2025-01-18 18:27:10.770 | INFO     | __main__:<module>:63 - Got args Namespace(max_steps=20000, eval_interval=1000, eval_steps=100, batch_size=16, gradient_accum=32, model_size='124M', tensorboard=1)
2025-01-18 18:27:10.770 | INFO     | __main__:<module>:88 - Training 124M model with config.n_layer=12, config.n_embd=768config.n_head=12, config.context_size=1024, config.dropout=0.0, config.vocab_size=50304
2025-01-18 18:27:13.013 | DEBUG    | __main__:<module>:138 - ddp_rank=0 ddp_local_rank=0
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:152 - Training data is 9,035,582,489 tokens
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:153 - Evaluation data is 4,434,606 tokens
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:160 - job_name='gpt2-training-124M-2025-01-18-18-27-10'
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:161 - Tokens / step: 524,288
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:162 - Total training tokens: 10,485,760,000
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:163 - Effective batch size with grad accumulation: batch_size * gradient_accumulation_steps=512
2025-01-18 18:27:13.013 | DEBUG    | __main__:<module>:164 - gradient_accumulation_steps_per_gpu=32
2025-01-18 18:27:13.013 | DEBUG    | __main__:<module>:165 - Directories: train_dir='/home/v-youransun/repgpt/input/data/train', eval_dir='/home/v-youransun/repgpt/input/data/eval' model_dir='/home/v-youransun/repgpt/model' log_dir='/home/v-youransun/repgpt/output/tensorboard/nov/gpt2-training-124M-2025-01-18-18-27-10'
2025-01-18 18:27:13.013 | INFO     | __main__:<module>:166 - Loaded dataset 2.2438719272613525
2025-01-18 18:27:16.330 | INFO     | __main__:<module>:200 - OptimizedModule(
  (_orig_mod): DistributedDataParallel(
    (module): GPT2(
      (token_embedding_table): Embedding(50304, 768)
      (position_embedding_table): Embedding(1024, 768)
      (blocks): Sequential(
        (0): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (layer_norm_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lm_head): Linear(in_features=768, out_features=50304, bias=False)
    )
  )
)
2025-01-18 18:27:16.330 | INFO     | __main__:<module>:203 - Training model with 123,587,328 parameters for max_steps=20,000 on total_training_tokens=10,485,760,000
2025-01-18 18:27:16.330 | INFO     | __main__:<module>:206 - Decayed parameter tensors: 74, with 124,354,560 parameters
2025-01-18 18:27:16.330 | INFO     | __main__:<module>:207 - Non-decayed parameter tensors: 25, with 19,200 parameters
[rank0]:W0118 18:27:16.349000 2655739 torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2025-01-18 18:27:27.479 | INFO     | __main__:<module>:265 - Step 0/20,000 loss: 10.9093 (T) 10.9085 (V) | lr=0.0e+00
2025-01-18 18:27:27.481 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 18:27:41.835 | DEBUG    | __main__:<module>:313 - Training step 0: loss = 10.9002 | 25501.29ms | Tokens/s = 20,559.3
2025-01-18 18:28:11.152 | DEBUG    | __main__:<module>:313 - Training step 10: loss = 9.4048 | 2945.61ms | Tokens/s = 177,989.6
2025-01-18 18:28:40.794 | DEBUG    | __main__:<module>:313 - Training step 20: loss = 8.7016 | 2977.33ms | Tokens/s = 176,093.1
2025-01-18 18:29:10.728 | DEBUG    | __main__:<module>:313 - Training step 30: loss = 7.9520 | 3001.09ms | Tokens/s = 174,699.2
2025-01-18 18:29:40.778 | DEBUG    | __main__:<module>:313 - Training step 40: loss = 7.2986 | 3015.31ms | Tokens/s = 173,875.3
2025-01-18 18:30:11.011 | DEBUG    | __main__:<module>:313 - Training step 50: loss = 6.8464 | 3028.70ms | Tokens/s = 173,106.7
2025-01-18 18:30:41.353 | DEBUG    | __main__:<module>:313 - Training step 60: loss = 6.5739 | 3039.38ms | Tokens/s = 172,498.3
2025-01-18 18:31:11.769 | DEBUG    | __main__:<module>:313 - Training step 70: loss = 6.7303 | 3042.65ms | Tokens/s = 172,312.8
2025-01-18 18:31:42.212 | DEBUG    | __main__:<module>:313 - Training step 80: loss = 6.4262 | 3042.89ms | Tokens/s = 172,299.5
2025-01-18 18:32:12.657 | DEBUG    | __main__:<module>:313 - Training step 90: loss = 6.2590 | 3044.86ms | Tokens/s = 172,188.1
2025-01-18 18:32:43.119 | DEBUG    | __main__:<module>:313 - Training step 100: loss = 6.0612 | 3045.91ms | Tokens/s = 172,128.3
2025-01-18 18:33:13.580 | DEBUG    | __main__:<module>:313 - Training step 110: loss = 6.3446 | 3045.78ms | Tokens/s = 172,135.6
2025-01-18 18:33:44.058 | DEBUG    | __main__:<module>:313 - Training step 120: loss = 6.1752 | 3048.88ms | Tokens/s = 171,961.0
2025-01-18 18:34:14.530 | DEBUG    | __main__:<module>:313 - Training step 130: loss = 6.0243 | 3047.02ms | Tokens/s = 172,065.8
2025-01-18 18:34:45.014 | DEBUG    | __main__:<module>:313 - Training step 140: loss = 5.8910 | 3046.64ms | Tokens/s = 172,087.5
2025-01-18 18:35:15.506 | DEBUG    | __main__:<module>:313 - Training step 150: loss = 5.8142 | 3048.47ms | Tokens/s = 171,984.0
2025-01-18 18:35:46.001 | DEBUG    | __main__:<module>:313 - Training step 160: loss = 5.9405 | 3049.92ms | Tokens/s = 171,902.1
2025-01-18 18:36:16.508 | DEBUG    | __main__:<module>:313 - Training step 170: loss = 5.8743 | 3050.17ms | Tokens/s = 171,888.3
2025-01-18 18:36:47.016 | DEBUG    | __main__:<module>:313 - Training step 180: loss = 5.7413 | 3048.84ms | Tokens/s = 171,963.3
2025-01-18 18:37:17.525 | DEBUG    | __main__:<module>:313 - Training step 190: loss = 5.9669 | 3051.97ms | Tokens/s = 171,786.5
2025-01-18 18:37:48.049 | DEBUG    | __main__:<module>:313 - Training step 200: loss = 5.6730 | 3052.34ms | Tokens/s = 171,765.7
2025-01-18 18:38:18.566 | DEBUG    | __main__:<module>:313 - Training step 210: loss = 5.5261 | 3051.83ms | Tokens/s = 171,794.4
2025-01-18 18:38:49.083 | DEBUG    | __main__:<module>:313 - Training step 220: loss = 5.7142 | 3051.88ms | Tokens/s = 171,791.6
2025-01-18 18:39:19.608 | DEBUG    | __main__:<module>:313 - Training step 230: loss = 5.5799 | 3053.19ms | Tokens/s = 171,718.0
2025-01-18 18:39:50.121 | DEBUG    | __main__:<module>:313 - Training step 240: loss = 5.4910 | 3050.65ms | Tokens/s = 171,860.8
2025-01-18 18:40:20.621 | DEBUG    | __main__:<module>:313 - Training step 250: loss = 5.5409 | 3051.39ms | Tokens/s = 171,819.4
2025-01-18 18:40:51.127 | DEBUG    | __main__:<module>:313 - Training step 260: loss = 5.6117 | 3049.75ms | Tokens/s = 171,911.6
2025-01-18 18:41:21.639 | DEBUG    | __main__:<module>:313 - Training step 270: loss = 5.4698 | 3050.98ms | Tokens/s = 171,842.2
2025-01-18 18:41:52.156 | DEBUG    | __main__:<module>:313 - Training step 280: loss = 5.3181 | 3051.28ms | Tokens/s = 171,825.9
2025-01-18 18:42:22.688 | DEBUG    | __main__:<module>:313 - Training step 290: loss = 5.3472 | 3050.51ms | Tokens/s = 171,869.0
2025-01-18 18:42:53.213 | DEBUG    | __main__:<module>:313 - Training step 300: loss = 5.3406 | 3051.85ms | Tokens/s = 171,793.4
2025-01-18 18:43:23.754 | DEBUG    | __main__:<module>:313 - Training step 310: loss = 5.1867 | 3056.27ms | Tokens/s = 171,545.1
2025-01-18 18:43:54.290 | DEBUG    | __main__:<module>:313 - Training step 320: loss = 5.1745 | 3054.04ms | Tokens/s = 171,670.2
2025-01-18 18:44:24.831 | DEBUG    | __main__:<module>:313 - Training step 330: loss = 5.1372 | 3053.65ms | Tokens/s = 171,692.1
2025-01-18 18:44:55.372 | DEBUG    | __main__:<module>:313 - Training step 340: loss = 5.1932 | 3054.84ms | Tokens/s = 171,625.4
2025-01-18 18:45:25.933 | DEBUG    | __main__:<module>:313 - Training step 350: loss = 5.0609 | 3057.27ms | Tokens/s = 171,489.0
2025-01-18 18:45:56.491 | DEBUG    | __main__:<module>:313 - Training step 360: loss = 5.0512 | 3056.36ms | Tokens/s = 171,540.1
2025-01-18 18:46:27.054 | DEBUG    | __main__:<module>:313 - Training step 370: loss = 5.1545 | 3055.75ms | Tokens/s = 171,574.4
2025-01-18 18:46:57.612 | DEBUG    | __main__:<module>:313 - Training step 380: loss = 4.8735 | 3056.53ms | Tokens/s = 171,530.2
2025-01-18 18:47:28.151 | DEBUG    | __main__:<module>:313 - Training step 390: loss = 5.0188 | 3052.80ms | Tokens/s = 171,739.8
2025-01-18 18:47:58.682 | DEBUG    | __main__:<module>:313 - Training step 400: loss = 5.0003 | 3052.05ms | Tokens/s = 171,782.5
2025-01-18 18:48:29.210 | DEBUG    | __main__:<module>:313 - Training step 410: loss = 4.7431 | 3053.38ms | Tokens/s = 171,707.5
2025-01-18 18:48:59.747 | DEBUG    | __main__:<module>:313 - Training step 420: loss = 4.7346 | 3052.96ms | Tokens/s = 171,731.1
2025-01-18 18:49:30.291 | DEBUG    | __main__:<module>:313 - Training step 430: loss = 4.8615 | 3055.23ms | Tokens/s = 171,603.7
2025-01-18 18:50:00.856 | DEBUG    | __main__:<module>:313 - Training step 440: loss = 4.8891 | 3056.92ms | Tokens/s = 171,508.5
2025-01-18 18:50:31.419 | DEBUG    | __main__:<module>:313 - Training step 450: loss = 4.8421 | 3057.62ms | Tokens/s = 171,469.1
2025-01-18 18:51:01.982 | DEBUG    | __main__:<module>:313 - Training step 460: loss = 4.7084 | 3056.80ms | Tokens/s = 171,515.3
2025-01-18 18:51:32.547 | DEBUG    | __main__:<module>:313 - Training step 470: loss = 4.5683 | 3056.38ms | Tokens/s = 171,539.0
2025-01-18 18:52:03.126 | DEBUG    | __main__:<module>:313 - Training step 480: loss = 4.5544 | 3057.01ms | Tokens/s = 171,503.3
2025-01-18 18:52:33.701 | DEBUG    | __main__:<module>:313 - Training step 490: loss = 4.6925 | 3055.27ms | Tokens/s = 171,601.0
2025-01-18 18:53:04.266 | DEBUG    | __main__:<module>:313 - Training step 500: loss = 4.3970 | 3053.69ms | Tokens/s = 171,690.2
2025-01-18 18:53:34.807 | DEBUG    | __main__:<module>:313 - Training step 510: loss = 4.5261 | 3052.25ms | Tokens/s = 171,770.8
2025-01-18 18:54:05.345 | DEBUG    | __main__:<module>:313 - Training step 520: loss = 4.5513 | 3054.47ms | Tokens/s = 171,646.0
2025-01-18 18:54:35.896 | DEBUG    | __main__:<module>:313 - Training step 530: loss = 4.4412 | 3053.91ms | Tokens/s = 171,677.7
2025-01-18 18:55:06.456 | DEBUG    | __main__:<module>:313 - Training step 540: loss = 4.5348 | 3054.48ms | Tokens/s = 171,645.7
2025-01-18 18:55:37.021 | DEBUG    | __main__:<module>:313 - Training step 550: loss = 4.4594 | 3055.94ms | Tokens/s = 171,563.4
2025-01-18 18:56:07.587 | DEBUG    | __main__:<module>:313 - Training step 560: loss = 4.4429 | 3054.88ms | Tokens/s = 171,623.0
2025-01-18 18:56:38.138 | DEBUG    | __main__:<module>:313 - Training step 570: loss = 4.4436 | 3055.77ms | Tokens/s = 171,572.9
2025-01-18 18:57:08.689 | DEBUG    | __main__:<module>:313 - Training step 580: loss = 4.2740 | 3055.19ms | Tokens/s = 171,605.5
2025-01-18 18:57:39.243 | DEBUG    | __main__:<module>:313 - Training step 590: loss = 4.2603 | 3056.70ms | Tokens/s = 171,521.1
2025-01-18 18:58:09.807 | DEBUG    | __main__:<module>:313 - Training step 600: loss = 4.3976 | 3052.99ms | Tokens/s = 171,729.1
2025-01-18 18:58:40.373 | DEBUG    | __main__:<module>:313 - Training step 610: loss = 4.4399 | 3055.86ms | Tokens/s = 171,567.9
2025-01-18 18:59:10.934 | DEBUG    | __main__:<module>:313 - Training step 620: loss = 4.3475 | 3056.01ms | Tokens/s = 171,559.5
2025-01-18 18:59:41.498 | DEBUG    | __main__:<module>:313 - Training step 630: loss = 4.2149 | 3056.24ms | Tokens/s = 171,546.5
2025-01-18 19:00:12.064 | DEBUG    | __main__:<module>:313 - Training step 640: loss = 4.3541 | 3060.02ms | Tokens/s = 171,334.7
2025-01-18 19:00:42.639 | DEBUG    | __main__:<module>:313 - Training step 650: loss = 4.1815 | 3057.46ms | Tokens/s = 171,478.4
2025-01-18 19:01:13.211 | DEBUG    | __main__:<module>:313 - Training step 660: loss = 4.1654 | 3055.14ms | Tokens/s = 171,608.6
2025-01-18 19:01:43.779 | DEBUG    | __main__:<module>:313 - Training step 670: loss = 4.2139 | 3057.48ms | Tokens/s = 171,477.4
2025-01-18 19:02:14.346 | DEBUG    | __main__:<module>:313 - Training step 680: loss = 4.1772 | 3056.10ms | Tokens/s = 171,554.9
2025-01-18 19:02:44.907 | DEBUG    | __main__:<module>:313 - Training step 690: loss = 4.1100 | 3056.45ms | Tokens/s = 171,534.8
2025-01-18 19:03:15.471 | DEBUG    | __main__:<module>:313 - Training step 700: loss = 4.2235 | 3054.75ms | Tokens/s = 171,630.5
2025-01-18 19:03:46.028 | DEBUG    | __main__:<module>:313 - Training step 710: loss = 4.1758 | 3054.11ms | Tokens/s = 171,666.3
2025-01-18 19:04:16.582 | DEBUG    | __main__:<module>:313 - Training step 720: loss = 4.0791 | 3056.16ms | Tokens/s = 171,551.3
2025-01-18 19:04:47.140 | DEBUG    | __main__:<module>:313 - Training step 730: loss = 4.0669 | 3056.69ms | Tokens/s = 171,521.5
2025-01-18 19:05:17.702 | DEBUG    | __main__:<module>:313 - Training step 740: loss = 4.1086 | 3057.18ms | Tokens/s = 171,493.8
2025-01-18 19:05:48.268 | DEBUG    | __main__:<module>:313 - Training step 750: loss = 4.0324 | 3056.92ms | Tokens/s = 171,508.4
2025-01-18 19:06:18.833 | DEBUG    | __main__:<module>:313 - Training step 760: loss = 4.0662 | 3055.73ms | Tokens/s = 171,575.2
2025-01-18 19:06:49.385 | DEBUG    | __main__:<module>:313 - Training step 770: loss = 4.1015 | 3053.52ms | Tokens/s = 171,699.6
2025-01-18 19:07:19.920 | DEBUG    | __main__:<module>:313 - Training step 780: loss = 4.0742 | 3052.24ms | Tokens/s = 171,771.7
2025-01-18 19:07:50.455 | DEBUG    | __main__:<module>:313 - Training step 790: loss = 4.0820 | 3053.01ms | Tokens/s = 171,728.4
2025-01-18 19:08:21.003 | DEBUG    | __main__:<module>:313 - Training step 800: loss = 4.0011 | 3054.28ms | Tokens/s = 171,656.8
2025-01-18 19:08:51.542 | DEBUG    | __main__:<module>:313 - Training step 810: loss = 4.1374 | 3052.88ms | Tokens/s = 171,735.3
2025-01-18 19:09:22.045 | DEBUG    | __main__:<module>:313 - Training step 820: loss = 4.1108 | 3047.83ms | Tokens/s = 172,019.9
2025-01-18 19:09:52.510 | DEBUG    | __main__:<module>:313 - Training step 830: loss = 4.0735 | 3045.21ms | Tokens/s = 172,168.2
2025-01-18 19:10:22.948 | DEBUG    | __main__:<module>:313 - Training step 840: loss = 4.2367 | 3041.88ms | Tokens/s = 172,356.5
2025-01-18 19:10:53.370 | DEBUG    | __main__:<module>:313 - Training step 850: loss = 4.0657 | 3043.71ms | Tokens/s = 172,253.1
2025-01-18 19:11:23.782 | DEBUG    | __main__:<module>:313 - Training step 860: loss = 3.8037 | 3042.48ms | Tokens/s = 172,322.6
2025-01-18 19:11:54.179 | DEBUG    | __main__:<module>:313 - Training step 870: loss = 4.2519 | 3037.85ms | Tokens/s = 172,585.4
2025-01-18 19:12:24.575 | DEBUG    | __main__:<module>:313 - Training step 880: loss = 3.9945 | 3039.82ms | Tokens/s = 172,473.3
2025-01-18 19:12:54.959 | DEBUG    | __main__:<module>:313 - Training step 890: loss = 3.9930 | 3038.12ms | Tokens/s = 172,569.7
2025-01-18 19:13:25.333 | DEBUG    | __main__:<module>:313 - Training step 900: loss = 3.9796 | 3038.01ms | Tokens/s = 172,576.0
2025-01-18 19:13:55.708 | DEBUG    | __main__:<module>:313 - Training step 910: loss = 4.0147 | 3036.59ms | Tokens/s = 172,656.9
2025-01-18 19:14:26.075 | DEBUG    | __main__:<module>:313 - Training step 920: loss = 3.8806 | 3034.84ms | Tokens/s = 172,756.2
2025-01-18 19:14:56.430 | DEBUG    | __main__:<module>:313 - Training step 930: loss = 4.0172 | 3036.35ms | Tokens/s = 172,670.7
2025-01-18 19:15:26.785 | DEBUG    | __main__:<module>:313 - Training step 940: loss = 3.9417 | 3035.80ms | Tokens/s = 172,701.5
2025-01-18 19:15:57.153 | DEBUG    | __main__:<module>:313 - Training step 950: loss = 3.8647 | 3037.30ms | Tokens/s = 172,616.6
2025-01-18 19:16:27.520 | DEBUG    | __main__:<module>:313 - Training step 960: loss = 3.9890 | 3035.51ms | Tokens/s = 172,718.2
2025-01-18 19:16:57.887 | DEBUG    | __main__:<module>:313 - Training step 970: loss = 4.3602 | 3036.52ms | Tokens/s = 172,660.9
2025-01-18 19:17:28.256 | DEBUG    | __main__:<module>:313 - Training step 980: loss = 4.1486 | 3036.08ms | Tokens/s = 172,685.9
2025-01-18 19:17:58.629 | DEBUG    | __main__:<module>:313 - Training step 990: loss = 4.0041 | 3037.78ms | Tokens/s = 172,589.0
2025-01-18 19:18:32.420 | INFO     | __main__:<module>:265 - Step 1,000/20,000 loss: 3.9605 (T) 3.9496 (V) | lr=5.0e-03
2025-01-18 19:18:32.421 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 19:18:45.675 | DEBUG    | __main__:<module>:313 - Training step 1000: loss = 3.9327 | 19702.17ms | Tokens/s = 26,610.7
2025-01-18 19:19:15.957 | DEBUG    | __main__:<module>:313 - Training step 1010: loss = 3.8698 | 3037.21ms | Tokens/s = 172,621.8
2025-01-18 19:19:46.397 | DEBUG    | __main__:<module>:313 - Training step 1020: loss = 3.9890 | 3047.61ms | Tokens/s = 172,032.7
2025-01-18 19:20:16.900 | DEBUG    | __main__:<module>:313 - Training step 1030: loss = 3.9571 | 3049.84ms | Tokens/s = 171,906.5
2025-01-18 19:20:47.402 | DEBUG    | __main__:<module>:313 - Training step 1040: loss = 4.1043 | 3049.79ms | Tokens/s = 171,909.7
2025-01-18 19:21:17.903 | DEBUG    | __main__:<module>:313 - Training step 1050: loss = 4.0720 | 3051.07ms | Tokens/s = 171,837.2
2025-01-18 19:21:48.409 | DEBUG    | __main__:<module>:313 - Training step 1060: loss = 4.0896 | 3049.99ms | Tokens/s = 171,898.1
2025-01-18 19:22:18.916 | DEBUG    | __main__:<module>:313 - Training step 1070: loss = 3.9518 | 3051.74ms | Tokens/s = 171,799.9
2025-01-18 19:22:49.426 | DEBUG    | __main__:<module>:313 - Training step 1080: loss = 3.9527 | 3050.46ms | Tokens/s = 171,872.0
2025-01-18 19:23:19.936 | DEBUG    | __main__:<module>:313 - Training step 1090: loss = 3.8433 | 3050.34ms | Tokens/s = 171,878.6
2025-01-18 19:23:50.445 | DEBUG    | __main__:<module>:313 - Training step 1100: loss = 3.8542 | 3049.42ms | Tokens/s = 171,930.5
2025-01-18 19:24:20.952 | DEBUG    | __main__:<module>:313 - Training step 1110: loss = 3.9474 | 3050.14ms | Tokens/s = 171,889.8
2025-01-18 19:24:51.465 | DEBUG    | __main__:<module>:313 - Training step 1120: loss = 3.7191 | 3050.47ms | Tokens/s = 171,871.5
2025-01-18 19:25:21.972 | DEBUG    | __main__:<module>:313 - Training step 1130: loss = 4.0277 | 3049.68ms | Tokens/s = 171,915.7
2025-01-18 19:25:52.485 | DEBUG    | __main__:<module>:313 - Training step 1140: loss = 3.9777 | 3050.15ms | Tokens/s = 171,889.4
2025-01-18 19:26:23.008 | DEBUG    | __main__:<module>:313 - Training step 1150: loss = 3.9717 | 3053.12ms | Tokens/s = 171,722.0
2025-01-18 19:26:53.525 | DEBUG    | __main__:<module>:313 - Training step 1160: loss = 4.0088 | 3049.28ms | Tokens/s = 171,938.1
2025-01-18 19:27:24.039 | DEBUG    | __main__:<module>:313 - Training step 1170: loss = 4.0311 | 3051.86ms | Tokens/s = 171,792.9
2025-01-18 19:27:54.560 | DEBUG    | __main__:<module>:313 - Training step 1180: loss = 4.0222 | 3051.60ms | Tokens/s = 171,807.8
2025-01-18 19:28:25.080 | DEBUG    | __main__:<module>:313 - Training step 1190: loss = 3.8710 | 3053.53ms | Tokens/s = 171,698.8
2025-01-18 19:28:55.595 | DEBUG    | __main__:<module>:313 - Training step 1200: loss = 3.8104 | 3053.06ms | Tokens/s = 171,725.2
2025-01-18 19:29:26.118 | DEBUG    | __main__:<module>:313 - Training step 1210: loss = 3.8784 | 3054.35ms | Tokens/s = 171,652.8
2025-01-18 19:29:56.651 | DEBUG    | __main__:<module>:313 - Training step 1220: loss = 3.7091 | 3054.75ms | Tokens/s = 171,630.3
2025-01-18 19:30:27.178 | DEBUG    | __main__:<module>:313 - Training step 1230: loss = 3.7791 | 3053.50ms | Tokens/s = 171,700.7
2025-01-18 19:30:57.702 | DEBUG    | __main__:<module>:313 - Training step 1240: loss = 3.9515 | 3052.07ms | Tokens/s = 171,781.1
2025-01-18 19:31:28.231 | DEBUG    | __main__:<module>:313 - Training step 1250: loss = 3.8494 | 3050.89ms | Tokens/s = 171,847.6
2025-01-18 19:31:58.757 | DEBUG    | __main__:<module>:313 - Training step 1260: loss = 3.8905 | 3053.45ms | Tokens/s = 171,703.3
2025-01-18 19:32:29.280 | DEBUG    | __main__:<module>:313 - Training step 1270: loss = 3.7625 | 3055.14ms | Tokens/s = 171,608.4
2025-01-18 19:32:59.808 | DEBUG    | __main__:<module>:313 - Training step 1280: loss = 3.7656 | 3052.62ms | Tokens/s = 171,750.2
2025-01-18 19:33:30.335 | DEBUG    | __main__:<module>:313 - Training step 1290: loss = 3.8425 | 3050.62ms | Tokens/s = 171,862.5
2025-01-18 19:34:00.870 | DEBUG    | __main__:<module>:313 - Training step 1300: loss = 3.8903 | 3050.43ms | Tokens/s = 171,873.4
2025-01-18 19:34:31.406 | DEBUG    | __main__:<module>:313 - Training step 1310: loss = 3.8910 | 3053.16ms | Tokens/s = 171,719.6
2025-01-18 19:35:01.944 | DEBUG    | __main__:<module>:313 - Training step 1320: loss = 3.7999 | 3055.10ms | Tokens/s = 171,610.8
2025-01-18 19:35:32.481 | DEBUG    | __main__:<module>:313 - Training step 1330: loss = 3.8003 | 3053.41ms | Tokens/s = 171,705.6
2025-01-18 19:36:03.009 | DEBUG    | __main__:<module>:313 - Training step 1340: loss = 3.8327 | 3051.66ms | Tokens/s = 171,804.0
2025-01-18 19:36:33.537 | DEBUG    | __main__:<module>:313 - Training step 1350: loss = 3.6609 | 3054.31ms | Tokens/s = 171,655.1
2025-01-18 19:37:04.067 | DEBUG    | __main__:<module>:313 - Training step 1360: loss = 3.6719 | 3054.08ms | Tokens/s = 171,668.0
2025-01-18 19:37:34.601 | DEBUG    | __main__:<module>:313 - Training step 1370: loss = 3.6886 | 3053.01ms | Tokens/s = 171,728.4
2025-01-18 19:38:05.130 | DEBUG    | __main__:<module>:313 - Training step 1380: loss = 3.8449 | 3053.38ms | Tokens/s = 171,707.3
2025-01-18 19:38:35.658 | DEBUG    | __main__:<module>:313 - Training step 1390: loss = 3.9414 | 3051.76ms | Tokens/s = 171,798.5
2025-01-18 19:39:06.185 | DEBUG    | __main__:<module>:313 - Training step 1400: loss = 3.7000 | 3051.14ms | Tokens/s = 171,833.6
2025-01-18 19:39:36.714 | DEBUG    | __main__:<module>:313 - Training step 1410: loss = 3.7501 | 3052.78ms | Tokens/s = 171,741.1
2025-01-18 19:40:07.242 | DEBUG    | __main__:<module>:313 - Training step 1420: loss = 3.7289 | 3052.94ms | Tokens/s = 171,732.0
2025-01-18 19:40:37.784 | DEBUG    | __main__:<module>:313 - Training step 1430: loss = 3.8577 | 3055.08ms | Tokens/s = 171,612.1
2025-01-18 19:41:08.322 | DEBUG    | __main__:<module>:313 - Training step 1440: loss = 3.7431 | 3053.21ms | Tokens/s = 171,716.7
2025-01-18 19:41:38.847 | DEBUG    | __main__:<module>:313 - Training step 1450: loss = 3.8193 | 3053.05ms | Tokens/s = 171,726.1
2025-01-18 19:42:09.379 | DEBUG    | __main__:<module>:313 - Training step 1460: loss = 3.6277 | 3053.67ms | Tokens/s = 171,690.8
2025-01-18 19:42:39.918 | DEBUG    | __main__:<module>:313 - Training step 1470: loss = 3.7094 | 3054.58ms | Tokens/s = 171,640.2
2025-01-18 19:43:10.451 | DEBUG    | __main__:<module>:313 - Training step 1480: loss = 3.8511 | 3051.96ms | Tokens/s = 171,787.4
2025-01-18 19:43:40.987 | DEBUG    | __main__:<module>:313 - Training step 1490: loss = 3.4951 | 3052.99ms | Tokens/s = 171,729.3
2025-01-18 19:44:11.523 | DEBUG    | __main__:<module>:313 - Training step 1500: loss = 3.7011 | 3053.15ms | Tokens/s = 171,720.4
2025-01-18 19:44:42.056 | DEBUG    | __main__:<module>:313 - Training step 1510: loss = 3.8964 | 3051.42ms | Tokens/s = 171,817.9
2025-01-18 19:45:12.596 | DEBUG    | __main__:<module>:313 - Training step 1520: loss = 3.7523 | 3054.73ms | Tokens/s = 171,631.6
2025-01-18 19:45:43.125 | DEBUG    | __main__:<module>:313 - Training step 1530: loss = 3.7730 | 3053.07ms | Tokens/s = 171,724.8
2025-01-18 19:46:13.649 | DEBUG    | __main__:<module>:313 - Training step 1540: loss = 3.7879 | 3054.23ms | Tokens/s = 171,659.7
2025-01-18 19:46:44.172 | DEBUG    | __main__:<module>:313 - Training step 1550: loss = 3.9012 | 3053.42ms | Tokens/s = 171,705.0
2025-01-18 19:47:14.698 | DEBUG    | __main__:<module>:313 - Training step 1560: loss = 3.6085 | 3054.89ms | Tokens/s = 171,622.5
2025-01-18 19:47:45.226 | DEBUG    | __main__:<module>:313 - Training step 1570: loss = 3.6329 | 3052.52ms | Tokens/s = 171,755.8
2025-01-18 19:48:15.759 | DEBUG    | __main__:<module>:313 - Training step 1580: loss = 3.6352 | 3055.72ms | Tokens/s = 171,575.8
2025-01-18 19:48:46.297 | DEBUG    | __main__:<module>:313 - Training step 1590: loss = 3.6544 | 3053.95ms | Tokens/s = 171,675.2
2025-01-18 19:49:16.839 | DEBUG    | __main__:<module>:313 - Training step 1600: loss = 3.7939 | 3054.24ms | Tokens/s = 171,659.3
2025-01-18 19:49:47.382 | DEBUG    | __main__:<module>:313 - Training step 1610: loss = 3.8679 | 3054.75ms | Tokens/s = 171,630.5
2025-01-18 19:50:17.917 | DEBUG    | __main__:<module>:313 - Training step 1620: loss = 3.7718 | 3052.21ms | Tokens/s = 171,773.3
2025-01-18 19:50:48.457 | DEBUG    | __main__:<module>:313 - Training step 1630: loss = 3.5502 | 3055.15ms | Tokens/s = 171,607.8
2025-01-18 19:51:19.000 | DEBUG    | __main__:<module>:313 - Training step 1640: loss = 3.7695 | 3055.96ms | Tokens/s = 171,562.6
2025-01-18 19:51:49.535 | DEBUG    | __main__:<module>:313 - Training step 1650: loss = 3.6577 | 3054.57ms | Tokens/s = 171,640.4
2025-01-18 19:52:20.071 | DEBUG    | __main__:<module>:313 - Training step 1660: loss = 3.7955 | 3052.44ms | Tokens/s = 171,760.1
2025-01-18 19:52:50.602 | DEBUG    | __main__:<module>:313 - Training step 1670: loss = 3.5917 | 3054.67ms | Tokens/s = 171,634.9
2025-01-18 19:53:21.135 | DEBUG    | __main__:<module>:313 - Training step 1680: loss = 3.5393 | 3053.02ms | Tokens/s = 171,727.9
2025-01-18 19:53:51.659 | DEBUG    | __main__:<module>:313 - Training step 1690: loss = 3.6695 | 3053.37ms | Tokens/s = 171,708.2
2025-01-18 19:54:22.175 | DEBUG    | __main__:<module>:313 - Training step 1700: loss = 3.6987 | 3052.57ms | Tokens/s = 171,752.8
2025-01-18 19:54:52.688 | DEBUG    | __main__:<module>:313 - Training step 1710: loss = 3.3011 | 3052.16ms | Tokens/s = 171,775.8
2025-01-18 19:55:23.206 | DEBUG    | __main__:<module>:313 - Training step 1720: loss = 3.7459 | 3053.14ms | Tokens/s = 171,721.1
2025-01-18 19:55:53.724 | DEBUG    | __main__:<module>:313 - Training step 1730: loss = 3.7653 | 3051.43ms | Tokens/s = 171,817.4
2025-01-18 19:56:24.243 | DEBUG    | __main__:<module>:313 - Training step 1740: loss = 3.7391 | 3052.29ms | Tokens/s = 171,768.8
2025-01-18 19:56:54.773 | DEBUG    | __main__:<module>:313 - Training step 1750: loss = 3.5269 | 3051.65ms | Tokens/s = 171,804.6
2025-01-18 19:57:25.308 | DEBUG    | __main__:<module>:313 - Training step 1760: loss = 3.7307 | 3055.41ms | Tokens/s = 171,593.3
2025-01-18 19:57:55.842 | DEBUG    | __main__:<module>:313 - Training step 1770: loss = 3.7061 | 3052.32ms | Tokens/s = 171,767.2
2025-01-18 19:58:26.367 | DEBUG    | __main__:<module>:313 - Training step 1780: loss = 3.6768 | 3051.42ms | Tokens/s = 171,817.6
2025-01-18 19:58:56.880 | DEBUG    | __main__:<module>:313 - Training step 1790: loss = 3.6863 | 3051.32ms | Tokens/s = 171,823.2
2025-01-18 19:59:27.379 | DEBUG    | __main__:<module>:313 - Training step 1800: loss = 3.7453 | 3046.54ms | Tokens/s = 172,093.2
2025-01-18 19:59:57.867 | DEBUG    | __main__:<module>:313 - Training step 1810: loss = 3.7080 | 3047.82ms | Tokens/s = 172,020.4
2025-01-18 20:00:28.352 | DEBUG    | __main__:<module>:313 - Training step 1820: loss = 3.6009 | 3047.68ms | Tokens/s = 172,028.5
2025-01-18 20:00:58.844 | DEBUG    | __main__:<module>:313 - Training step 1830: loss = 3.5953 | 3048.76ms | Tokens/s = 171,967.5
2025-01-18 20:01:29.324 | DEBUG    | __main__:<module>:313 - Training step 1840: loss = 3.5205 | 3047.50ms | Tokens/s = 172,038.9
2025-01-18 20:01:59.809 | DEBUG    | __main__:<module>:313 - Training step 1850: loss = 3.6258 | 3048.20ms | Tokens/s = 171,999.0
2025-01-18 20:02:30.289 | DEBUG    | __main__:<module>:313 - Training step 1860: loss = 3.4819 | 3047.61ms | Tokens/s = 172,032.3
2025-01-18 20:03:00.770 | DEBUG    | __main__:<module>:313 - Training step 1870: loss = 3.7829 | 3049.75ms | Tokens/s = 171,911.8
2025-01-18 20:03:31.258 | DEBUG    | __main__:<module>:313 - Training step 1880: loss = 3.6058 | 3050.21ms | Tokens/s = 171,886.0
2025-01-18 20:04:01.756 | DEBUG    | __main__:<module>:313 - Training step 1890: loss = 3.6719 | 3048.91ms | Tokens/s = 171,959.4
2025-01-18 20:04:32.236 | DEBUG    | __main__:<module>:313 - Training step 1900: loss = 3.5237 | 3046.23ms | Tokens/s = 172,110.5
2025-01-18 20:05:02.732 | DEBUG    | __main__:<module>:313 - Training step 1910: loss = 3.6214 | 3048.47ms | Tokens/s = 171,984.1
2025-01-18 20:05:33.220 | DEBUG    | __main__:<module>:313 - Training step 1920: loss = 3.8036 | 3048.97ms | Tokens/s = 171,955.5
2025-01-18 20:06:03.698 | DEBUG    | __main__:<module>:313 - Training step 1930: loss = 3.7840 | 3047.07ms | Tokens/s = 172,063.0
2025-01-18 20:06:34.174 | DEBUG    | __main__:<module>:313 - Training step 1940: loss = 3.6289 | 3047.42ms | Tokens/s = 172,043.4
2025-01-18 20:07:04.642 | DEBUG    | __main__:<module>:313 - Training step 1950: loss = 3.6776 | 3044.63ms | Tokens/s = 172,201.0
2025-01-18 20:07:35.108 | DEBUG    | __main__:<module>:313 - Training step 1960: loss = 3.7756 | 3046.11ms | Tokens/s = 172,117.0
2025-01-18 20:08:05.570 | DEBUG    | __main__:<module>:313 - Training step 1970: loss = 3.5962 | 3046.29ms | Tokens/s = 172,106.9
2025-01-18 20:08:36.056 | DEBUG    | __main__:<module>:313 - Training step 1980: loss = 3.6635 | 3048.66ms | Tokens/s = 171,973.3
2025-01-18 20:09:06.537 | DEBUG    | __main__:<module>:313 - Training step 1990: loss = 3.5746 | 3047.88ms | Tokens/s = 172,017.1
2025-01-18 20:09:40.437 | INFO     | __main__:<module>:265 - Step 2,000/20,000 loss: 3.6553 (T) 3.6418 (V) | lr=1.0e-02
2025-01-18 20:09:40.439 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 20:09:54.400 | DEBUG    | __main__:<module>:313 - Training step 2000: loss = 3.6207 | 20433.30ms | Tokens/s = 25,658.5
2025-01-18 20:10:24.723 | DEBUG    | __main__:<module>:313 - Training step 2010: loss = 3.7170 | 3041.93ms | Tokens/s = 172,353.8
2025-01-18 20:10:55.170 | DEBUG    | __main__:<module>:313 - Training step 2020: loss = 3.6530 | 3047.71ms | Tokens/s = 172,027.0
2025-01-18 20:11:25.660 | DEBUG    | __main__:<module>:313 - Training step 2030: loss = 3.4762 | 3048.25ms | Tokens/s = 171,996.5
2025-01-18 20:11:56.153 | DEBUG    | __main__:<module>:313 - Training step 2040: loss = 3.3761 | 3048.40ms | Tokens/s = 171,987.9
2025-01-18 20:12:26.625 | DEBUG    | __main__:<module>:313 - Training step 2050: loss = 3.6118 | 3047.45ms | Tokens/s = 172,041.8
2025-01-18 20:12:57.103 | DEBUG    | __main__:<module>:313 - Training step 2060: loss = 3.4721 | 3046.89ms | Tokens/s = 172,073.0
2025-01-18 20:13:27.587 | DEBUG    | __main__:<module>:313 - Training step 2070: loss = 3.3980 | 3049.87ms | Tokens/s = 171,905.2
2025-01-18 20:13:58.070 | DEBUG    | __main__:<module>:313 - Training step 2080: loss = 3.6256 | 3047.03ms | Tokens/s = 172,065.2
2025-01-18 20:14:28.537 | DEBUG    | __main__:<module>:313 - Training step 2090: loss = 3.7662 | 3046.98ms | Tokens/s = 172,067.9
2025-01-18 20:14:59.007 | DEBUG    | __main__:<module>:313 - Training step 2100: loss = 3.7730 | 3046.17ms | Tokens/s = 172,114.1
2025-01-18 20:15:29.469 | DEBUG    | __main__:<module>:313 - Training step 2110: loss = 3.6449 | 3045.49ms | Tokens/s = 172,152.3
2025-01-18 20:15:59.927 | DEBUG    | __main__:<module>:313 - Training step 2120: loss = 3.6082 | 3044.11ms | Tokens/s = 172,230.0
2025-01-18 20:16:30.383 | DEBUG    | __main__:<module>:313 - Training step 2130: loss = 3.6010 | 3045.74ms | Tokens/s = 172,138.2
2025-01-18 20:17:00.842 | DEBUG    | __main__:<module>:313 - Training step 2140: loss = 3.6778 | 3047.69ms | Tokens/s = 172,028.1
2025-01-18 20:17:31.313 | DEBUG    | __main__:<module>:313 - Training step 2150: loss = 3.6178 | 3048.31ms | Tokens/s = 171,992.8
2025-01-18 20:18:01.814 | DEBUG    | __main__:<module>:313 - Training step 2160: loss = 3.5359 | 3053.43ms | Tokens/s = 171,704.4
2025-01-18 20:18:32.315 | DEBUG    | __main__:<module>:313 - Training step 2170: loss = 3.7126 | 3048.15ms | Tokens/s = 172,002.2
2025-01-18 20:19:02.800 | DEBUG    | __main__:<module>:313 - Training step 2180: loss = 3.5678 | 3047.48ms | Tokens/s = 172,039.7
2025-01-18 20:19:33.276 | DEBUG    | __main__:<module>:313 - Training step 2190: loss = 3.8203 | 3048.15ms | Tokens/s = 172,002.3
2025-01-18 20:20:03.741 | DEBUG    | __main__:<module>:313 - Training step 2200: loss = 3.6876 | 3044.51ms | Tokens/s = 172,207.9
2025-01-18 20:20:34.199 | DEBUG    | __main__:<module>:313 - Training step 2210: loss = 3.7483 | 3047.28ms | Tokens/s = 172,051.3
2025-01-18 20:21:04.658 | DEBUG    | __main__:<module>:313 - Training step 2220: loss = 3.6194 | 3046.45ms | Tokens/s = 172,097.9
2025-01-18 20:21:35.114 | DEBUG    | __main__:<module>:313 - Training step 2230: loss = 3.6312 | 3047.74ms | Tokens/s = 172,025.2
2025-01-18 20:22:05.599 | DEBUG    | __main__:<module>:313 - Training step 2240: loss = 3.5938 | 3048.68ms | Tokens/s = 171,972.3
2025-01-18 20:22:36.067 | DEBUG    | __main__:<module>:313 - Training step 2250: loss = 3.6899 | 3047.90ms | Tokens/s = 172,015.9
2025-01-18 20:23:06.532 | DEBUG    | __main__:<module>:313 - Training step 2260: loss = 3.6518 | 3046.03ms | Tokens/s = 172,122.0
2025-01-18 20:23:36.996 | DEBUG    | __main__:<module>:313 - Training step 2270: loss = 3.6520 | 3046.46ms | Tokens/s = 172,097.5
2025-01-18 20:24:07.448 | DEBUG    | __main__:<module>:313 - Training step 2280: loss = 3.5743 | 3043.63ms | Tokens/s = 172,257.6
2025-01-18 20:24:37.895 | DEBUG    | __main__:<module>:313 - Training step 2290: loss = 3.5090 | 3043.56ms | Tokens/s = 172,261.2
2025-01-18 20:25:08.350 | DEBUG    | __main__:<module>:313 - Training step 2300: loss = 3.5702 | 3045.36ms | Tokens/s = 172,159.6
2025-01-18 20:25:38.836 | DEBUG    | __main__:<module>:313 - Training step 2310: loss = 3.5954 | 3049.51ms | Tokens/s = 171,925.5
2025-01-18 20:26:09.327 | DEBUG    | __main__:<module>:313 - Training step 2320: loss = 3.6573 | 3048.61ms | Tokens/s = 171,976.0
2025-01-18 20:26:39.806 | DEBUG    | __main__:<module>:313 - Training step 2330: loss = 3.6395 | 3047.78ms | Tokens/s = 172,023.1
2025-01-18 20:27:10.279 | DEBUG    | __main__:<module>:313 - Training step 2340: loss = 3.4287 | 3045.89ms | Tokens/s = 172,129.9
2025-01-18 20:27:40.745 | DEBUG    | __main__:<module>:313 - Training step 2350: loss = 3.6560 | 3043.90ms | Tokens/s = 172,242.4
2025-01-18 20:28:11.199 | DEBUG    | __main__:<module>:313 - Training step 2360: loss = 3.4089 | 3045.55ms | Tokens/s = 172,149.1
2025-01-18 20:28:41.659 | DEBUG    | __main__:<module>:313 - Training step 2370: loss = 3.5062 | 3046.42ms | Tokens/s = 172,099.8
2025-01-18 20:29:12.153 | DEBUG    | __main__:<module>:313 - Training step 2380: loss = 3.5105 | 3049.35ms | Tokens/s = 171,934.3
2025-01-18 20:29:42.651 | DEBUG    | __main__:<module>:313 - Training step 2390: loss = 3.6201 | 3049.86ms | Tokens/s = 171,905.6
2025-01-18 20:30:13.128 | DEBUG    | __main__:<module>:313 - Training step 2400: loss = 3.5147 | 3045.33ms | Tokens/s = 172,161.2
2025-01-18 20:30:43.593 | DEBUG    | __main__:<module>:313 - Training step 2410: loss = 3.5803 | 3044.17ms | Tokens/s = 172,226.7
2025-01-18 20:31:14.044 | DEBUG    | __main__:<module>:313 - Training step 2420: loss = 3.5049 | 3045.02ms | Tokens/s = 172,178.9
2025-01-18 20:31:44.519 | DEBUG    | __main__:<module>:313 - Training step 2430: loss = 3.4955 | 3048.20ms | Tokens/s = 171,999.1
2025-01-18 20:32:15.012 | DEBUG    | __main__:<module>:313 - Training step 2440: loss = 3.7353 | 3048.11ms | Tokens/s = 172,004.2
2025-01-18 20:32:45.487 | DEBUG    | __main__:<module>:313 - Training step 2450: loss = 3.5383 | 3046.41ms | Tokens/s = 172,100.1
2025-01-18 20:33:15.973 | DEBUG    | __main__:<module>:313 - Training step 2460: loss = 3.6446 | 3048.85ms | Tokens/s = 171,962.6
2025-01-18 20:33:46.479 | DEBUG    | __main__:<module>:313 - Training step 2470: loss = 3.6876 | 3049.93ms | Tokens/s = 171,901.6
2025-01-18 20:34:16.966 | DEBUG    | __main__:<module>:313 - Training step 2480: loss = 3.5667 | 3046.96ms | Tokens/s = 172,069.1
2025-01-18 20:34:47.446 | DEBUG    | __main__:<module>:313 - Training step 2490: loss = 3.5602 | 3048.18ms | Tokens/s = 172,000.2
2025-01-18 20:35:17.935 | DEBUG    | __main__:<module>:313 - Training step 2500: loss = 3.6857 | 3047.00ms | Tokens/s = 172,067.0
2025-01-18 20:35:48.404 | DEBUG    | __main__:<module>:313 - Training step 2510: loss = 3.6111 | 3047.16ms | Tokens/s = 172,057.7
2025-01-18 20:36:18.867 | DEBUG    | __main__:<module>:313 - Training step 2520: loss = 3.4873 | 3047.57ms | Tokens/s = 172,034.8
2025-01-18 20:36:49.325 | DEBUG    | __main__:<module>:313 - Training step 2530: loss = 3.5873 | 3048.15ms | Tokens/s = 172,002.1
2025-01-18 20:37:19.782 | DEBUG    | __main__:<module>:313 - Training step 2540: loss = 3.5325 | 3043.81ms | Tokens/s = 172,247.4
2025-01-18 20:37:50.230 | DEBUG    | __main__:<module>:313 - Training step 2550: loss = 3.5076 | 3045.30ms | Tokens/s = 172,162.9
2025-01-18 20:38:20.704 | DEBUG    | __main__:<module>:313 - Training step 2560: loss = 3.6554 | 3050.13ms | Tokens/s = 171,890.6
2025-01-18 20:38:51.205 | DEBUG    | __main__:<module>:313 - Training step 2570: loss = 3.5729 | 3048.55ms | Tokens/s = 171,979.4
2025-01-18 20:39:21.689 | DEBUG    | __main__:<module>:313 - Training step 2580: loss = 3.4609 | 3046.57ms | Tokens/s = 172,091.5
2025-01-18 20:39:52.157 | DEBUG    | __main__:<module>:313 - Training step 2590: loss = 3.3166 | 3045.97ms | Tokens/s = 172,124.9
2025-01-18 20:40:22.620 | DEBUG    | __main__:<module>:313 - Training step 2600: loss = 3.5306 | 3044.01ms | Tokens/s = 172,236.1
2025-01-18 20:40:53.084 | DEBUG    | __main__:<module>:313 - Training step 2610: loss = 3.4640 | 3047.86ms | Tokens/s = 172,018.2
2025-01-18 20:41:23.567 | DEBUG    | __main__:<module>:313 - Training step 2620: loss = 3.5738 | 3047.79ms | Tokens/s = 172,022.6
2025-01-18 20:41:54.039 | DEBUG    | __main__:<module>:313 - Training step 2630: loss = 3.6306 | 3045.25ms | Tokens/s = 172,165.9
2025-01-18 20:42:24.499 | DEBUG    | __main__:<module>:313 - Training step 2640: loss = 3.4745 | 3044.04ms | Tokens/s = 172,234.1
2025-01-18 20:42:54.952 | DEBUG    | __main__:<module>:313 - Training step 2650: loss = 3.5221 | 3045.38ms | Tokens/s = 172,158.8
2025-01-18 20:43:25.407 | DEBUG    | __main__:<module>:313 - Training step 2660: loss = 3.5493 | 3045.95ms | Tokens/s = 172,126.1
2025-01-18 20:43:55.861 | DEBUG    | __main__:<module>:313 - Training step 2670: loss = 3.7702 | 3044.57ms | Tokens/s = 172,204.4
2025-01-18 20:44:26.309 | DEBUG    | __main__:<module>:313 - Training step 2680: loss = 3.6534 | 3043.61ms | Tokens/s = 172,258.7
2025-01-18 20:44:56.772 | DEBUG    | __main__:<module>:313 - Training step 2690: loss = 3.6886 | 3046.92ms | Tokens/s = 172,071.6
2025-01-18 20:45:27.264 | DEBUG    | __main__:<module>:313 - Training step 2700: loss = 3.4637 | 3046.57ms | Tokens/s = 172,091.0
2025-01-18 20:45:57.738 | DEBUG    | __main__:<module>:313 - Training step 2710: loss = 3.6451 | 3046.69ms | Tokens/s = 172,084.4
2025-01-18 20:46:28.198 | DEBUG    | __main__:<module>:313 - Training step 2720: loss = 3.4806 | 3046.46ms | Tokens/s = 172,097.4
2025-01-18 20:46:58.665 | DEBUG    | __main__:<module>:313 - Training step 2730: loss = 3.5292 | 3048.03ms | Tokens/s = 172,009.1
2025-01-18 20:47:29.152 | DEBUG    | __main__:<module>:313 - Training step 2740: loss = 3.8031 | 3047.94ms | Tokens/s = 172,013.8
2025-01-18 20:47:59.635 | DEBUG    | __main__:<module>:313 - Training step 2750: loss = 3.6879 | 3046.59ms | Tokens/s = 172,090.2
2025-01-18 20:48:30.104 | DEBUG    | __main__:<module>:313 - Training step 2760: loss = 3.5539 | 3044.15ms | Tokens/s = 172,228.1
2025-01-18 20:49:00.577 | DEBUG    | __main__:<module>:313 - Training step 2770: loss = 3.6933 | 3048.98ms | Tokens/s = 171,955.1
2025-01-18 20:49:31.059 | DEBUG    | __main__:<module>:313 - Training step 2780: loss = 3.4095 | 3046.80ms | Tokens/s = 172,078.0
2025-01-18 20:50:01.527 | DEBUG    | __main__:<module>:313 - Training step 2790: loss = 3.5241 | 3045.81ms | Tokens/s = 172,134.3
2025-01-18 20:50:32.014 | DEBUG    | __main__:<module>:313 - Training step 2800: loss = 3.6294 | 3048.70ms | Tokens/s = 171,971.1
2025-01-18 20:51:02.506 | DEBUG    | __main__:<module>:313 - Training step 2810: loss = 3.7191 | 3048.86ms | Tokens/s = 171,962.2
2025-01-18 20:51:32.992 | DEBUG    | __main__:<module>:313 - Training step 2820: loss = 3.4976 | 3049.37ms | Tokens/s = 171,933.0
2025-01-18 20:52:03.464 | DEBUG    | __main__:<module>:313 - Training step 2830: loss = 3.5201 | 3047.54ms | Tokens/s = 172,036.7
2025-01-18 20:52:33.921 | DEBUG    | __main__:<module>:313 - Training step 2840: loss = 3.4832 | 3047.61ms | Tokens/s = 172,032.2
2025-01-18 20:53:04.369 | DEBUG    | __main__:<module>:313 - Training step 2850: loss = 3.6195 | 3044.76ms | Tokens/s = 172,193.6
2025-01-18 20:53:34.827 | DEBUG    | __main__:<module>:313 - Training step 2860: loss = 3.4870 | 3047.53ms | Tokens/s = 172,037.0
2025-01-18 20:54:05.319 | DEBUG    | __main__:<module>:313 - Training step 2870: loss = 3.5161 | 3051.83ms | Tokens/s = 171,794.6
2025-01-18 20:54:35.797 | DEBUG    | __main__:<module>:313 - Training step 2880: loss = 3.3444 | 3046.26ms | Tokens/s = 172,109.0
2025-01-18 20:55:06.268 | DEBUG    | __main__:<module>:313 - Training step 2890: loss = 3.5104 | 3048.09ms | Tokens/s = 172,005.6
2025-01-18 20:55:36.728 | DEBUG    | __main__:<module>:313 - Training step 2900: loss = 3.4060 | 3045.30ms | Tokens/s = 172,163.0
2025-01-18 20:56:07.179 | DEBUG    | __main__:<module>:313 - Training step 2910: loss = 3.2787 | 3042.90ms | Tokens/s = 172,298.5
2025-01-18 20:56:37.626 | DEBUG    | __main__:<module>:313 - Training step 2920: loss = 3.6257 | 3044.80ms | Tokens/s = 172,191.5
2025-01-18 20:57:08.073 | DEBUG    | __main__:<module>:313 - Training step 2930: loss = 3.6661 | 3042.33ms | Tokens/s = 172,331.2
2025-01-18 20:57:38.526 | DEBUG    | __main__:<module>:313 - Training step 2940: loss = 3.5164 | 3046.38ms | Tokens/s = 172,101.8
2025-01-18 20:58:09.014 | DEBUG    | __main__:<module>:313 - Training step 2950: loss = 3.6744 | 3047.13ms | Tokens/s = 172,059.4
2025-01-18 20:58:39.486 | DEBUG    | __main__:<module>:313 - Training step 2960: loss = 3.3918 | 3047.38ms | Tokens/s = 172,045.5
2025-01-18 20:59:09.945 | DEBUG    | __main__:<module>:313 - Training step 2970: loss = 3.2727 | 3045.79ms | Tokens/s = 172,135.6
2025-01-18 20:59:40.424 | DEBUG    | __main__:<module>:313 - Training step 2980: loss = 3.8681 | 3048.76ms | Tokens/s = 171,967.5
2025-01-18 21:00:10.912 | DEBUG    | __main__:<module>:313 - Training step 2990: loss = 3.6259 | 3046.89ms | Tokens/s = 172,073.4
2025-01-18 21:00:44.798 | INFO     | __main__:<module>:265 - Step 3,000/20,000 loss: 3.5604 (T) 3.5413 (V) | lr=9.9e-03
2025-01-18 21:00:44.800 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 21:00:58.267 | DEBUG    | __main__:<module>:313 - Training step 3000: loss = 3.4732 | 19935.06ms | Tokens/s = 26,299.8
2025-01-18 21:01:28.583 | DEBUG    | __main__:<module>:313 - Training step 3010: loss = 3.5510 | 3039.78ms | Tokens/s = 172,475.5
2025-01-18 21:01:59.015 | DEBUG    | __main__:<module>:313 - Training step 3020: loss = 3.7277 | 3045.74ms | Tokens/s = 172,138.3
2025-01-18 21:02:29.480 | DEBUG    | __main__:<module>:313 - Training step 3030: loss = 3.6406 | 3048.54ms | Tokens/s = 171,980.1
2025-01-18 21:02:59.945 | DEBUG    | __main__:<module>:313 - Training step 3040: loss = 3.4435 | 3045.13ms | Tokens/s = 172,172.4
2025-01-18 21:03:30.395 | DEBUG    | __main__:<module>:313 - Training step 3050: loss = 3.6230 | 3043.36ms | Tokens/s = 172,272.7
2025-01-18 21:04:00.840 | DEBUG    | __main__:<module>:313 - Training step 3060: loss = 3.5795 | 3042.33ms | Tokens/s = 172,331.0
2025-01-18 21:04:31.285 | DEBUG    | __main__:<module>:313 - Training step 3070: loss = 3.4909 | 3043.71ms | Tokens/s = 172,252.8
2025-01-18 21:05:01.715 | DEBUG    | __main__:<module>:313 - Training step 3080: loss = 3.6635 | 3043.04ms | Tokens/s = 172,291.0
2025-01-18 21:05:32.146 | DEBUG    | __main__:<module>:313 - Training step 3090: loss = 3.5702 | 3041.73ms | Tokens/s = 172,364.8
2025-01-18 21:06:02.577 | DEBUG    | __main__:<module>:313 - Training step 3100: loss = 3.5108 | 3042.56ms | Tokens/s = 172,317.9
2025-01-18 21:06:33.019 | DEBUG    | __main__:<module>:313 - Training step 3110: loss = 3.4756 | 3046.18ms | Tokens/s = 172,113.1
2025-01-18 21:07:03.490 | DEBUG    | __main__:<module>:313 - Training step 3120: loss = 3.4900 | 3047.95ms | Tokens/s = 172,013.3
2025-01-18 21:07:33.978 | DEBUG    | __main__:<module>:313 - Training step 3130: loss = 3.4272 | 3048.30ms | Tokens/s = 171,993.8
2025-01-18 21:08:04.453 | DEBUG    | __main__:<module>:313 - Training step 3140: loss = 3.6112 | 3047.52ms | Tokens/s = 172,037.9
2025-01-18 21:08:34.911 | DEBUG    | __main__:<module>:313 - Training step 3150: loss = 3.5275 | 3047.14ms | Tokens/s = 172,059.3
2025-01-18 21:09:05.364 | DEBUG    | __main__:<module>:313 - Training step 3160: loss = 3.6119 | 3043.92ms | Tokens/s = 172,240.9
2025-01-18 21:09:35.811 | DEBUG    | __main__:<module>:313 - Training step 3170: loss = 3.5263 | 3045.36ms | Tokens/s = 172,159.6
2025-01-18 21:10:06.258 | DEBUG    | __main__:<module>:313 - Training step 3180: loss = 3.6429 | 3042.79ms | Tokens/s = 172,305.3
2025-01-18 21:10:36.704 | DEBUG    | __main__:<module>:313 - Training step 3190: loss = 3.4403 | 3047.31ms | Tokens/s = 172,049.3
2025-01-18 21:11:07.190 | DEBUG    | __main__:<module>:313 - Training step 3200: loss = 3.4539 | 3050.81ms | Tokens/s = 171,852.2
2025-01-18 21:11:37.678 | DEBUG    | __main__:<module>:313 - Training step 3210: loss = 3.3385 | 3046.24ms | Tokens/s = 172,110.1
2025-01-18 21:12:08.146 | DEBUG    | __main__:<module>:313 - Training step 3220: loss = 3.5081 | 3047.30ms | Tokens/s = 172,049.9
2025-01-18 21:12:38.599 | DEBUG    | __main__:<module>:313 - Training step 3230: loss = 3.5405 | 3045.77ms | Tokens/s = 172,136.2
2025-01-18 21:13:09.042 | DEBUG    | __main__:<module>:313 - Training step 3240: loss = 3.4511 | 3045.30ms | Tokens/s = 172,162.8
2025-01-18 21:13:39.486 | DEBUG    | __main__:<module>:313 - Training step 3250: loss = 3.3358 | 3045.92ms | Tokens/s = 172,127.9
2025-01-18 21:14:09.931 | DEBUG    | __main__:<module>:313 - Training step 3260: loss = 3.5306 | 3044.99ms | Tokens/s = 172,180.5
2025-01-18 21:14:40.410 | DEBUG    | __main__:<module>:313 - Training step 3270: loss = 3.5099 | 3050.19ms | Tokens/s = 171,887.2
2025-01-18 21:15:10.918 | DEBUG    | __main__:<module>:313 - Training step 3280: loss = 3.6127 | 3052.52ms | Tokens/s = 171,755.8
2025-01-18 21:15:41.397 | DEBUG    | __main__:<module>:313 - Training step 3290: loss = 3.6451 | 3047.29ms | Tokens/s = 172,050.7
2025-01-18 21:16:11.867 | DEBUG    | __main__:<module>:313 - Training step 3300: loss = 3.5822 | 3044.88ms | Tokens/s = 172,186.6
2025-01-18 21:16:42.318 | DEBUG    | __main__:<module>:313 - Training step 3310: loss = 3.5193 | 3044.01ms | Tokens/s = 172,236.2
2025-01-18 21:17:12.761 | DEBUG    | __main__:<module>:313 - Training step 3320: loss = 3.6092 | 3043.44ms | Tokens/s = 172,268.2
2025-01-18 21:17:43.203 | DEBUG    | __main__:<module>:313 - Training step 3330: loss = 3.5437 | 3043.75ms | Tokens/s = 172,250.6
2025-01-18 21:18:13.673 | DEBUG    | __main__:<module>:313 - Training step 3340: loss = 3.5346 | 3048.71ms | Tokens/s = 171,970.6
2025-01-18 21:18:44.138 | DEBUG    | __main__:<module>:313 - Training step 3350: loss = 3.3841 | 3045.83ms | Tokens/s = 172,133.3
2025-01-18 21:19:14.585 | DEBUG    | __main__:<module>:313 - Training step 3360: loss = 3.4786 | 3041.43ms | Tokens/s = 172,382.1
2025-01-18 21:19:45.030 | DEBUG    | __main__:<module>:313 - Training step 3370: loss = 3.3222 | 3043.48ms | Tokens/s = 172,266.0
2025-01-18 21:20:15.465 | DEBUG    | __main__:<module>:313 - Training step 3380: loss = 3.4195 | 3041.64ms | Tokens/s = 172,370.2
2025-01-18 21:20:45.901 | DEBUG    | __main__:<module>:313 - Training step 3390: loss = 3.7045 | 3041.89ms | Tokens/s = 172,356.1
2025-01-18 21:21:16.345 | DEBUG    | __main__:<module>:313 - Training step 3400: loss = 3.4594 | 3045.41ms | Tokens/s = 172,156.7
2025-01-18 21:21:46.829 | DEBUG    | __main__:<module>:313 - Training step 3410: loss = 3.5066 | 3050.08ms | Tokens/s = 171,893.0
2025-01-18 21:22:17.301 | DEBUG    | __main__:<module>:313 - Training step 3420: loss = 3.5879 | 3046.03ms | Tokens/s = 172,121.8
2025-01-18 21:22:47.761 | DEBUG    | __main__:<module>:313 - Training step 3430: loss = 3.6072 | 3044.30ms | Tokens/s = 172,219.4
2025-01-18 21:23:18.212 | DEBUG    | __main__:<module>:313 - Training step 3440: loss = 3.2967 | 3045.29ms | Tokens/s = 172,163.6
2025-01-18 21:23:48.665 | DEBUG    | __main__:<module>:313 - Training step 3450: loss = 3.5728 | 3046.56ms | Tokens/s = 172,091.6
2025-01-18 21:24:19.134 | DEBUG    | __main__:<module>:313 - Training step 3460: loss = 3.3245 | 3048.77ms | Tokens/s = 171,967.1
2025-01-18 21:24:49.624 | DEBUG    | __main__:<module>:313 - Training step 3470: loss = 3.5052 | 3047.51ms | Tokens/s = 172,038.0
2025-01-18 21:25:20.091 | DEBUG    | __main__:<module>:313 - Training step 3480: loss = 3.3415 | 3046.29ms | Tokens/s = 172,106.8
2025-01-18 21:25:50.540 | DEBUG    | __main__:<module>:313 - Training step 3490: loss = 3.2962 | 3043.41ms | Tokens/s = 172,270.0
2025-01-18 21:26:21.007 | DEBUG    | __main__:<module>:313 - Training step 3500: loss = 3.6474 | 3046.16ms | Tokens/s = 172,114.6
2025-01-18 21:26:51.463 | DEBUG    | __main__:<module>:313 - Training step 3510: loss = 3.5081 | 3045.00ms | Tokens/s = 172,180.2
2025-01-18 21:27:21.905 | DEBUG    | __main__:<module>:313 - Training step 3520: loss = 3.3861 | 3043.66ms | Tokens/s = 172,255.9
2025-01-18 21:27:52.346 | DEBUG    | __main__:<module>:313 - Training step 3530: loss = 3.4528 | 3043.03ms | Tokens/s = 172,291.4
2025-01-18 21:28:22.801 | DEBUG    | __main__:<module>:313 - Training step 3540: loss = 3.3082 | 3046.28ms | Tokens/s = 172,107.5
2025-01-18 21:28:53.286 | DEBUG    | __main__:<module>:313 - Training step 3550: loss = 3.4126 | 3047.77ms | Tokens/s = 172,023.6
2025-01-18 21:29:23.753 | DEBUG    | __main__:<module>:313 - Training step 3560: loss = 3.5167 | 3045.21ms | Tokens/s = 172,168.1
2025-01-18 21:29:54.216 | DEBUG    | __main__:<module>:313 - Training step 3570: loss = 3.4150 | 3045.71ms | Tokens/s = 172,139.7
2025-01-18 21:30:24.671 | DEBUG    | __main__:<module>:313 - Training step 3580: loss = 3.3509 | 3046.79ms | Tokens/s = 172,078.8
2025-01-18 21:30:55.121 | DEBUG    | __main__:<module>:313 - Training step 3590: loss = 3.5616 | 3044.62ms | Tokens/s = 172,201.7
2025-01-18 21:31:25.567 | DEBUG    | __main__:<module>:313 - Training step 3600: loss = 3.6074 | 3043.80ms | Tokens/s = 172,247.9
2025-01-18 21:31:56.034 | DEBUG    | __main__:<module>:313 - Training step 3610: loss = 3.3572 | 3048.54ms | Tokens/s = 171,979.9
2025-01-18 21:32:26.520 | DEBUG    | __main__:<module>:313 - Training step 3620: loss = 3.4601 | 3046.77ms | Tokens/s = 172,080.1
2025-01-18 21:32:56.992 | DEBUG    | __main__:<module>:313 - Training step 3630: loss = 3.6346 | 3047.65ms | Tokens/s = 172,030.2
2025-01-18 21:33:27.447 | DEBUG    | __main__:<module>:313 - Training step 3640: loss = 3.4512 | 3044.74ms | Tokens/s = 172,194.9
2025-01-18 21:33:57.904 | DEBUG    | __main__:<module>:313 - Training step 3650: loss = 3.4904 | 3045.29ms | Tokens/s = 172,163.6
2025-01-18 21:34:28.360 | DEBUG    | __main__:<module>:313 - Training step 3660: loss = 3.4166 | 3043.01ms | Tokens/s = 172,292.3
2025-01-18 21:34:58.811 | DEBUG    | __main__:<module>:313 - Training step 3670: loss = 3.4470 | 3047.57ms | Tokens/s = 172,034.6
2025-01-18 21:35:29.284 | DEBUG    | __main__:<module>:313 - Training step 3680: loss = 3.5323 | 3046.89ms | Tokens/s = 172,073.3
2025-01-18 21:35:59.751 | DEBUG    | __main__:<module>:313 - Training step 3690: loss = 3.3670 | 3046.02ms | Tokens/s = 172,122.2
2025-01-18 21:36:30.198 | DEBUG    | __main__:<module>:313 - Training step 3700: loss = 3.3970 | 3044.11ms | Tokens/s = 172,230.2
2025-01-18 21:37:00.641 | DEBUG    | __main__:<module>:313 - Training step 3710: loss = 3.5916 | 3042.25ms | Tokens/s = 172,335.5
2025-01-18 21:37:31.086 | DEBUG    | __main__:<module>:313 - Training step 3720: loss = 3.3357 | 3045.60ms | Tokens/s = 172,145.9
2025-01-18 21:38:01.521 | DEBUG    | __main__:<module>:313 - Training step 3730: loss = 3.3759 | 3043.12ms | Tokens/s = 172,286.1
2025-01-18 21:38:31.956 | DEBUG    | __main__:<module>:313 - Training step 3740: loss = 3.4148 | 3041.69ms | Tokens/s = 172,367.3
2025-01-18 21:39:02.407 | DEBUG    | __main__:<module>:313 - Training step 3750: loss = 3.6268 | 3047.15ms | Tokens/s = 172,058.2
2025-01-18 21:39:32.885 | DEBUG    | __main__:<module>:313 - Training step 3760: loss = 3.5128 | 3047.71ms | Tokens/s = 172,026.7
2025-01-18 21:40:03.349 | DEBUG    | __main__:<module>:313 - Training step 3770: loss = 3.5078 | 3047.68ms | Tokens/s = 172,028.4
2025-01-18 21:40:33.805 | DEBUG    | __main__:<module>:313 - Training step 3780: loss = 3.5726 | 3046.24ms | Tokens/s = 172,110.0
2025-01-18 21:41:04.250 | DEBUG    | __main__:<module>:313 - Training step 3790: loss = 3.4825 | 3043.51ms | Tokens/s = 172,264.0
2025-01-18 21:41:34.698 | DEBUG    | __main__:<module>:313 - Training step 3800: loss = 3.5340 | 3044.96ms | Tokens/s = 172,182.0
2025-01-18 21:42:05.176 | DEBUG    | __main__:<module>:313 - Training step 3810: loss = 3.4569 | 3046.47ms | Tokens/s = 172,097.1
2025-01-18 21:42:35.650 | DEBUG    | __main__:<module>:313 - Training step 3820: loss = 3.3765 | 3045.89ms | Tokens/s = 172,129.9
2025-01-18 21:43:06.104 | DEBUG    | __main__:<module>:313 - Training step 3830: loss = 3.3019 | 3044.87ms | Tokens/s = 172,187.3
2025-01-18 21:43:36.553 | DEBUG    | __main__:<module>:313 - Training step 3840: loss = 3.3673 | 3045.32ms | Tokens/s = 172,162.0
2025-01-18 21:44:06.987 | DEBUG    | __main__:<module>:313 - Training step 3850: loss = 3.4755 | 3045.11ms | Tokens/s = 172,173.7
2025-01-18 21:44:37.429 | DEBUG    | __main__:<module>:313 - Training step 3860: loss = 3.6257 | 3042.59ms | Tokens/s = 172,316.5
2025-01-18 21:45:07.886 | DEBUG    | __main__:<module>:313 - Training step 3870: loss = 3.4893 | 3046.26ms | Tokens/s = 172,109.0
2025-01-18 21:45:38.365 | DEBUG    | __main__:<module>:313 - Training step 3880: loss = 3.5502 | 3047.22ms | Tokens/s = 172,054.6
2025-01-18 21:46:08.836 | DEBUG    | __main__:<module>:313 - Training step 3890: loss = 3.4957 | 3044.54ms | Tokens/s = 172,206.2
2025-01-18 21:46:39.292 | DEBUG    | __main__:<module>:313 - Training step 3900: loss = 3.6454 | 3043.22ms | Tokens/s = 172,280.6
2025-01-18 21:47:09.741 | DEBUG    | __main__:<module>:313 - Training step 3910: loss = 3.3096 | 3043.29ms | Tokens/s = 172,276.6
2025-01-18 21:47:40.188 | DEBUG    | __main__:<module>:313 - Training step 3920: loss = 3.5085 | 3043.15ms | Tokens/s = 172,284.6
2025-01-18 21:48:10.639 | DEBUG    | __main__:<module>:313 - Training step 3930: loss = 3.3111 | 3046.34ms | Tokens/s = 172,104.4
2025-01-18 21:48:41.102 | DEBUG    | __main__:<module>:313 - Training step 3940: loss = 3.4839 | 3044.68ms | Tokens/s = 172,198.1
2025-01-18 21:49:11.551 | DEBUG    | __main__:<module>:313 - Training step 3950: loss = 3.5133 | 3045.49ms | Tokens/s = 172,152.5
2025-01-18 21:49:41.993 | DEBUG    | __main__:<module>:313 - Training step 3960: loss = 3.4519 | 3042.92ms | Tokens/s = 172,297.8
2025-01-18 21:50:12.428 | DEBUG    | __main__:<module>:313 - Training step 3970: loss = 3.4346 | 3044.68ms | Tokens/s = 172,198.1
2025-01-18 21:50:42.882 | DEBUG    | __main__:<module>:313 - Training step 3980: loss = 3.3841 | 3048.24ms | Tokens/s = 171,997.2
2025-01-18 21:51:13.370 | DEBUG    | __main__:<module>:313 - Training step 3990: loss = 3.4294 | 3048.46ms | Tokens/s = 171,984.3
2025-01-18 21:51:47.263 | INFO     | __main__:<module>:265 - Step 4,000/20,000 loss: 3.4566 (T) 3.4906 (V) | lr=9.7e-03
2025-01-18 21:51:47.265 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 21:52:00.950 | DEBUG    | __main__:<module>:313 - Training step 4000: loss = 3.3888 | 20152.57ms | Tokens/s = 26,015.9
2025-01-18 21:52:31.274 | DEBUG    | __main__:<module>:313 - Training step 4010: loss = 3.4815 | 3040.62ms | Tokens/s = 172,428.0
2025-01-18 21:53:01.710 | DEBUG    | __main__:<module>:313 - Training step 4020: loss = 3.5505 | 3045.61ms | Tokens/s = 172,145.6
2025-01-18 21:53:32.172 | DEBUG    | __main__:<module>:313 - Training step 4030: loss = 3.4229 | 3045.64ms | Tokens/s = 172,143.8
2025-01-18 21:54:02.651 | DEBUG    | __main__:<module>:313 - Training step 4040: loss = 3.3529 | 3045.90ms | Tokens/s = 172,129.0
2025-01-18 21:54:33.114 | DEBUG    | __main__:<module>:313 - Training step 4050: loss = 3.3810 | 3046.69ms | Tokens/s = 172,084.6
2025-01-18 21:55:03.566 | DEBUG    | __main__:<module>:313 - Training step 4060: loss = 3.4343 | 3044.44ms | Tokens/s = 172,211.9
2025-01-18 21:55:34.014 | DEBUG    | __main__:<module>:313 - Training step 4070: loss = 3.4348 | 3042.10ms | Tokens/s = 172,343.9
2025-01-18 21:56:04.454 | DEBUG    | __main__:<module>:313 - Training step 4080: loss = 3.3175 | 3045.09ms | Tokens/s = 172,174.6
2025-01-18 21:56:34.887 | DEBUG    | __main__:<module>:313 - Training step 4090: loss = 3.4203 | 3041.70ms | Tokens/s = 172,366.7
2025-01-18 21:57:05.315 | DEBUG    | __main__:<module>:313 - Training step 4100: loss = 3.4560 | 3041.89ms | Tokens/s = 172,355.9
2025-01-18 21:57:35.743 | DEBUG    | __main__:<module>:313 - Training step 4110: loss = 3.5988 | 3042.53ms | Tokens/s = 172,319.9
2025-01-18 21:58:06.169 | DEBUG    | __main__:<module>:313 - Training step 4120: loss = 3.5988 | 3041.46ms | Tokens/s = 172,380.2
2025-01-18 21:58:36.605 | DEBUG    | __main__:<module>:313 - Training step 4130: loss = 3.3559 | 3045.35ms | Tokens/s = 172,160.1
2025-01-18 21:59:07.056 | DEBUG    | __main__:<module>:313 - Training step 4140: loss = 3.5435 | 3045.60ms | Tokens/s = 172,146.2
2025-01-18 21:59:37.491 | DEBUG    | __main__:<module>:313 - Training step 4150: loss = 3.4223 | 3042.79ms | Tokens/s = 172,305.1
2025-01-18 22:00:07.923 | DEBUG    | __main__:<module>:313 - Training step 4160: loss = 3.5480 | 3041.68ms | Tokens/s = 172,368.1
2025-01-18 22:00:38.356 | DEBUG    | __main__:<module>:313 - Training step 4170: loss = 3.5546 | 3043.46ms | Tokens/s = 172,266.9
2025-01-18 22:01:08.784 | DEBUG    | __main__:<module>:313 - Training step 4180: loss = 3.3678 | 3040.96ms | Tokens/s = 172,408.9
2025-01-18 22:01:39.210 | DEBUG    | __main__:<module>:313 - Training step 4190: loss = 3.4459 | 3044.04ms | Tokens/s = 172,234.5
2025-01-18 22:02:09.660 | DEBUG    | __main__:<module>:313 - Training step 4200: loss = 3.4732 | 3045.27ms | Tokens/s = 172,164.4
2025-01-18 22:02:40.139 | DEBUG    | __main__:<module>:313 - Training step 4210: loss = 3.3044 | 3047.56ms | Tokens/s = 172,035.1
2025-01-18 22:03:10.596 | DEBUG    | __main__:<module>:313 - Training step 4220: loss = 3.5213 | 3046.33ms | Tokens/s = 172,104.9
2025-01-18 22:03:41.040 | DEBUG    | __main__:<module>:313 - Training step 4230: loss = 3.4070 | 3043.75ms | Tokens/s = 172,250.8
2025-01-18 22:04:11.478 | DEBUG    | __main__:<module>:313 - Training step 4240: loss = 3.5451 | 3042.76ms | Tokens/s = 172,306.5
2025-01-18 22:04:41.918 | DEBUG    | __main__:<module>:313 - Training step 4250: loss = 3.5205 | 3044.69ms | Tokens/s = 172,197.4
2025-01-18 22:05:12.374 | DEBUG    | __main__:<module>:313 - Training step 4260: loss = 3.3444 | 3045.44ms | Tokens/s = 172,155.0
2025-01-18 22:05:42.823 | DEBUG    | __main__:<module>:313 - Training step 4270: loss = 3.6439 | 3043.14ms | Tokens/s = 172,285.5
2025-01-18 22:06:13.259 | DEBUG    | __main__:<module>:313 - Training step 4280: loss = 3.6180 | 3042.57ms | Tokens/s = 172,317.2
2025-01-18 22:06:43.700 | DEBUG    | __main__:<module>:313 - Training step 4290: loss = 3.4847 | 3045.04ms | Tokens/s = 172,177.8
2025-01-18 22:07:14.144 | DEBUG    | __main__:<module>:313 - Training step 4300: loss = 3.3467 | 3041.85ms | Tokens/s = 172,358.5
2025-01-18 22:07:44.585 | DEBUG    | __main__:<module>:313 - Training step 4310: loss = 3.6253 | 3044.93ms | Tokens/s = 172,183.8
2025-01-18 22:08:15.045 | DEBUG    | __main__:<module>:313 - Training step 4320: loss = 3.6099 | 3047.59ms | Tokens/s = 172,033.5
2025-01-18 22:08:45.504 | DEBUG    | __main__:<module>:313 - Training step 4330: loss = 3.3835 | 3044.73ms | Tokens/s = 172,195.2
2025-01-18 22:09:15.951 | DEBUG    | __main__:<module>:313 - Training step 4340: loss = 3.3605 | 3044.48ms | Tokens/s = 172,209.3
2025-01-18 22:09:46.412 | DEBUG    | __main__:<module>:313 - Training step 4350: loss = 3.5037 | 3045.03ms | Tokens/s = 172,178.1
2025-01-18 22:10:16.863 | DEBUG    | __main__:<module>:313 - Training step 4360: loss = 3.4475 | 3044.88ms | Tokens/s = 172,186.7
2025-01-18 22:10:47.299 | DEBUG    | __main__:<module>:313 - Training step 4370: loss = 3.4190 | 3042.44ms | Tokens/s = 172,324.8
2025-01-18 22:11:17.750 | DEBUG    | __main__:<module>:313 - Training step 4380: loss = 3.5946 | 3044.52ms | Tokens/s = 172,207.4
2025-01-18 22:11:48.226 | DEBUG    | __main__:<module>:313 - Training step 4390: loss = 3.4511 | 3047.75ms | Tokens/s = 172,024.4
2025-01-18 22:12:18.681 | DEBUG    | __main__:<module>:313 - Training step 4400: loss = 3.4561 | 3043.88ms | Tokens/s = 172,243.1
2025-01-18 22:12:49.135 | DEBUG    | __main__:<module>:313 - Training step 4410: loss = 3.4067 | 3045.66ms | Tokens/s = 172,142.5
2025-01-18 22:13:19.572 | DEBUG    | __main__:<module>:313 - Training step 4420: loss = 3.6195 | 3042.53ms | Tokens/s = 172,319.8
2025-01-18 22:13:50.002 | DEBUG    | __main__:<module>:313 - Training step 4430: loss = 3.3900 | 3044.45ms | Tokens/s = 172,210.9
2025-01-18 22:14:20.428 | DEBUG    | __main__:<module>:313 - Training step 4440: loss = 3.4563 | 3042.42ms | Tokens/s = 172,325.8
2025-01-18 22:14:50.862 | DEBUG    | __main__:<module>:313 - Training step 4450: loss = 3.4570 | 3044.90ms | Tokens/s = 172,185.4
2025-01-18 22:15:21.342 | DEBUG    | __main__:<module>:313 - Training step 4460: loss = 3.4429 | 3049.53ms | Tokens/s = 171,924.4
2025-01-18 22:15:51.809 | DEBUG    | __main__:<module>:313 - Training step 4470: loss = 3.4518 | 3044.45ms | Tokens/s = 172,210.9
2025-01-18 22:16:22.263 | DEBUG    | __main__:<module>:313 - Training step 4480: loss = 3.5181 | 3048.50ms | Tokens/s = 171,982.3
2025-01-18 22:16:52.746 | DEBUG    | __main__:<module>:313 - Training step 4490: loss = 3.4759 | 3047.77ms | Tokens/s = 172,023.6
2025-01-18 22:17:23.218 | DEBUG    | __main__:<module>:313 - Training step 4500: loss = 3.6263 | 3045.69ms | Tokens/s = 172,140.9
2025-01-18 22:17:53.699 | DEBUG    | __main__:<module>:313 - Training step 4510: loss = 3.3371 | 3048.76ms | Tokens/s = 171,967.8
2025-01-18 22:18:24.178 | DEBUG    | __main__:<module>:313 - Training step 4520: loss = 3.5227 | 3047.76ms | Tokens/s = 172,024.0
2025-01-18 22:18:54.658 | DEBUG    | __main__:<module>:313 - Training step 4530: loss = 3.5379 | 3047.12ms | Tokens/s = 172,060.2
2025-01-18 22:19:25.115 | DEBUG    | __main__:<module>:313 - Training step 4540: loss = 3.2604 | 3046.08ms | Tokens/s = 172,118.7
2025-01-18 22:19:55.559 | DEBUG    | __main__:<module>:313 - Training step 4550: loss = 3.5147 | 3044.08ms | Tokens/s = 172,231.8
2025-01-18 22:20:25.999 | DEBUG    | __main__:<module>:313 - Training step 4560: loss = 3.3233 | 3043.47ms | Tokens/s = 172,266.4
2025-01-18 22:20:56.447 | DEBUG    | __main__:<module>:313 - Training step 4570: loss = 3.3882 | 3047.86ms | Tokens/s = 172,018.7
2025-01-18 22:21:26.916 | DEBUG    | __main__:<module>:313 - Training step 4580: loss = 3.3922 | 3044.60ms | Tokens/s = 172,202.7
2025-01-18 22:21:57.378 | DEBUG    | __main__:<module>:313 - Training step 4590: loss = 3.3537 | 3044.19ms | Tokens/s = 172,225.7
2025-01-18 22:22:27.824 | DEBUG    | __main__:<module>:313 - Training step 4600: loss = 3.3363 | 3042.13ms | Tokens/s = 172,342.2
2025-01-18 22:22:58.269 | DEBUG    | __main__:<module>:313 - Training step 4610: loss = 3.5435 | 3042.60ms | Tokens/s = 172,315.6
2025-01-18 22:23:28.702 | DEBUG    | __main__:<module>:313 - Training step 4620: loss = 3.4647 | 3042.34ms | Tokens/s = 172,330.6
2025-01-18 22:23:59.131 | DEBUG    | __main__:<module>:313 - Training step 4630: loss = 3.6000 | 3041.24ms | Tokens/s = 172,392.9
2025-01-18 22:24:29.561 | DEBUG    | __main__:<module>:313 - Training step 4640: loss = 3.5746 | 3045.06ms | Tokens/s = 172,176.7
2025-01-18 22:24:59.982 | DEBUG    | __main__:<module>:313 - Training step 4650: loss = 3.3841 | 3041.84ms | Tokens/s = 172,358.6
2025-01-18 22:25:30.410 | DEBUG    | __main__:<module>:313 - Training step 4660: loss = 3.3934 | 3043.02ms | Tokens/s = 172,292.0
2025-01-18 22:26:00.833 | DEBUG    | __main__:<module>:313 - Training step 4670: loss = 3.4103 | 3039.73ms | Tokens/s = 172,478.5
2025-01-18 22:26:31.260 | DEBUG    | __main__:<module>:313 - Training step 4680: loss = 3.4662 | 3044.53ms | Tokens/s = 172,206.6
2025-01-18 22:27:01.713 | DEBUG    | __main__:<module>:313 - Training step 4690: loss = 3.6234 | 3046.60ms | Tokens/s = 172,089.7
2025-01-18 22:27:32.162 | DEBUG    | __main__:<module>:313 - Training step 4700: loss = 3.5018 | 3044.59ms | Tokens/s = 172,202.9
2025-01-18 22:28:02.601 | DEBUG    | __main__:<module>:313 - Training step 4710: loss = 3.1711 | 3043.45ms | Tokens/s = 172,267.8
2025-01-18 22:28:33.028 | DEBUG    | __main__:<module>:313 - Training step 4720: loss = 3.5016 | 3042.86ms | Tokens/s = 172,301.1
2025-01-18 22:29:03.484 | DEBUG    | __main__:<module>:313 - Training step 4730: loss = 3.2750 | 3047.03ms | Tokens/s = 172,065.2
2025-01-18 22:29:33.941 | DEBUG    | __main__:<module>:313 - Training step 4740: loss = 3.4044 | 3044.32ms | Tokens/s = 172,218.5
2025-01-18 22:30:04.387 | DEBUG    | __main__:<module>:313 - Training step 4750: loss = 3.5549 | 3043.69ms | Tokens/s = 172,253.9
2025-01-18 22:30:34.824 | DEBUG    | __main__:<module>:313 - Training step 4760: loss = 3.4149 | 3043.58ms | Tokens/s = 172,260.2
2025-01-18 22:31:05.263 | DEBUG    | __main__:<module>:313 - Training step 4770: loss = 3.6216 | 3044.52ms | Tokens/s = 172,207.2
2025-01-18 22:31:35.737 | DEBUG    | __main__:<module>:313 - Training step 4780: loss = 3.7270 | 3049.28ms | Tokens/s = 171,938.2
2025-01-18 22:32:06.217 | DEBUG    | __main__:<module>:313 - Training step 4790: loss = 3.3733 | 3047.64ms | Tokens/s = 172,030.9
2025-01-18 22:32:36.685 | DEBUG    | __main__:<module>:313 - Training step 4800: loss = 3.5258 | 3044.05ms | Tokens/s = 172,233.6
2025-01-18 22:33:07.133 | DEBUG    | __main__:<module>:313 - Training step 4810: loss = 3.5481 | 3042.96ms | Tokens/s = 172,295.6
2025-01-18 22:33:37.573 | DEBUG    | __main__:<module>:313 - Training step 4820: loss = 3.4777 | 3043.63ms | Tokens/s = 172,257.7
2025-01-18 22:34:08.001 | DEBUG    | __main__:<module>:313 - Training step 4830: loss = 3.4094 | 3044.40ms | Tokens/s = 172,213.8
2025-01-18 22:34:38.449 | DEBUG    | __main__:<module>:313 - Training step 4840: loss = 3.4637 | 3047.21ms | Tokens/s = 172,054.8
2025-01-18 22:35:08.907 | DEBUG    | __main__:<module>:313 - Training step 4850: loss = 3.3830 | 3045.43ms | Tokens/s = 172,155.7
2025-01-18 22:35:39.344 | DEBUG    | __main__:<module>:313 - Training step 4860: loss = 3.2680 | 3042.82ms | Tokens/s = 172,303.1
2025-01-18 22:36:09.777 | DEBUG    | __main__:<module>:313 - Training step 4870: loss = 3.4192 | 3041.77ms | Tokens/s = 172,363.0
2025-01-18 22:36:40.212 | DEBUG    | __main__:<module>:313 - Training step 4880: loss = 3.3695 | 3042.46ms | Tokens/s = 172,323.8
2025-01-18 22:37:10.634 | DEBUG    | __main__:<module>:313 - Training step 4890: loss = 3.3383 | 3042.66ms | Tokens/s = 172,312.2
2025-01-18 22:37:41.055 | DEBUG    | __main__:<module>:313 - Training step 4900: loss = 3.5340 | 3039.96ms | Tokens/s = 172,465.3
2025-01-18 22:38:11.478 | DEBUG    | __main__:<module>:313 - Training step 4910: loss = 3.5910 | 3043.12ms | Tokens/s = 172,286.4
2025-01-18 22:38:41.940 | DEBUG    | __main__:<module>:313 - Training step 4920: loss = 3.4280 | 3047.49ms | Tokens/s = 172,039.5
2025-01-18 22:39:12.410 | DEBUG    | __main__:<module>:313 - Training step 4930: loss = 3.4898 | 3046.32ms | Tokens/s = 172,105.2
2025-01-18 22:39:42.865 | DEBUG    | __main__:<module>:313 - Training step 4940: loss = 3.3389 | 3046.22ms | Tokens/s = 172,110.9
2025-01-18 22:40:13.306 | DEBUG    | __main__:<module>:313 - Training step 4950: loss = 3.3576 | 3044.67ms | Tokens/s = 172,198.6
2025-01-18 22:40:43.743 | DEBUG    | __main__:<module>:313 - Training step 4960: loss = 3.4914 | 3043.25ms | Tokens/s = 172,279.2
2025-01-18 22:41:14.200 | DEBUG    | __main__:<module>:313 - Training step 4970: loss = 3.3996 | 3046.58ms | Tokens/s = 172,090.4
2025-01-18 22:41:44.667 | DEBUG    | __main__:<module>:313 - Training step 4980: loss = 3.5425 | 3046.86ms | Tokens/s = 172,075.1
2025-01-18 22:42:15.126 | DEBUG    | __main__:<module>:313 - Training step 4990: loss = 3.4440 | 3046.88ms | Tokens/s = 172,073.9
2025-01-18 22:42:48.982 | INFO     | __main__:<module>:265 - Step 5,000/20,000 loss: 3.4301 (T) 3.4529 (V) | lr=9.3e-03
2025-01-18 22:42:48.983 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 22:43:02.691 | DEBUG    | __main__:<module>:313 - Training step 5000: loss = 3.2921 | 20171.73ms | Tokens/s = 25,991.2
2025-01-18 22:43:32.975 | DEBUG    | __main__:<module>:313 - Training step 5010: loss = 3.3523 | 3036.02ms | Tokens/s = 172,689.3
2025-01-18 22:44:03.367 | DEBUG    | __main__:<module>:313 - Training step 5020: loss = 3.5272 | 3041.53ms | Tokens/s = 172,376.1
2025-01-18 22:44:33.801 | DEBUG    | __main__:<module>:313 - Training step 5030: loss = 3.3942 | 3044.37ms | Tokens/s = 172,215.6
2025-01-18 22:45:04.263 | DEBUG    | __main__:<module>:313 - Training step 5040: loss = 3.4474 | 3046.11ms | Tokens/s = 172,117.2
2025-01-18 22:45:34.725 | DEBUG    | __main__:<module>:313 - Training step 5050: loss = 3.5192 | 3043.45ms | Tokens/s = 172,267.9
2025-01-18 22:46:05.167 | DEBUG    | __main__:<module>:313 - Training step 5060: loss = 3.3852 | 3044.11ms | Tokens/s = 172,230.6
2025-01-18 22:46:35.606 | DEBUG    | __main__:<module>:313 - Training step 5070: loss = 3.4404 | 3044.99ms | Tokens/s = 172,180.6
2025-01-18 22:47:06.067 | DEBUG    | __main__:<module>:313 - Training step 5080: loss = 3.5582 | 3046.44ms | Tokens/s = 172,098.5
2025-01-18 22:47:36.525 | DEBUG    | __main__:<module>:313 - Training step 5090: loss = 3.4613 | 3043.45ms | Tokens/s = 172,267.9
2025-01-18 22:48:06.965 | DEBUG    | __main__:<module>:313 - Training step 5100: loss = 3.6504 | 3043.19ms | Tokens/s = 172,282.2
2025-01-18 22:48:37.393 | DEBUG    | __main__:<module>:313 - Training step 5110: loss = 3.3764 | 3043.21ms | Tokens/s = 172,281.2
2025-01-18 22:49:07.841 | DEBUG    | __main__:<module>:313 - Training step 5120: loss = 3.5777 | 3046.53ms | Tokens/s = 172,093.3
2025-01-18 22:49:38.298 | DEBUG    | __main__:<module>:313 - Training step 5130: loss = 3.3882 | 3043.77ms | Tokens/s = 172,249.3
2025-01-18 22:50:08.757 | DEBUG    | __main__:<module>:313 - Training step 5140: loss = 3.6148 | 3043.24ms | Tokens/s = 172,279.5
2025-01-18 22:50:39.199 | DEBUG    | __main__:<module>:313 - Training step 5150: loss = 3.3945 | 3043.55ms | Tokens/s = 172,262.0
2025-01-18 22:51:09.629 | DEBUG    | __main__:<module>:313 - Training step 5160: loss = 3.4685 | 3042.45ms | Tokens/s = 172,324.4
2025-01-18 22:51:40.060 | DEBUG    | __main__:<module>:313 - Training step 5170: loss = 3.4110 | 3043.80ms | Tokens/s = 172,248.1
2025-01-18 22:52:10.484 | DEBUG    | __main__:<module>:313 - Training step 5180: loss = 3.5234 | 3039.33ms | Tokens/s = 172,501.0
2025-01-18 22:52:40.924 | DEBUG    | __main__:<module>:313 - Training step 5190: loss = 3.4629 | 3045.31ms | Tokens/s = 172,162.4
2025-01-18 22:53:11.379 | DEBUG    | __main__:<module>:313 - Training step 5200: loss = 3.3845 | 3043.03ms | Tokens/s = 172,291.2
2025-01-18 22:53:41.820 | DEBUG    | __main__:<module>:313 - Training step 5210: loss = 3.4593 | 3043.43ms | Tokens/s = 172,268.6
2025-01-18 22:54:12.257 | DEBUG    | __main__:<module>:313 - Training step 5220: loss = 3.5012 | 3043.78ms | Tokens/s = 172,249.2
2025-01-18 22:54:42.678 | DEBUG    | __main__:<module>:313 - Training step 5230: loss = 3.2798 | 3039.08ms | Tokens/s = 172,515.4
2025-01-18 22:55:13.091 | DEBUG    | __main__:<module>:313 - Training step 5240: loss = 3.5739 | 3041.02ms | Tokens/s = 172,405.4
2025-01-18 22:55:43.498 | DEBUG    | __main__:<module>:313 - Training step 5250: loss = 3.4054 | 3040.01ms | Tokens/s = 172,462.5
2025-01-18 22:56:13.906 | DEBUG    | __main__:<module>:313 - Training step 5260: loss = 3.5842 | 3039.48ms | Tokens/s = 172,492.7
2025-01-18 22:56:44.320 | DEBUG    | __main__:<module>:313 - Training step 5270: loss = 3.3852 | 3041.13ms | Tokens/s = 172,398.8
2025-01-18 22:57:14.759 | DEBUG    | __main__:<module>:313 - Training step 5280: loss = 3.4657 | 3043.29ms | Tokens/s = 172,276.5
2025-01-18 22:57:45.188 | DEBUG    | __main__:<module>:313 - Training step 5290: loss = 3.4197 | 3041.44ms | Tokens/s = 172,381.5
2025-01-18 22:58:15.611 | DEBUG    | __main__:<module>:313 - Training step 5300: loss = 3.3885 | 3043.08ms | Tokens/s = 172,288.7
2025-01-18 22:58:46.043 | DEBUG    | __main__:<module>:313 - Training step 5310: loss = 3.5955 | 3044.30ms | Tokens/s = 172,219.3
2025-01-18 22:59:16.466 | DEBUG    | __main__:<module>:313 - Training step 5320: loss = 3.4324 | 3041.97ms | Tokens/s = 172,351.8
2025-01-18 22:59:46.884 | DEBUG    | __main__:<module>:313 - Training step 5330: loss = 3.4678 | 3039.29ms | Tokens/s = 172,503.2
2025-01-18 23:00:17.312 | DEBUG    | __main__:<module>:313 - Training step 5340: loss = 3.4671 | 3043.87ms | Tokens/s = 172,244.0
2025-01-18 23:00:47.742 | DEBUG    | __main__:<module>:313 - Training step 5350: loss = 3.3442 | 3043.03ms | Tokens/s = 172,291.4
2025-01-18 23:01:18.177 | DEBUG    | __main__:<module>:313 - Training step 5360: loss = 3.4407 | 3043.77ms | Tokens/s = 172,249.5
2025-01-18 23:01:48.603 | DEBUG    | __main__:<module>:313 - Training step 5370: loss = 3.5048 | 3041.97ms | Tokens/s = 172,351.5
2025-01-18 23:02:19.028 | DEBUG    | __main__:<module>:313 - Training step 5380: loss = 3.4302 | 3044.55ms | Tokens/s = 172,205.4
2025-01-18 23:02:49.469 | DEBUG    | __main__:<module>:313 - Training step 5390: loss = 3.4848 | 3041.68ms | Tokens/s = 172,367.7
2025-01-18 23:03:19.904 | DEBUG    | __main__:<module>:313 - Training step 5400: loss = 3.4684 | 3041.10ms | Tokens/s = 172,400.6
2025-01-18 23:03:50.330 | DEBUG    | __main__:<module>:313 - Training step 5410: loss = 3.6225 | 3043.90ms | Tokens/s = 172,242.0
2025-01-18 23:04:20.778 | DEBUG    | __main__:<module>:313 - Training step 5420: loss = 3.4988 | 3045.20ms | Tokens/s = 172,168.5
2025-01-18 23:04:51.253 | DEBUG    | __main__:<module>:313 - Training step 5430: loss = 3.3062 | 3048.91ms | Tokens/s = 171,959.2
2025-01-18 23:05:21.709 | DEBUG    | __main__:<module>:313 - Training step 5440: loss = 3.4819 | 3044.52ms | Tokens/s = 172,207.3
2025-01-18 23:05:52.155 | DEBUG    | __main__:<module>:313 - Training step 5450: loss = 3.4603 | 3043.36ms | Tokens/s = 172,272.6
2025-01-18 23:06:22.584 | DEBUG    | __main__:<module>:313 - Training step 5460: loss = 3.5196 | 3043.15ms | Tokens/s = 172,284.6
2025-01-18 23:06:53.007 | DEBUG    | __main__:<module>:313 - Training step 5470: loss = 3.5500 | 3042.82ms | Tokens/s = 172,303.2
2025-01-18 23:07:23.454 | DEBUG    | __main__:<module>:313 - Training step 5480: loss = 3.4114 | 3047.05ms | Tokens/s = 172,064.3
2025-01-18 23:07:53.911 | DEBUG    | __main__:<module>:313 - Training step 5490: loss = 3.4615 | 3043.38ms | Tokens/s = 172,271.6
2025-01-18 23:08:24.355 | DEBUG    | __main__:<module>:313 - Training step 5500: loss = 3.3729 | 3045.98ms | Tokens/s = 172,124.3
2025-01-18 23:08:54.813 | DEBUG    | __main__:<module>:313 - Training step 5510: loss = 3.4035 | 3043.31ms | Tokens/s = 172,275.3
2025-01-18 23:09:25.257 | DEBUG    | __main__:<module>:313 - Training step 5520: loss = 3.5728 | 3043.97ms | Tokens/s = 172,238.3
2025-01-18 23:09:55.682 | DEBUG    | __main__:<module>:313 - Training step 5530: loss = 3.5018 | 3041.26ms | Tokens/s = 172,391.8
2025-01-18 23:10:26.104 | DEBUG    | __main__:<module>:313 - Training step 5540: loss = 3.4622 | 3041.82ms | Tokens/s = 172,360.2
2025-01-18 23:10:56.514 | DEBUG    | __main__:<module>:313 - Training step 5550: loss = 3.3669 | 3040.09ms | Tokens/s = 172,458.3
2025-01-18 23:11:26.947 | DEBUG    | __main__:<module>:313 - Training step 5560: loss = 3.2479 | 3047.26ms | Tokens/s = 172,052.0
2025-01-18 23:11:57.419 | DEBUG    | __main__:<module>:313 - Training step 5570: loss = 3.1462 | 3045.91ms | Tokens/s = 172,128.4
2025-01-18 23:12:27.872 | DEBUG    | __main__:<module>:313 - Training step 5580: loss = 3.4388 | 3045.61ms | Tokens/s = 172,145.7
2025-01-18 23:12:58.311 | DEBUG    | __main__:<module>:313 - Training step 5590: loss = 3.3983 | 3044.18ms | Tokens/s = 172,226.2
2025-01-18 23:13:28.745 | DEBUG    | __main__:<module>:313 - Training step 5600: loss = 3.3628 | 3041.37ms | Tokens/s = 172,385.6
2025-01-18 23:13:59.193 | DEBUG    | __main__:<module>:313 - Training step 5610: loss = 3.4332 | 3045.07ms | Tokens/s = 172,176.1
2025-01-18 23:14:29.634 | DEBUG    | __main__:<module>:313 - Training step 5620: loss = 3.5200 | 3041.65ms | Tokens/s = 172,369.5
2025-01-18 23:15:00.054 | DEBUG    | __main__:<module>:313 - Training step 5630: loss = 3.3674 | 3043.07ms | Tokens/s = 172,288.9
2025-01-18 23:15:30.473 | DEBUG    | __main__:<module>:313 - Training step 5640: loss = 3.4747 | 3040.77ms | Tokens/s = 172,419.7
2025-01-18 23:16:00.884 | DEBUG    | __main__:<module>:313 - Training step 5650: loss = 3.4067 | 3040.34ms | Tokens/s = 172,444.0
2025-01-18 23:16:31.306 | DEBUG    | __main__:<module>:313 - Training step 5660: loss = 3.3984 | 3042.79ms | Tokens/s = 172,304.9
2025-01-18 23:17:01.763 | DEBUG    | __main__:<module>:313 - Training step 5670: loss = 3.4314 | 3045.28ms | Tokens/s = 172,164.2
2025-01-18 23:17:32.233 | DEBUG    | __main__:<module>:313 - Training step 5680: loss = 3.3668 | 3047.29ms | Tokens/s = 172,050.4
2025-01-18 23:18:02.688 | DEBUG    | __main__:<module>:313 - Training step 5690: loss = 3.3565 | 3043.67ms | Tokens/s = 172,255.0
2025-01-18 23:18:33.127 | DEBUG    | __main__:<module>:313 - Training step 5700: loss = 3.4720 | 3040.31ms | Tokens/s = 172,445.4
2025-01-18 23:19:03.553 | DEBUG    | __main__:<module>:313 - Training step 5710: loss = 3.4455 | 3040.61ms | Tokens/s = 172,428.5
2025-01-18 23:19:33.967 | DEBUG    | __main__:<module>:313 - Training step 5720: loss = 3.4482 | 3041.94ms | Tokens/s = 172,353.3
2025-01-18 23:20:04.392 | DEBUG    | __main__:<module>:313 - Training step 5730: loss = 3.5663 | 3042.28ms | Tokens/s = 172,334.0
2025-01-18 23:20:34.853 | DEBUG    | __main__:<module>:313 - Training step 5740: loss = 3.4367 | 3044.49ms | Tokens/s = 172,209.0
2025-01-18 23:21:05.306 | DEBUG    | __main__:<module>:313 - Training step 5750: loss = 3.5194 | 3045.34ms | Tokens/s = 172,160.7
2025-01-18 23:21:35.762 | DEBUG    | __main__:<module>:313 - Training step 5760: loss = 3.4324 | 3044.95ms | Tokens/s = 172,182.6
2025-01-18 23:22:06.202 | DEBUG    | __main__:<module>:313 - Training step 5770: loss = 3.2823 | 3045.25ms | Tokens/s = 172,165.6
2025-01-18 23:22:36.623 | DEBUG    | __main__:<module>:313 - Training step 5780: loss = 3.5266 | 3041.38ms | Tokens/s = 172,384.8
2025-01-18 23:23:07.048 | DEBUG    | __main__:<module>:313 - Training step 5790: loss = 3.5545 | 3040.85ms | Tokens/s = 172,415.1
2025-01-18 23:23:37.477 | DEBUG    | __main__:<module>:313 - Training step 5800: loss = 3.3223 | 3042.65ms | Tokens/s = 172,313.0
2025-01-18 23:24:07.931 | DEBUG    | __main__:<module>:313 - Training step 5810: loss = 3.4554 | 3044.65ms | Tokens/s = 172,199.9
2025-01-18 23:24:38.363 | DEBUG    | __main__:<module>:313 - Training step 5820: loss = 3.3951 | 3042.81ms | Tokens/s = 172,303.8
2025-01-18 23:25:08.781 | DEBUG    | __main__:<module>:313 - Training step 5830: loss = 3.4832 | 3041.45ms | Tokens/s = 172,380.7
2025-01-18 23:25:39.196 | DEBUG    | __main__:<module>:313 - Training step 5840: loss = 3.5341 | 3041.62ms | Tokens/s = 172,371.5
2025-01-18 23:26:09.604 | DEBUG    | __main__:<module>:313 - Training step 5850: loss = 3.3096 | 3042.56ms | Tokens/s = 172,317.9
2025-01-18 23:26:40.050 | DEBUG    | __main__:<module>:313 - Training step 5860: loss = 3.3341 | 3046.75ms | Tokens/s = 172,081.1
2025-01-18 23:27:10.507 | DEBUG    | __main__:<module>:313 - Training step 5870: loss = 3.6664 | 3046.80ms | Tokens/s = 172,078.1
2025-01-18 23:27:40.951 | DEBUG    | __main__:<module>:313 - Training step 5880: loss = 3.5085 | 3041.90ms | Tokens/s = 172,355.4
2025-01-18 23:28:11.376 | DEBUG    | __main__:<module>:313 - Training step 5890: loss = 3.3598 | 3041.21ms | Tokens/s = 172,394.3
2025-01-18 23:28:41.796 | DEBUG    | __main__:<module>:313 - Training step 5900: loss = 3.2260 | 3041.06ms | Tokens/s = 172,403.1
2025-01-18 23:29:12.206 | DEBUG    | __main__:<module>:313 - Training step 5910: loss = 3.4836 | 3040.58ms | Tokens/s = 172,430.3
2025-01-18 23:29:42.617 | DEBUG    | __main__:<module>:313 - Training step 5920: loss = 3.6569 | 3040.91ms | Tokens/s = 172,411.5
2025-01-18 23:30:13.028 | DEBUG    | __main__:<module>:313 - Training step 5930: loss = 3.5709 | 3041.47ms | Tokens/s = 172,380.0
2025-01-18 23:30:43.448 | DEBUG    | __main__:<module>:313 - Training step 5940: loss = 3.1134 | 3043.78ms | Tokens/s = 172,248.7
2025-01-18 23:31:13.899 | DEBUG    | __main__:<module>:313 - Training step 5950: loss = 3.4759 | 3047.35ms | Tokens/s = 172,047.4
2025-01-18 23:31:44.332 | DEBUG    | __main__:<module>:313 - Training step 5960: loss = 3.4550 | 3039.18ms | Tokens/s = 172,509.4
2025-01-18 23:32:14.750 | DEBUG    | __main__:<module>:313 - Training step 5970: loss = 3.3541 | 3041.29ms | Tokens/s = 172,390.1
2025-01-18 23:32:45.157 | DEBUG    | __main__:<module>:313 - Training step 5980: loss = 3.4928 | 3040.42ms | Tokens/s = 172,439.2
2025-01-18 23:33:15.566 | DEBUG    | __main__:<module>:313 - Training step 5990: loss = 3.4092 | 3044.98ms | Tokens/s = 172,181.2
2025-01-18 23:33:49.425 | INFO     | __main__:<module>:265 - Step 6,000/20,000 loss: 3.4219 (T) 3.4397 (V) | lr=8.8e-03
2025-01-18 23:33:49.427 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 23:34:02.816 | DEBUG    | __main__:<module>:313 - Training step 6000: loss = 3.3639 | 19853.04ms | Tokens/s = 26,408.5
2025-01-18 23:34:33.121 | DEBUG    | __main__:<module>:313 - Training step 6010: loss = 3.2961 | 3037.49ms | Tokens/s = 172,605.5
2025-01-18 23:35:03.532 | DEBUG    | __main__:<module>:313 - Training step 6020: loss = 3.5271 | 3043.43ms | Tokens/s = 172,268.8
2025-01-18 23:35:33.961 | DEBUG    | __main__:<module>:313 - Training step 6030: loss = 3.4462 | 3042.92ms | Tokens/s = 172,297.5
2025-01-18 23:36:04.373 | DEBUG    | __main__:<module>:313 - Training step 6040: loss = 3.4358 | 3040.31ms | Tokens/s = 172,445.7
2025-01-18 23:36:34.794 | DEBUG    | __main__:<module>:313 - Training step 6050: loss = 3.3612 | 3042.38ms | Tokens/s = 172,328.4
2025-01-18 23:37:05.236 | DEBUG    | __main__:<module>:313 - Training step 6060: loss = 3.1892 | 3045.39ms | Tokens/s = 172,158.1
2025-01-18 23:37:35.711 | DEBUG    | __main__:<module>:313 - Training step 6070: loss = 3.3949 | 3047.73ms | Tokens/s = 172,025.8
2025-01-18 23:38:06.182 | DEBUG    | __main__:<module>:313 - Training step 6080: loss = 3.4151 | 3047.14ms | Tokens/s = 172,059.0
2025-01-18 23:38:36.631 | DEBUG    | __main__:<module>:313 - Training step 6090: loss = 3.4272 | 3043.38ms | Tokens/s = 172,271.7
2025-01-18 23:39:07.060 | DEBUG    | __main__:<module>:313 - Training step 6100: loss = 3.3838 | 3039.78ms | Tokens/s = 172,475.7
2025-01-18 23:39:37.515 | DEBUG    | __main__:<module>:313 - Training step 6110: loss = 3.5730 | 3046.58ms | Tokens/s = 172,090.5
2025-01-18 23:40:07.967 | DEBUG    | __main__:<module>:313 - Training step 6120: loss = 3.3674 | 3045.19ms | Tokens/s = 172,169.3
2025-01-18 23:40:38.399 | DEBUG    | __main__:<module>:313 - Training step 6130: loss = 3.5147 | 3042.35ms | Tokens/s = 172,329.9
2025-01-18 23:41:08.823 | DEBUG    | __main__:<module>:313 - Training step 6140: loss = 3.4100 | 3041.77ms | Tokens/s = 172,363.0
2025-01-18 23:41:39.266 | DEBUG    | __main__:<module>:313 - Training step 6150: loss = 3.5449 | 3046.14ms | Tokens/s = 172,115.6
2025-01-18 23:42:09.723 | DEBUG    | __main__:<module>:313 - Training step 6160: loss = 3.5097 | 3044.66ms | Tokens/s = 172,199.2
2025-01-18 23:42:40.149 | DEBUG    | __main__:<module>:313 - Training step 6170: loss = 3.4558 | 3039.99ms | Tokens/s = 172,463.7
2025-01-18 23:43:10.567 | DEBUG    | __main__:<module>:313 - Training step 6180: loss = 3.4156 | 3042.21ms | Tokens/s = 172,337.8
2025-01-18 23:43:40.978 | DEBUG    | __main__:<module>:313 - Training step 6190: loss = 3.4855 | 3044.08ms | Tokens/s = 172,231.8
2025-01-18 23:44:11.423 | DEBUG    | __main__:<module>:313 - Training step 6200: loss = 3.3684 | 3045.18ms | Tokens/s = 172,170.0
2025-01-18 23:44:41.889 | DEBUG    | __main__:<module>:313 - Training step 6210: loss = 3.5031 | 3048.63ms | Tokens/s = 171,975.1
2025-01-18 23:45:12.348 | DEBUG    | __main__:<module>:313 - Training step 6220: loss = 3.4260 | 3044.88ms | Tokens/s = 172,186.9
2025-01-18 23:45:42.775 | DEBUG    | __main__:<module>:313 - Training step 6230: loss = 3.4118 | 3041.26ms | Tokens/s = 172,391.6
2025-01-18 23:46:13.191 | DEBUG    | __main__:<module>:313 - Training step 6240: loss = 3.5584 | 3040.64ms | Tokens/s = 172,427.0
2025-01-18 23:46:43.597 | DEBUG    | __main__:<module>:313 - Training step 6250: loss = 3.4924 | 3038.36ms | Tokens/s = 172,556.5
2025-01-18 23:47:14.005 | DEBUG    | __main__:<module>:313 - Training step 6260: loss = 3.4452 | 3043.30ms | Tokens/s = 172,276.1
2025-01-18 23:47:44.412 | DEBUG    | __main__:<module>:313 - Training step 6270: loss = 3.3588 | 3041.17ms | Tokens/s = 172,396.9
2025-01-18 23:48:14.845 | DEBUG    | __main__:<module>:313 - Training step 6280: loss = 3.5631 | 3044.99ms | Tokens/s = 172,180.7
2025-01-18 23:48:45.298 | DEBUG    | __main__:<module>:313 - Training step 6290: loss = 3.3939 | 3043.55ms | Tokens/s = 172,261.9
2025-01-18 23:49:15.749 | DEBUG    | __main__:<module>:313 - Training step 6300: loss = 3.3243 | 3044.14ms | Tokens/s = 172,228.8
2025-01-18 23:49:46.226 | DEBUG    | __main__:<module>:313 - Training step 6310: loss = 3.4478 | 3047.50ms | Tokens/s = 172,038.7
2025-01-18 23:50:16.672 | DEBUG    | __main__:<module>:313 - Training step 6320: loss = 3.3598 | 3044.72ms | Tokens/s = 172,195.7
2025-01-18 23:50:47.117 | DEBUG    | __main__:<module>:313 - Training step 6330: loss = 3.4258 | 3046.97ms | Tokens/s = 172,068.9
2025-01-18 23:51:17.555 | DEBUG    | __main__:<module>:313 - Training step 6340: loss = 3.4339 | 3041.90ms | Tokens/s = 172,355.2
2025-01-18 23:51:47.978 | DEBUG    | __main__:<module>:313 - Training step 6350: loss = 3.4402 | 3042.69ms | Tokens/s = 172,310.6
2025-01-18 23:52:18.392 | DEBUG    | __main__:<module>:313 - Training step 6360: loss = 3.3510 | 3041.41ms | Tokens/s = 172,383.2
2025-01-18 23:52:48.804 | DEBUG    | __main__:<module>:313 - Training step 6370: loss = 3.5442 | 3042.30ms | Tokens/s = 172,332.6
2025-01-18 23:53:19.218 | DEBUG    | __main__:<module>:313 - Training step 6380: loss = 3.5215 | 3040.97ms | Tokens/s = 172,407.9
2025-01-18 23:53:49.661 | DEBUG    | __main__:<module>:313 - Training step 6390: loss = 3.4258 | 3046.41ms | Tokens/s = 172,100.0
2025-01-18 23:54:20.115 | DEBUG    | __main__:<module>:313 - Training step 6400: loss = 3.3797 | 3043.79ms | Tokens/s = 172,248.3
2025-01-18 23:54:50.552 | DEBUG    | __main__:<module>:313 - Training step 6410: loss = 3.2889 | 3043.95ms | Tokens/s = 172,239.6
2025-01-18 23:55:20.997 | DEBUG    | __main__:<module>:313 - Training step 6420: loss = 3.4314 | 3044.17ms | Tokens/s = 172,226.8
2025-01-18 23:55:51.455 | DEBUG    | __main__:<module>:313 - Training step 6430: loss = 3.2703 | 3043.80ms | Tokens/s = 172,247.7
2025-01-18 23:56:21.889 | DEBUG    | __main__:<module>:313 - Training step 6440: loss = 3.5211 | 3041.93ms | Tokens/s = 172,353.6
2025-01-18 23:56:52.323 | DEBUG    | __main__:<module>:313 - Training step 6450: loss = 3.2786 | 3044.91ms | Tokens/s = 172,185.2
2025-01-18 23:57:22.774 | DEBUG    | __main__:<module>:313 - Training step 6460: loss = 3.3688 | 3045.16ms | Tokens/s = 172,170.8
2025-01-18 23:57:53.215 | DEBUG    | __main__:<module>:313 - Training step 6470: loss = 3.5485 | 3040.84ms | Tokens/s = 172,415.6
2025-01-18 23:58:23.640 | DEBUG    | __main__:<module>:313 - Training step 6480: loss = 3.3934 | 3043.80ms | Tokens/s = 172,247.6
2025-01-18 23:58:54.099 | DEBUG    | __main__:<module>:313 - Training step 6490: loss = 3.2448 | 3047.08ms | Tokens/s = 172,062.6
2025-01-18 23:59:24.556 | DEBUG    | __main__:<module>:313 - Training step 6500: loss = 3.4952 | 3040.48ms | Tokens/s = 172,435.9
2025-01-18 23:59:54.991 | DEBUG    | __main__:<module>:313 - Training step 6510: loss = 3.1615 | 3041.88ms | Tokens/s = 172,356.3
2025-01-19 00:00:25.414 | DEBUG    | __main__:<module>:313 - Training step 6520: loss = 3.2466 | 3042.59ms | Tokens/s = 172,316.2
2025-01-19 00:00:55.826 | DEBUG    | __main__:<module>:313 - Training step 6530: loss = 3.2208 | 3041.46ms | Tokens/s = 172,380.7
2025-01-19 00:01:26.234 | DEBUG    | __main__:<module>:313 - Training step 6540: loss = 3.2976 | 3040.05ms | Tokens/s = 172,460.5
2025-01-19 00:01:56.684 | DEBUG    | __main__:<module>:313 - Training step 6550: loss = 3.3000 | 3047.91ms | Tokens/s = 172,015.8
2025-01-19 00:02:27.155 | DEBUG    | __main__:<module>:313 - Training step 6560: loss = 3.6208 | 3046.47ms | Tokens/s = 172,096.7
2025-01-19 00:02:57.607 | DEBUG    | __main__:<module>:313 - Training step 6570: loss = 3.2781 | 3043.23ms | Tokens/s = 172,279.9
2025-01-19 00:03:28.047 | DEBUG    | __main__:<module>:313 - Training step 6580: loss = 3.2640 | 3044.78ms | Tokens/s = 172,192.5
2025-01-19 00:03:58.467 | DEBUG    | __main__:<module>:313 - Training step 6590: loss = 3.5126 | 3043.65ms | Tokens/s = 172,256.4
2025-01-19 00:04:28.912 | DEBUG    | __main__:<module>:313 - Training step 6600: loss = 3.4584 | 3044.45ms | Tokens/s = 172,211.1
2025-01-19 00:04:59.363 | DEBUG    | __main__:<module>:313 - Training step 6610: loss = 3.4473 | 3043.54ms | Tokens/s = 172,262.8
2025-01-19 00:05:29.795 | DEBUG    | __main__:<module>:313 - Training step 6620: loss = 3.4099 | 3042.00ms | Tokens/s = 172,349.6
2025-01-19 00:06:00.213 | DEBUG    | __main__:<module>:313 - Training step 6630: loss = 3.2642 | 3040.97ms | Tokens/s = 172,408.4
2025-01-19 00:06:30.623 | DEBUG    | __main__:<module>:313 - Training step 6640: loss = 3.3749 | 3040.93ms | Tokens/s = 172,410.3
2025-01-19 00:07:01.032 | DEBUG    | __main__:<module>:313 - Training step 6650: loss = 3.5727 | 3039.17ms | Tokens/s = 172,510.4
2025-01-19 00:07:31.449 | DEBUG    | __main__:<module>:313 - Training step 6660: loss = 3.4531 | 3043.12ms | Tokens/s = 172,286.4
2025-01-19 00:08:01.905 | DEBUG    | __main__:<module>:313 - Training step 6670: loss = 3.4672 | 3046.46ms | Tokens/s = 172,097.6
2025-01-19 00:08:32.357 | DEBUG    | __main__:<module>:313 - Training step 6680: loss = 3.3410 | 3043.56ms | Tokens/s = 172,261.4
2025-01-19 00:09:02.791 | DEBUG    | __main__:<module>:313 - Training step 6690: loss = 3.3242 | 3043.63ms | Tokens/s = 172,257.2
2025-01-19 00:09:33.233 | DEBUG    | __main__:<module>:313 - Training step 6700: loss = 3.5353 | 3043.77ms | Tokens/s = 172,249.4
2025-01-19 00:10:03.678 | DEBUG    | __main__:<module>:313 - Training step 6710: loss = 3.3401 | 3044.06ms | Tokens/s = 172,232.9
2025-01-19 00:10:34.101 | DEBUG    | __main__:<module>:313 - Training step 6720: loss = 3.3370 | 3040.99ms | Tokens/s = 172,406.9
2025-01-19 00:11:04.512 | DEBUG    | __main__:<module>:313 - Training step 6730: loss = 3.5142 | 3042.42ms | Tokens/s = 172,325.7
2025-01-19 00:11:34.918 | DEBUG    | __main__:<module>:313 - Training step 6740: loss = 3.4434 | 3041.54ms | Tokens/s = 172,375.7
2025-01-19 00:12:05.335 | DEBUG    | __main__:<module>:313 - Training step 6750: loss = 3.4706 | 3043.82ms | Tokens/s = 172,246.5
2025-01-19 00:12:35.781 | DEBUG    | __main__:<module>:313 - Training step 6760: loss = 3.4815 | 3044.28ms | Tokens/s = 172,220.6
2025-01-19 00:13:06.230 | DEBUG    | __main__:<module>:313 - Training step 6770: loss = 3.2976 | 3043.95ms | Tokens/s = 172,239.3
2025-01-19 00:13:36.656 | DEBUG    | __main__:<module>:313 - Training step 6780: loss = 3.3733 | 3041.68ms | Tokens/s = 172,367.8
2025-01-19 00:14:07.080 | DEBUG    | __main__:<module>:313 - Training step 6790: loss = 3.4342 | 3042.57ms | Tokens/s = 172,317.3
2025-01-19 00:14:37.490 | DEBUG    | __main__:<module>:313 - Training step 6800: loss = 3.3718 | 3039.63ms | Tokens/s = 172,484.2
2025-01-19 00:15:07.897 | DEBUG    | __main__:<module>:313 - Training step 6810: loss = 3.3884 | 3041.32ms | Tokens/s = 172,388.4
2025-01-19 00:15:38.302 | DEBUG    | __main__:<module>:313 - Training step 6820: loss = 3.3808 | 3038.40ms | Tokens/s = 172,553.8
2025-01-19 00:16:08.720 | DEBUG    | __main__:<module>:313 - Training step 6830: loss = 3.5293 | 3043.33ms | Tokens/s = 172,274.3
2025-01-19 00:16:39.175 | DEBUG    | __main__:<module>:313 - Training step 6840: loss = 3.5344 | 3047.82ms | Tokens/s = 172,020.7
2025-01-19 00:17:09.624 | DEBUG    | __main__:<module>:313 - Training step 6850: loss = 3.3769 | 3046.31ms | Tokens/s = 172,106.1
2025-01-19 00:17:40.062 | DEBUG    | __main__:<module>:313 - Training step 6860: loss = 3.3324 | 3041.96ms | Tokens/s = 172,352.2
2025-01-19 00:18:10.484 | DEBUG    | __main__:<module>:313 - Training step 6870: loss = 3.3318 | 3043.83ms | Tokens/s = 172,246.0
2025-01-19 00:18:40.929 | DEBUG    | __main__:<module>:313 - Training step 6880: loss = 3.3494 | 3044.97ms | Tokens/s = 172,181.6
2025-01-19 00:19:11.366 | DEBUG    | __main__:<module>:313 - Training step 6890: loss = 3.3324 | 3042.07ms | Tokens/s = 172,345.7
2025-01-19 00:19:41.786 | DEBUG    | __main__:<module>:313 - Training step 6900: loss = 3.4509 | 3038.53ms | Tokens/s = 172,546.7
2025-01-19 00:20:12.197 | DEBUG    | __main__:<module>:313 - Training step 6910: loss = 3.3680 | 3038.81ms | Tokens/s = 172,530.8
2025-01-19 00:20:42.602 | DEBUG    | __main__:<module>:313 - Training step 6920: loss = 3.4018 | 3041.16ms | Tokens/s = 172,397.2
2025-01-19 00:21:13.035 | DEBUG    | __main__:<module>:313 - Training step 6930: loss = 3.4071 | 3044.12ms | Tokens/s = 172,229.6
2025-01-19 00:21:43.465 | DEBUG    | __main__:<module>:313 - Training step 6940: loss = 3.4038 | 3042.56ms | Tokens/s = 172,317.9
2025-01-19 00:22:13.882 | DEBUG    | __main__:<module>:313 - Training step 6950: loss = 3.4982 | 3042.64ms | Tokens/s = 172,313.6
2025-01-19 00:22:44.316 | DEBUG    | __main__:<module>:313 - Training step 6960: loss = 3.3666 | 3044.88ms | Tokens/s = 172,186.6
2025-01-19 00:23:14.767 | DEBUG    | __main__:<module>:313 - Training step 6970: loss = 3.3333 | 3044.59ms | Tokens/s = 172,203.2
2025-01-19 00:23:45.204 | DEBUG    | __main__:<module>:313 - Training step 6980: loss = 3.5423 | 3042.91ms | Tokens/s = 172,298.4
2025-01-19 00:24:15.628 | DEBUG    | __main__:<module>:313 - Training step 6990: loss = 3.3430 | 3041.57ms | Tokens/s = 172,374.1
2025-01-19 00:24:49.457 | INFO     | __main__:<module>:265 - Step 7,000/20,000 loss: 3.3935 (T) 3.3855 (V) | lr=8.2e-03
2025-01-19 00:24:49.459 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 00:25:03.045 | DEBUG    | __main__:<module>:313 - Training step 7000: loss = 3.4199 | 20044.51ms | Tokens/s = 26,156.2
2025-01-19 00:25:33.320 | DEBUG    | __main__:<module>:313 - Training step 7010: loss = 3.3057 | 3034.79ms | Tokens/s = 172,759.0
2025-01-19 00:26:03.703 | DEBUG    | __main__:<module>:313 - Training step 7020: loss = 3.6168 | 3040.35ms | Tokens/s = 172,443.2
2025-01-19 00:26:34.124 | DEBUG    | __main__:<module>:313 - Training step 7030: loss = 3.4839 | 3042.92ms | Tokens/s = 172,297.6
2025-01-19 00:27:04.555 | DEBUG    | __main__:<module>:313 - Training step 7040: loss = 3.5512 | 3040.55ms | Tokens/s = 172,432.1
2025-01-19 00:27:34.976 | DEBUG    | __main__:<module>:313 - Training step 7050: loss = 3.4102 | 3042.51ms | Tokens/s = 172,321.1
2025-01-19 00:28:05.384 | DEBUG    | __main__:<module>:313 - Training step 7060: loss = 3.2592 | 3040.48ms | Tokens/s = 172,436.1
2025-01-19 00:28:35.804 | DEBUG    | __main__:<module>:313 - Training step 7070: loss = 3.4794 | 3044.62ms | Tokens/s = 172,201.5
2025-01-19 00:29:06.257 | DEBUG    | __main__:<module>:313 - Training step 7080: loss = 3.5068 | 3045.96ms | Tokens/s = 172,125.6
2025-01-19 00:29:36.706 | DEBUG    | __main__:<module>:313 - Training step 7090: loss = 3.5239 | 3042.75ms | Tokens/s = 172,307.1
2025-01-19 00:30:07.147 | DEBUG    | __main__:<module>:313 - Training step 7100: loss = 3.5193 | 3044.60ms | Tokens/s = 172,202.7
2025-01-19 00:30:37.604 | DEBUG    | __main__:<module>:313 - Training step 7110: loss = 3.4804 | 3042.50ms | Tokens/s = 172,321.7
2025-01-19 00:31:08.042 | DEBUG    | __main__:<module>:313 - Training step 7120: loss = 3.5025 | 3043.35ms | Tokens/s = 172,273.4
2025-01-19 00:31:38.459 | DEBUG    | __main__:<module>:313 - Training step 7130: loss = 3.3919 | 3040.96ms | Tokens/s = 172,408.5
2025-01-19 00:32:08.877 | DEBUG    | __main__:<module>:313 - Training step 7140: loss = 3.4220 | 3041.63ms | Tokens/s = 172,371.0
2025-01-19 00:32:39.286 | DEBUG    | __main__:<module>:313 - Training step 7150: loss = 3.4758 | 3039.12ms | Tokens/s = 172,513.0
2025-01-19 00:33:09.692 | DEBUG    | __main__:<module>:313 - Training step 7160: loss = 3.5653 | 3038.85ms | Tokens/s = 172,528.7
2025-01-19 00:33:40.088 | DEBUG    | __main__:<module>:313 - Training step 7170: loss = 3.3735 | 3037.59ms | Tokens/s = 172,599.8
2025-01-19 00:34:10.492 | DEBUG    | __main__:<module>:313 - Training step 7180: loss = 3.4262 | 3043.46ms | Tokens/s = 172,267.2
2025-01-19 00:34:40.939 | DEBUG    | __main__:<module>:313 - Training step 7190: loss = 3.4742 | 3046.61ms | Tokens/s = 172,089.1
2025-01-19 00:35:11.380 | DEBUG    | __main__:<module>:313 - Training step 7200: loss = 3.5176 | 3041.67ms | Tokens/s = 172,368.2
2025-01-19 00:35:41.805 | DEBUG    | __main__:<module>:313 - Training step 7210: loss = 3.4613 | 3043.07ms | Tokens/s = 172,289.3
2025-01-19 00:36:12.215 | DEBUG    | __main__:<module>:313 - Training step 7220: loss = 3.5702 | 3041.98ms | Tokens/s = 172,350.7
2025-01-19 00:36:42.644 | DEBUG    | __main__:<module>:313 - Training step 7230: loss = 3.2078 | 3041.95ms | Tokens/s = 172,352.4
2025-01-19 00:37:13.062 | DEBUG    | __main__:<module>:313 - Training step 7240: loss = 3.4985 | 3041.73ms | Tokens/s = 172,365.2
2025-01-19 00:37:43.472 | DEBUG    | __main__:<module>:313 - Training step 7250: loss = 3.4617 | 3040.39ms | Tokens/s = 172,440.8
2025-01-19 00:38:13.872 | DEBUG    | __main__:<module>:313 - Training step 7260: loss = 3.3958 | 3040.63ms | Tokens/s = 172,427.2
2025-01-19 00:38:44.301 | DEBUG    | __main__:<module>:313 - Training step 7270: loss = 3.4316 | 3044.95ms | Tokens/s = 172,183.0
2025-01-19 00:39:14.743 | DEBUG    | __main__:<module>:313 - Training step 7280: loss = 3.3093 | 3044.21ms | Tokens/s = 172,224.6
2025-01-19 00:39:45.173 | DEBUG    | __main__:<module>:313 - Training step 7290: loss = 3.2554 | 3044.09ms | Tokens/s = 172,231.4
2025-01-19 00:40:15.618 | DEBUG    | __main__:<module>:313 - Training step 7300: loss = 3.3006 | 3042.97ms | Tokens/s = 172,294.9
2025-01-19 00:40:46.051 | DEBUG    | __main__:<module>:313 - Training step 7310: loss = 3.3120 | 3041.99ms | Tokens/s = 172,350.4
2025-01-19 00:41:16.477 | DEBUG    | __main__:<module>:313 - Training step 7320: loss = 3.3707 | 3043.69ms | Tokens/s = 172,253.8
2025-01-19 00:41:46.938 | DEBUG    | __main__:<module>:313 - Training step 7330: loss = 3.7041 | 3045.65ms | Tokens/s = 172,143.4
2025-01-19 00:42:17.395 | DEBUG    | __main__:<module>:313 - Training step 7340: loss = 3.4061 | 3043.85ms | Tokens/s = 172,245.0
2025-01-19 00:42:47.844 | DEBUG    | __main__:<module>:313 - Training step 7350: loss = 3.5130 | 3046.45ms | Tokens/s = 172,098.1
2025-01-19 00:43:18.291 | DEBUG    | __main__:<module>:313 - Training step 7360: loss = 3.3116 | 3043.37ms | Tokens/s = 172,272.1
2025-01-19 00:43:48.715 | DEBUG    | __main__:<module>:313 - Training step 7370: loss = 3.4400 | 3041.78ms | Tokens/s = 172,362.5
2025-01-19 00:44:19.130 | DEBUG    | __main__:<module>:313 - Training step 7380: loss = 3.4294 | 3039.99ms | Tokens/s = 172,463.6
2025-01-19 00:44:49.533 | DEBUG    | __main__:<module>:313 - Training step 7390: loss = 3.2957 | 3038.99ms | Tokens/s = 172,520.5
2025-01-19 00:45:19.931 | DEBUG    | __main__:<module>:313 - Training step 7400: loss = 3.2612 | 3039.66ms | Tokens/s = 172,482.5
2025-01-19 00:45:50.330 | DEBUG    | __main__:<module>:313 - Training step 7410: loss = 3.3997 | 3038.91ms | Tokens/s = 172,525.0
2025-01-19 00:46:20.729 | DEBUG    | __main__:<module>:313 - Training step 7420: loss = 3.4038 | 3038.26ms | Tokens/s = 172,562.1
2025-01-19 00:46:51.133 | DEBUG    | __main__:<module>:313 - Training step 7430: loss = 3.4966 | 3039.43ms | Tokens/s = 172,495.2
2025-01-19 00:47:21.581 | DEBUG    | __main__:<module>:313 - Training step 7440: loss = 3.5248 | 3047.96ms | Tokens/s = 172,012.9
2025-01-19 00:47:52.016 | DEBUG    | __main__:<module>:313 - Training step 7450: loss = 3.3322 | 3042.28ms | Tokens/s = 172,334.1
2025-01-19 00:48:22.432 | DEBUG    | __main__:<module>:313 - Training step 7460: loss = 3.3945 | 3039.87ms | Tokens/s = 172,470.4
2025-01-19 00:48:52.835 | DEBUG    | __main__:<module>:313 - Training step 7470: loss = 3.3376 | 3039.35ms | Tokens/s = 172,499.8
2025-01-19 00:49:23.237 | DEBUG    | __main__:<module>:313 - Training step 7480: loss = 3.5788 | 3042.18ms | Tokens/s = 172,339.7
2025-01-19 00:49:53.660 | DEBUG    | __main__:<module>:313 - Training step 7490: loss = 3.5990 | 3044.16ms | Tokens/s = 172,227.4
2025-01-19 00:50:24.110 | DEBUG    | __main__:<module>:313 - Training step 7500: loss = 3.5093 | 3045.07ms | Tokens/s = 172,175.8
2025-01-19 00:50:54.555 | DEBUG    | __main__:<module>:313 - Training step 7510: loss = 3.5477 | 3043.31ms | Tokens/s = 172,275.4
2025-01-19 00:51:24.979 | DEBUG    | __main__:<module>:313 - Training step 7520: loss = 3.4910 | 3042.51ms | Tokens/s = 172,320.8
2025-01-19 00:51:55.390 | DEBUG    | __main__:<module>:313 - Training step 7530: loss = 3.4032 | 3040.65ms | Tokens/s = 172,426.5
2025-01-19 00:52:25.793 | DEBUG    | __main__:<module>:313 - Training step 7540: loss = 3.4531 | 3039.20ms | Tokens/s = 172,508.6
2025-01-19 00:52:56.196 | DEBUG    | __main__:<module>:313 - Training step 7550: loss = 3.3243 | 3040.88ms | Tokens/s = 172,413.4
2025-01-19 00:53:26.594 | DEBUG    | __main__:<module>:313 - Training step 7560: loss = 3.3794 | 3038.83ms | Tokens/s = 172,529.5
2025-01-19 00:53:56.987 | DEBUG    | __main__:<module>:313 - Training step 7570: loss = 3.4143 | 3038.21ms | Tokens/s = 172,564.5
2025-01-19 00:54:27.376 | DEBUG    | __main__:<module>:313 - Training step 7580: loss = 3.2281 | 3039.83ms | Tokens/s = 172,472.9
2025-01-19 00:54:57.791 | DEBUG    | __main__:<module>:313 - Training step 7590: loss = 3.4898 | 3041.76ms | Tokens/s = 172,363.1
2025-01-19 00:55:28.231 | DEBUG    | __main__:<module>:313 - Training step 7600: loss = 3.2848 | 3042.62ms | Tokens/s = 172,314.5
2025-01-19 00:55:58.651 | DEBUG    | __main__:<module>:313 - Training step 7610: loss = 3.3942 | 3042.08ms | Tokens/s = 172,345.0
2025-01-19 00:56:29.060 | DEBUG    | __main__:<module>:313 - Training step 7620: loss = 3.3810 | 3041.42ms | Tokens/s = 172,382.4
2025-01-19 00:56:59.488 | DEBUG    | __main__:<module>:313 - Training step 7630: loss = 3.5757 | 3045.90ms | Tokens/s = 172,128.8
2025-01-19 00:57:29.942 | DEBUG    | __main__:<module>:313 - Training step 7640: loss = 3.3738 | 3045.63ms | Tokens/s = 172,144.5
2025-01-19 00:58:00.392 | DEBUG    | __main__:<module>:313 - Training step 7650: loss = 3.4517 | 3043.36ms | Tokens/s = 172,272.9
2025-01-19 00:58:30.815 | DEBUG    | __main__:<module>:313 - Training step 7660: loss = 3.4213 | 3041.78ms | Tokens/s = 172,362.2
2025-01-19 00:59:01.225 | DEBUG    | __main__:<module>:313 - Training step 7670: loss = 3.3258 | 3039.44ms | Tokens/s = 172,495.0
2025-01-19 00:59:31.635 | DEBUG    | __main__:<module>:313 - Training step 7680: loss = 3.1883 | 3042.29ms | Tokens/s = 172,333.3
2025-01-19 01:00:02.049 | DEBUG    | __main__:<module>:313 - Training step 7690: loss = 3.4609 | 3043.28ms | Tokens/s = 172,277.5
2025-01-19 01:00:32.494 | DEBUG    | __main__:<module>:313 - Training step 7700: loss = 3.1952 | 3044.27ms | Tokens/s = 172,221.3
2025-01-19 01:01:02.926 | DEBUG    | __main__:<module>:313 - Training step 7710: loss = 3.4431 | 3039.93ms | Tokens/s = 172,467.4
2025-01-19 01:01:33.349 | DEBUG    | __main__:<module>:313 - Training step 7720: loss = 3.3957 | 3042.26ms | Tokens/s = 172,335.0
2025-01-19 01:02:03.806 | DEBUG    | __main__:<module>:313 - Training step 7730: loss = 3.5069 | 3047.08ms | Tokens/s = 172,062.6
2025-01-19 01:02:34.278 | DEBUG    | __main__:<module>:313 - Training step 7740: loss = 3.4277 | 3046.43ms | Tokens/s = 172,098.9
2025-01-19 01:03:04.723 | DEBUG    | __main__:<module>:313 - Training step 7750: loss = 3.3422 | 3040.94ms | Tokens/s = 172,409.7
2025-01-19 01:03:35.154 | DEBUG    | __main__:<module>:313 - Training step 7760: loss = 3.3272 | 3041.56ms | Tokens/s = 172,374.9
2025-01-19 01:04:05.569 | DEBUG    | __main__:<module>:313 - Training step 7770: loss = 3.3977 | 3041.07ms | Tokens/s = 172,402.7
2025-01-19 01:04:35.970 | DEBUG    | __main__:<module>:313 - Training step 7780: loss = 3.4847 | 3039.55ms | Tokens/s = 172,488.7
2025-01-19 01:05:06.362 | DEBUG    | __main__:<module>:313 - Training step 7790: loss = 3.3179 | 3041.59ms | Tokens/s = 172,373.2
2025-01-19 01:05:36.758 | DEBUG    | __main__:<module>:313 - Training step 7800: loss = 3.3763 | 3041.15ms | Tokens/s = 172,397.7
2025-01-19 01:06:07.194 | DEBUG    | __main__:<module>:313 - Training step 7810: loss = 3.2408 | 3044.92ms | Tokens/s = 172,184.7
2025-01-19 01:06:37.653 | DEBUG    | __main__:<module>:313 - Training step 7820: loss = 3.2339 | 3046.53ms | Tokens/s = 172,093.4
2025-01-19 01:07:08.096 | DEBUG    | __main__:<module>:313 - Training step 7830: loss = 3.5018 | 3043.78ms | Tokens/s = 172,249.1
2025-01-19 01:07:38.521 | DEBUG    | __main__:<module>:313 - Training step 7840: loss = 3.3215 | 3040.51ms | Tokens/s = 172,434.3
2025-01-19 01:08:08.927 | DEBUG    | __main__:<module>:313 - Training step 7850: loss = 3.3080 | 3038.40ms | Tokens/s = 172,554.2
2025-01-19 01:08:39.325 | DEBUG    | __main__:<module>:313 - Training step 7860: loss = 3.3980 | 3039.18ms | Tokens/s = 172,509.9
2025-01-19 01:09:09.733 | DEBUG    | __main__:<module>:313 - Training step 7870: loss = 3.4363 | 3043.99ms | Tokens/s = 172,237.1
2025-01-19 01:09:40.176 | DEBUG    | __main__:<module>:313 - Training step 7880: loss = 3.5126 | 3044.59ms | Tokens/s = 172,203.2
2025-01-19 01:10:10.632 | DEBUG    | __main__:<module>:313 - Training step 7890: loss = 3.2947 | 3043.03ms | Tokens/s = 172,291.5
2025-01-19 01:10:41.067 | DEBUG    | __main__:<module>:313 - Training step 7900: loss = 3.3861 | 3043.30ms | Tokens/s = 172,276.4
2025-01-19 01:11:11.485 | DEBUG    | __main__:<module>:313 - Training step 7910: loss = 3.4359 | 3043.73ms | Tokens/s = 172,251.6
2025-01-19 01:11:41.926 | DEBUG    | __main__:<module>:313 - Training step 7920: loss = 3.3759 | 3045.45ms | Tokens/s = 172,154.6
2025-01-19 01:12:12.381 | DEBUG    | __main__:<module>:313 - Training step 7930: loss = 3.3246 | 3043.70ms | Tokens/s = 172,253.6
2025-01-19 01:12:42.811 | DEBUG    | __main__:<module>:313 - Training step 7940: loss = 3.4833 | 3043.16ms | Tokens/s = 172,283.8
2025-01-19 01:13:13.224 | DEBUG    | __main__:<module>:313 - Training step 7950: loss = 3.3492 | 3039.91ms | Tokens/s = 172,468.4
2025-01-19 01:13:43.627 | DEBUG    | __main__:<module>:313 - Training step 7960: loss = 3.4659 | 3039.81ms | Tokens/s = 172,473.9
2025-01-19 01:14:14.024 | DEBUG    | __main__:<module>:313 - Training step 7970: loss = 3.2245 | 3039.79ms | Tokens/s = 172,475.3
2025-01-19 01:14:44.457 | DEBUG    | __main__:<module>:313 - Training step 7980: loss = 3.3801 | 3043.53ms | Tokens/s = 172,263.0
2025-01-19 01:15:14.918 | DEBUG    | __main__:<module>:313 - Training step 7990: loss = 3.5067 | 3047.58ms | Tokens/s = 172,034.2
2025-01-19 01:15:48.795 | INFO     | __main__:<module>:265 - Step 8,000/20,000 loss: 3.3572 (T) 3.3686 (V) | lr=7.5e-03
2025-01-19 01:15:48.797 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 01:16:04.853 | DEBUG    | __main__:<module>:313 - Training step 8000: loss = 3.3518 | 22519.11ms | Tokens/s = 23,281.9
2025-01-19 01:16:35.114 | DEBUG    | __main__:<module>:313 - Training step 8010: loss = 3.3037 | 3033.30ms | Tokens/s = 172,844.3
2025-01-19 01:17:05.496 | DEBUG    | __main__:<module>:313 - Training step 8020: loss = 3.2669 | 3039.82ms | Tokens/s = 172,473.6
2025-01-19 01:17:35.923 | DEBUG    | __main__:<module>:313 - Training step 8030: loss = 3.3231 | 3042.86ms | Tokens/s = 172,301.1
2025-01-19 01:18:06.374 | DEBUG    | __main__:<module>:313 - Training step 8040: loss = 3.3769 | 3044.60ms | Tokens/s = 172,202.8
2025-01-19 01:18:36.800 | DEBUG    | __main__:<module>:313 - Training step 8050: loss = 3.5571 | 3041.78ms | Tokens/s = 172,362.2
2025-01-19 01:19:07.241 | DEBUG    | __main__:<module>:313 - Training step 8060: loss = 3.3847 | 3044.82ms | Tokens/s = 172,190.2
2025-01-19 01:19:37.703 | DEBUG    | __main__:<module>:313 - Training step 8070: loss = 3.5943 | 3044.69ms | Tokens/s = 172,197.7
2025-01-19 01:20:08.142 | DEBUG    | __main__:<module>:313 - Training step 8080: loss = 3.3932 | 3043.21ms | Tokens/s = 172,281.1
2025-01-19 01:20:38.563 | DEBUG    | __main__:<module>:313 - Training step 8090: loss = 3.4201 | 3040.54ms | Tokens/s = 172,432.7
2025-01-19 01:21:08.968 | DEBUG    | __main__:<module>:313 - Training step 8100: loss = 3.0993 | 3040.46ms | Tokens/s = 172,437.2
2025-01-19 01:21:39.375 | DEBUG    | __main__:<module>:313 - Training step 8110: loss = 3.2672 | 3040.35ms | Tokens/s = 172,443.5
2025-01-19 01:22:09.807 | DEBUG    | __main__:<module>:313 - Training step 8120: loss = 3.2727 | 3045.07ms | Tokens/s = 172,175.9
2025-01-19 01:22:40.273 | DEBUG    | __main__:<module>:313 - Training step 8130: loss = 3.4190 | 3048.44ms | Tokens/s = 171,985.4
2025-01-19 01:23:10.727 | DEBUG    | __main__:<module>:313 - Training step 8140: loss = 3.2564 | 3043.59ms | Tokens/s = 172,259.8
2025-01-19 01:23:41.154 | DEBUG    | __main__:<module>:313 - Training step 8150: loss = 3.3805 | 3040.18ms | Tokens/s = 172,453.0
2025-01-19 01:24:11.573 | DEBUG    | __main__:<module>:313 - Training step 8160: loss = 3.3405 | 3040.16ms | Tokens/s = 172,454.3
2025-01-19 01:24:42.005 | DEBUG    | __main__:<module>:313 - Training step 8170: loss = 3.3284 | 3046.06ms | Tokens/s = 172,120.0
2025-01-19 01:25:12.465 | DEBUG    | __main__:<module>:313 - Training step 8180: loss = 3.4742 | 3046.31ms | Tokens/s = 172,105.8
2025-01-19 01:25:42.920 | DEBUG    | __main__:<module>:313 - Training step 8190: loss = 3.1814 | 3044.51ms | Tokens/s = 172,207.7
2025-01-19 01:26:13.352 | DEBUG    | __main__:<module>:313 - Training step 8200: loss = 3.3283 | 3041.27ms | Tokens/s = 172,391.0
2025-01-19 01:26:43.768 | DEBUG    | __main__:<module>:313 - Training step 8210: loss = 3.2771 | 3041.01ms | Tokens/s = 172,405.7
2025-01-19 01:27:14.192 | DEBUG    | __main__:<module>:313 - Training step 8220: loss = 3.3346 | 3043.26ms | Tokens/s = 172,278.2
2025-01-19 01:27:44.623 | DEBUG    | __main__:<module>:313 - Training step 8230: loss = 3.3463 | 3042.44ms | Tokens/s = 172,324.8
2025-01-19 01:28:15.040 | DEBUG    | __main__:<module>:313 - Training step 8240: loss = 3.3334 | 3042.70ms | Tokens/s = 172,309.9
2025-01-19 01:28:45.487 | DEBUG    | __main__:<module>:313 - Training step 8250: loss = 3.4421 | 3046.17ms | Tokens/s = 172,114.0
2025-01-19 01:29:15.944 | DEBUG    | __main__:<module>:313 - Training step 8260: loss = 3.1674 | 3043.88ms | Tokens/s = 172,243.1
2025-01-19 01:29:46.378 | DEBUG    | __main__:<module>:313 - Training step 8270: loss = 3.3852 | 3041.09ms | Tokens/s = 172,401.2
2025-01-19 01:30:16.799 | DEBUG    | __main__:<module>:313 - Training step 8280: loss = 3.4202 | 3042.82ms | Tokens/s = 172,303.5
2025-01-19 01:30:47.211 | DEBUG    | __main__:<module>:313 - Training step 8290: loss = 3.3491 | 3043.33ms | Tokens/s = 172,274.6
2025-01-19 01:31:17.660 | DEBUG    | __main__:<module>:313 - Training step 8300: loss = 3.4297 | 3047.36ms | Tokens/s = 172,046.8
2025-01-19 01:31:48.116 | DEBUG    | __main__:<module>:313 - Training step 8310: loss = 3.4314 | 3044.08ms | Tokens/s = 172,231.8
2025-01-19 01:32:18.576 | DEBUG    | __main__:<module>:313 - Training step 8320: loss = 3.4026 | 3046.96ms | Tokens/s = 172,069.1
2025-01-19 01:32:49.012 | DEBUG    | __main__:<module>:313 - Training step 8330: loss = 3.3729 | 3044.05ms | Tokens/s = 172,233.5
2025-01-19 01:33:19.439 | DEBUG    | __main__:<module>:313 - Training step 8340: loss = 3.1968 | 3041.59ms | Tokens/s = 172,372.8
2025-01-19 01:33:49.866 | DEBUG    | __main__:<module>:313 - Training step 8350: loss = 3.4212 | 3045.04ms | Tokens/s = 172,177.7
2025-01-19 01:34:20.303 | DEBUG    | __main__:<module>:313 - Training step 8360: loss = 3.3971 | 3040.24ms | Tokens/s = 172,449.6
2025-01-19 01:34:50.716 | DEBUG    | __main__:<module>:313 - Training step 8370: loss = 3.4647 | 3039.54ms | Tokens/s = 172,489.2
2025-01-19 01:35:21.124 | DEBUG    | __main__:<module>:313 - Training step 8380: loss = 3.1985 | 3041.24ms | Tokens/s = 172,393.1
2025-01-19 01:35:51.529 | DEBUG    | __main__:<module>:313 - Training step 8390: loss = 3.4812 | 3042.15ms | Tokens/s = 172,341.4
2025-01-19 01:36:21.961 | DEBUG    | __main__:<module>:313 - Training step 8400: loss = 3.2202 | 3042.21ms | Tokens/s = 172,338.1
2025-01-19 01:36:52.416 | DEBUG    | __main__:<module>:313 - Training step 8410: loss = 3.4299 | 3045.66ms | Tokens/s = 172,142.7
2025-01-19 01:37:22.862 | DEBUG    | __main__:<module>:313 - Training step 8420: loss = 3.0424 | 3042.28ms | Tokens/s = 172,333.7
2025-01-19 01:37:53.286 | DEBUG    | __main__:<module>:313 - Training step 8430: loss = 3.4795 | 3042.95ms | Tokens/s = 172,296.1
2025-01-19 01:38:23.694 | DEBUG    | __main__:<module>:313 - Training step 8440: loss = 3.0498 | 3041.77ms | Tokens/s = 172,362.8
2025-01-19 01:38:54.103 | DEBUG    | __main__:<module>:313 - Training step 8450: loss = 3.4498 | 3041.76ms | Tokens/s = 172,363.4
2025-01-19 01:39:24.546 | DEBUG    | __main__:<module>:313 - Training step 8460: loss = 3.2853 | 3046.04ms | Tokens/s = 172,121.1
2025-01-19 01:39:55.008 | DEBUG    | __main__:<module>:313 - Training step 8470: loss = 3.4351 | 3045.82ms | Tokens/s = 172,133.4
2025-01-19 01:40:25.450 | DEBUG    | __main__:<module>:313 - Training step 8480: loss = 3.2755 | 3044.26ms | Tokens/s = 172,221.5
2025-01-19 01:40:55.875 | DEBUG    | __main__:<module>:313 - Training step 8490: loss = 3.1313 | 3041.03ms | Tokens/s = 172,404.9
2025-01-19 01:41:26.303 | DEBUG    | __main__:<module>:313 - Training step 8500: loss = 3.2397 | 3043.63ms | Tokens/s = 172,257.3
2025-01-19 01:41:56.768 | DEBUG    | __main__:<module>:313 - Training step 8510: loss = 3.2068 | 3049.68ms | Tokens/s = 171,915.5
2025-01-19 01:42:27.224 | DEBUG    | __main__:<module>:313 - Training step 8520: loss = 3.3489 | 3044.52ms | Tokens/s = 172,206.9
2025-01-19 01:42:57.659 | DEBUG    | __main__:<module>:313 - Training step 8530: loss = 3.3037 | 3042.12ms | Tokens/s = 172,343.1
2025-01-19 01:43:28.080 | DEBUG    | __main__:<module>:313 - Training step 8540: loss = 3.2932 | 3041.97ms | Tokens/s = 172,351.2
2025-01-19 01:43:58.521 | DEBUG    | __main__:<module>:313 - Training step 8550: loss = 3.4884 | 3045.36ms | Tokens/s = 172,159.9
2025-01-19 01:44:28.973 | DEBUG    | __main__:<module>:313 - Training step 8560: loss = 3.1206 | 3044.82ms | Tokens/s = 172,190.1
2025-01-19 01:44:59.415 | DEBUG    | __main__:<module>:313 - Training step 8570: loss = 3.2841 | 3043.81ms | Tokens/s = 172,247.0
2025-01-19 01:45:29.845 | DEBUG    | __main__:<module>:313 - Training step 8580: loss = 3.2350 | 3043.05ms | Tokens/s = 172,290.2
2025-01-19 01:46:00.296 | DEBUG    | __main__:<module>:313 - Training step 8590: loss = 3.3212 | 3044.07ms | Tokens/s = 172,232.8
2025-01-19 01:46:30.730 | DEBUG    | __main__:<module>:313 - Training step 8600: loss = 3.3853 | 3040.25ms | Tokens/s = 172,448.9
2025-01-19 01:47:01.148 | DEBUG    | __main__:<module>:313 - Training step 8610: loss = 3.2854 | 3041.10ms | Tokens/s = 172,400.8
2025-01-19 01:47:31.573 | DEBUG    | __main__:<module>:313 - Training step 8620: loss = 3.4115 | 3043.55ms | Tokens/s = 172,261.8
2025-01-19 01:48:02.005 | DEBUG    | __main__:<module>:313 - Training step 8630: loss = 3.4021 | 3044.22ms | Tokens/s = 172,224.2
2025-01-19 01:48:32.423 | DEBUG    | __main__:<module>:313 - Training step 8640: loss = 3.3016 | 3042.70ms | Tokens/s = 172,310.3
2025-01-19 01:49:02.837 | DEBUG    | __main__:<module>:313 - Training step 8650: loss = 3.3270 | 3042.22ms | Tokens/s = 172,337.1
2025-01-19 01:49:33.240 | DEBUG    | __main__:<module>:313 - Training step 8660: loss = 3.2166 | 3040.55ms | Tokens/s = 172,431.9
2025-01-19 01:50:03.669 | DEBUG    | __main__:<module>:313 - Training step 8670: loss = 3.2634 | 3042.91ms | Tokens/s = 172,298.1
2025-01-19 01:50:34.110 | DEBUG    | __main__:<module>:313 - Training step 8680: loss = 3.2706 | 3041.40ms | Tokens/s = 172,383.5
2025-01-19 01:51:04.535 | DEBUG    | __main__:<module>:313 - Training step 8690: loss = 3.1803 | 3043.36ms | Tokens/s = 172,272.8
2025-01-19 01:51:34.977 | DEBUG    | __main__:<module>:313 - Training step 8700: loss = 3.4714 | 3040.33ms | Tokens/s = 172,444.5
2025-01-19 01:52:05.406 | DEBUG    | __main__:<module>:313 - Training step 8710: loss = 3.3351 | 3043.79ms | Tokens/s = 172,248.5
2025-01-19 01:52:35.844 | DEBUG    | __main__:<module>:313 - Training step 8720: loss = 3.4547 | 3045.89ms | Tokens/s = 172,129.4
2025-01-19 01:53:06.289 | DEBUG    | __main__:<module>:313 - Training step 8730: loss = 3.2875 | 3042.29ms | Tokens/s = 172,333.5
2025-01-19 01:53:36.739 | DEBUG    | __main__:<module>:313 - Training step 8740: loss = 3.0811 | 3045.76ms | Tokens/s = 172,137.2
2025-01-19 01:54:07.176 | DEBUG    | __main__:<module>:313 - Training step 8750: loss = 3.3222 | 3041.78ms | Tokens/s = 172,362.4
2025-01-19 01:54:37.600 | DEBUG    | __main__:<module>:313 - Training step 8760: loss = 3.1882 | 3043.57ms | Tokens/s = 172,261.1
2025-01-19 01:55:08.043 | DEBUG    | __main__:<module>:313 - Training step 8770: loss = 3.4271 | 3043.52ms | Tokens/s = 172,263.5
2025-01-19 01:55:38.468 | DEBUG    | __main__:<module>:313 - Training step 8780: loss = 3.3778 | 3040.38ms | Tokens/s = 172,441.5
2025-01-19 01:56:08.887 | DEBUG    | __main__:<module>:313 - Training step 8790: loss = 3.2210 | 3043.47ms | Tokens/s = 172,266.8
2025-01-19 01:56:39.330 | DEBUG    | __main__:<module>:313 - Training step 8800: loss = 3.3520 | 3044.25ms | Tokens/s = 172,222.5
2025-01-19 01:57:09.797 | DEBUG    | __main__:<module>:313 - Training step 8810: loss = 3.3179 | 3045.60ms | Tokens/s = 172,146.2
2025-01-19 01:57:40.236 | DEBUG    | __main__:<module>:313 - Training step 8820: loss = 3.3091 | 3044.18ms | Tokens/s = 172,226.3
2025-01-19 01:58:10.658 | DEBUG    | __main__:<module>:313 - Training step 8830: loss = 3.2908 | 3043.14ms | Tokens/s = 172,285.4
2025-01-19 01:58:41.112 | DEBUG    | __main__:<module>:313 - Training step 8840: loss = 3.5260 | 3045.05ms | Tokens/s = 172,177.0
2025-01-19 01:59:11.561 | DEBUG    | __main__:<module>:313 - Training step 8850: loss = 3.2935 | 3044.48ms | Tokens/s = 172,209.2
2025-01-19 01:59:41.990 | DEBUG    | __main__:<module>:313 - Training step 8860: loss = 3.4233 | 3045.02ms | Tokens/s = 172,178.6
2025-01-19 02:00:12.405 | DEBUG    | __main__:<module>:313 - Training step 8870: loss = 3.2197 | 3040.72ms | Tokens/s = 172,422.3
2025-01-19 02:00:42.810 | DEBUG    | __main__:<module>:313 - Training step 8880: loss = 3.3410 | 3039.95ms | Tokens/s = 172,466.0
2025-01-19 02:01:13.201 | DEBUG    | __main__:<module>:313 - Training step 8890: loss = 3.4839 | 3038.53ms | Tokens/s = 172,546.8
2025-01-19 02:01:43.625 | DEBUG    | __main__:<module>:313 - Training step 8900: loss = 3.2347 | 3042.96ms | Tokens/s = 172,295.4
2025-01-19 02:02:14.076 | DEBUG    | __main__:<module>:313 - Training step 8910: loss = 3.2732 | 3046.43ms | Tokens/s = 172,098.9
2025-01-19 02:02:44.536 | DEBUG    | __main__:<module>:313 - Training step 8920: loss = 3.4363 | 3043.87ms | Tokens/s = 172,243.9
2025-01-19 02:03:14.972 | DEBUG    | __main__:<module>:313 - Training step 8930: loss = 3.3340 | 3042.98ms | Tokens/s = 172,294.4
2025-01-19 02:03:45.395 | DEBUG    | __main__:<module>:313 - Training step 8940: loss = 3.3112 | 3041.47ms | Tokens/s = 172,379.7
2025-01-19 02:04:15.813 | DEBUG    | __main__:<module>:313 - Training step 8950: loss = 3.4150 | 3041.84ms | Tokens/s = 172,358.9
2025-01-19 02:04:46.245 | DEBUG    | __main__:<module>:313 - Training step 8960: loss = 3.3773 | 3041.69ms | Tokens/s = 172,367.5
2025-01-19 02:05:16.671 | DEBUG    | __main__:<module>:313 - Training step 8970: loss = 3.3303 | 3040.28ms | Tokens/s = 172,447.1
2025-01-19 02:05:47.083 | DEBUG    | __main__:<module>:313 - Training step 8980: loss = 3.4861 | 3041.70ms | Tokens/s = 172,366.8
2025-01-19 02:06:17.514 | DEBUG    | __main__:<module>:313 - Training step 8990: loss = 3.2092 | 3043.56ms | Tokens/s = 172,261.2
2025-01-19 02:06:51.347 | INFO     | __main__:<module>:265 - Step 9,000/20,000 loss: 3.3348 (T) 3.3645 (V) | lr=6.7e-03
2025-01-19 02:06:51.349 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 02:07:05.038 | DEBUG    | __main__:<module>:313 - Training step 9000: loss = 3.2057 | 20141.94ms | Tokens/s = 26,029.7
2025-01-19 02:07:35.297 | DEBUG    | __main__:<module>:313 - Training step 9010: loss = 3.5697 | 3033.59ms | Tokens/s = 172,827.8
2025-01-19 02:08:05.680 | DEBUG    | __main__:<module>:313 - Training step 9020: loss = 3.1334 | 3042.57ms | Tokens/s = 172,317.6
2025-01-19 02:08:36.100 | DEBUG    | __main__:<module>:313 - Training step 9030: loss = 3.4134 | 3045.64ms | Tokens/s = 172,143.7
2025-01-19 02:09:06.541 | DEBUG    | __main__:<module>:313 - Training step 9040: loss = 3.0735 | 3044.96ms | Tokens/s = 172,182.0
2025-01-19 02:09:36.968 | DEBUG    | __main__:<module>:313 - Training step 9050: loss = 3.2907 | 3040.84ms | Tokens/s = 172,415.7
2025-01-19 02:10:07.382 | DEBUG    | __main__:<module>:313 - Training step 9060: loss = 3.3863 | 3040.18ms | Tokens/s = 172,452.9
2025-01-19 02:10:37.784 | DEBUG    | __main__:<module>:313 - Training step 9070: loss = 3.2319 | 3039.91ms | Tokens/s = 172,468.2
2025-01-19 02:11:08.205 | DEBUG    | __main__:<module>:313 - Training step 9080: loss = 3.2195 | 3046.10ms | Tokens/s = 172,117.9
2025-01-19 02:11:38.651 | DEBUG    | __main__:<module>:313 - Training step 9090: loss = 3.2347 | 3043.36ms | Tokens/s = 172,272.9
2025-01-19 02:12:09.100 | DEBUG    | __main__:<module>:313 - Training step 9100: loss = 3.5007 | 3044.58ms | Tokens/s = 172,203.8
2025-01-19 02:12:39.524 | DEBUG    | __main__:<module>:313 - Training step 9110: loss = 3.4565 | 3042.12ms | Tokens/s = 172,342.7
2025-01-19 02:13:09.931 | DEBUG    | __main__:<module>:313 - Training step 9120: loss = 3.5577 | 3040.13ms | Tokens/s = 172,455.9
2025-01-19 02:13:40.338 | DEBUG    | __main__:<module>:313 - Training step 9130: loss = 3.2762 | 3039.86ms | Tokens/s = 172,471.2
2025-01-19 02:14:10.733 | DEBUG    | __main__:<module>:313 - Training step 9140: loss = 3.1788 | 3039.22ms | Tokens/s = 172,507.6
2025-01-19 02:14:41.159 | DEBUG    | __main__:<module>:313 - Training step 9150: loss = 3.3424 | 3044.57ms | Tokens/s = 172,204.3
2025-01-19 02:15:11.594 | DEBUG    | __main__:<module>:313 - Training step 9160: loss = 3.3734 | 3041.27ms | Tokens/s = 172,391.0
2025-01-19 02:15:42.009 | DEBUG    | __main__:<module>:313 - Training step 9170: loss = 3.3020 | 3040.76ms | Tokens/s = 172,420.0
2025-01-19 02:16:12.427 | DEBUG    | __main__:<module>:313 - Training step 9180: loss = 3.2732 | 3042.88ms | Tokens/s = 172,300.2
2025-01-19 02:16:42.869 | DEBUG    | __main__:<module>:313 - Training step 9190: loss = 3.4323 | 3044.62ms | Tokens/s = 172,201.2
2025-01-19 02:17:13.306 | DEBUG    | __main__:<module>:313 - Training step 9200: loss = 3.3111 | 3038.56ms | Tokens/s = 172,544.6
2025-01-19 02:17:43.723 | DEBUG    | __main__:<module>:313 - Training step 9210: loss = 3.3140 | 3041.40ms | Tokens/s = 172,383.9
2025-01-19 02:18:14.125 | DEBUG    | __main__:<module>:313 - Training step 9220: loss = 3.4454 | 3040.14ms | Tokens/s = 172,455.4
2025-01-19 02:18:44.534 | DEBUG    | __main__:<module>:313 - Training step 9230: loss = 3.4533 | 3043.93ms | Tokens/s = 172,240.8
2025-01-19 02:19:14.971 | DEBUG    | __main__:<module>:313 - Training step 9240: loss = 3.2324 | 3043.25ms | Tokens/s = 172,278.8
2025-01-19 02:19:45.405 | DEBUG    | __main__:<module>:313 - Training step 9250: loss = 3.1427 | 3041.04ms | Tokens/s = 172,404.1
2025-01-19 02:20:15.831 | DEBUG    | __main__:<module>:313 - Training step 9260: loss = 3.3719 | 3043.81ms | Tokens/s = 172,247.4
2025-01-19 02:20:46.278 | DEBUG    | __main__:<module>:313 - Training step 9270: loss = 3.2442 | 3046.35ms | Tokens/s = 172,103.8
2025-01-19 02:21:16.710 | DEBUG    | __main__:<module>:313 - Training step 9280: loss = 3.1774 | 3041.55ms | Tokens/s = 172,375.2
2025-01-19 02:21:47.124 | DEBUG    | __main__:<module>:313 - Training step 9290: loss = 3.4173 | 3039.92ms | Tokens/s = 172,467.6
2025-01-19 02:22:17.534 | DEBUG    | __main__:<module>:313 - Training step 9300: loss = 3.4244 | 3040.56ms | Tokens/s = 172,431.6
2025-01-19 02:22:47.955 | DEBUG    | __main__:<module>:313 - Training step 9310: loss = 3.2651 | 3044.07ms | Tokens/s = 172,232.4
2025-01-19 02:23:18.406 | DEBUG    | __main__:<module>:313 - Training step 9320: loss = 3.3424 | 3043.08ms | Tokens/s = 172,288.9
2025-01-19 02:23:48.834 | DEBUG    | __main__:<module>:313 - Training step 9330: loss = 3.2012 | 3040.95ms | Tokens/s = 172,409.3
2025-01-19 02:24:19.248 | DEBUG    | __main__:<module>:313 - Training step 9340: loss = 3.1105 | 3040.92ms | Tokens/s = 172,410.9
2025-01-19 02:24:49.656 | DEBUG    | __main__:<module>:313 - Training step 9350: loss = 3.1663 | 3040.33ms | Tokens/s = 172,444.3
2025-01-19 02:25:20.078 | DEBUG    | __main__:<module>:313 - Training step 9360: loss = 3.2961 | 3044.65ms | Tokens/s = 172,199.8
2025-01-19 02:25:50.536 | DEBUG    | __main__:<module>:313 - Training step 9370: loss = 3.1278 | 3046.09ms | Tokens/s = 172,118.4
2025-01-19 02:26:21.007 | DEBUG    | __main__:<module>:313 - Training step 9380: loss = 3.3665 | 3045.54ms | Tokens/s = 172,149.7
2025-01-19 02:26:51.458 | DEBUG    | __main__:<module>:313 - Training step 9390: loss = 3.2829 | 3046.14ms | Tokens/s = 172,115.7
2025-01-19 02:27:21.891 | DEBUG    | __main__:<module>:313 - Training step 9400: loss = 3.3886 | 3043.02ms | Tokens/s = 172,292.3
2025-01-19 02:27:52.312 | DEBUG    | __main__:<module>:313 - Training step 9410: loss = 3.2331 | 3041.14ms | Tokens/s = 172,398.8
2025-01-19 02:28:22.719 | DEBUG    | __main__:<module>:313 - Training step 9420: loss = 3.6952 | 3040.68ms | Tokens/s = 172,424.5
2025-01-19 02:28:53.146 | DEBUG    | __main__:<module>:313 - Training step 9430: loss = 3.3156 | 3043.34ms | Tokens/s = 172,273.6
2025-01-19 02:29:23.592 | DEBUG    | __main__:<module>:313 - Training step 9440: loss = 3.3339 | 3043.53ms | Tokens/s = 172,262.9
2025-01-19 02:29:54.011 | DEBUG    | __main__:<module>:313 - Training step 9450: loss = 3.3977 | 3042.51ms | Tokens/s = 172,320.8
2025-01-19 02:30:24.419 | DEBUG    | __main__:<module>:313 - Training step 9460: loss = 3.4024 | 3040.71ms | Tokens/s = 172,422.6
2025-01-19 02:30:54.851 | DEBUG    | __main__:<module>:313 - Training step 9470: loss = 3.1435 | 3042.81ms | Tokens/s = 172,304.1
2025-01-19 02:31:25.266 | DEBUG    | __main__:<module>:313 - Training step 9480: loss = 3.2228 | 3040.72ms | Tokens/s = 172,422.5
2025-01-19 02:31:55.677 | DEBUG    | __main__:<module>:313 - Training step 9490: loss = 3.3870 | 3042.58ms | Tokens/s = 172,317.0
2025-01-19 02:32:26.119 | DEBUG    | __main__:<module>:313 - Training step 9500: loss = 3.3430 | 3044.32ms | Tokens/s = 172,218.4
2025-01-19 02:32:56.571 | DEBUG    | __main__:<module>:313 - Training step 9510: loss = 3.2552 | 3044.13ms | Tokens/s = 172,229.2
2025-01-19 02:33:27.009 | DEBUG    | __main__:<module>:313 - Training step 9520: loss = 3.3127 | 3044.55ms | Tokens/s = 172,205.2
2025-01-19 02:33:57.424 | DEBUG    | __main__:<module>:313 - Training step 9530: loss = 3.2314 | 3041.86ms | Tokens/s = 172,357.6
2025-01-19 02:34:27.831 | DEBUG    | __main__:<module>:313 - Training step 9540: loss = 3.2827 | 3042.87ms | Tokens/s = 172,300.4
2025-01-19 02:34:58.240 | DEBUG    | __main__:<module>:313 - Training step 9550: loss = 3.4342 | 3040.75ms | Tokens/s = 172,420.5
2025-01-19 02:35:28.672 | DEBUG    | __main__:<module>:313 - Training step 9560: loss = 3.1647 | 3043.35ms | Tokens/s = 172,273.3
2025-01-19 02:35:59.131 | DEBUG    | __main__:<module>:313 - Training step 9570: loss = 3.3312 | 3045.17ms | Tokens/s = 172,170.6
2025-01-19 02:36:29.590 | DEBUG    | __main__:<module>:313 - Training step 9580: loss = 3.2746 | 3043.70ms | Tokens/s = 172,253.7
2025-01-19 02:37:00.028 | DEBUG    | __main__:<module>:313 - Training step 9590: loss = 3.3256 | 3044.33ms | Tokens/s = 172,217.9
2025-01-19 02:37:30.487 | DEBUG    | __main__:<module>:313 - Training step 9600: loss = 3.3576 | 3046.00ms | Tokens/s = 172,123.6
2025-01-19 02:38:00.926 | DEBUG    | __main__:<module>:313 - Training step 9610: loss = 3.3776 | 3043.30ms | Tokens/s = 172,275.9
2025-01-19 02:38:31.355 | DEBUG    | __main__:<module>:313 - Training step 9620: loss = 3.2273 | 3044.58ms | Tokens/s = 172,203.6
2025-01-19 02:39:01.813 | DEBUG    | __main__:<module>:313 - Training step 9630: loss = 3.3991 | 3046.84ms | Tokens/s = 172,076.1
2025-01-19 02:39:32.272 | DEBUG    | __main__:<module>:313 - Training step 9640: loss = 3.2024 | 3044.39ms | Tokens/s = 172,214.7
2025-01-19 02:40:02.705 | DEBUG    | __main__:<module>:313 - Training step 9650: loss = 3.3982 | 3041.86ms | Tokens/s = 172,357.8
2025-01-19 02:40:33.161 | DEBUG    | __main__:<module>:313 - Training step 9660: loss = 3.3722 | 3047.70ms | Tokens/s = 172,027.2
2025-01-19 02:41:03.609 | DEBUG    | __main__:<module>:313 - Training step 9670: loss = 3.2500 | 3043.88ms | Tokens/s = 172,243.5
2025-01-19 02:41:34.033 | DEBUG    | __main__:<module>:313 - Training step 9680: loss = 3.3077 | 3041.76ms | Tokens/s = 172,363.1
2025-01-19 02:42:04.480 | DEBUG    | __main__:<module>:313 - Training step 9690: loss = 3.1349 | 3046.48ms | Tokens/s = 172,096.5
2025-01-19 02:42:34.936 | DEBUG    | __main__:<module>:313 - Training step 9700: loss = 3.4246 | 3044.23ms | Tokens/s = 172,223.3
2025-01-19 02:43:05.372 | DEBUG    | __main__:<module>:313 - Training step 9710: loss = 3.5391 | 3043.02ms | Tokens/s = 172,292.2
2025-01-19 02:43:35.804 | DEBUG    | __main__:<module>:313 - Training step 9720: loss = 3.2254 | 3045.59ms | Tokens/s = 172,146.5
2025-01-19 02:44:06.250 | DEBUG    | __main__:<module>:313 - Training step 9730: loss = 3.3250 | 3043.58ms | Tokens/s = 172,260.1
2025-01-19 02:44:36.674 | DEBUG    | __main__:<module>:313 - Training step 9740: loss = 3.3073 | 3041.72ms | Tokens/s = 172,365.7
2025-01-19 02:45:07.090 | DEBUG    | __main__:<module>:313 - Training step 9750: loss = 3.4547 | 3043.14ms | Tokens/s = 172,285.4
2025-01-19 02:45:37.539 | DEBUG    | __main__:<module>:313 - Training step 9760: loss = 3.2214 | 3046.16ms | Tokens/s = 172,114.6
2025-01-19 02:46:07.995 | DEBUG    | __main__:<module>:313 - Training step 9770: loss = 3.4682 | 3045.41ms | Tokens/s = 172,157.1
2025-01-19 02:46:38.427 | DEBUG    | __main__:<module>:313 - Training step 9780: loss = 3.3284 | 3043.02ms | Tokens/s = 172,291.8
2025-01-19 02:47:08.853 | DEBUG    | __main__:<module>:313 - Training step 9790: loss = 3.3678 | 3043.75ms | Tokens/s = 172,250.7
2025-01-19 02:47:39.301 | DEBUG    | __main__:<module>:313 - Training step 9800: loss = 3.2497 | 3041.20ms | Tokens/s = 172,394.8
2025-01-19 02:48:09.730 | DEBUG    | __main__:<module>:313 - Training step 9810: loss = 3.5032 | 3043.31ms | Tokens/s = 172,275.8
2025-01-19 02:48:40.146 | DEBUG    | __main__:<module>:313 - Training step 9820: loss = 3.3154 | 3041.83ms | Tokens/s = 172,359.1
2025-01-19 02:49:10.550 | DEBUG    | __main__:<module>:313 - Training step 9830: loss = 3.3385 | 3040.95ms | Tokens/s = 172,409.6
2025-01-19 02:49:40.952 | DEBUG    | __main__:<module>:313 - Training step 9840: loss = 3.3389 | 3039.51ms | Tokens/s = 172,491.1
2025-01-19 02:50:11.378 | DEBUG    | __main__:<module>:313 - Training step 9850: loss = 3.3467 | 3044.92ms | Tokens/s = 172,184.6
2025-01-19 02:50:41.831 | DEBUG    | __main__:<module>:313 - Training step 9860: loss = 3.3631 | 3043.91ms | Tokens/s = 172,241.4
2025-01-19 02:51:12.266 | DEBUG    | __main__:<module>:313 - Training step 9870: loss = 3.3295 | 3043.30ms | Tokens/s = 172,276.1
2025-01-19 02:51:42.724 | DEBUG    | __main__:<module>:313 - Training step 9880: loss = 3.3785 | 3046.36ms | Tokens/s = 172,102.8
2025-01-19 02:52:13.176 | DEBUG    | __main__:<module>:313 - Training step 9890: loss = 3.2879 | 3042.75ms | Tokens/s = 172,307.4
2025-01-19 02:52:43.609 | DEBUG    | __main__:<module>:313 - Training step 9900: loss = 3.1142 | 3042.00ms | Tokens/s = 172,349.9
2025-01-19 02:53:14.039 | DEBUG    | __main__:<module>:313 - Training step 9910: loss = 3.4706 | 3044.01ms | Tokens/s = 172,236.0
2025-01-19 02:53:44.480 | DEBUG    | __main__:<module>:313 - Training step 9920: loss = 3.3791 | 3044.78ms | Tokens/s = 172,192.3
2025-01-19 02:54:14.902 | DEBUG    | __main__:<module>:313 - Training step 9930: loss = 3.2159 | 3040.42ms | Tokens/s = 172,439.4
2025-01-19 02:54:45.325 | DEBUG    | __main__:<module>:313 - Training step 9940: loss = 3.2646 | 3045.16ms | Tokens/s = 172,171.1
2025-01-19 02:55:15.758 | DEBUG    | __main__:<module>:313 - Training step 9950: loss = 3.3619 | 3042.31ms | Tokens/s = 172,331.9
2025-01-19 02:55:46.177 | DEBUG    | __main__:<module>:313 - Training step 9960: loss = 3.2118 | 3041.40ms | Tokens/s = 172,383.5
2025-01-19 02:56:16.615 | DEBUG    | __main__:<module>:313 - Training step 9970: loss = 3.5236 | 3043.97ms | Tokens/s = 172,238.4
2025-01-19 02:56:47.077 | DEBUG    | __main__:<module>:313 - Training step 9980: loss = 3.2649 | 3048.09ms | Tokens/s = 172,005.4
2025-01-19 02:57:17.535 | DEBUG    | __main__:<module>:313 - Training step 9990: loss = 3.3018 | 3046.13ms | Tokens/s = 172,115.9
2025-01-19 02:57:51.405 | INFO     | __main__:<module>:265 - Step 10,000/20,000 loss: 3.3057 (T) 3.3177 (V) | lr=5.9e-03
2025-01-19 02:57:51.406 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 02:58:04.863 | DEBUG    | __main__:<module>:313 - Training step 10000: loss = 3.4200 | 19914.90ms | Tokens/s = 26,326.4
2025-01-19 02:58:35.141 | DEBUG    | __main__:<module>:313 - Training step 10010: loss = 3.2540 | 3035.48ms | Tokens/s = 172,720.2
2025-01-19 02:59:05.536 | DEBUG    | __main__:<module>:313 - Training step 10020: loss = 3.4057 | 3041.98ms | Tokens/s = 172,351.0
2025-01-19 02:59:35.960 | DEBUG    | __main__:<module>:313 - Training step 10030: loss = 3.3890 | 3041.96ms | Tokens/s = 172,352.0
2025-01-19 03:00:06.393 | DEBUG    | __main__:<module>:313 - Training step 10040: loss = 3.1668 | 3042.72ms | Tokens/s = 172,309.2
2025-01-19 03:00:36.810 | DEBUG    | __main__:<module>:313 - Training step 10050: loss = 3.5077 | 3041.45ms | Tokens/s = 172,380.7
2025-01-19 03:01:07.213 | DEBUG    | __main__:<module>:313 - Training step 10060: loss = 3.3218 | 3038.51ms | Tokens/s = 172,547.7
2025-01-19 03:01:37.639 | DEBUG    | __main__:<module>:313 - Training step 10070: loss = 3.3461 | 3045.55ms | Tokens/s = 172,149.0
2025-01-19 03:02:08.089 | DEBUG    | __main__:<module>:313 - Training step 10080: loss = 3.2449 | 3045.52ms | Tokens/s = 172,150.5
2025-01-19 03:02:38.527 | DEBUG    | __main__:<module>:313 - Training step 10090: loss = 3.3458 | 3041.94ms | Tokens/s = 172,352.9
2025-01-19 03:03:08.939 | DEBUG    | __main__:<module>:313 - Training step 10100: loss = 3.1794 | 3039.06ms | Tokens/s = 172,516.6
2025-01-19 03:03:39.378 | DEBUG    | __main__:<module>:313 - Training step 10110: loss = 3.1828 | 3046.05ms | Tokens/s = 172,120.8
2025-01-19 03:04:09.832 | DEBUG    | __main__:<module>:313 - Training step 10120: loss = 3.3331 | 3044.71ms | Tokens/s = 172,196.1
2025-01-19 03:04:40.272 | DEBUG    | __main__:<module>:313 - Training step 10130: loss = 3.2163 | 3043.78ms | Tokens/s = 172,249.1
2025-01-19 03:05:10.691 | DEBUG    | __main__:<module>:313 - Training step 10140: loss = 3.1783 | 3041.20ms | Tokens/s = 172,395.2
2025-01-19 03:05:41.105 | DEBUG    | __main__:<module>:313 - Training step 10150: loss = 3.3833 | 3040.42ms | Tokens/s = 172,439.6
2025-01-19 03:06:11.546 | DEBUG    | __main__:<module>:313 - Training step 10160: loss = 3.2783 | 3043.97ms | Tokens/s = 172,238.0
2025-01-19 03:06:41.976 | DEBUG    | __main__:<module>:313 - Training step 10170: loss = 3.3513 | 3041.51ms | Tokens/s = 172,377.4
2025-01-19 03:07:12.395 | DEBUG    | __main__:<module>:313 - Training step 10180: loss = 3.1121 | 3040.79ms | Tokens/s = 172,418.4
2025-01-19 03:07:42.838 | DEBUG    | __main__:<module>:313 - Training step 10190: loss = 3.3019 | 3044.09ms | Tokens/s = 172,231.4
2025-01-19 03:08:13.286 | DEBUG    | __main__:<module>:313 - Training step 10200: loss = 3.3926 | 3041.26ms | Tokens/s = 172,391.6
2025-01-19 03:08:43.716 | DEBUG    | __main__:<module>:313 - Training step 10210: loss = 3.2122 | 3041.82ms | Tokens/s = 172,359.9
2025-01-19 03:09:14.133 | DEBUG    | __main__:<module>:313 - Training step 10220: loss = 3.3577 | 3041.78ms | Tokens/s = 172,362.3
2025-01-19 03:09:44.541 | DEBUG    | __main__:<module>:313 - Training step 10230: loss = 3.5076 | 3037.98ms | Tokens/s = 172,577.7
2025-01-19 03:10:14.963 | DEBUG    | __main__:<module>:313 - Training step 10240: loss = 3.1905 | 3041.93ms | Tokens/s = 172,353.5
2025-01-19 03:10:45.404 | DEBUG    | __main__:<module>:313 - Training step 10250: loss = 3.3401 | 3043.27ms | Tokens/s = 172,277.9
2025-01-19 03:11:15.827 | DEBUG    | __main__:<module>:313 - Training step 10260: loss = 3.2194 | 3039.25ms | Tokens/s = 172,505.9
2025-01-19 03:11:46.232 | DEBUG    | __main__:<module>:313 - Training step 10270: loss = 3.4335 | 3040.08ms | Tokens/s = 172,458.4
2025-01-19 03:12:16.620 | DEBUG    | __main__:<module>:313 - Training step 10280: loss = 3.4768 | 3037.54ms | Tokens/s = 172,602.9
2025-01-19 03:12:47.033 | DEBUG    | __main__:<module>:313 - Training step 10290: loss = 3.4071 | 3043.92ms | Tokens/s = 172,240.8
2025-01-19 03:13:17.479 | DEBUG    | __main__:<module>:313 - Training step 10300: loss = 3.4072 | 3046.11ms | Tokens/s = 172,117.1
2025-01-19 03:13:47.921 | DEBUG    | __main__:<module>:313 - Training step 10310: loss = 3.3229 | 3045.90ms | Tokens/s = 172,128.8
2025-01-19 03:14:18.337 | DEBUG    | __main__:<module>:313 - Training step 10320: loss = 3.3234 | 3039.09ms | Tokens/s = 172,514.9
2025-01-19 03:14:48.749 | DEBUG    | __main__:<module>:313 - Training step 10330: loss = 3.2629 | 3041.31ms | Tokens/s = 172,388.9
2025-01-19 03:15:19.188 | DEBUG    | __main__:<module>:313 - Training step 10340: loss = 3.0892 | 3044.13ms | Tokens/s = 172,228.9
2025-01-19 03:15:49.620 | DEBUG    | __main__:<module>:313 - Training step 10350: loss = 3.2171 | 3043.11ms | Tokens/s = 172,287.0
2025-01-19 03:16:20.072 | DEBUG    | __main__:<module>:313 - Training step 10360: loss = 3.4715 | 3046.22ms | Tokens/s = 172,110.9
2025-01-19 03:16:50.539 | DEBUG    | __main__:<module>:313 - Training step 10370: loss = 3.4088 | 3046.59ms | Tokens/s = 172,090.3
2025-01-19 03:17:20.994 | DEBUG    | __main__:<module>:313 - Training step 10380: loss = 3.2233 | 3044.84ms | Tokens/s = 172,189.0
2025-01-19 03:17:51.421 | DEBUG    | __main__:<module>:313 - Training step 10390: loss = 3.2311 | 3042.90ms | Tokens/s = 172,299.0
2025-01-19 03:18:21.827 | DEBUG    | __main__:<module>:313 - Training step 10400: loss = 3.3121 | 3038.10ms | Tokens/s = 172,570.8
2025-01-19 03:18:52.219 | DEBUG    | __main__:<module>:313 - Training step 10410: loss = 3.2444 | 3040.04ms | Tokens/s = 172,461.1
2025-01-19 03:19:22.614 | DEBUG    | __main__:<module>:313 - Training step 10420: loss = 3.3967 | 3041.70ms | Tokens/s = 172,366.8
2025-01-19 03:19:53.042 | DEBUG    | __main__:<module>:313 - Training step 10430: loss = 3.4163 | 3043.80ms | Tokens/s = 172,247.8
2025-01-19 03:20:23.501 | DEBUG    | __main__:<module>:313 - Training step 10440: loss = 3.4181 | 3048.40ms | Tokens/s = 171,988.0
2025-01-19 03:20:53.955 | DEBUG    | __main__:<module>:313 - Training step 10450: loss = 3.2528 | 3045.24ms | Tokens/s = 172,166.3
2025-01-19 03:21:24.385 | DEBUG    | __main__:<module>:313 - Training step 10460: loss = 3.0264 | 3041.74ms | Tokens/s = 172,364.7
2025-01-19 03:21:54.795 | DEBUG    | __main__:<module>:313 - Training step 10470: loss = 3.4788 | 3038.48ms | Tokens/s = 172,549.6
2025-01-19 03:22:25.195 | DEBUG    | __main__:<module>:313 - Training step 10480: loss = 3.2678 | 3039.38ms | Tokens/s = 172,498.6
2025-01-19 03:22:55.593 | DEBUG    | __main__:<module>:313 - Training step 10490: loss = 3.3332 | 3039.77ms | Tokens/s = 172,476.4
2025-01-19 03:23:25.971 | DEBUG    | __main__:<module>:313 - Training step 10500: loss = 3.4739 | 3037.95ms | Tokens/s = 172,579.8
2025-01-19 03:23:56.364 | DEBUG    | __main__:<module>:313 - Training step 10510: loss = 3.2706 | 3042.12ms | Tokens/s = 172,343.1
2025-01-19 03:24:26.795 | DEBUG    | __main__:<module>:313 - Training step 10520: loss = 3.3221 | 3044.26ms | Tokens/s = 172,221.6
2025-01-19 03:24:57.253 | DEBUG    | __main__:<module>:313 - Training step 10530: loss = 3.2030 | 3045.33ms | Tokens/s = 172,161.4
2025-01-19 03:25:27.687 | DEBUG    | __main__:<module>:313 - Training step 10540: loss = 3.4555 | 3040.76ms | Tokens/s = 172,419.8
2025-01-19 03:25:58.103 | DEBUG    | __main__:<module>:313 - Training step 10550: loss = 3.2879 | 3039.87ms | Tokens/s = 172,470.7
2025-01-19 03:26:28.502 | DEBUG    | __main__:<module>:313 - Training step 10560: loss = 3.4266 | 3040.27ms | Tokens/s = 172,447.9
2025-01-19 03:26:58.899 | DEBUG    | __main__:<module>:313 - Training step 10570: loss = 3.1150 | 3039.83ms | Tokens/s = 172,472.9
2025-01-19 03:27:29.318 | DEBUG    | __main__:<module>:313 - Training step 10580: loss = 3.2004 | 3039.58ms | Tokens/s = 172,487.2
2025-01-19 03:27:59.723 | DEBUG    | __main__:<module>:313 - Training step 10590: loss = 3.2236 | 3041.97ms | Tokens/s = 172,351.5
2025-01-19 03:28:30.142 | DEBUG    | __main__:<module>:313 - Training step 10600: loss = 3.4445 | 3040.89ms | Tokens/s = 172,412.7
2025-01-19 03:29:00.598 | DEBUG    | __main__:<module>:313 - Training step 10610: loss = 3.3568 | 3045.29ms | Tokens/s = 172,163.4
2025-01-19 03:29:31.030 | DEBUG    | __main__:<module>:313 - Training step 10620: loss = 3.2071 | 3039.87ms | Tokens/s = 172,470.5
2025-01-19 03:30:01.440 | DEBUG    | __main__:<module>:313 - Training step 10630: loss = 3.4239 | 3040.49ms | Tokens/s = 172,435.6
2025-01-19 03:30:31.841 | DEBUG    | __main__:<module>:313 - Training step 10640: loss = 3.1693 | 3039.54ms | Tokens/s = 172,489.5
2025-01-19 03:31:02.227 | DEBUG    | __main__:<module>:313 - Training step 10650: loss = 3.3035 | 3037.97ms | Tokens/s = 172,578.5
2025-01-19 03:31:32.613 | DEBUG    | __main__:<module>:313 - Training step 10660: loss = 3.3988 | 3041.17ms | Tokens/s = 172,396.9
2025-01-19 03:32:03.038 | DEBUG    | __main__:<module>:313 - Training step 10670: loss = 3.4626 | 3043.66ms | Tokens/s = 172,255.7
2025-01-19 03:32:33.489 | DEBUG    | __main__:<module>:313 - Training step 10680: loss = 3.2897 | 3047.10ms | Tokens/s = 172,061.3
2025-01-19 03:33:03.934 | DEBUG    | __main__:<module>:313 - Training step 10690: loss = 3.3366 | 3042.87ms | Tokens/s = 172,300.2
2025-01-19 03:33:34.358 | DEBUG    | __main__:<module>:313 - Training step 10700: loss = 3.2755 | 3040.75ms | Tokens/s = 172,420.5
2025-01-19 03:34:04.806 | DEBUG    | __main__:<module>:313 - Training step 10710: loss = 3.2880 | 3046.94ms | Tokens/s = 172,070.3
2025-01-19 03:34:35.275 | DEBUG    | __main__:<module>:313 - Training step 10720: loss = 3.1868 | 3047.44ms | Tokens/s = 172,041.9
2025-01-19 03:35:05.718 | DEBUG    | __main__:<module>:313 - Training step 10730: loss = 3.2241 | 3044.21ms | Tokens/s = 172,224.4
2025-01-19 03:35:36.162 | DEBUG    | __main__:<module>:313 - Training step 10740: loss = 3.2514 | 3046.36ms | Tokens/s = 172,103.0
2025-01-19 03:36:06.617 | DEBUG    | __main__:<module>:313 - Training step 10750: loss = 3.1785 | 3044.85ms | Tokens/s = 172,188.7
2025-01-19 03:36:37.056 | DEBUG    | __main__:<module>:313 - Training step 10760: loss = 3.2590 | 3042.53ms | Tokens/s = 172,319.8
2025-01-19 03:37:07.478 | DEBUG    | __main__:<module>:313 - Training step 10770: loss = 3.1610 | 3042.84ms | Tokens/s = 172,302.3
2025-01-19 03:37:37.902 | DEBUG    | __main__:<module>:313 - Training step 10780: loss = 3.1188 | 3044.53ms | Tokens/s = 172,206.6
2025-01-19 03:38:08.353 | DEBUG    | __main__:<module>:313 - Training step 10790: loss = 3.1973 | 3046.24ms | Tokens/s = 172,110.0
2025-01-19 03:38:38.817 | DEBUG    | __main__:<module>:313 - Training step 10800: loss = 3.2785 | 3045.65ms | Tokens/s = 172,143.4
2025-01-19 03:39:09.261 | DEBUG    | __main__:<module>:313 - Training step 10810: loss = 3.2142 | 3041.55ms | Tokens/s = 172,375.4
2025-01-19 03:39:39.688 | DEBUG    | __main__:<module>:313 - Training step 10820: loss = 3.2613 | 3043.46ms | Tokens/s = 172,266.8
2025-01-19 03:40:10.090 | DEBUG    | __main__:<module>:313 - Training step 10830: loss = 3.3183 | 3039.18ms | Tokens/s = 172,509.8
2025-01-19 03:40:40.488 | DEBUG    | __main__:<module>:313 - Training step 10840: loss = 3.1946 | 3038.53ms | Tokens/s = 172,546.3
2025-01-19 03:41:10.888 | DEBUG    | __main__:<module>:313 - Training step 10850: loss = 3.2915 | 3040.59ms | Tokens/s = 172,429.8
2025-01-19 03:41:41.324 | DEBUG    | __main__:<module>:313 - Training step 10860: loss = 3.2003 | 3044.98ms | Tokens/s = 172,181.1
2025-01-19 03:42:11.776 | DEBUG    | __main__:<module>:313 - Training step 10870: loss = 3.1960 | 3042.69ms | Tokens/s = 172,310.5
2025-01-19 03:42:42.210 | DEBUG    | __main__:<module>:313 - Training step 10880: loss = 3.2806 | 3041.24ms | Tokens/s = 172,392.7
2025-01-19 03:43:12.628 | DEBUG    | __main__:<module>:313 - Training step 10890: loss = 3.2698 | 3042.35ms | Tokens/s = 172,330.2
2025-01-19 03:43:43.038 | DEBUG    | __main__:<module>:313 - Training step 10900: loss = 3.0859 | 3039.83ms | Tokens/s = 172,473.0
2025-01-19 03:44:13.440 | DEBUG    | __main__:<module>:313 - Training step 10910: loss = 3.2721 | 3041.45ms | Tokens/s = 172,380.7
2025-01-19 03:44:43.848 | DEBUG    | __main__:<module>:313 - Training step 10920: loss = 3.1951 | 3040.87ms | Tokens/s = 172,413.9
2025-01-19 03:45:14.290 | DEBUG    | __main__:<module>:313 - Training step 10930: loss = 3.1793 | 3045.71ms | Tokens/s = 172,139.6
2025-01-19 03:45:44.720 | DEBUG    | __main__:<module>:313 - Training step 10940: loss = 3.3273 | 3045.58ms | Tokens/s = 172,147.4
2025-01-19 03:46:15.150 | DEBUG    | __main__:<module>:313 - Training step 10950: loss = 3.2165 | 3042.99ms | Tokens/s = 172,293.7
2025-01-19 03:46:45.561 | DEBUG    | __main__:<module>:313 - Training step 10960: loss = 3.3123 | 3040.92ms | Tokens/s = 172,410.8
2025-01-19 03:47:15.973 | DEBUG    | __main__:<module>:313 - Training step 10970: loss = 3.2849 | 3040.87ms | Tokens/s = 172,414.0
2025-01-19 03:47:46.413 | DEBUG    | __main__:<module>:313 - Training step 10980: loss = 3.2101 | 3045.99ms | Tokens/s = 172,123.7
2025-01-19 03:48:16.877 | DEBUG    | __main__:<module>:313 - Training step 10990: loss = 3.2106 | 3047.32ms | Tokens/s = 172,048.8
2025-01-19 03:48:50.716 | INFO     | __main__:<module>:265 - Step 11,000/20,000 loss: 3.2765 (T) 3.2611 (V) | lr=5.0e-03
2025-01-19 03:48:50.718 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 03:49:04.447 | DEBUG    | __main__:<module>:313 - Training step 11000: loss = 3.2073 | 20182.09ms | Tokens/s = 25,977.9
2025-01-19 03:49:34.703 | DEBUG    | __main__:<module>:313 - Training step 11010: loss = 3.2609 | 3032.40ms | Tokens/s = 172,895.2
2025-01-19 03:50:05.077 | DEBUG    | __main__:<module>:313 - Training step 11020: loss = 3.2925 | 3037.33ms | Tokens/s = 172,614.7
2025-01-19 03:50:35.490 | DEBUG    | __main__:<module>:313 - Training step 11030: loss = 3.3504 | 3041.60ms | Tokens/s = 172,372.6
2025-01-19 03:51:05.924 | DEBUG    | __main__:<module>:313 - Training step 11040: loss = 3.2934 | 3044.07ms | Tokens/s = 172,232.7
2025-01-19 03:51:36.349 | DEBUG    | __main__:<module>:313 - Training step 11050: loss = 3.4150 | 3042.60ms | Tokens/s = 172,315.5
2025-01-19 03:52:06.758 | DEBUG    | __main__:<module>:313 - Training step 11060: loss = 3.2664 | 3040.23ms | Tokens/s = 172,449.9
2025-01-19 03:52:37.156 | DEBUG    | __main__:<module>:313 - Training step 11070: loss = 3.2383 | 3039.16ms | Tokens/s = 172,510.7
2025-01-19 03:53:07.580 | DEBUG    | __main__:<module>:313 - Training step 11080: loss = 3.1911 | 3045.16ms | Tokens/s = 172,170.8
2025-01-19 03:53:38.018 | DEBUG    | __main__:<module>:313 - Training step 11090: loss = 3.4317 | 3041.55ms | Tokens/s = 172,375.4
2025-01-19 03:54:08.429 | DEBUG    | __main__:<module>:313 - Training step 11100: loss = 3.1764 | 3038.50ms | Tokens/s = 172,548.2
2025-01-19 03:54:38.831 | DEBUG    | __main__:<module>:313 - Training step 11110: loss = 3.2366 | 3040.20ms | Tokens/s = 172,451.7
2025-01-19 03:55:09.222 | DEBUG    | __main__:<module>:313 - Training step 11120: loss = 3.3454 | 3039.39ms | Tokens/s = 172,498.0
2025-01-19 03:55:39.626 | DEBUG    | __main__:<module>:313 - Training step 11130: loss = 3.2998 | 3041.70ms | Tokens/s = 172,366.7
2025-01-19 03:56:10.060 | DEBUG    | __main__:<module>:313 - Training step 11140: loss = 3.1523 | 3045.68ms | Tokens/s = 172,141.5
2025-01-19 03:56:40.518 | DEBUG    | __main__:<module>:313 - Training step 11150: loss = 3.3634 | 3047.20ms | Tokens/s = 172,055.6
2025-01-19 03:57:10.970 | DEBUG    | __main__:<module>:313 - Training step 11160: loss = 3.3926 | 3042.78ms | Tokens/s = 172,305.7
2025-01-19 03:57:41.392 | DEBUG    | __main__:<module>:313 - Training step 11170: loss = 3.2154 | 3042.17ms | Tokens/s = 172,340.1
2025-01-19 03:58:11.798 | DEBUG    | __main__:<module>:313 - Training step 11180: loss = 3.1654 | 3039.76ms | Tokens/s = 172,476.7
2025-01-19 03:58:42.208 | DEBUG    | __main__:<module>:313 - Training step 11190: loss = 3.4303 | 3041.33ms | Tokens/s = 172,387.8
2025-01-19 03:59:12.617 | DEBUG    | __main__:<module>:313 - Training step 11200: loss = 3.2783 | 3040.91ms | Tokens/s = 172,411.4
2025-01-19 03:59:43.055 | DEBUG    | __main__:<module>:313 - Training step 11210: loss = 3.2601 | 3045.03ms | Tokens/s = 172,178.0
2025-01-19 04:00:13.487 | DEBUG    | __main__:<module>:313 - Training step 11220: loss = 3.1410 | 3039.64ms | Tokens/s = 172,483.6
2025-01-19 04:00:43.896 | DEBUG    | __main__:<module>:313 - Training step 11230: loss = 3.2800 | 3042.40ms | Tokens/s = 172,326.9
2025-01-19 04:01:14.338 | DEBUG    | __main__:<module>:313 - Training step 11240: loss = 3.1755 | 3047.15ms | Tokens/s = 172,058.8
2025-01-19 04:01:44.792 | DEBUG    | __main__:<module>:313 - Training step 11250: loss = 3.2474 | 3045.96ms | Tokens/s = 172,125.6
2025-01-19 04:02:15.226 | DEBUG    | __main__:<module>:313 - Training step 11260: loss = 3.1401 | 3042.64ms | Tokens/s = 172,313.3
2025-01-19 04:02:45.638 | DEBUG    | __main__:<module>:313 - Training step 11270: loss = 3.3062 | 3041.54ms | Tokens/s = 172,375.6
2025-01-19 04:03:16.043 | DEBUG    | __main__:<module>:313 - Training step 11280: loss = 3.3772 | 3041.61ms | Tokens/s = 172,371.9
2025-01-19 04:03:46.445 | DEBUG    | __main__:<module>:313 - Training step 11290: loss = 2.9792 | 3041.72ms | Tokens/s = 172,365.6
2025-01-19 04:04:16.879 | DEBUG    | __main__:<module>:313 - Training step 11300: loss = 3.1968 | 3044.67ms | Tokens/s = 172,198.8
2025-01-19 04:04:47.319 | DEBUG    | __main__:<module>:313 - Training step 11310: loss = 3.3396 | 3042.60ms | Tokens/s = 172,315.8
2025-01-19 04:05:17.738 | DEBUG    | __main__:<module>:313 - Training step 11320: loss = 2.9552 | 3041.46ms | Tokens/s = 172,380.4
2025-01-19 04:05:48.152 | DEBUG    | __main__:<module>:313 - Training step 11330: loss = 3.3287 | 3039.95ms | Tokens/s = 172,466.0
2025-01-19 04:06:18.580 | DEBUG    | __main__:<module>:313 - Training step 11340: loss = 3.1598 | 3044.91ms | Tokens/s = 172,185.0
2025-01-19 04:06:49.008 | DEBUG    | __main__:<module>:313 - Training step 11350: loss = 3.1334 | 3041.11ms | Tokens/s = 172,400.5
2025-01-19 04:07:19.421 | DEBUG    | __main__:<module>:313 - Training step 11360: loss = 3.2696 | 3041.23ms | Tokens/s = 172,393.4
2025-01-19 04:07:49.819 | DEBUG    | __main__:<module>:313 - Training step 11370: loss = 3.3015 | 3038.52ms | Tokens/s = 172,547.1
2025-01-19 04:08:20.213 | DEBUG    | __main__:<module>:313 - Training step 11380: loss = 3.2226 | 3036.69ms | Tokens/s = 172,651.0
2025-01-19 04:08:50.600 | DEBUG    | __main__:<module>:313 - Training step 11390: loss = 3.1161 | 3038.30ms | Tokens/s = 172,559.7
2025-01-19 04:09:20.992 | DEBUG    | __main__:<module>:313 - Training step 11400: loss = 3.2181 | 3043.41ms | Tokens/s = 172,270.0
2025-01-19 04:09:51.415 | DEBUG    | __main__:<module>:313 - Training step 11410: loss = 3.2910 | 3044.06ms | Tokens/s = 172,233.0
2025-01-19 04:10:21.875 | DEBUG    | __main__:<module>:313 - Training step 11420: loss = 3.2052 | 3044.25ms | Tokens/s = 172,222.1
2025-01-19 04:10:52.302 | DEBUG    | __main__:<module>:313 - Training step 11430: loss = 3.3911 | 3042.09ms | Tokens/s = 172,344.9
2025-01-19 04:11:22.734 | DEBUG    | __main__:<module>:313 - Training step 11440: loss = 3.1685 | 3045.29ms | Tokens/s = 172,163.3
2025-01-19 04:11:53.180 | DEBUG    | __main__:<module>:313 - Training step 11450: loss = 3.1940 | 3045.60ms | Tokens/s = 172,146.0
2025-01-19 04:12:23.604 | DEBUG    | __main__:<module>:313 - Training step 11460: loss = 3.1927 | 3041.15ms | Tokens/s = 172,398.1
2025-01-19 04:12:54.009 | DEBUG    | __main__:<module>:313 - Training step 11470: loss = 3.1187 | 3042.52ms | Tokens/s = 172,320.5
2025-01-19 04:13:24.432 | DEBUG    | __main__:<module>:313 - Training step 11480: loss = 3.4403 | 3044.63ms | Tokens/s = 172,200.8
2025-01-19 04:13:54.868 | DEBUG    | __main__:<module>:313 - Training step 11490: loss = 3.4233 | 3041.59ms | Tokens/s = 172,372.9
2025-01-19 04:14:25.288 | DEBUG    | __main__:<module>:313 - Training step 11500: loss = 3.2743 | 3041.94ms | Tokens/s = 172,353.0
2025-01-19 04:14:55.732 | DEBUG    | __main__:<module>:313 - Training step 11510: loss = 3.3047 | 3044.46ms | Tokens/s = 172,210.7
2025-01-19 04:15:26.165 | DEBUG    | __main__:<module>:313 - Training step 11520: loss = 3.1406 | 3041.78ms | Tokens/s = 172,362.2
2025-01-19 04:15:56.583 | DEBUG    | __main__:<module>:313 - Training step 11530: loss = 3.1624 | 3041.16ms | Tokens/s = 172,397.2
2025-01-19 04:16:27.014 | DEBUG    | __main__:<module>:313 - Training step 11540: loss = 3.1506 | 3044.72ms | Tokens/s = 172,195.9
2025-01-19 04:16:57.451 | DEBUG    | __main__:<module>:313 - Training step 11550: loss = 3.3121 | 3041.66ms | Tokens/s = 172,369.1
2025-01-19 04:17:27.875 | DEBUG    | __main__:<module>:313 - Training step 11560: loss = 3.3164 | 3041.16ms | Tokens/s = 172,397.4
2025-01-19 04:17:58.281 | DEBUG    | __main__:<module>:313 - Training step 11570: loss = 3.2449 | 3041.75ms | Tokens/s = 172,363.8
2025-01-19 04:18:28.689 | DEBUG    | __main__:<module>:313 - Training step 11580: loss = 3.2272 | 3039.81ms | Tokens/s = 172,474.1
2025-01-19 04:18:59.103 | DEBUG    | __main__:<module>:313 - Training step 11590: loss = 3.2338 | 3037.73ms | Tokens/s = 172,591.8
2025-01-19 04:19:29.507 | DEBUG    | __main__:<module>:313 - Training step 11600: loss = 3.1893 | 3039.69ms | Tokens/s = 172,480.7
2025-01-19 04:19:59.931 | DEBUG    | __main__:<module>:313 - Training step 11610: loss = 3.3735 | 3043.96ms | Tokens/s = 172,238.9
2025-01-19 04:20:30.372 | DEBUG    | __main__:<module>:313 - Training step 11620: loss = 3.2318 | 3045.40ms | Tokens/s = 172,157.5
2025-01-19 04:21:00.790 | DEBUG    | __main__:<module>:313 - Training step 11630: loss = 3.2451 | 3040.37ms | Tokens/s = 172,442.0
2025-01-19 04:21:31.199 | DEBUG    | __main__:<module>:313 - Training step 11640: loss = 3.2746 | 3039.80ms | Tokens/s = 172,474.3
2025-01-19 04:22:01.594 | DEBUG    | __main__:<module>:313 - Training step 11650: loss = 3.2718 | 3039.87ms | Tokens/s = 172,470.8
2025-01-19 04:22:32.002 | DEBUG    | __main__:<module>:313 - Training step 11660: loss = 3.2175 | 3043.32ms | Tokens/s = 172,275.0
2025-01-19 04:23:02.447 | DEBUG    | __main__:<module>:313 - Training step 11670: loss = 3.2585 | 3047.06ms | Tokens/s = 172,063.6
2025-01-19 04:23:32.905 | DEBUG    | __main__:<module>:313 - Training step 11680: loss = 3.2556 | 3044.66ms | Tokens/s = 172,199.3
2025-01-19 04:24:03.338 | DEBUG    | __main__:<module>:313 - Training step 11690: loss = 3.3239 | 3040.94ms | Tokens/s = 172,409.7
2025-01-19 04:24:33.749 | DEBUG    | __main__:<module>:313 - Training step 11700: loss = 3.1218 | 3040.32ms | Tokens/s = 172,445.0
2025-01-19 04:25:04.140 | DEBUG    | __main__:<module>:313 - Training step 11710: loss = 3.2689 | 3037.93ms | Tokens/s = 172,580.5
2025-01-19 04:25:34.522 | DEBUG    | __main__:<module>:313 - Training step 11720: loss = 3.3464 | 3038.74ms | Tokens/s = 172,534.4
2025-01-19 04:26:04.929 | DEBUG    | __main__:<module>:313 - Training step 11730: loss = 3.3451 | 3043.25ms | Tokens/s = 172,278.9
2025-01-19 04:26:35.352 | DEBUG    | __main__:<module>:313 - Training step 11740: loss = 3.2299 | 3041.05ms | Tokens/s = 172,403.5
2025-01-19 04:27:05.766 | DEBUG    | __main__:<module>:313 - Training step 11750: loss = 3.3319 | 3042.05ms | Tokens/s = 172,346.9
2025-01-19 04:27:36.187 | DEBUG    | __main__:<module>:313 - Training step 11760: loss = 3.2005 | 3044.51ms | Tokens/s = 172,207.7
2025-01-19 04:28:06.641 | DEBUG    | __main__:<module>:313 - Training step 11770: loss = 3.2858 | 3046.52ms | Tokens/s = 172,094.2
2025-01-19 04:28:37.085 | DEBUG    | __main__:<module>:313 - Training step 11780: loss = 3.5206 | 3041.43ms | Tokens/s = 172,382.3
2025-01-19 04:29:07.504 | DEBUG    | __main__:<module>:313 - Training step 11790: loss = 3.3050 | 3041.02ms | Tokens/s = 172,405.0
2025-01-19 04:29:37.904 | DEBUG    | __main__:<module>:313 - Training step 11800: loss = 3.2768 | 3036.98ms | Tokens/s = 172,634.8
2025-01-19 04:30:08.322 | DEBUG    | __main__:<module>:313 - Training step 11810: loss = 3.1968 | 3044.39ms | Tokens/s = 172,214.4
2025-01-19 04:30:38.767 | DEBUG    | __main__:<module>:313 - Training step 11820: loss = 3.5371 | 3044.47ms | Tokens/s = 172,209.9
2025-01-19 04:31:09.229 | DEBUG    | __main__:<module>:313 - Training step 11830: loss = 3.1294 | 3044.59ms | Tokens/s = 172,203.4
2025-01-19 04:31:39.657 | DEBUG    | __main__:<module>:313 - Training step 11840: loss = 3.1823 | 3038.97ms | Tokens/s = 172,521.6
2025-01-19 04:32:10.078 | DEBUG    | __main__:<module>:313 - Training step 11850: loss = 3.2107 | 3042.59ms | Tokens/s = 172,316.3
2025-01-19 04:32:40.518 | DEBUG    | __main__:<module>:313 - Training step 11860: loss = 3.1461 | 3044.11ms | Tokens/s = 172,230.5
2025-01-19 04:33:10.945 | DEBUG    | __main__:<module>:313 - Training step 11870: loss = 3.2648 | 3044.04ms | Tokens/s = 172,234.3
2025-01-19 04:33:41.354 | DEBUG    | __main__:<module>:313 - Training step 11880: loss = 3.2591 | 3040.24ms | Tokens/s = 172,449.7
2025-01-19 04:34:11.755 | DEBUG    | __main__:<module>:313 - Training step 11890: loss = 3.1292 | 3040.01ms | Tokens/s = 172,462.6
2025-01-19 04:34:42.174 | DEBUG    | __main__:<module>:313 - Training step 11900: loss = 3.3484 | 3041.73ms | Tokens/s = 172,364.9
2025-01-19 04:35:12.623 | DEBUG    | __main__:<module>:313 - Training step 11910: loss = 3.2909 | 3044.19ms | Tokens/s = 172,225.7
2025-01-19 04:35:43.051 | DEBUG    | __main__:<module>:313 - Training step 11920: loss = 2.9024 | 3040.28ms | Tokens/s = 172,447.4
2025-01-19 04:36:13.460 | DEBUG    | __main__:<module>:313 - Training step 11930: loss = 3.2330 | 3039.91ms | Tokens/s = 172,468.4
2025-01-19 04:36:43.863 | DEBUG    | __main__:<module>:313 - Training step 11940: loss = 3.1669 | 3042.53ms | Tokens/s = 172,320.0
2025-01-19 04:37:14.284 | DEBUG    | __main__:<module>:313 - Training step 11950: loss = 3.2360 | 3044.47ms | Tokens/s = 172,209.9
2025-01-19 04:37:44.709 | DEBUG    | __main__:<module>:313 - Training step 11960: loss = 3.1941 | 3038.69ms | Tokens/s = 172,537.5
2025-01-19 04:38:15.123 | DEBUG    | __main__:<module>:313 - Training step 11970: loss = 3.2187 | 3042.09ms | Tokens/s = 172,344.5
2025-01-19 04:38:45.564 | DEBUG    | __main__:<module>:313 - Training step 11980: loss = 3.4633 | 3042.95ms | Tokens/s = 172,296.0
2025-01-19 04:39:15.985 | DEBUG    | __main__:<module>:313 - Training step 11990: loss = 3.1401 | 3041.94ms | Tokens/s = 172,353.0
2025-01-19 04:39:49.842 | INFO     | __main__:<module>:265 - Step 12,000/20,000 loss: 3.2370 (T) 3.2450 (V) | lr=4.1e-03
2025-01-19 04:39:49.844 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 04:40:03.297 | DEBUG    | __main__:<module>:313 - Training step 12000: loss = 3.2786 | 19918.80ms | Tokens/s = 26,321.3
2025-01-19 04:40:33.597 | DEBUG    | __main__:<module>:313 - Training step 12010: loss = 3.0994 | 3035.35ms | Tokens/s = 172,727.2
2025-01-19 04:41:03.990 | DEBUG    | __main__:<module>:313 - Training step 12020: loss = 3.1637 | 3042.26ms | Tokens/s = 172,335.0
2025-01-19 04:41:34.394 | DEBUG    | __main__:<module>:313 - Training step 12030: loss = 3.2420 | 3042.33ms | Tokens/s = 172,331.0
2025-01-19 04:42:04.829 | DEBUG    | __main__:<module>:313 - Training step 12040: loss = 3.2935 | 3043.69ms | Tokens/s = 172,253.9
2025-01-19 04:42:35.285 | DEBUG    | __main__:<module>:313 - Training step 12050: loss = 3.3299 | 3045.90ms | Tokens/s = 172,129.1
2025-01-19 04:43:05.748 | DEBUG    | __main__:<module>:313 - Training step 12060: loss = 3.1188 | 3045.74ms | Tokens/s = 172,138.1
2025-01-19 04:43:36.189 | DEBUG    | __main__:<module>:313 - Training step 12070: loss = 3.1568 | 3043.48ms | Tokens/s = 172,265.9
2025-01-19 04:44:06.624 | DEBUG    | __main__:<module>:313 - Training step 12080: loss = 3.2745 | 3044.71ms | Tokens/s = 172,196.6
2025-01-19 04:44:37.046 | DEBUG    | __main__:<module>:313 - Training step 12090: loss = 3.1690 | 3040.32ms | Tokens/s = 172,444.8
2025-01-19 04:45:07.463 | DEBUG    | __main__:<module>:313 - Training step 12100: loss = 3.1115 | 3039.12ms | Tokens/s = 172,513.1
2025-01-19 04:45:37.881 | DEBUG    | __main__:<module>:313 - Training step 12110: loss = 3.2100 | 3041.83ms | Tokens/s = 172,359.4
2025-01-19 04:46:08.338 | DEBUG    | __main__:<module>:313 - Training step 12120: loss = 3.2433 | 3047.11ms | Tokens/s = 172,061.0
2025-01-19 04:46:38.786 | DEBUG    | __main__:<module>:313 - Training step 12130: loss = 3.2764 | 3044.66ms | Tokens/s = 172,198.9
2025-01-19 04:47:09.231 | DEBUG    | __main__:<module>:313 - Training step 12140: loss = 3.2049 | 3045.45ms | Tokens/s = 172,154.6
2025-01-19 04:47:39.664 | DEBUG    | __main__:<module>:313 - Training step 12150: loss = 3.3770 | 3042.35ms | Tokens/s = 172,330.0
2025-01-19 04:48:10.087 | DEBUG    | __main__:<module>:313 - Training step 12160: loss = 3.4213 | 3041.93ms | Tokens/s = 172,353.8
2025-01-19 04:48:40.520 | DEBUG    | __main__:<module>:313 - Training step 12170: loss = 3.1265 | 3042.47ms | Tokens/s = 172,323.2
2025-01-19 04:49:10.992 | DEBUG    | __main__:<module>:313 - Training step 12180: loss = 3.4837 | 3046.55ms | Tokens/s = 172,092.5
2025-01-19 04:49:41.444 | DEBUG    | __main__:<module>:313 - Training step 12190: loss = 3.3044 | 3042.67ms | Tokens/s = 172,311.8
2025-01-19 04:50:11.889 | DEBUG    | __main__:<module>:313 - Training step 12200: loss = 3.2407 | 3043.13ms | Tokens/s = 172,285.7
2025-01-19 04:50:42.318 | DEBUG    | __main__:<module>:313 - Training step 12210: loss = 3.2264 | 3042.68ms | Tokens/s = 172,311.0
2025-01-19 04:51:12.752 | DEBUG    | __main__:<module>:313 - Training step 12220: loss = 3.3171 | 3042.14ms | Tokens/s = 172,341.9
2025-01-19 04:51:43.189 | DEBUG    | __main__:<module>:313 - Training step 12230: loss = 3.1477 | 3044.65ms | Tokens/s = 172,199.5
2025-01-19 04:52:13.655 | DEBUG    | __main__:<module>:313 - Training step 12240: loss = 3.0402 | 3047.20ms | Tokens/s = 172,055.6
2025-01-19 04:52:44.108 | DEBUG    | __main__:<module>:313 - Training step 12250: loss = 3.0987 | 3043.66ms | Tokens/s = 172,256.0
2025-01-19 04:53:14.550 | DEBUG    | __main__:<module>:313 - Training step 12260: loss = 3.2647 | 3042.01ms | Tokens/s = 172,349.4
2025-01-19 04:53:44.992 | DEBUG    | __main__:<module>:313 - Training step 12270: loss = 3.0894 | 3043.92ms | Tokens/s = 172,241.3
2025-01-19 04:54:15.423 | DEBUG    | __main__:<module>:313 - Training step 12280: loss = 3.2726 | 3042.20ms | Tokens/s = 172,338.4
2025-01-19 04:54:45.849 | DEBUG    | __main__:<module>:313 - Training step 12290: loss = 3.2077 | 3042.91ms | Tokens/s = 172,298.5
2025-01-19 04:55:16.271 | DEBUG    | __main__:<module>:313 - Training step 12300: loss = 3.2441 | 3040.45ms | Tokens/s = 172,437.4
2025-01-19 04:55:46.691 | DEBUG    | __main__:<module>:313 - Training step 12310: loss = 3.2426 | 3040.74ms | Tokens/s = 172,421.0
2025-01-19 04:56:17.111 | DEBUG    | __main__:<module>:313 - Training step 12320: loss = 3.3497 | 3040.42ms | Tokens/s = 172,439.4
2025-01-19 04:56:47.535 | DEBUG    | __main__:<module>:313 - Training step 12330: loss = 3.2151 | 3043.18ms | Tokens/s = 172,283.1
2025-01-19 04:57:17.954 | DEBUG    | __main__:<module>:313 - Training step 12340: loss = 3.0682 | 3046.02ms | Tokens/s = 172,122.5
2025-01-19 04:57:48.418 | DEBUG    | __main__:<module>:313 - Training step 12350: loss = 3.3210 | 3046.24ms | Tokens/s = 172,110.0
2025-01-19 04:58:18.873 | DEBUG    | __main__:<module>:313 - Training step 12360: loss = 3.2250 | 3045.21ms | Tokens/s = 172,168.3
2025-01-19 04:58:49.322 | DEBUG    | __main__:<module>:313 - Training step 12370: loss = 3.1623 | 3043.74ms | Tokens/s = 172,251.5
2025-01-19 04:59:19.766 | DEBUG    | __main__:<module>:313 - Training step 12380: loss = 3.3623 | 3044.69ms | Tokens/s = 172,197.7
2025-01-19 04:59:50.231 | DEBUG    | __main__:<module>:313 - Training step 12390: loss = 3.3432 | 3046.94ms | Tokens/s = 172,070.6
2025-01-19 05:00:20.696 | DEBUG    | __main__:<module>:313 - Training step 12400: loss = 3.2749 | 3043.98ms | Tokens/s = 172,237.4
2025-01-19 05:00:51.155 | DEBUG    | __main__:<module>:313 - Training step 12410: loss = 3.1492 | 3045.88ms | Tokens/s = 172,130.0
2025-01-19 05:01:21.605 | DEBUG    | __main__:<module>:313 - Training step 12420: loss = 3.1391 | 3044.48ms | Tokens/s = 172,209.5
2025-01-19 05:01:52.039 | DEBUG    | __main__:<module>:313 - Training step 12430: loss = 3.2426 | 3043.75ms | Tokens/s = 172,250.7
2025-01-19 05:02:22.475 | DEBUG    | __main__:<module>:313 - Training step 12440: loss = 3.1953 | 3043.56ms | Tokens/s = 172,261.5
2025-01-19 05:02:52.910 | DEBUG    | __main__:<module>:313 - Training step 12450: loss = 3.1049 | 3044.13ms | Tokens/s = 172,229.4
2025-01-19 05:03:23.339 | DEBUG    | __main__:<module>:313 - Training step 12460: loss = 3.2155 | 3042.67ms | Tokens/s = 172,311.8
2025-01-19 05:03:53.771 | DEBUG    | __main__:<module>:313 - Training step 12470: loss = 2.9743 | 3044.34ms | Tokens/s = 172,217.1
2025-01-19 05:04:24.213 | DEBUG    | __main__:<module>:313 - Training step 12480: loss = 3.2651 | 3043.96ms | Tokens/s = 172,238.9
2025-01-19 05:04:54.645 | DEBUG    | __main__:<module>:313 - Training step 12490: loss = 3.2950 | 3043.41ms | Tokens/s = 172,269.8
2025-01-19 05:05:25.071 | DEBUG    | __main__:<module>:313 - Training step 12500: loss = 3.2177 | 3042.06ms | Tokens/s = 172,346.3
2025-01-19 05:05:55.500 | DEBUG    | __main__:<module>:313 - Training step 12510: loss = 3.1804 | 3040.99ms | Tokens/s = 172,406.8
2025-01-19 05:06:25.927 | DEBUG    | __main__:<module>:313 - Training step 12520: loss = 3.1661 | 3040.40ms | Tokens/s = 172,440.7
2025-01-19 05:06:56.356 | DEBUG    | __main__:<module>:313 - Training step 12530: loss = 3.2181 | 3043.43ms | Tokens/s = 172,268.7
2025-01-19 05:07:26.781 | DEBUG    | __main__:<module>:313 - Training step 12540: loss = 3.2013 | 3039.75ms | Tokens/s = 172,477.1
2025-01-19 05:07:57.234 | DEBUG    | __main__:<module>:313 - Training step 12550: loss = 3.3466 | 3046.00ms | Tokens/s = 172,123.4
2025-01-19 05:08:27.694 | DEBUG    | __main__:<module>:313 - Training step 12560: loss = 3.2339 | 3044.14ms | Tokens/s = 172,228.8
2025-01-19 05:08:58.140 | DEBUG    | __main__:<module>:313 - Training step 12570: loss = 3.1936 | 3045.14ms | Tokens/s = 172,172.1
2025-01-19 05:09:28.580 | DEBUG    | __main__:<module>:313 - Training step 12580: loss = 3.2479 | 3044.91ms | Tokens/s = 172,184.8
2025-01-19 05:09:59.020 | DEBUG    | __main__:<module>:313 - Training step 12590: loss = 3.2115 | 3045.69ms | Tokens/s = 172,141.1
2025-01-19 05:10:29.452 | DEBUG    | __main__:<module>:313 - Training step 12600: loss = 3.1585 | 3043.32ms | Tokens/s = 172,275.3
2025-01-19 05:10:59.880 | DEBUG    | __main__:<module>:313 - Training step 12610: loss = 3.1724 | 3043.11ms | Tokens/s = 172,286.9
2025-01-19 05:11:30.345 | DEBUG    | __main__:<module>:313 - Training step 12620: loss = 3.1718 | 3051.03ms | Tokens/s = 171,839.4
2025-01-19 05:12:00.836 | DEBUG    | __main__:<module>:313 - Training step 12630: loss = 3.3166 | 3047.79ms | Tokens/s = 172,022.5
2025-01-19 05:12:31.306 | DEBUG    | __main__:<module>:313 - Training step 12640: loss = 2.9974 | 3045.80ms | Tokens/s = 172,134.8
2025-01-19 05:13:01.774 | DEBUG    | __main__:<module>:313 - Training step 12650: loss = 3.2217 | 3046.61ms | Tokens/s = 172,088.8
2025-01-19 05:13:32.230 | DEBUG    | __main__:<module>:313 - Training step 12660: loss = 3.2377 | 3047.77ms | Tokens/s = 172,023.3
2025-01-19 05:14:02.704 | DEBUG    | __main__:<module>:313 - Training step 12670: loss = 3.2138 | 3046.49ms | Tokens/s = 172,095.6
2025-01-19 05:14:33.155 | DEBUG    | __main__:<module>:313 - Training step 12680: loss = 3.0964 | 3043.71ms | Tokens/s = 172,253.1
2025-01-19 05:15:03.606 | DEBUG    | __main__:<module>:313 - Training step 12690: loss = 3.2034 | 3045.41ms | Tokens/s = 172,157.0
2025-01-19 05:15:34.046 | DEBUG    | __main__:<module>:313 - Training step 12700: loss = 3.2978 | 3043.69ms | Tokens/s = 172,254.0
2025-01-19 05:16:04.479 | DEBUG    | __main__:<module>:313 - Training step 12710: loss = 3.3374 | 3042.76ms | Tokens/s = 172,306.7
2025-01-19 05:16:34.915 | DEBUG    | __main__:<module>:313 - Training step 12720: loss = 3.2275 | 3044.39ms | Tokens/s = 172,214.6
2025-01-19 05:17:05.343 | DEBUG    | __main__:<module>:313 - Training step 12730: loss = 3.1833 | 3044.37ms | Tokens/s = 172,215.8
2025-01-19 05:17:35.774 | DEBUG    | __main__:<module>:313 - Training step 12740: loss = 3.2341 | 3044.10ms | Tokens/s = 172,231.0
2025-01-19 05:18:06.209 | DEBUG    | __main__:<module>:313 - Training step 12750: loss = 3.3329 | 3047.11ms | Tokens/s = 172,060.5
2025-01-19 05:18:36.675 | DEBUG    | __main__:<module>:313 - Training step 12760: loss = 3.2182 | 3047.82ms | Tokens/s = 172,020.5
2025-01-19 05:19:07.126 | DEBUG    | __main__:<module>:313 - Training step 12770: loss = 3.2069 | 3045.91ms | Tokens/s = 172,128.6
2025-01-19 05:19:37.567 | DEBUG    | __main__:<module>:313 - Training step 12780: loss = 3.2356 | 3044.22ms | Tokens/s = 172,223.9
2025-01-19 05:20:08.004 | DEBUG    | __main__:<module>:313 - Training step 12790: loss = 3.1982 | 3043.16ms | Tokens/s = 172,283.8
2025-01-19 05:20:38.439 | DEBUG    | __main__:<module>:313 - Training step 12800: loss = 3.1172 | 3041.48ms | Tokens/s = 172,379.3
2025-01-19 05:21:08.874 | DEBUG    | __main__:<module>:313 - Training step 12810: loss = 3.2037 | 3043.95ms | Tokens/s = 172,239.4
2025-01-19 05:21:39.306 | DEBUG    | __main__:<module>:313 - Training step 12820: loss = 3.1984 | 3044.89ms | Tokens/s = 172,186.4
2025-01-19 05:22:09.744 | DEBUG    | __main__:<module>:313 - Training step 12830: loss = 3.0956 | 3043.67ms | Tokens/s = 172,255.0
2025-01-19 05:22:40.183 | DEBUG    | __main__:<module>:313 - Training step 12840: loss = 3.2449 | 3045.09ms | Tokens/s = 172,174.7
2025-01-19 05:23:10.644 | DEBUG    | __main__:<module>:313 - Training step 12850: loss = 3.1254 | 3045.86ms | Tokens/s = 172,131.2
2025-01-19 05:23:41.100 | DEBUG    | __main__:<module>:313 - Training step 12860: loss = 3.2448 | 3045.70ms | Tokens/s = 172,140.3
2025-01-19 05:24:11.551 | DEBUG    | __main__:<module>:313 - Training step 12870: loss = 3.2466 | 3044.90ms | Tokens/s = 172,185.4
2025-01-19 05:24:41.992 | DEBUG    | __main__:<module>:313 - Training step 12880: loss = 3.2397 | 3042.52ms | Tokens/s = 172,320.5
2025-01-19 05:25:12.453 | DEBUG    | __main__:<module>:313 - Training step 12890: loss = 3.2533 | 3047.98ms | Tokens/s = 172,011.8
2025-01-19 05:25:42.904 | DEBUG    | __main__:<module>:313 - Training step 12900: loss = 3.1039 | 3044.72ms | Tokens/s = 172,195.9
2025-01-19 05:26:13.342 | DEBUG    | __main__:<module>:313 - Training step 12910: loss = 3.1857 | 3044.17ms | Tokens/s = 172,227.1
2025-01-19 05:26:43.776 | DEBUG    | __main__:<module>:313 - Training step 12920: loss = 3.1502 | 3045.06ms | Tokens/s = 172,176.6
2025-01-19 05:27:14.211 | DEBUG    | __main__:<module>:313 - Training step 12930: loss = 3.3191 | 3041.61ms | Tokens/s = 172,372.0
2025-01-19 05:27:44.637 | DEBUG    | __main__:<module>:313 - Training step 12940: loss = 3.0801 | 3043.93ms | Tokens/s = 172,240.3
2025-01-19 05:28:15.067 | DEBUG    | __main__:<module>:313 - Training step 12950: loss = 3.2437 | 3044.49ms | Tokens/s = 172,209.1
2025-01-19 05:28:45.539 | DEBUG    | __main__:<module>:313 - Training step 12960: loss = 3.1312 | 3048.92ms | Tokens/s = 171,958.8
2025-01-19 05:29:16.010 | DEBUG    | __main__:<module>:313 - Training step 12970: loss = 3.2261 | 3047.24ms | Tokens/s = 172,053.6
2025-01-19 05:29:46.463 | DEBUG    | __main__:<module>:313 - Training step 12980: loss = 3.1852 | 3045.82ms | Tokens/s = 172,133.7
2025-01-19 05:30:16.912 | DEBUG    | __main__:<module>:313 - Training step 12990: loss = 3.3468 | 3045.40ms | Tokens/s = 172,157.1
2025-01-19 05:30:50.766 | INFO     | __main__:<module>:265 - Step 13,000/20,000 loss: 3.2095 (T) 3.2063 (V) | lr=3.3e-03
2025-01-19 05:30:50.768 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 05:31:04.152 | DEBUG    | __main__:<module>:313 - Training step 13000: loss = 3.2692 | 19842.76ms | Tokens/s = 26,422.1
2025-01-19 05:31:34.446 | DEBUG    | __main__:<module>:313 - Training step 13010: loss = 3.1206 | 3038.00ms | Tokens/s = 172,576.5
2025-01-19 05:32:04.865 | DEBUG    | __main__:<module>:313 - Training step 13020: loss = 3.3133 | 3042.83ms | Tokens/s = 172,302.5
2025-01-19 05:32:35.328 | DEBUG    | __main__:<module>:313 - Training step 13030: loss = 3.0879 | 3048.06ms | Tokens/s = 172,006.8
2025-01-19 05:33:05.797 | DEBUG    | __main__:<module>:313 - Training step 13040: loss = 3.2644 | 3046.43ms | Tokens/s = 172,099.3
2025-01-19 05:33:36.249 | DEBUG    | __main__:<module>:313 - Training step 13050: loss = 3.1995 | 3046.48ms | Tokens/s = 172,096.3
2025-01-19 05:34:06.716 | DEBUG    | __main__:<module>:313 - Training step 13060: loss = 3.3176 | 3046.99ms | Tokens/s = 172,067.2
2025-01-19 05:34:37.194 | DEBUG    | __main__:<module>:313 - Training step 13070: loss = 3.0054 | 3047.92ms | Tokens/s = 172,015.1
2025-01-19 05:35:07.654 | DEBUG    | __main__:<module>:313 - Training step 13080: loss = 3.2510 | 3046.89ms | Tokens/s = 172,072.9
2025-01-19 05:35:38.110 | DEBUG    | __main__:<module>:313 - Training step 13090: loss = 3.1434 | 3047.40ms | Tokens/s = 172,044.2
2025-01-19 05:36:08.572 | DEBUG    | __main__:<module>:313 - Training step 13100: loss = 3.1917 | 3047.67ms | Tokens/s = 172,029.2
2025-01-19 05:36:39.034 | DEBUG    | __main__:<module>:313 - Training step 13110: loss = 3.2768 | 3045.70ms | Tokens/s = 172,140.4
2025-01-19 05:37:09.483 | DEBUG    | __main__:<module>:313 - Training step 13120: loss = 3.3043 | 3043.24ms | Tokens/s = 172,279.7
2025-01-19 05:37:39.923 | DEBUG    | __main__:<module>:313 - Training step 13130: loss = 3.0278 | 3044.52ms | Tokens/s = 172,207.0
2025-01-19 05:38:10.358 | DEBUG    | __main__:<module>:313 - Training step 13140: loss = 3.3776 | 3043.93ms | Tokens/s = 172,240.5
2025-01-19 05:38:40.790 | DEBUG    | __main__:<module>:313 - Training step 13150: loss = 3.2946 | 3043.80ms | Tokens/s = 172,248.1
2025-01-19 05:39:11.229 | DEBUG    | __main__:<module>:313 - Training step 13160: loss = 3.1709 | 3043.11ms | Tokens/s = 172,287.0
2025-01-19 05:39:41.672 | DEBUG    | __main__:<module>:313 - Training step 13170: loss = 3.3380 | 3044.14ms | Tokens/s = 172,228.7
2025-01-19 05:40:12.110 | DEBUG    | __main__:<module>:313 - Training step 13180: loss = 3.2376 | 3044.68ms | Tokens/s = 172,197.9
2025-01-19 05:40:42.550 | DEBUG    | __main__:<module>:313 - Training step 13190: loss = 3.2412 | 3044.83ms | Tokens/s = 172,189.8
2025-01-19 05:41:13.019 | DEBUG    | __main__:<module>:313 - Training step 13200: loss = 3.2655 | 3044.72ms | Tokens/s = 172,196.0
2025-01-19 05:41:43.488 | DEBUG    | __main__:<module>:313 - Training step 13210: loss = 3.2385 | 3047.43ms | Tokens/s = 172,042.5
2025-01-19 05:42:13.957 | DEBUG    | __main__:<module>:313 - Training step 13220: loss = 3.1632 | 3047.37ms | Tokens/s = 172,045.8
2025-01-19 05:42:44.416 | DEBUG    | __main__:<module>:313 - Training step 13230: loss = 3.1865 | 3048.22ms | Tokens/s = 171,998.0
2025-01-19 05:43:14.861 | DEBUG    | __main__:<module>:313 - Training step 13240: loss = 3.2981 | 3042.85ms | Tokens/s = 172,301.9
2025-01-19 05:43:45.325 | DEBUG    | __main__:<module>:313 - Training step 13250: loss = 3.2815 | 3046.36ms | Tokens/s = 172,103.2
2025-01-19 05:44:15.777 | DEBUG    | __main__:<module>:313 - Training step 13260: loss = 3.1782 | 3043.15ms | Tokens/s = 172,284.4
2025-01-19 05:44:46.221 | DEBUG    | __main__:<module>:313 - Training step 13270: loss = 3.1681 | 3045.86ms | Tokens/s = 172,131.3
2025-01-19 05:45:16.662 | DEBUG    | __main__:<module>:313 - Training step 13280: loss = 3.2616 | 3043.49ms | Tokens/s = 172,265.5
2025-01-19 05:45:47.118 | DEBUG    | __main__:<module>:313 - Training step 13290: loss = 3.1924 | 3047.70ms | Tokens/s = 172,027.2
2025-01-19 05:46:17.581 | DEBUG    | __main__:<module>:313 - Training step 13300: loss = 3.1846 | 3045.14ms | Tokens/s = 172,172.1
2025-01-19 05:46:48.029 | DEBUG    | __main__:<module>:313 - Training step 13310: loss = 3.2449 | 3046.51ms | Tokens/s = 172,094.6
2025-01-19 05:47:18.475 | DEBUG    | __main__:<module>:313 - Training step 13320: loss = 3.2851 | 3044.12ms | Tokens/s = 172,229.5
2025-01-19 05:47:48.911 | DEBUG    | __main__:<module>:313 - Training step 13330: loss = 2.9603 | 3043.02ms | Tokens/s = 172,292.3
2025-01-19 05:48:19.355 | DEBUG    | __main__:<module>:313 - Training step 13340: loss = 3.3909 | 3045.92ms | Tokens/s = 172,127.8
2025-01-19 05:48:49.797 | DEBUG    | __main__:<module>:313 - Training step 13350: loss = 3.2634 | 3043.29ms | Tokens/s = 172,276.9
2025-01-19 05:49:20.232 | DEBUG    | __main__:<module>:313 - Training step 13360: loss = 3.2084 | 3045.19ms | Tokens/s = 172,169.4
2025-01-19 05:49:50.659 | DEBUG    | __main__:<module>:313 - Training step 13370: loss = 3.2865 | 3040.77ms | Tokens/s = 172,419.8
2025-01-19 05:50:21.086 | DEBUG    | __main__:<module>:313 - Training step 13380: loss = 3.1923 | 3045.13ms | Tokens/s = 172,172.4
2025-01-19 05:50:51.508 | DEBUG    | __main__:<module>:313 - Training step 13390: loss = 3.1029 | 3041.49ms | Tokens/s = 172,378.8
2025-01-19 05:51:21.935 | DEBUG    | __main__:<module>:313 - Training step 13400: loss = 3.2881 | 3044.00ms | Tokens/s = 172,236.6
2025-01-19 05:51:52.383 | DEBUG    | __main__:<module>:313 - Training step 13410: loss = 3.1771 | 3047.10ms | Tokens/s = 172,061.4
2025-01-19 05:52:22.837 | DEBUG    | __main__:<module>:313 - Training step 13420: loss = 3.1160 | 3045.83ms | Tokens/s = 172,133.3
2025-01-19 05:52:53.280 | DEBUG    | __main__:<module>:313 - Training step 13430: loss = 3.2394 | 3042.08ms | Tokens/s = 172,345.5
2025-01-19 05:53:23.723 | DEBUG    | __main__:<module>:313 - Training step 13440: loss = 3.0617 | 3044.49ms | Tokens/s = 172,208.7
2025-01-19 05:53:54.179 | DEBUG    | __main__:<module>:313 - Training step 13450: loss = 3.2966 | 3047.13ms | Tokens/s = 172,059.4
2025-01-19 05:54:24.647 | DEBUG    | __main__:<module>:313 - Training step 13460: loss = 3.3654 | 3046.58ms | Tokens/s = 172,090.7
2025-01-19 05:54:55.102 | DEBUG    | __main__:<module>:313 - Training step 13470: loss = 3.1448 | 3044.51ms | Tokens/s = 172,207.7
2025-01-19 05:55:25.553 | DEBUG    | __main__:<module>:313 - Training step 13480: loss = 2.9488 | 3044.18ms | Tokens/s = 172,226.4
2025-01-19 05:55:55.986 | DEBUG    | __main__:<module>:313 - Training step 13490: loss = 3.1933 | 3042.52ms | Tokens/s = 172,320.5
2025-01-19 05:56:26.418 | DEBUG    | __main__:<module>:313 - Training step 13500: loss = 3.0234 | 3042.98ms | Tokens/s = 172,294.4
2025-01-19 05:56:56.845 | DEBUG    | __main__:<module>:313 - Training step 13510: loss = 3.2506 | 3042.12ms | Tokens/s = 172,342.7
2025-01-19 05:57:27.274 | DEBUG    | __main__:<module>:313 - Training step 13520: loss = 3.2322 | 3041.80ms | Tokens/s = 172,360.8
2025-01-19 05:57:57.700 | DEBUG    | __main__:<module>:313 - Training step 13530: loss = 3.1721 | 3040.88ms | Tokens/s = 172,413.0
2025-01-19 05:58:28.133 | DEBUG    | __main__:<module>:313 - Training step 13540: loss = 3.2057 | 3041.70ms | Tokens/s = 172,366.5
2025-01-19 05:58:58.581 | DEBUG    | __main__:<module>:313 - Training step 13550: loss = 3.0037 | 3047.62ms | Tokens/s = 172,032.0
2025-01-19 05:59:29.032 | DEBUG    | __main__:<module>:313 - Training step 13560: loss = 3.1830 | 3045.91ms | Tokens/s = 172,128.3
2025-01-19 05:59:59.475 | DEBUG    | __main__:<module>:313 - Training step 13570: loss = 3.1473 | 3044.89ms | Tokens/s = 172,186.1
2025-01-19 06:00:29.911 | DEBUG    | __main__:<module>:313 - Training step 13580: loss = 3.3536 | 3043.73ms | Tokens/s = 172,251.9
2025-01-19 06:01:00.353 | DEBUG    | __main__:<module>:313 - Training step 13590: loss = 3.1790 | 3045.37ms | Tokens/s = 172,159.1
2025-01-19 06:01:30.801 | DEBUG    | __main__:<module>:313 - Training step 13600: loss = 3.2302 | 3042.39ms | Tokens/s = 172,327.4
2025-01-19 06:02:01.241 | DEBUG    | __main__:<module>:313 - Training step 13610: loss = 2.9615 | 3044.13ms | Tokens/s = 172,229.2
2025-01-19 06:02:31.684 | DEBUG    | __main__:<module>:313 - Training step 13620: loss = 3.1731 | 3042.86ms | Tokens/s = 172,301.0
2025-01-19 06:03:02.127 | DEBUG    | __main__:<module>:313 - Training step 13630: loss = 3.2434 | 3043.39ms | Tokens/s = 172,271.1
2025-01-19 06:03:32.577 | DEBUG    | __main__:<module>:313 - Training step 13640: loss = 3.0676 | 3045.38ms | Tokens/s = 172,158.3
2025-01-19 06:04:03.061 | DEBUG    | __main__:<module>:313 - Training step 13650: loss = 3.1983 | 3047.35ms | Tokens/s = 172,047.1
2025-01-19 06:04:33.532 | DEBUG    | __main__:<module>:313 - Training step 13660: loss = 3.2270 | 3045.99ms | Tokens/s = 172,124.2
2025-01-19 06:05:03.994 | DEBUG    | __main__:<module>:313 - Training step 13670: loss = 3.2480 | 3046.14ms | Tokens/s = 172,115.7
2025-01-19 06:05:34.445 | DEBUG    | __main__:<module>:313 - Training step 13680: loss = 3.2014 | 3044.22ms | Tokens/s = 172,223.9
2025-01-19 06:06:04.898 | DEBUG    | __main__:<module>:313 - Training step 13690: loss = 3.3010 | 3044.55ms | Tokens/s = 172,205.3
2025-01-19 06:06:35.358 | DEBUG    | __main__:<module>:313 - Training step 13700: loss = 3.0880 | 3044.21ms | Tokens/s = 172,224.4
2025-01-19 06:07:05.815 | DEBUG    | __main__:<module>:313 - Training step 13710: loss = 3.0819 | 3046.57ms | Tokens/s = 172,091.0
2025-01-19 06:07:36.268 | DEBUG    | __main__:<module>:313 - Training step 13720: loss = 3.0922 | 3045.48ms | Tokens/s = 172,152.6
2025-01-19 06:08:06.718 | DEBUG    | __main__:<module>:313 - Training step 13730: loss = 3.2896 | 3044.43ms | Tokens/s = 172,212.1
2025-01-19 06:08:37.167 | DEBUG    | __main__:<module>:313 - Training step 13740: loss = 3.2229 | 3045.88ms | Tokens/s = 172,130.1
2025-01-19 06:09:07.608 | DEBUG    | __main__:<module>:313 - Training step 13750: loss = 3.1110 | 3044.39ms | Tokens/s = 172,214.6
2025-01-19 06:09:38.051 | DEBUG    | __main__:<module>:313 - Training step 13760: loss = 3.2537 | 3044.24ms | Tokens/s = 172,223.0
2025-01-19 06:10:08.485 | DEBUG    | __main__:<module>:313 - Training step 13770: loss = 3.2985 | 3041.38ms | Tokens/s = 172,384.9
2025-01-19 06:10:38.931 | DEBUG    | __main__:<module>:313 - Training step 13780: loss = 3.2562 | 3047.52ms | Tokens/s = 172,037.4
2025-01-19 06:11:09.401 | DEBUG    | __main__:<module>:313 - Training step 13790: loss = 3.1624 | 3047.09ms | Tokens/s = 172,062.1
2025-01-19 06:11:39.862 | DEBUG    | __main__:<module>:313 - Training step 13800: loss = 3.2080 | 3044.68ms | Tokens/s = 172,198.3
2025-01-19 06:12:10.307 | DEBUG    | __main__:<module>:313 - Training step 13810: loss = 3.1945 | 3042.84ms | Tokens/s = 172,302.3
2025-01-19 06:12:40.758 | DEBUG    | __main__:<module>:313 - Training step 13820: loss = 3.2246 | 3045.47ms | Tokens/s = 172,153.3
2025-01-19 06:13:11.204 | DEBUG    | __main__:<module>:313 - Training step 13830: loss = 3.0088 | 3043.50ms | Tokens/s = 172,264.9
2025-01-19 06:13:41.652 | DEBUG    | __main__:<module>:313 - Training step 13840: loss = 3.1102 | 3045.49ms | Tokens/s = 172,152.1
2025-01-19 06:14:12.099 | DEBUG    | __main__:<module>:313 - Training step 13850: loss = 3.1643 | 3045.04ms | Tokens/s = 172,177.6
2025-01-19 06:14:42.533 | DEBUG    | __main__:<module>:313 - Training step 13860: loss = 3.0758 | 3044.48ms | Tokens/s = 172,209.6
2025-01-19 06:15:12.979 | DEBUG    | __main__:<module>:313 - Training step 13870: loss = 3.1269 | 3047.51ms | Tokens/s = 172,038.1
2025-01-19 06:15:43.442 | DEBUG    | __main__:<module>:313 - Training step 13880: loss = 3.1511 | 3046.14ms | Tokens/s = 172,115.3
2025-01-19 06:16:13.898 | DEBUG    | __main__:<module>:313 - Training step 13890: loss = 3.1157 | 3044.34ms | Tokens/s = 172,217.2
2025-01-19 06:16:44.346 | DEBUG    | __main__:<module>:313 - Training step 13900: loss = 3.1305 | 3043.66ms | Tokens/s = 172,255.8
2025-01-19 06:17:14.796 | DEBUG    | __main__:<module>:313 - Training step 13910: loss = 3.2766 | 3045.58ms | Tokens/s = 172,147.1
2025-01-19 06:17:45.265 | DEBUG    | __main__:<module>:313 - Training step 13920: loss = 2.9973 | 3046.22ms | Tokens/s = 172,111.0
2025-01-19 06:18:15.732 | DEBUG    | __main__:<module>:313 - Training step 13930: loss = 3.2341 | 3044.60ms | Tokens/s = 172,202.6
2025-01-19 06:18:46.191 | DEBUG    | __main__:<module>:313 - Training step 13940: loss = 3.1983 | 3045.79ms | Tokens/s = 172,135.0
2025-01-19 06:19:16.657 | DEBUG    | __main__:<module>:313 - Training step 13950: loss = 3.0705 | 3047.65ms | Tokens/s = 172,030.4
2025-01-19 06:19:47.137 | DEBUG    | __main__:<module>:313 - Training step 13960: loss = 3.2663 | 3047.95ms | Tokens/s = 172,013.3
2025-01-19 06:20:17.588 | DEBUG    | __main__:<module>:313 - Training step 13970: loss = 3.2020 | 3045.20ms | Tokens/s = 172,168.9
2025-01-19 06:20:48.032 | DEBUG    | __main__:<module>:313 - Training step 13980: loss = 3.0699 | 3042.18ms | Tokens/s = 172,339.4
2025-01-19 06:21:18.486 | DEBUG    | __main__:<module>:313 - Training step 13990: loss = 3.0455 | 3046.76ms | Tokens/s = 172,080.3
2025-01-19 06:21:52.385 | INFO     | __main__:<module>:265 - Step 14,000/20,000 loss: 3.1577 (T) 3.1881 (V) | lr=2.5e-03
2025-01-19 06:21:52.386 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 06:22:05.832 | DEBUG    | __main__:<module>:313 - Training step 14000: loss = 2.9144 | 19916.29ms | Tokens/s = 26,324.6
2025-01-19 06:22:36.154 | DEBUG    | __main__:<module>:313 - Training step 14010: loss = 3.2604 | 3039.29ms | Tokens/s = 172,503.6
2025-01-19 06:23:06.591 | DEBUG    | __main__:<module>:313 - Training step 14020: loss = 3.0696 | 3045.77ms | Tokens/s = 172,136.2
2025-01-19 06:23:37.050 | DEBUG    | __main__:<module>:313 - Training step 14030: loss = 3.0869 | 3044.80ms | Tokens/s = 172,191.1
2025-01-19 06:24:07.493 | DEBUG    | __main__:<module>:313 - Training step 14040: loss = 3.0087 | 3042.67ms | Tokens/s = 172,311.8
2025-01-19 06:24:37.925 | DEBUG    | __main__:<module>:313 - Training step 14050: loss = 3.2142 | 3044.46ms | Tokens/s = 172,210.2
2025-01-19 06:25:08.358 | DEBUG    | __main__:<module>:313 - Training step 14060: loss = 3.2804 | 3043.71ms | Tokens/s = 172,252.8
2025-01-19 06:25:38.801 | DEBUG    | __main__:<module>:313 - Training step 14070: loss = 3.1611 | 3043.75ms | Tokens/s = 172,250.8
2025-01-19 06:26:09.246 | DEBUG    | __main__:<module>:313 - Training step 14080: loss = 3.1554 | 3044.59ms | Tokens/s = 172,202.9
2025-01-19 06:26:39.695 | DEBUG    | __main__:<module>:313 - Training step 14090: loss = 3.1003 | 3045.25ms | Tokens/s = 172,165.8
2025-01-19 06:27:10.136 | DEBUG    | __main__:<module>:313 - Training step 14100: loss = 3.0272 | 3043.24ms | Tokens/s = 172,279.8
2025-01-19 06:27:40.570 | DEBUG    | __main__:<module>:313 - Training step 14110: loss = 3.2011 | 3044.23ms | Tokens/s = 172,223.4
2025-01-19 06:28:11.008 | DEBUG    | __main__:<module>:313 - Training step 14120: loss = 2.9954 | 3044.04ms | Tokens/s = 172,234.4
2025-01-19 06:28:41.440 | DEBUG    | __main__:<module>:313 - Training step 14130: loss = 3.1958 | 3043.54ms | Tokens/s = 172,262.4
2025-01-19 06:29:11.876 | DEBUG    | __main__:<module>:313 - Training step 14140: loss = 3.1275 | 3044.59ms | Tokens/s = 172,203.1
2025-01-19 06:29:42.307 | DEBUG    | __main__:<module>:313 - Training step 14150: loss = 3.4363 | 3043.38ms | Tokens/s = 172,271.4
2025-01-19 06:30:12.740 | DEBUG    | __main__:<module>:313 - Training step 14160: loss = 3.2950 | 3042.92ms | Tokens/s = 172,297.6
2025-01-19 06:30:43.170 | DEBUG    | __main__:<module>:313 - Training step 14170: loss = 3.2976 | 3041.53ms | Tokens/s = 172,376.4
2025-01-19 06:31:13.595 | DEBUG    | __main__:<module>:313 - Training step 14180: loss = 3.1564 | 3042.04ms | Tokens/s = 172,347.5
2025-01-19 06:31:44.025 | DEBUG    | __main__:<module>:313 - Training step 14190: loss = 3.2236 | 3041.32ms | Tokens/s = 172,388.3
2025-01-19 06:32:14.460 | DEBUG    | __main__:<module>:313 - Training step 14200: loss = 3.2261 | 3044.44ms | Tokens/s = 172,211.5
2025-01-19 06:32:44.898 | DEBUG    | __main__:<module>:313 - Training step 14210: loss = 3.0396 | 3043.84ms | Tokens/s = 172,245.4
2025-01-19 06:33:15.357 | DEBUG    | __main__:<module>:313 - Training step 14220: loss = 3.0986 | 3045.75ms | Tokens/s = 172,137.8
2025-01-19 06:33:45.801 | DEBUG    | __main__:<module>:313 - Training step 14230: loss = 3.0889 | 3045.91ms | Tokens/s = 172,128.7
2025-01-19 06:34:16.240 | DEBUG    | __main__:<module>:313 - Training step 14240: loss = 3.1107 | 3044.26ms | Tokens/s = 172,221.7
2025-01-19 06:34:46.680 | DEBUG    | __main__:<module>:313 - Training step 14250: loss = 3.0692 | 3045.57ms | Tokens/s = 172,147.9
2025-01-19 06:35:17.126 | DEBUG    | __main__:<module>:313 - Training step 14260: loss = 3.0734 | 3045.60ms | Tokens/s = 172,146.3
2025-01-19 06:35:47.570 | DEBUG    | __main__:<module>:313 - Training step 14270: loss = 3.1641 | 3043.88ms | Tokens/s = 172,243.4
2025-01-19 06:36:18.017 | DEBUG    | __main__:<module>:313 - Training step 14280: loss = 3.2774 | 3045.61ms | Tokens/s = 172,145.3
2025-01-19 06:36:48.462 | DEBUG    | __main__:<module>:313 - Training step 14290: loss = 3.1592 | 3043.48ms | Tokens/s = 172,266.2
2025-01-19 06:37:18.904 | DEBUG    | __main__:<module>:313 - Training step 14300: loss = 3.2494 | 3043.31ms | Tokens/s = 172,275.7
2025-01-19 06:37:49.340 | DEBUG    | __main__:<module>:313 - Training step 14310: loss = 3.0764 | 3041.74ms | Tokens/s = 172,364.6
2025-01-19 06:38:19.779 | DEBUG    | __main__:<module>:313 - Training step 14320: loss = 3.2282 | 3043.34ms | Tokens/s = 172,273.8
2025-01-19 06:38:50.210 | DEBUG    | __main__:<module>:313 - Training step 14330: loss = 3.0855 | 3044.32ms | Tokens/s = 172,218.7
2025-01-19 06:39:20.645 | DEBUG    | __main__:<module>:313 - Training step 14340: loss = 3.1844 | 3044.17ms | Tokens/s = 172,226.8
2025-01-19 06:39:51.077 | DEBUG    | __main__:<module>:313 - Training step 14350: loss = 3.2181 | 3042.72ms | Tokens/s = 172,309.1
2025-01-19 06:40:21.508 | DEBUG    | __main__:<module>:313 - Training step 14360: loss = 3.0794 | 3041.82ms | Tokens/s = 172,359.7
2025-01-19 06:40:51.941 | DEBUG    | __main__:<module>:313 - Training step 14370: loss = 2.9855 | 3044.73ms | Tokens/s = 172,195.3
2025-01-19 06:41:22.377 | DEBUG    | __main__:<module>:313 - Training step 14380: loss = 3.0789 | 3044.23ms | Tokens/s = 172,223.3
2025-01-19 06:41:52.808 | DEBUG    | __main__:<module>:313 - Training step 14390: loss = 3.5072 | 3044.31ms | Tokens/s = 172,218.9
2025-01-19 06:42:23.232 | DEBUG    | __main__:<module>:313 - Training step 14400: loss = 3.2078 | 3042.85ms | Tokens/s = 172,301.4
2025-01-19 06:42:53.690 | DEBUG    | __main__:<module>:313 - Training step 14410: loss = 3.3236 | 3046.80ms | Tokens/s = 172,078.0
2025-01-19 06:43:24.148 | DEBUG    | __main__:<module>:313 - Training step 14420: loss = 3.1883 | 3044.07ms | Tokens/s = 172,232.4
2025-01-19 06:43:54.599 | DEBUG    | __main__:<module>:313 - Training step 14430: loss = 3.1663 | 3044.20ms | Tokens/s = 172,224.9
2025-01-19 06:44:25.044 | DEBUG    | __main__:<module>:313 - Training step 14440: loss = 3.0772 | 3044.14ms | Tokens/s = 172,228.8
2025-01-19 06:44:55.494 | DEBUG    | __main__:<module>:313 - Training step 14450: loss = 3.1158 | 3044.59ms | Tokens/s = 172,203.1
2025-01-19 06:45:25.932 | DEBUG    | __main__:<module>:313 - Training step 14460: loss = 3.0499 | 3045.14ms | Tokens/s = 172,172.2
2025-01-19 06:45:56.369 | DEBUG    | __main__:<module>:313 - Training step 14470: loss = 3.0995 | 3045.15ms | Tokens/s = 172,171.4
2025-01-19 06:46:26.807 | DEBUG    | __main__:<module>:313 - Training step 14480: loss = 3.2859 | 3043.04ms | Tokens/s = 172,290.7
2025-01-19 06:46:57.243 | DEBUG    | __main__:<module>:313 - Training step 14490: loss = 3.0470 | 3043.26ms | Tokens/s = 172,278.6
2025-01-19 06:47:27.682 | DEBUG    | __main__:<module>:313 - Training step 14500: loss = 3.1646 | 3041.98ms | Tokens/s = 172,351.1
2025-01-19 06:47:58.141 | DEBUG    | __main__:<module>:313 - Training step 14510: loss = 3.2271 | 3048.94ms | Tokens/s = 171,957.6
2025-01-19 06:48:28.606 | DEBUG    | __main__:<module>:313 - Training step 14520: loss = 3.3339 | 3046.36ms | Tokens/s = 172,103.2
2025-01-19 06:48:59.062 | DEBUG    | __main__:<module>:313 - Training step 14530: loss = 3.1287 | 3045.77ms | Tokens/s = 172,136.5
2025-01-19 06:49:29.506 | DEBUG    | __main__:<module>:313 - Training step 14540: loss = 3.0476 | 3044.39ms | Tokens/s = 172,214.5
2025-01-19 06:49:59.946 | DEBUG    | __main__:<module>:313 - Training step 14550: loss = 3.1974 | 3042.96ms | Tokens/s = 172,295.1
2025-01-19 06:50:30.385 | DEBUG    | __main__:<module>:313 - Training step 14560: loss = 3.2604 | 3043.83ms | Tokens/s = 172,246.3
2025-01-19 06:51:00.820 | DEBUG    | __main__:<module>:313 - Training step 14570: loss = 3.2145 | 3043.47ms | Tokens/s = 172,266.3
2025-01-19 06:51:31.249 | DEBUG    | __main__:<module>:313 - Training step 14580: loss = 3.1334 | 3041.56ms | Tokens/s = 172,374.8
2025-01-19 06:52:01.676 | DEBUG    | __main__:<module>:313 - Training step 14590: loss = 3.1675 | 3043.65ms | Tokens/s = 172,256.2
2025-01-19 06:52:32.103 | DEBUG    | __main__:<module>:313 - Training step 14600: loss = 3.2166 | 3043.52ms | Tokens/s = 172,263.7
2025-01-19 06:53:02.537 | DEBUG    | __main__:<module>:313 - Training step 14610: loss = 3.1570 | 3044.39ms | Tokens/s = 172,214.5
2025-01-19 06:53:32.970 | DEBUG    | __main__:<module>:313 - Training step 14620: loss = 3.2608 | 3043.35ms | Tokens/s = 172,273.1
2025-01-19 06:54:03.399 | DEBUG    | __main__:<module>:313 - Training step 14630: loss = 3.0305 | 3042.21ms | Tokens/s = 172,337.9
2025-01-19 06:54:33.831 | DEBUG    | __main__:<module>:313 - Training step 14640: loss = 3.1402 | 3043.74ms | Tokens/s = 172,251.2
2025-01-19 06:55:04.260 | DEBUG    | __main__:<module>:313 - Training step 14650: loss = 3.2329 | 3044.98ms | Tokens/s = 172,181.1
2025-01-19 06:55:34.688 | DEBUG    | __main__:<module>:313 - Training step 14660: loss = 3.1528 | 3042.90ms | Tokens/s = 172,298.9
2025-01-19 06:56:05.114 | DEBUG    | __main__:<module>:313 - Training step 14670: loss = 3.0612 | 3042.06ms | Tokens/s = 172,346.3
2025-01-19 06:56:35.574 | DEBUG    | __main__:<module>:313 - Training step 14680: loss = 3.0315 | 3048.12ms | Tokens/s = 172,003.9
2025-01-19 06:57:06.049 | DEBUG    | __main__:<module>:313 - Training step 14690: loss = 3.2181 | 3047.05ms | Tokens/s = 172,063.9
2025-01-19 06:57:36.508 | DEBUG    | __main__:<module>:313 - Training step 14700: loss = 3.1683 | 3045.54ms | Tokens/s = 172,149.2
2025-01-19 06:58:06.957 | DEBUG    | __main__:<module>:313 - Training step 14710: loss = 3.0901 | 3043.89ms | Tokens/s = 172,242.6
2025-01-19 06:58:37.403 | DEBUG    | __main__:<module>:313 - Training step 14720: loss = 3.1148 | 3044.58ms | Tokens/s = 172,203.5
2025-01-19 06:59:07.843 | DEBUG    | __main__:<module>:313 - Training step 14730: loss = 3.0074 | 3046.29ms | Tokens/s = 172,107.2
2025-01-19 06:59:38.282 | DEBUG    | __main__:<module>:313 - Training step 14740: loss = 3.1067 | 3044.25ms | Tokens/s = 172,222.2
2025-01-19 07:00:08.720 | DEBUG    | __main__:<module>:313 - Training step 14750: loss = 3.0340 | 3044.62ms | Tokens/s = 172,201.4
2025-01-19 07:00:39.163 | DEBUG    | __main__:<module>:313 - Training step 14760: loss = 3.2432 | 3046.31ms | Tokens/s = 172,106.0
2025-01-19 07:01:09.637 | DEBUG    | __main__:<module>:313 - Training step 14770: loss = 3.1534 | 3046.77ms | Tokens/s = 172,080.0
2025-01-19 07:01:40.090 | DEBUG    | __main__:<module>:313 - Training step 14780: loss = 3.3004 | 3045.09ms | Tokens/s = 172,174.7
2025-01-19 07:02:10.546 | DEBUG    | __main__:<module>:313 - Training step 14790: loss = 3.1454 | 3047.00ms | Tokens/s = 172,066.9
2025-01-19 07:02:40.997 | DEBUG    | __main__:<module>:313 - Training step 14800: loss = 3.0410 | 3043.88ms | Tokens/s = 172,243.4
2025-01-19 07:03:11.441 | DEBUG    | __main__:<module>:313 - Training step 14810: loss = 3.2322 | 3044.26ms | Tokens/s = 172,221.9
2025-01-19 07:03:41.878 | DEBUG    | __main__:<module>:313 - Training step 14820: loss = 3.0920 | 3043.76ms | Tokens/s = 172,250.0
2025-01-19 07:04:12.316 | DEBUG    | __main__:<module>:313 - Training step 14830: loss = 3.0997 | 3045.52ms | Tokens/s = 172,150.6
2025-01-19 07:04:42.768 | DEBUG    | __main__:<module>:313 - Training step 14840: loss = 3.1511 | 3042.44ms | Tokens/s = 172,325.1
2025-01-19 07:05:13.198 | DEBUG    | __main__:<module>:313 - Training step 14850: loss = 3.2290 | 3041.05ms | Tokens/s = 172,403.5
2025-01-19 07:05:43.611 | DEBUG    | __main__:<module>:313 - Training step 14860: loss = 3.2511 | 3041.20ms | Tokens/s = 172,395.3
2025-01-19 07:06:14.019 | DEBUG    | __main__:<module>:313 - Training step 14870: loss = 3.1388 | 3042.48ms | Tokens/s = 172,322.6
2025-01-19 07:06:44.466 | DEBUG    | __main__:<module>:313 - Training step 14880: loss = 3.1274 | 3047.90ms | Tokens/s = 172,016.2
2025-01-19 07:07:14.942 | DEBUG    | __main__:<module>:313 - Training step 14890: loss = 3.0208 | 3049.45ms | Tokens/s = 171,928.5
2025-01-19 07:07:45.396 | DEBUG    | __main__:<module>:313 - Training step 14900: loss = 3.3881 | 3044.51ms | Tokens/s = 172,207.6
2025-01-19 07:08:15.826 | DEBUG    | __main__:<module>:313 - Training step 14910: loss = 3.0658 | 3042.18ms | Tokens/s = 172,339.8
2025-01-19 07:08:46.268 | DEBUG    | __main__:<module>:313 - Training step 14920: loss = 3.1061 | 3043.70ms | Tokens/s = 172,253.7
2025-01-19 07:09:16.696 | DEBUG    | __main__:<module>:313 - Training step 14930: loss = 3.0978 | 3041.51ms | Tokens/s = 172,377.3
2025-01-19 07:09:47.118 | DEBUG    | __main__:<module>:313 - Training step 14940: loss = 3.1098 | 3043.88ms | Tokens/s = 172,243.5
2025-01-19 07:10:17.566 | DEBUG    | __main__:<module>:313 - Training step 14950: loss = 2.9531 | 3045.93ms | Tokens/s = 172,127.1
2025-01-19 07:10:48.043 | DEBUG    | __main__:<module>:313 - Training step 14960: loss = 3.1643 | 3046.31ms | Tokens/s = 172,105.6
2025-01-19 07:11:18.494 | DEBUG    | __main__:<module>:313 - Training step 14970: loss = 3.1252 | 3043.57ms | Tokens/s = 172,261.1
2025-01-19 07:11:48.945 | DEBUG    | __main__:<module>:313 - Training step 14980: loss = 3.1933 | 3046.42ms | Tokens/s = 172,100.0
2025-01-19 07:12:19.399 | DEBUG    | __main__:<module>:313 - Training step 14990: loss = 3.1494 | 3044.97ms | Tokens/s = 172,181.5
2025-01-19 07:12:53.238 | INFO     | __main__:<module>:265 - Step 15,000/20,000 loss: 3.1313 (T) 3.1301 (V) | lr=1.8e-03
2025-01-19 07:12:53.239 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 07:13:06.651 | DEBUG    | __main__:<module>:313 - Training step 15000: loss = 3.0627 | 19862.77ms | Tokens/s = 26,395.5
2025-01-19 07:13:36.916 | DEBUG    | __main__:<module>:313 - Training step 15010: loss = 3.1465 | 3036.13ms | Tokens/s = 172,682.7
2025-01-19 07:14:07.311 | DEBUG    | __main__:<module>:313 - Training step 15020: loss = 3.1947 | 3041.72ms | Tokens/s = 172,365.5
2025-01-19 07:14:37.757 | DEBUG    | __main__:<module>:313 - Training step 15030: loss = 3.1153 | 3047.97ms | Tokens/s = 172,012.0
2025-01-19 07:15:08.200 | DEBUG    | __main__:<module>:313 - Training step 15040: loss = 3.1385 | 3043.30ms | Tokens/s = 172,276.1
2025-01-19 07:15:38.628 | DEBUG    | __main__:<module>:313 - Training step 15050: loss = 3.0927 | 3042.68ms | Tokens/s = 172,311.1
2025-01-19 07:16:09.051 | DEBUG    | __main__:<module>:313 - Training step 15060: loss = 3.0542 | 3040.86ms | Tokens/s = 172,414.5
2025-01-19 07:16:39.462 | DEBUG    | __main__:<module>:313 - Training step 15070: loss = 3.0960 | 3040.38ms | Tokens/s = 172,441.5
2025-01-19 07:17:09.881 | DEBUG    | __main__:<module>:313 - Training step 15080: loss = 3.1108 | 3042.51ms | Tokens/s = 172,320.7
2025-01-19 07:17:40.332 | DEBUG    | __main__:<module>:313 - Training step 15090: loss = 3.0399 | 3046.26ms | Tokens/s = 172,108.6
2025-01-19 07:18:10.783 | DEBUG    | __main__:<module>:313 - Training step 15100: loss = 3.1097 | 3044.19ms | Tokens/s = 172,225.6
2025-01-19 07:18:41.210 | DEBUG    | __main__:<module>:313 - Training step 15110: loss = 2.9820 | 3042.48ms | Tokens/s = 172,322.6
2025-01-19 07:19:11.614 | DEBUG    | __main__:<module>:313 - Training step 15120: loss = 3.2471 | 3038.29ms | Tokens/s = 172,560.2
2025-01-19 07:19:42.034 | DEBUG    | __main__:<module>:313 - Training step 15130: loss = 3.1204 | 3043.15ms | Tokens/s = 172,284.6
2025-01-19 07:20:12.496 | DEBUG    | __main__:<module>:313 - Training step 15140: loss = 3.0229 | 3050.38ms | Tokens/s = 171,876.5
2025-01-19 07:20:42.978 | DEBUG    | __main__:<module>:313 - Training step 15150: loss = 2.8764 | 3046.93ms | Tokens/s = 172,071.2
2025-01-19 07:21:13.440 | DEBUG    | __main__:<module>:313 - Training step 15160: loss = 3.0942 | 3045.86ms | Tokens/s = 172,131.5
2025-01-19 07:21:43.887 | DEBUG    | __main__:<module>:313 - Training step 15170: loss = 2.9899 | 3044.70ms | Tokens/s = 172,196.7
2025-01-19 07:22:14.312 | DEBUG    | __main__:<module>:313 - Training step 15180: loss = 3.1690 | 3041.50ms | Tokens/s = 172,378.2
2025-01-19 07:22:44.727 | DEBUG    | __main__:<module>:313 - Training step 15190: loss = 3.1479 | 3040.05ms | Tokens/s = 172,460.4
2025-01-19 07:23:15.126 | DEBUG    | __main__:<module>:313 - Training step 15200: loss = 3.2496 | 3039.47ms | Tokens/s = 172,493.1
2025-01-19 07:23:45.519 | DEBUG    | __main__:<module>:313 - Training step 15210: loss = 2.9303 | 3040.03ms | Tokens/s = 172,461.5
2025-01-19 07:24:15.926 | DEBUG    | __main__:<module>:313 - Training step 15220: loss = 3.0624 | 3041.97ms | Tokens/s = 172,351.2
2025-01-19 07:24:46.367 | DEBUG    | __main__:<module>:313 - Training step 15230: loss = 3.0966 | 3046.80ms | Tokens/s = 172,078.3
2025-01-19 07:25:16.831 | DEBUG    | __main__:<module>:313 - Training step 15240: loss = 3.1863 | 3044.41ms | Tokens/s = 172,213.2
2025-01-19 07:25:47.278 | DEBUG    | __main__:<module>:313 - Training step 15250: loss = 3.2522 | 3044.08ms | Tokens/s = 172,231.9
2025-01-19 07:26:17.712 | DEBUG    | __main__:<module>:313 - Training step 15260: loss = 3.0675 | 3043.34ms | Tokens/s = 172,273.9
2025-01-19 07:26:48.131 | DEBUG    | __main__:<module>:313 - Training step 15270: loss = 3.0773 | 3039.90ms | Tokens/s = 172,468.8
2025-01-19 07:27:18.555 | DEBUG    | __main__:<module>:313 - Training step 15280: loss = 3.0334 | 3045.43ms | Tokens/s = 172,155.5
2025-01-19 07:27:48.994 | DEBUG    | __main__:<module>:313 - Training step 15290: loss = 2.9762 | 3045.32ms | Tokens/s = 172,161.8
2025-01-19 07:28:19.451 | DEBUG    | __main__:<module>:313 - Training step 15300: loss = 3.0934 | 3045.83ms | Tokens/s = 172,133.2
2025-01-19 07:28:49.896 | DEBUG    | __main__:<module>:313 - Training step 15310: loss = 3.1024 | 3045.52ms | Tokens/s = 172,150.8
2025-01-19 07:29:20.329 | DEBUG    | __main__:<module>:313 - Training step 15320: loss = 2.9915 | 3044.86ms | Tokens/s = 172,188.1
2025-01-19 07:29:50.767 | DEBUG    | __main__:<module>:313 - Training step 15330: loss = 3.1859 | 3045.34ms | Tokens/s = 172,160.7
2025-01-19 07:30:21.236 | DEBUG    | __main__:<module>:313 - Training step 15340: loss = 3.1603 | 3047.00ms | Tokens/s = 172,067.1
2025-01-19 07:30:51.697 | DEBUG    | __main__:<module>:313 - Training step 15350: loss = 3.1487 | 3044.60ms | Tokens/s = 172,202.4
2025-01-19 07:31:22.140 | DEBUG    | __main__:<module>:313 - Training step 15360: loss = 3.0297 | 3043.53ms | Tokens/s = 172,263.4
2025-01-19 07:31:52.569 | DEBUG    | __main__:<module>:313 - Training step 15370: loss = 3.1254 | 3041.12ms | Tokens/s = 172,399.7
2025-01-19 07:32:22.987 | DEBUG    | __main__:<module>:313 - Training step 15380: loss = 3.0391 | 3042.10ms | Tokens/s = 172,344.0
2025-01-19 07:32:53.410 | DEBUG    | __main__:<module>:313 - Training step 15390: loss = 3.1389 | 3043.27ms | Tokens/s = 172,277.7
2025-01-19 07:33:23.827 | DEBUG    | __main__:<module>:313 - Training step 15400: loss = 3.0569 | 3039.98ms | Tokens/s = 172,464.5
2025-01-19 07:33:54.250 | DEBUG    | __main__:<module>:313 - Training step 15410: loss = 3.0162 | 3041.57ms | Tokens/s = 172,374.1
2025-01-19 07:34:24.681 | DEBUG    | __main__:<module>:313 - Training step 15420: loss = 2.9183 | 3043.90ms | Tokens/s = 172,242.1
2025-01-19 07:34:55.128 | DEBUG    | __main__:<module>:313 - Training step 15430: loss = 3.1032 | 3044.17ms | Tokens/s = 172,226.8
2025-01-19 07:35:25.569 | DEBUG    | __main__:<module>:313 - Training step 15440: loss = 3.1999 | 3043.97ms | Tokens/s = 172,238.4
2025-01-19 07:35:55.993 | DEBUG    | __main__:<module>:313 - Training step 15450: loss = 3.1963 | 3040.55ms | Tokens/s = 172,431.7
2025-01-19 07:36:26.407 | DEBUG    | __main__:<module>:313 - Training step 15460: loss = 3.1143 | 3042.38ms | Tokens/s = 172,328.0
2025-01-19 07:36:56.828 | DEBUG    | __main__:<module>:313 - Training step 15470: loss = 3.1314 | 3041.52ms | Tokens/s = 172,377.2
2025-01-19 07:37:27.246 | DEBUG    | __main__:<module>:313 - Training step 15480: loss = 3.0902 | 3043.01ms | Tokens/s = 172,292.8
2025-01-19 07:37:57.665 | DEBUG    | __main__:<module>:313 - Training step 15490: loss = 3.1474 | 3044.27ms | Tokens/s = 172,221.3
2025-01-19 07:38:28.122 | DEBUG    | __main__:<module>:313 - Training step 15500: loss = 3.1846 | 3045.85ms | Tokens/s = 172,131.8
2025-01-19 07:38:58.597 | DEBUG    | __main__:<module>:313 - Training step 15510: loss = 3.0958 | 3045.00ms | Tokens/s = 172,180.1
2025-01-19 07:39:29.059 | DEBUG    | __main__:<module>:313 - Training step 15520: loss = 3.1716 | 3046.66ms | Tokens/s = 172,086.2
2025-01-19 07:39:59.501 | DEBUG    | __main__:<module>:313 - Training step 15530: loss = 2.9924 | 3041.35ms | Tokens/s = 172,386.8
2025-01-19 07:40:29.921 | DEBUG    | __main__:<module>:313 - Training step 15540: loss = 3.2978 | 3039.63ms | Tokens/s = 172,484.1
2025-01-19 07:41:00.359 | DEBUG    | __main__:<module>:313 - Training step 15550: loss = 3.1015 | 3045.63ms | Tokens/s = 172,144.4
2025-01-19 07:41:30.816 | DEBUG    | __main__:<module>:313 - Training step 15560: loss = 3.2556 | 3045.11ms | Tokens/s = 172,173.7
2025-01-19 07:42:01.255 | DEBUG    | __main__:<module>:313 - Training step 15570: loss = 3.1617 | 3045.39ms | Tokens/s = 172,158.0
2025-01-19 07:42:31.694 | DEBUG    | __main__:<module>:313 - Training step 15580: loss = 2.9458 | 3041.83ms | Tokens/s = 172,359.4
2025-01-19 07:43:02.118 | DEBUG    | __main__:<module>:313 - Training step 15590: loss = 3.0074 | 3044.39ms | Tokens/s = 172,214.7
2025-01-19 07:43:32.544 | DEBUG    | __main__:<module>:313 - Training step 15600: loss = 2.9291 | 3041.55ms | Tokens/s = 172,375.3
2025-01-19 07:44:02.990 | DEBUG    | __main__:<module>:313 - Training step 15610: loss = 2.9135 | 3046.20ms | Tokens/s = 172,111.9
2025-01-19 07:44:33.467 | DEBUG    | __main__:<module>:313 - Training step 15620: loss = 3.1270 | 3046.37ms | Tokens/s = 172,102.7
2025-01-19 07:45:03.928 | DEBUG    | __main__:<module>:313 - Training step 15630: loss = 2.9248 | 3047.66ms | Tokens/s = 172,029.5
2025-01-19 07:45:34.373 | DEBUG    | __main__:<module>:313 - Training step 15640: loss = 3.1027 | 3044.48ms | Tokens/s = 172,209.5
2025-01-19 07:46:04.818 | DEBUG    | __main__:<module>:313 - Training step 15650: loss = 3.1078 | 3046.36ms | Tokens/s = 172,103.3
2025-01-19 07:46:35.265 | DEBUG    | __main__:<module>:313 - Training step 15660: loss = 3.1127 | 3047.66ms | Tokens/s = 172,029.5
2025-01-19 07:47:05.746 | DEBUG    | __main__:<module>:313 - Training step 15670: loss = 3.0692 | 3048.08ms | Tokens/s = 172,006.0
2025-01-19 07:47:36.213 | DEBUG    | __main__:<module>:313 - Training step 15680: loss = 2.9336 | 3047.50ms | Tokens/s = 172,038.5
2025-01-19 07:48:06.658 | DEBUG    | __main__:<module>:313 - Training step 15690: loss = 3.0701 | 3043.90ms | Tokens/s = 172,242.0
2025-01-19 07:48:37.105 | DEBUG    | __main__:<module>:313 - Training step 15700: loss = 2.9615 | 3045.13ms | Tokens/s = 172,172.9
2025-01-19 07:49:07.577 | DEBUG    | __main__:<module>:313 - Training step 15710: loss = 2.9735 | 3047.26ms | Tokens/s = 172,052.3
2025-01-19 07:49:38.042 | DEBUG    | __main__:<module>:313 - Training step 15720: loss = 3.0667 | 3047.05ms | Tokens/s = 172,064.0
2025-01-19 07:50:08.494 | DEBUG    | __main__:<module>:313 - Training step 15730: loss = 3.0302 | 3045.67ms | Tokens/s = 172,142.1
2025-01-19 07:50:38.930 | DEBUG    | __main__:<module>:313 - Training step 15740: loss = 3.1402 | 3044.71ms | Tokens/s = 172,196.4
2025-01-19 07:51:09.362 | DEBUG    | __main__:<module>:313 - Training step 15750: loss = 3.0518 | 3043.25ms | Tokens/s = 172,278.9
2025-01-19 07:51:39.797 | DEBUG    | __main__:<module>:313 - Training step 15760: loss = 2.9967 | 3043.77ms | Tokens/s = 172,249.3
2025-01-19 07:52:10.235 | DEBUG    | __main__:<module>:313 - Training step 15770: loss = 3.0929 | 3044.73ms | Tokens/s = 172,195.0
2025-01-19 07:52:40.673 | DEBUG    | __main__:<module>:313 - Training step 15780: loss = 3.1458 | 3043.11ms | Tokens/s = 172,287.2
2025-01-19 07:53:11.103 | DEBUG    | __main__:<module>:313 - Training step 15790: loss = 3.0394 | 3041.92ms | Tokens/s = 172,354.5
2025-01-19 07:53:41.520 | DEBUG    | __main__:<module>:313 - Training step 15800: loss = 3.0225 | 3039.90ms | Tokens/s = 172,468.9
2025-01-19 07:54:11.940 | DEBUG    | __main__:<module>:313 - Training step 15810: loss = 3.0430 | 3042.29ms | Tokens/s = 172,333.6
2025-01-19 07:54:42.372 | DEBUG    | __main__:<module>:313 - Training step 15820: loss = 3.0035 | 3041.60ms | Tokens/s = 172,372.6
2025-01-19 07:55:12.815 | DEBUG    | __main__:<module>:313 - Training step 15830: loss = 3.1166 | 3046.67ms | Tokens/s = 172,085.7
2025-01-19 07:55:43.277 | DEBUG    | __main__:<module>:313 - Training step 15840: loss = 3.2011 | 3045.96ms | Tokens/s = 172,125.5
2025-01-19 07:56:13.734 | DEBUG    | __main__:<module>:313 - Training step 15850: loss = 3.0226 | 3044.10ms | Tokens/s = 172,230.8
2025-01-19 07:56:44.207 | DEBUG    | __main__:<module>:313 - Training step 15860: loss = 3.2422 | 3048.10ms | Tokens/s = 172,005.1
2025-01-19 07:57:14.688 | DEBUG    | __main__:<module>:313 - Training step 15870: loss = 3.1308 | 3047.69ms | Tokens/s = 172,028.2
2025-01-19 07:57:45.157 | DEBUG    | __main__:<module>:313 - Training step 15880: loss = 2.9199 | 3046.78ms | Tokens/s = 172,079.6
2025-01-19 07:58:15.612 | DEBUG    | __main__:<module>:313 - Training step 15890: loss = 2.9648 | 3045.67ms | Tokens/s = 172,142.0
2025-01-19 07:58:46.060 | DEBUG    | __main__:<module>:313 - Training step 15900: loss = 3.2380 | 3044.03ms | Tokens/s = 172,234.8
2025-01-19 07:59:16.499 | DEBUG    | __main__:<module>:313 - Training step 15910: loss = 3.0182 | 3043.79ms | Tokens/s = 172,248.5
2025-01-19 07:59:46.933 | DEBUG    | __main__:<module>:313 - Training step 15920: loss = 3.0471 | 3044.75ms | Tokens/s = 172,194.2
2025-01-19 08:00:17.382 | DEBUG    | __main__:<module>:313 - Training step 15930: loss = 3.2280 | 3046.49ms | Tokens/s = 172,095.9
2025-01-19 08:00:47.855 | DEBUG    | __main__:<module>:313 - Training step 15940: loss = 3.1195 | 3045.87ms | Tokens/s = 172,131.0
2025-01-19 08:01:18.297 | DEBUG    | __main__:<module>:313 - Training step 15950: loss = 2.8974 | 3041.14ms | Tokens/s = 172,398.8
2025-01-19 08:01:48.730 | DEBUG    | __main__:<module>:313 - Training step 15960: loss = 3.0647 | 3041.08ms | Tokens/s = 172,401.7
2025-01-19 08:02:19.161 | DEBUG    | __main__:<module>:313 - Training step 15970: loss = 3.0015 | 3044.00ms | Tokens/s = 172,236.6
2025-01-19 08:02:49.607 | DEBUG    | __main__:<module>:313 - Training step 15980: loss = 3.0728 | 3047.03ms | Tokens/s = 172,065.1
2025-01-19 08:03:20.076 | DEBUG    | __main__:<module>:313 - Training step 15990: loss = 3.0514 | 3047.07ms | Tokens/s = 172,062.8
2025-01-19 08:03:53.942 | INFO     | __main__:<module>:265 - Step 16,000/20,000 loss: 3.0723 (T) 3.0888 (V) | lr=1.2e-03
2025-01-19 08:03:53.943 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 08:04:07.229 | DEBUG    | __main__:<module>:313 - Training step 16000: loss = 3.1885 | 19741.93ms | Tokens/s = 26,557.1
2025-01-19 08:04:37.518 | DEBUG    | __main__:<module>:313 - Training step 16010: loss = 3.1647 | 3038.35ms | Tokens/s = 172,556.7
2025-01-19 08:05:07.931 | DEBUG    | __main__:<module>:313 - Training step 16020: loss = 3.0839 | 3043.65ms | Tokens/s = 172,256.3
2025-01-19 08:05:38.384 | DEBUG    | __main__:<module>:313 - Training step 16030: loss = 3.0390 | 3045.40ms | Tokens/s = 172,157.5
2025-01-19 08:06:08.829 | DEBUG    | __main__:<module>:313 - Training step 16040: loss = 3.0635 | 3042.26ms | Tokens/s = 172,335.3
2025-01-19 08:06:39.264 | DEBUG    | __main__:<module>:313 - Training step 16050: loss = 3.1804 | 3043.64ms | Tokens/s = 172,257.0
2025-01-19 08:07:09.686 | DEBUG    | __main__:<module>:313 - Training step 16060: loss = 3.1369 | 3042.63ms | Tokens/s = 172,314.2
2025-01-19 08:07:40.114 | DEBUG    | __main__:<module>:313 - Training step 16070: loss = 3.0649 | 3042.81ms | Tokens/s = 172,303.7
2025-01-19 08:08:10.549 | DEBUG    | __main__:<module>:313 - Training step 16080: loss = 3.1608 | 3042.18ms | Tokens/s = 172,339.6
2025-01-19 08:08:40.988 | DEBUG    | __main__:<module>:313 - Training step 16090: loss = 3.2443 | 3042.96ms | Tokens/s = 172,295.7
2025-01-19 08:09:11.415 | DEBUG    | __main__:<module>:313 - Training step 16100: loss = 3.0542 | 3040.54ms | Tokens/s = 172,432.5
2025-01-19 08:09:41.848 | DEBUG    | __main__:<module>:313 - Training step 16110: loss = 3.1065 | 3043.03ms | Tokens/s = 172,291.3
2025-01-19 08:10:12.308 | DEBUG    | __main__:<module>:313 - Training step 16120: loss = 3.1133 | 3046.55ms | Tokens/s = 172,092.5
2025-01-19 08:10:42.765 | DEBUG    | __main__:<module>:313 - Training step 16130: loss = 2.9470 | 3043.74ms | Tokens/s = 172,251.1
2025-01-19 08:11:13.219 | DEBUG    | __main__:<module>:313 - Training step 16140: loss = 3.1578 | 3048.54ms | Tokens/s = 171,980.2
2025-01-19 08:11:43.678 | DEBUG    | __main__:<module>:313 - Training step 16150: loss = 2.8832 | 3044.65ms | Tokens/s = 172,200.0
2025-01-19 08:12:14.126 | DEBUG    | __main__:<module>:313 - Training step 16160: loss = 2.8451 | 3046.72ms | Tokens/s = 172,082.5
2025-01-19 08:12:44.566 | DEBUG    | __main__:<module>:313 - Training step 16170: loss = 3.0148 | 3042.07ms | Tokens/s = 172,345.6
2025-01-19 08:13:15.022 | DEBUG    | __main__:<module>:313 - Training step 16180: loss = 2.8675 | 3043.36ms | Tokens/s = 172,272.8
2025-01-19 08:13:45.458 | DEBUG    | __main__:<module>:313 - Training step 16190: loss = 3.1476 | 3042.73ms | Tokens/s = 172,308.6
2025-01-19 08:14:15.879 | DEBUG    | __main__:<module>:313 - Training step 16200: loss = 3.1264 | 3042.23ms | Tokens/s = 172,336.8
2025-01-19 08:14:46.302 | DEBUG    | __main__:<module>:313 - Training step 16210: loss = 3.0505 | 3042.79ms | Tokens/s = 172,305.2
2025-01-19 08:15:16.722 | DEBUG    | __main__:<module>:313 - Training step 16220: loss = 3.0760 | 3040.12ms | Tokens/s = 172,456.4
2025-01-19 08:15:47.166 | DEBUG    | __main__:<module>:313 - Training step 16230: loss = 3.0463 | 3045.69ms | Tokens/s = 172,140.9
2025-01-19 08:16:17.646 | DEBUG    | __main__:<module>:313 - Training step 16240: loss = 3.0391 | 3048.93ms | Tokens/s = 171,957.8
2025-01-19 08:16:48.110 | DEBUG    | __main__:<module>:313 - Training step 16250: loss = 3.1098 | 3046.60ms | Tokens/s = 172,089.6
2025-01-19 08:17:18.558 | DEBUG    | __main__:<module>:313 - Training step 16260: loss = 3.1177 | 3043.51ms | Tokens/s = 172,264.4
2025-01-19 08:17:48.999 | DEBUG    | __main__:<module>:313 - Training step 16270: loss = 2.9869 | 3043.71ms | Tokens/s = 172,252.9
2025-01-19 08:18:19.440 | DEBUG    | __main__:<module>:313 - Training step 16280: loss = 3.0068 | 3044.70ms | Tokens/s = 172,196.8
2025-01-19 08:18:49.887 | DEBUG    | __main__:<module>:313 - Training step 16290: loss = 3.1366 | 3043.71ms | Tokens/s = 172,253.1
2025-01-19 08:19:20.315 | DEBUG    | __main__:<module>:313 - Training step 16300: loss = 3.0113 | 3040.64ms | Tokens/s = 172,426.8
2025-01-19 08:19:50.737 | DEBUG    | __main__:<module>:313 - Training step 16310: loss = 3.1442 | 3043.30ms | Tokens/s = 172,276.0
2025-01-19 08:20:21.165 | DEBUG    | __main__:<module>:313 - Training step 16320: loss = 3.0755 | 3043.16ms | Tokens/s = 172,284.0
2025-01-19 08:20:51.598 | DEBUG    | __main__:<module>:313 - Training step 16330: loss = 3.1157 | 3041.59ms | Tokens/s = 172,373.1
2025-01-19 08:21:22.019 | DEBUG    | __main__:<module>:313 - Training step 16340: loss = 2.9475 | 3041.02ms | Tokens/s = 172,405.4
2025-01-19 08:21:52.445 | DEBUG    | __main__:<module>:313 - Training step 16350: loss = 3.0409 | 3044.00ms | Tokens/s = 172,236.3
2025-01-19 08:22:22.879 | DEBUG    | __main__:<module>:313 - Training step 16360: loss = 3.1956 | 3043.09ms | Tokens/s = 172,288.3
2025-01-19 08:22:53.300 | DEBUG    | __main__:<module>:313 - Training step 16370: loss = 2.8129 | 3042.75ms | Tokens/s = 172,307.4
2025-01-19 08:23:23.721 | DEBUG    | __main__:<module>:313 - Training step 16380: loss = 2.8335 | 3042.97ms | Tokens/s = 172,294.6
2025-01-19 08:23:54.168 | DEBUG    | __main__:<module>:313 - Training step 16390: loss = 3.0419 | 3044.62ms | Tokens/s = 172,201.3
2025-01-19 08:24:24.647 | DEBUG    | __main__:<module>:313 - Training step 16400: loss = 2.9268 | 3047.82ms | Tokens/s = 172,020.5
2025-01-19 08:24:55.122 | DEBUG    | __main__:<module>:313 - Training step 16410: loss = 3.0738 | 3046.77ms | Tokens/s = 172,079.9
2025-01-19 08:25:25.586 | DEBUG    | __main__:<module>:313 - Training step 16420: loss = 2.9570 | 3048.07ms | Tokens/s = 172,006.7
2025-01-19 08:25:56.061 | DEBUG    | __main__:<module>:313 - Training step 16430: loss = 3.2331 | 3045.09ms | Tokens/s = 172,174.6
2025-01-19 08:26:26.517 | DEBUG    | __main__:<module>:313 - Training step 16440: loss = 2.9611 | 3045.18ms | Tokens/s = 172,169.7
2025-01-19 08:26:56.962 | DEBUG    | __main__:<module>:313 - Training step 16450: loss = 3.1003 | 3041.89ms | Tokens/s = 172,356.3
2025-01-19 08:27:27.406 | DEBUG    | __main__:<module>:313 - Training step 16460: loss = 2.9079 | 3043.36ms | Tokens/s = 172,272.8
2025-01-19 08:27:57.850 | DEBUG    | __main__:<module>:313 - Training step 16470: loss = 3.2417 | 3045.53ms | Tokens/s = 172,150.2
2025-01-19 08:28:28.294 | DEBUG    | __main__:<module>:313 - Training step 16480: loss = 2.9948 | 3043.75ms | Tokens/s = 172,250.5
2025-01-19 08:28:58.735 | DEBUG    | __main__:<module>:313 - Training step 16490: loss = 2.9616 | 3041.71ms | Tokens/s = 172,366.3
2025-01-19 08:29:29.164 | DEBUG    | __main__:<module>:313 - Training step 16500: loss = 3.1304 | 3041.31ms | Tokens/s = 172,388.7
2025-01-19 08:29:59.595 | DEBUG    | __main__:<module>:313 - Training step 16510: loss = 2.9525 | 3045.61ms | Tokens/s = 172,145.6
2025-01-19 08:30:30.037 | DEBUG    | __main__:<module>:313 - Training step 16520: loss = 3.0357 | 3044.15ms | Tokens/s = 172,228.2
2025-01-19 08:31:00.519 | DEBUG    | __main__:<module>:313 - Training step 16530: loss = 3.0920 | 3049.07ms | Tokens/s = 171,950.1
2025-01-19 08:31:30.995 | DEBUG    | __main__:<module>:313 - Training step 16540: loss = 2.9674 | 3046.34ms | Tokens/s = 172,104.3
2025-01-19 08:32:01.440 | DEBUG    | __main__:<module>:313 - Training step 16550: loss = 2.8926 | 3044.56ms | Tokens/s = 172,205.1
2025-01-19 08:32:31.882 | DEBUG    | __main__:<module>:313 - Training step 16560: loss = 3.0608 | 3044.51ms | Tokens/s = 172,207.7
2025-01-19 08:33:02.321 | DEBUG    | __main__:<module>:313 - Training step 16570: loss = 3.0354 | 3044.91ms | Tokens/s = 172,184.9
2025-01-19 08:33:32.764 | DEBUG    | __main__:<module>:313 - Training step 16580: loss = 3.0838 | 3044.45ms | Tokens/s = 172,210.9
2025-01-19 08:34:03.200 | DEBUG    | __main__:<module>:313 - Training step 16590: loss = 3.0623 | 3041.58ms | Tokens/s = 172,373.6
2025-01-19 08:34:33.643 | DEBUG    | __main__:<module>:313 - Training step 16600: loss = 3.0419 | 3044.35ms | Tokens/s = 172,216.9
2025-01-19 08:35:04.118 | DEBUG    | __main__:<module>:313 - Training step 16610: loss = 3.0087 | 3048.74ms | Tokens/s = 171,968.8
2025-01-19 08:35:34.581 | DEBUG    | __main__:<module>:313 - Training step 16620: loss = 3.2826 | 3046.19ms | Tokens/s = 172,112.8
2025-01-19 08:36:05.034 | DEBUG    | __main__:<module>:313 - Training step 16630: loss = 3.1483 | 3043.84ms | Tokens/s = 172,245.6
2025-01-19 08:36:35.492 | DEBUG    | __main__:<module>:313 - Training step 16640: loss = 3.0901 | 3047.33ms | Tokens/s = 172,048.4
2025-01-19 08:37:05.944 | DEBUG    | __main__:<module>:313 - Training step 16650: loss = 3.0210 | 3044.69ms | Tokens/s = 172,197.7
2025-01-19 08:37:36.384 | DEBUG    | __main__:<module>:313 - Training step 16660: loss = 3.0918 | 3043.55ms | Tokens/s = 172,262.1
2025-01-19 08:38:06.820 | DEBUG    | __main__:<module>:313 - Training step 16670: loss = 2.8418 | 3043.73ms | Tokens/s = 172,251.7
2025-01-19 08:38:37.256 | DEBUG    | __main__:<module>:313 - Training step 16680: loss = 3.0466 | 3046.33ms | Tokens/s = 172,104.7
2025-01-19 08:39:07.697 | DEBUG    | __main__:<module>:313 - Training step 16690: loss = 3.0976 | 3045.43ms | Tokens/s = 172,155.9
2025-01-19 08:39:38.131 | DEBUG    | __main__:<module>:313 - Training step 16700: loss = 3.0537 | 3044.44ms | Tokens/s = 172,211.7
2025-01-19 08:40:08.554 | DEBUG    | __main__:<module>:313 - Training step 16710: loss = 3.2408 | 3042.63ms | Tokens/s = 172,314.2
2025-01-19 08:40:38.968 | DEBUG    | __main__:<module>:313 - Training step 16720: loss = 2.9828 | 3040.11ms | Tokens/s = 172,457.1
2025-01-19 08:41:09.386 | DEBUG    | __main__:<module>:313 - Training step 16730: loss = 3.0537 | 3044.11ms | Tokens/s = 172,230.2
2025-01-19 08:41:39.816 | DEBUG    | __main__:<module>:313 - Training step 16740: loss = 2.9484 | 3042.02ms | Tokens/s = 172,348.7
2025-01-19 08:42:10.270 | DEBUG    | __main__:<module>:313 - Training step 16750: loss = 3.0379 | 3046.92ms | Tokens/s = 172,071.5
2025-01-19 08:42:40.717 | DEBUG    | __main__:<module>:313 - Training step 16760: loss = 3.0813 | 3045.21ms | Tokens/s = 172,168.3
2025-01-19 08:43:11.157 | DEBUG    | __main__:<module>:313 - Training step 16770: loss = 2.9881 | 3043.89ms | Tokens/s = 172,242.6
2025-01-19 08:43:41.590 | DEBUG    | __main__:<module>:313 - Training step 16780: loss = 2.8704 | 3044.20ms | Tokens/s = 172,225.0
2025-01-19 08:44:12.019 | DEBUG    | __main__:<module>:313 - Training step 16790: loss = 2.9952 | 3041.48ms | Tokens/s = 172,379.0
2025-01-19 08:44:42.477 | DEBUG    | __main__:<module>:313 - Training step 16800: loss = 3.0370 | 3045.93ms | Tokens/s = 172,127.7
2025-01-19 08:45:12.949 | DEBUG    | __main__:<module>:313 - Training step 16810: loss = 2.9144 | 3046.35ms | Tokens/s = 172,103.5
2025-01-19 08:45:43.406 | DEBUG    | __main__:<module>:313 - Training step 16820: loss = 2.9147 | 3044.57ms | Tokens/s = 172,204.0
2025-01-19 08:46:13.843 | DEBUG    | __main__:<module>:313 - Training step 16830: loss = 2.9386 | 3042.50ms | Tokens/s = 172,321.3
2025-01-19 08:46:44.271 | DEBUG    | __main__:<module>:313 - Training step 16840: loss = 3.1618 | 3044.31ms | Tokens/s = 172,219.2
2025-01-19 08:47:14.716 | DEBUG    | __main__:<module>:313 - Training step 16850: loss = 3.0859 | 3044.07ms | Tokens/s = 172,232.3
2025-01-19 08:47:45.155 | DEBUG    | __main__:<module>:313 - Training step 16860: loss = 3.1489 | 3043.69ms | Tokens/s = 172,253.8
2025-01-19 08:48:15.614 | DEBUG    | __main__:<module>:313 - Training step 16870: loss = 3.0038 | 3045.09ms | Tokens/s = 172,174.9
2025-01-19 08:48:46.052 | DEBUG    | __main__:<module>:313 - Training step 16880: loss = 2.9138 | 3044.42ms | Tokens/s = 172,212.8
2025-01-19 08:49:16.469 | DEBUG    | __main__:<module>:313 - Training step 16890: loss = 3.0144 | 3040.52ms | Tokens/s = 172,433.8
2025-01-19 08:49:46.877 | DEBUG    | __main__:<module>:313 - Training step 16900: loss = 2.9029 | 3041.09ms | Tokens/s = 172,401.5
2025-01-19 08:50:17.282 | DEBUG    | __main__:<module>:313 - Training step 16910: loss = 3.2025 | 3040.85ms | Tokens/s = 172,415.2
2025-01-19 08:50:47.720 | DEBUG    | __main__:<module>:313 - Training step 16920: loss = 2.8901 | 3045.11ms | Tokens/s = 172,173.8
2025-01-19 08:51:18.180 | DEBUG    | __main__:<module>:313 - Training step 16930: loss = 3.1244 | 3048.26ms | Tokens/s = 171,995.8
2025-01-19 08:51:48.638 | DEBUG    | __main__:<module>:313 - Training step 16940: loss = 2.9560 | 3043.79ms | Tokens/s = 172,248.6
2025-01-19 08:52:19.103 | DEBUG    | __main__:<module>:313 - Training step 16950: loss = 2.8702 | 3046.35ms | Tokens/s = 172,103.9
2025-01-19 08:52:49.559 | DEBUG    | __main__:<module>:313 - Training step 16960: loss = 3.0027 | 3043.93ms | Tokens/s = 172,240.2
2025-01-19 08:53:19.992 | DEBUG    | __main__:<module>:313 - Training step 16970: loss = 2.9799 | 3042.61ms | Tokens/s = 172,315.2
2025-01-19 08:53:50.409 | DEBUG    | __main__:<module>:313 - Training step 16980: loss = 3.0007 | 3042.37ms | Tokens/s = 172,328.6
2025-01-19 08:54:20.818 | DEBUG    | __main__:<module>:313 - Training step 16990: loss = 3.0398 | 3039.72ms | Tokens/s = 172,478.9
2025-01-19 08:54:54.664 | INFO     | __main__:<module>:265 - Step 17,000/20,000 loss: 3.0424 (T) 3.0366 (V) | lr=6.7e-04
2025-01-19 08:54:54.665 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 08:55:08.228 | DEBUG    | __main__:<module>:313 - Training step 17000: loss = 2.9957 | 20016.43ms | Tokens/s = 26,192.9
2025-01-19 08:55:38.519 | DEBUG    | __main__:<module>:313 - Training step 17010: loss = 3.1498 | 3036.21ms | Tokens/s = 172,678.2
2025-01-19 08:56:08.916 | DEBUG    | __main__:<module>:313 - Training step 17020: loss = 3.0653 | 3039.94ms | Tokens/s = 172,466.8
2025-01-19 08:56:39.349 | DEBUG    | __main__:<module>:313 - Training step 17030: loss = 3.2449 | 3044.07ms | Tokens/s = 172,232.8
2025-01-19 08:57:09.810 | DEBUG    | __main__:<module>:313 - Training step 17040: loss = 3.0331 | 3046.92ms | Tokens/s = 172,071.7
2025-01-19 08:57:40.246 | DEBUG    | __main__:<module>:313 - Training step 17050: loss = 3.2489 | 3043.93ms | Tokens/s = 172,240.3
2025-01-19 08:58:10.659 | DEBUG    | __main__:<module>:313 - Training step 17060: loss = 3.1088 | 3041.77ms | Tokens/s = 172,362.9
2025-01-19 08:58:41.065 | DEBUG    | __main__:<module>:313 - Training step 17070: loss = 2.8944 | 3039.63ms | Tokens/s = 172,484.1
2025-01-19 08:59:11.485 | DEBUG    | __main__:<module>:313 - Training step 17080: loss = 2.9313 | 3042.69ms | Tokens/s = 172,310.8
2025-01-19 08:59:41.937 | DEBUG    | __main__:<module>:313 - Training step 17090: loss = 3.0109 | 3045.95ms | Tokens/s = 172,126.2
2025-01-19 09:00:12.382 | DEBUG    | __main__:<module>:313 - Training step 17100: loss = 3.0147 | 3041.59ms | Tokens/s = 172,373.2
2025-01-19 09:00:42.799 | DEBUG    | __main__:<module>:313 - Training step 17110: loss = 2.9498 | 3040.66ms | Tokens/s = 172,425.9
2025-01-19 09:01:13.228 | DEBUG    | __main__:<module>:313 - Training step 17120: loss = 2.9505 | 3045.59ms | Tokens/s = 172,146.4
2025-01-19 09:01:43.691 | DEBUG    | __main__:<module>:313 - Training step 17130: loss = 2.9126 | 3047.93ms | Tokens/s = 172,014.5
2025-01-19 09:02:14.170 | DEBUG    | __main__:<module>:313 - Training step 17140: loss = 3.1609 | 3047.75ms | Tokens/s = 172,024.4
2025-01-19 09:02:44.635 | DEBUG    | __main__:<module>:313 - Training step 17150: loss = 2.9693 | 3044.76ms | Tokens/s = 172,193.3
2025-01-19 09:03:15.073 | DEBUG    | __main__:<module>:313 - Training step 17160: loss = 2.9609 | 3042.86ms | Tokens/s = 172,301.1
2025-01-19 09:03:45.493 | DEBUG    | __main__:<module>:313 - Training step 17170: loss = 2.9884 | 3041.27ms | Tokens/s = 172,391.2
2025-01-19 09:04:15.925 | DEBUG    | __main__:<module>:313 - Training step 17180: loss = 2.9967 | 3043.76ms | Tokens/s = 172,250.2
2025-01-19 09:04:46.362 | DEBUG    | __main__:<module>:313 - Training step 17190: loss = 2.9776 | 3041.32ms | Tokens/s = 172,388.2
2025-01-19 09:05:16.782 | DEBUG    | __main__:<module>:313 - Training step 17200: loss = 3.1897 | 3040.48ms | Tokens/s = 172,435.7
2025-01-19 09:05:47.236 | DEBUG    | __main__:<module>:313 - Training step 17210: loss = 3.0998 | 3045.57ms | Tokens/s = 172,147.8
2025-01-19 09:06:17.701 | DEBUG    | __main__:<module>:313 - Training step 17220: loss = 3.0483 | 3046.06ms | Tokens/s = 172,120.0
2025-01-19 09:06:48.141 | DEBUG    | __main__:<module>:313 - Training step 17230: loss = 3.1854 | 3042.53ms | Tokens/s = 172,319.8
2025-01-19 09:07:18.593 | DEBUG    | __main__:<module>:313 - Training step 17240: loss = 3.0375 | 3046.81ms | Tokens/s = 172,077.8
2025-01-19 09:07:49.038 | DEBUG    | __main__:<module>:313 - Training step 17250: loss = 2.9155 | 3043.60ms | Tokens/s = 172,259.4
2025-01-19 09:08:19.461 | DEBUG    | __main__:<module>:313 - Training step 17260: loss = 2.9007 | 3041.36ms | Tokens/s = 172,385.8
2025-01-19 09:08:49.878 | DEBUG    | __main__:<module>:313 - Training step 17270: loss = 3.0709 | 3041.50ms | Tokens/s = 172,378.4
2025-01-19 09:09:20.316 | DEBUG    | __main__:<module>:313 - Training step 17280: loss = 2.9782 | 3043.90ms | Tokens/s = 172,242.3
2025-01-19 09:09:50.753 | DEBUG    | __main__:<module>:313 - Training step 17290: loss = 3.0819 | 3039.52ms | Tokens/s = 172,490.3
2025-01-19 09:10:21.171 | DEBUG    | __main__:<module>:313 - Training step 17300: loss = 2.9420 | 3039.64ms | Tokens/s = 172,483.6
2025-01-19 09:10:51.608 | DEBUG    | __main__:<module>:313 - Training step 17310: loss = 2.9549 | 3045.34ms | Tokens/s = 172,160.7
2025-01-19 09:11:22.059 | DEBUG    | __main__:<module>:313 - Training step 17320: loss = 2.9923 | 3044.85ms | Tokens/s = 172,188.6
2025-01-19 09:11:52.527 | DEBUG    | __main__:<module>:313 - Training step 17330: loss = 2.8474 | 3047.36ms | Tokens/s = 172,046.6
2025-01-19 09:12:22.976 | DEBUG    | __main__:<module>:313 - Training step 17340: loss = 2.9425 | 3042.28ms | Tokens/s = 172,333.9
2025-01-19 09:12:53.407 | DEBUG    | __main__:<module>:313 - Training step 17350: loss = 3.0614 | 3043.46ms | Tokens/s = 172,267.0
2025-01-19 09:13:23.862 | DEBUG    | __main__:<module>:313 - Training step 17360: loss = 3.0472 | 3047.48ms | Tokens/s = 172,039.9
2025-01-19 09:13:54.328 | DEBUG    | __main__:<module>:313 - Training step 17370: loss = 3.0367 | 3045.29ms | Tokens/s = 172,163.5
2025-01-19 09:14:24.770 | DEBUG    | __main__:<module>:313 - Training step 17380: loss = 3.0175 | 3041.33ms | Tokens/s = 172,387.8
2025-01-19 09:14:55.183 | DEBUG    | __main__:<module>:313 - Training step 17390: loss = 2.9055 | 3041.29ms | Tokens/s = 172,390.2
2025-01-19 09:15:25.594 | DEBUG    | __main__:<module>:313 - Training step 17400: loss = 2.9135 | 3039.99ms | Tokens/s = 172,463.6
2025-01-19 09:15:56.031 | DEBUG    | __main__:<module>:313 - Training step 17410: loss = 2.8618 | 3045.43ms | Tokens/s = 172,155.8
2025-01-19 09:16:26.484 | DEBUG    | __main__:<module>:313 - Training step 17420: loss = 3.1329 | 3043.63ms | Tokens/s = 172,257.6
2025-01-19 09:16:56.910 | DEBUG    | __main__:<module>:313 - Training step 17430: loss = 2.9771 | 3042.60ms | Tokens/s = 172,315.8
2025-01-19 09:17:27.320 | DEBUG    | __main__:<module>:313 - Training step 17440: loss = 3.1049 | 3041.56ms | Tokens/s = 172,374.7
2025-01-19 09:17:57.739 | DEBUG    | __main__:<module>:313 - Training step 17450: loss = 3.0314 | 3043.32ms | Tokens/s = 172,275.1
2025-01-19 09:18:28.194 | DEBUG    | __main__:<module>:313 - Training step 17460: loss = 2.9347 | 3047.95ms | Tokens/s = 172,013.4
2025-01-19 09:18:58.655 | DEBUG    | __main__:<module>:313 - Training step 17470: loss = 3.1869 | 3046.11ms | Tokens/s = 172,117.1
2025-01-19 09:19:29.094 | DEBUG    | __main__:<module>:313 - Training step 17480: loss = 3.0307 | 3041.76ms | Tokens/s = 172,363.4
2025-01-19 09:19:59.513 | DEBUG    | __main__:<module>:313 - Training step 17490: loss = 2.8622 | 3040.16ms | Tokens/s = 172,453.8
2025-01-19 09:20:29.954 | DEBUG    | __main__:<module>:313 - Training step 17500: loss = 2.8358 | 3045.75ms | Tokens/s = 172,137.4
2025-01-19 09:21:00.409 | DEBUG    | __main__:<module>:313 - Training step 17510: loss = 2.7083 | 3047.16ms | Tokens/s = 172,058.0
2025-01-19 09:21:30.863 | DEBUG    | __main__:<module>:313 - Training step 17520: loss = 2.9884 | 3044.28ms | Tokens/s = 172,220.6
2025-01-19 09:22:01.302 | DEBUG    | __main__:<module>:313 - Training step 17530: loss = 3.0044 | 3041.80ms | Tokens/s = 172,361.2
2025-01-19 09:22:31.721 | DEBUG    | __main__:<module>:313 - Training step 17540: loss = 3.0928 | 3042.06ms | Tokens/s = 172,346.4
2025-01-19 09:23:02.160 | DEBUG    | __main__:<module>:313 - Training step 17550: loss = 3.0945 | 3045.06ms | Tokens/s = 172,176.8
2025-01-19 09:23:32.609 | DEBUG    | __main__:<module>:313 - Training step 17560: loss = 2.8251 | 3045.16ms | Tokens/s = 172,170.8
2025-01-19 09:24:03.022 | DEBUG    | __main__:<module>:313 - Training step 17570: loss = 3.0550 | 3040.92ms | Tokens/s = 172,411.2
2025-01-19 09:24:33.427 | DEBUG    | __main__:<module>:313 - Training step 17580: loss = 3.2019 | 3038.40ms | Tokens/s = 172,554.1
2025-01-19 09:25:03.843 | DEBUG    | __main__:<module>:313 - Training step 17590: loss = 3.1016 | 3041.57ms | Tokens/s = 172,374.4
2025-01-19 09:25:34.291 | DEBUG    | __main__:<module>:313 - Training step 17600: loss = 3.1467 | 3043.75ms | Tokens/s = 172,250.8
2025-01-19 09:26:04.730 | DEBUG    | __main__:<module>:313 - Training step 17610: loss = 2.9647 | 3045.55ms | Tokens/s = 172,149.1
2025-01-19 09:26:35.175 | DEBUG    | __main__:<module>:313 - Training step 17620: loss = 3.1282 | 3042.99ms | Tokens/s = 172,293.8
2025-01-19 09:27:05.598 | DEBUG    | __main__:<module>:313 - Training step 17630: loss = 2.9558 | 3041.37ms | Tokens/s = 172,385.2
2025-01-19 09:27:36.020 | DEBUG    | __main__:<module>:313 - Training step 17640: loss = 3.1823 | 3040.62ms | Tokens/s = 172,428.0
2025-01-19 09:28:06.443 | DEBUG    | __main__:<module>:313 - Training step 17650: loss = 2.9675 | 3042.38ms | Tokens/s = 172,328.1
2025-01-19 09:28:36.886 | DEBUG    | __main__:<module>:313 - Training step 17660: loss = 3.1056 | 3044.31ms | Tokens/s = 172,219.2
2025-01-19 09:29:07.347 | DEBUG    | __main__:<module>:313 - Training step 17670: loss = 3.0360 | 3044.60ms | Tokens/s = 172,202.6
2025-01-19 09:29:37.789 | DEBUG    | __main__:<module>:313 - Training step 17680: loss = 2.9864 | 3040.45ms | Tokens/s = 172,437.6
2025-01-19 09:30:08.209 | DEBUG    | __main__:<module>:313 - Training step 17690: loss = 3.0923 | 3040.79ms | Tokens/s = 172,418.5
2025-01-19 09:30:38.609 | DEBUG    | __main__:<module>:313 - Training step 17700: loss = 2.9353 | 3038.58ms | Tokens/s = 172,543.9
2025-01-19 09:31:09.033 | DEBUG    | __main__:<module>:313 - Training step 17710: loss = 2.8965 | 3044.50ms | Tokens/s = 172,208.2
2025-01-19 09:31:39.481 | DEBUG    | __main__:<module>:313 - Training step 17720: loss = 3.0534 | 3045.28ms | Tokens/s = 172,164.3
2025-01-19 09:32:09.942 | DEBUG    | __main__:<module>:313 - Training step 17730: loss = 3.0342 | 3045.24ms | Tokens/s = 172,166.5
2025-01-19 09:32:40.394 | DEBUG    | __main__:<module>:313 - Training step 17740: loss = 2.9380 | 3044.96ms | Tokens/s = 172,182.0
2025-01-19 09:33:10.826 | DEBUG    | __main__:<module>:313 - Training step 17750: loss = 3.1242 | 3042.72ms | Tokens/s = 172,308.7
2025-01-19 09:33:41.238 | DEBUG    | __main__:<module>:313 - Training step 17760: loss = 3.1687 | 3040.40ms | Tokens/s = 172,440.3
2025-01-19 09:34:11.665 | DEBUG    | __main__:<module>:313 - Training step 17770: loss = 3.0850 | 3045.70ms | Tokens/s = 172,140.5
2025-01-19 09:34:42.118 | DEBUG    | __main__:<module>:313 - Training step 17780: loss = 3.1374 | 3045.71ms | Tokens/s = 172,140.0
2025-01-19 09:35:12.561 | DEBUG    | __main__:<module>:313 - Training step 17790: loss = 3.1112 | 3043.91ms | Tokens/s = 172,241.7
2025-01-19 09:35:42.984 | DEBUG    | __main__:<module>:313 - Training step 17800: loss = 3.0941 | 3040.39ms | Tokens/s = 172,441.2
2025-01-19 09:36:13.388 | DEBUG    | __main__:<module>:313 - Training step 17810: loss = 3.2450 | 3040.24ms | Tokens/s = 172,449.6
2025-01-19 09:36:43.813 | DEBUG    | __main__:<module>:313 - Training step 17820: loss = 3.0623 | 3043.43ms | Tokens/s = 172,268.5
2025-01-19 09:37:14.249 | DEBUG    | __main__:<module>:313 - Training step 17830: loss = 3.0414 | 3041.91ms | Tokens/s = 172,355.1
2025-01-19 09:37:44.680 | DEBUG    | __main__:<module>:313 - Training step 17840: loss = 2.8766 | 3042.27ms | Tokens/s = 172,334.4
2025-01-19 09:38:15.126 | DEBUG    | __main__:<module>:313 - Training step 17850: loss = 3.1640 | 3043.93ms | Tokens/s = 172,240.4
2025-01-19 09:38:45.585 | DEBUG    | __main__:<module>:313 - Training step 17860: loss = 3.0699 | 3044.67ms | Tokens/s = 172,198.8
2025-01-19 09:39:16.023 | DEBUG    | __main__:<module>:313 - Training step 17870: loss = 2.9899 | 3046.41ms | Tokens/s = 172,100.1
2025-01-19 09:39:46.463 | DEBUG    | __main__:<module>:313 - Training step 17880: loss = 2.9826 | 3043.78ms | Tokens/s = 172,248.8
2025-01-19 09:40:16.885 | DEBUG    | __main__:<module>:313 - Training step 17890: loss = 2.8184 | 3041.80ms | Tokens/s = 172,361.0
2025-01-19 09:40:47.288 | DEBUG    | __main__:<module>:313 - Training step 17900: loss = 3.0663 | 3038.01ms | Tokens/s = 172,576.3
2025-01-19 09:41:17.704 | DEBUG    | __main__:<module>:313 - Training step 17910: loss = 3.0573 | 3043.42ms | Tokens/s = 172,269.6
2025-01-19 09:41:48.156 | DEBUG    | __main__:<module>:313 - Training step 17920: loss = 3.0143 | 3046.43ms | Tokens/s = 172,099.3
2025-01-19 09:42:18.621 | DEBUG    | __main__:<module>:313 - Training step 17930: loss = 3.0677 | 3043.87ms | Tokens/s = 172,244.0
2025-01-19 09:42:49.049 | DEBUG    | __main__:<module>:313 - Training step 17940: loss = 2.9277 | 3040.60ms | Tokens/s = 172,429.3
2025-01-19 09:43:19.455 | DEBUG    | __main__:<module>:313 - Training step 17950: loss = 2.9288 | 3041.49ms | Tokens/s = 172,378.6
2025-01-19 09:43:49.888 | DEBUG    | __main__:<module>:313 - Training step 17960: loss = 3.0452 | 3042.98ms | Tokens/s = 172,294.5
2025-01-19 09:44:20.345 | DEBUG    | __main__:<module>:313 - Training step 17970: loss = 3.1166 | 3046.21ms | Tokens/s = 172,111.8
2025-01-19 09:44:50.795 | DEBUG    | __main__:<module>:313 - Training step 17980: loss = 2.8970 | 3042.40ms | Tokens/s = 172,326.8
2025-01-19 09:45:21.221 | DEBUG    | __main__:<module>:313 - Training step 17990: loss = 2.9255 | 3042.60ms | Tokens/s = 172,315.8
2025-01-19 09:45:55.035 | INFO     | __main__:<module>:265 - Step 18,000/20,000 loss: 2.9867 (T) 2.9939 (V) | lr=3.0e-04
2025-01-19 09:45:55.036 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 09:46:08.211 | DEBUG    | __main__:<module>:313 - Training step 18000: loss = 3.0839 | 19625.12ms | Tokens/s = 26,715.1
2025-01-19 09:46:38.468 | DEBUG    | __main__:<module>:313 - Training step 18010: loss = 3.0495 | 3035.38ms | Tokens/s = 172,725.6
2025-01-19 09:47:08.853 | DEBUG    | __main__:<module>:313 - Training step 18020: loss = 2.8639 | 3042.20ms | Tokens/s = 172,338.6
2025-01-19 09:47:39.266 | DEBUG    | __main__:<module>:313 - Training step 18030: loss = 3.0776 | 3042.19ms | Tokens/s = 172,339.1
2025-01-19 09:48:09.694 | DEBUG    | __main__:<module>:313 - Training step 18040: loss = 2.8567 | 3041.29ms | Tokens/s = 172,389.9
2025-01-19 09:48:40.098 | DEBUG    | __main__:<module>:313 - Training step 18050: loss = 3.0248 | 3039.92ms | Tokens/s = 172,467.5
2025-01-19 09:49:10.491 | DEBUG    | __main__:<module>:313 - Training step 18060: loss = 3.0284 | 3039.52ms | Tokens/s = 172,490.7
2025-01-19 09:49:40.910 | DEBUG    | __main__:<module>:313 - Training step 18070: loss = 2.9786 | 3042.99ms | Tokens/s = 172,293.6
2025-01-19 09:50:11.353 | DEBUG    | __main__:<module>:313 - Training step 18080: loss = 2.9847 | 3043.40ms | Tokens/s = 172,270.6
2025-01-19 09:50:41.786 | DEBUG    | __main__:<module>:313 - Training step 18090: loss = 3.1341 | 3044.42ms | Tokens/s = 172,212.5
2025-01-19 09:51:12.206 | DEBUG    | __main__:<module>:313 - Training step 18100: loss = 2.8643 | 3042.17ms | Tokens/s = 172,339.9
2025-01-19 09:51:42.644 | DEBUG    | __main__:<module>:313 - Training step 18110: loss = 2.9269 | 3045.28ms | Tokens/s = 172,164.3
2025-01-19 09:52:13.104 | DEBUG    | __main__:<module>:313 - Training step 18120: loss = 2.8639 | 3045.84ms | Tokens/s = 172,132.7
2025-01-19 09:52:43.547 | DEBUG    | __main__:<module>:313 - Training step 18130: loss = 2.9384 | 3044.78ms | Tokens/s = 172,192.5
2025-01-19 09:53:13.988 | DEBUG    | __main__:<module>:313 - Training step 18140: loss = 3.0888 | 3045.43ms | Tokens/s = 172,155.4
2025-01-19 09:53:44.440 | DEBUG    | __main__:<module>:313 - Training step 18150: loss = 3.0359 | 3043.88ms | Tokens/s = 172,243.5
2025-01-19 09:54:14.871 | DEBUG    | __main__:<module>:313 - Training step 18160: loss = 3.1024 | 3042.60ms | Tokens/s = 172,315.9
2025-01-19 09:54:45.319 | DEBUG    | __main__:<module>:313 - Training step 18170: loss = 2.8816 | 3044.40ms | Tokens/s = 172,214.0
2025-01-19 09:55:15.776 | DEBUG    | __main__:<module>:313 - Training step 18180: loss = 3.0212 | 3045.55ms | Tokens/s = 172,149.1
2025-01-19 09:55:46.199 | DEBUG    | __main__:<module>:313 - Training step 18190: loss = 3.1460 | 3040.52ms | Tokens/s = 172,433.5
2025-01-19 09:56:16.604 | DEBUG    | __main__:<module>:313 - Training step 18200: loss = 2.8581 | 3039.04ms | Tokens/s = 172,517.7
2025-01-19 09:56:47.003 | DEBUG    | __main__:<module>:313 - Training step 18210: loss = 3.0459 | 3042.08ms | Tokens/s = 172,345.2
2025-01-19 09:57:17.434 | DEBUG    | __main__:<module>:313 - Training step 18220: loss = 2.9170 | 3045.84ms | Tokens/s = 172,132.2
2025-01-19 09:57:47.848 | DEBUG    | __main__:<module>:313 - Training step 18230: loss = 3.0610 | 3041.73ms | Tokens/s = 172,365.2
2025-01-19 09:58:18.265 | DEBUG    | __main__:<module>:313 - Training step 18240: loss = 2.7934 | 3043.92ms | Tokens/s = 172,241.0
2025-01-19 09:58:48.710 | DEBUG    | __main__:<module>:313 - Training step 18250: loss = 3.1804 | 3043.78ms | Tokens/s = 172,248.8
2025-01-19 09:59:19.151 | DEBUG    | __main__:<module>:313 - Training step 18260: loss = 2.9610 | 3042.30ms | Tokens/s = 172,332.8
2025-01-19 09:59:49.583 | DEBUG    | __main__:<module>:313 - Training step 18270: loss = 3.0990 | 3043.44ms | Tokens/s = 172,268.3
2025-01-19 10:00:20.034 | DEBUG    | __main__:<module>:313 - Training step 18280: loss = 2.9181 | 3045.76ms | Tokens/s = 172,137.0
2025-01-19 10:00:50.489 | DEBUG    | __main__:<module>:313 - Training step 18290: loss = 3.0273 | 3047.62ms | Tokens/s = 172,031.7
2025-01-19 10:01:20.948 | DEBUG    | __main__:<module>:313 - Training step 18300: loss = 2.7089 | 3042.62ms | Tokens/s = 172,314.7
2025-01-19 10:01:51.370 | DEBUG    | __main__:<module>:313 - Training step 18310: loss = 2.9319 | 3040.10ms | Tokens/s = 172,457.2
2025-01-19 10:02:21.803 | DEBUG    | __main__:<module>:313 - Training step 18320: loss = 3.1490 | 3044.38ms | Tokens/s = 172,214.9
2025-01-19 10:02:52.260 | DEBUG    | __main__:<module>:313 - Training step 18330: loss = 2.9931 | 3046.51ms | Tokens/s = 172,094.5
2025-01-19 10:03:22.724 | DEBUG    | __main__:<module>:313 - Training step 18340: loss = 3.1566 | 3046.35ms | Tokens/s = 172,103.6
2025-01-19 10:03:53.163 | DEBUG    | __main__:<module>:313 - Training step 18350: loss = 3.0097 | 3042.46ms | Tokens/s = 172,323.9
2025-01-19 10:04:23.584 | DEBUG    | __main__:<module>:313 - Training step 18360: loss = 3.0048 | 3041.53ms | Tokens/s = 172,376.4
2025-01-19 10:04:53.989 | DEBUG    | __main__:<module>:313 - Training step 18370: loss = 2.7742 | 3040.56ms | Tokens/s = 172,431.2
2025-01-19 10:05:24.387 | DEBUG    | __main__:<module>:313 - Training step 18380: loss = 2.9477 | 3040.06ms | Tokens/s = 172,459.5
2025-01-19 10:05:54.793 | DEBUG    | __main__:<module>:313 - Training step 18390: loss = 2.8585 | 3040.78ms | Tokens/s = 172,419.1
2025-01-19 10:06:25.233 | DEBUG    | __main__:<module>:313 - Training step 18400: loss = 3.0588 | 3043.80ms | Tokens/s = 172,247.9
2025-01-19 10:06:55.688 | DEBUG    | __main__:<module>:313 - Training step 18410: loss = 3.0655 | 3044.55ms | Tokens/s = 172,205.5
2025-01-19 10:07:26.138 | DEBUG    | __main__:<module>:313 - Training step 18420: loss = 3.0569 | 3046.58ms | Tokens/s = 172,090.6
2025-01-19 10:07:56.595 | DEBUG    | __main__:<module>:313 - Training step 18430: loss = 2.9067 | 3045.52ms | Tokens/s = 172,150.8
2025-01-19 10:08:27.025 | DEBUG    | __main__:<module>:313 - Training step 18440: loss = 3.0006 | 3043.05ms | Tokens/s = 172,290.4
2025-01-19 10:08:57.465 | DEBUG    | __main__:<module>:313 - Training step 18450: loss = 2.9150 | 3044.69ms | Tokens/s = 172,197.7
2025-01-19 10:09:27.923 | DEBUG    | __main__:<module>:313 - Training step 18460: loss = 2.9374 | 3045.72ms | Tokens/s = 172,139.2
2025-01-19 10:09:58.358 | DEBUG    | __main__:<module>:313 - Training step 18470: loss = 3.0978 | 3042.23ms | Tokens/s = 172,336.5
2025-01-19 10:10:28.769 | DEBUG    | __main__:<module>:313 - Training step 18480: loss = 2.9820 | 3039.83ms | Tokens/s = 172,472.8
2025-01-19 10:10:59.182 | DEBUG    | __main__:<module>:313 - Training step 18490: loss = 2.8236 | 3042.16ms | Tokens/s = 172,340.4
2025-01-19 10:11:29.627 | DEBUG    | __main__:<module>:313 - Training step 18500: loss = 2.9303 | 3046.05ms | Tokens/s = 172,120.9
2025-01-19 10:12:00.088 | DEBUG    | __main__:<module>:313 - Training step 18510: loss = 2.7830 | 3046.50ms | Tokens/s = 172,095.1
2025-01-19 10:12:30.528 | DEBUG    | __main__:<module>:313 - Training step 18520: loss = 3.0672 | 3043.60ms | Tokens/s = 172,259.1
2025-01-19 10:13:00.943 | DEBUG    | __main__:<module>:313 - Training step 18530: loss = 2.9364 | 3041.30ms | Tokens/s = 172,389.4
2025-01-19 10:13:31.353 | DEBUG    | __main__:<module>:313 - Training step 18540: loss = 3.0547 | 3042.97ms | Tokens/s = 172,295.1
2025-01-19 10:14:01.783 | DEBUG    | __main__:<module>:313 - Training step 18550: loss = 2.9742 | 3041.36ms | Tokens/s = 172,385.9
2025-01-19 10:14:32.194 | DEBUG    | __main__:<module>:313 - Training step 18560: loss = 2.9304 | 3042.65ms | Tokens/s = 172,312.7
2025-01-19 10:15:02.628 | DEBUG    | __main__:<module>:313 - Training step 18570: loss = 2.9365 | 3044.65ms | Tokens/s = 172,199.5
2025-01-19 10:15:33.080 | DEBUG    | __main__:<module>:313 - Training step 18580: loss = 3.0771 | 3045.33ms | Tokens/s = 172,161.4
2025-01-19 10:16:03.534 | DEBUG    | __main__:<module>:313 - Training step 18590: loss = 2.9033 | 3044.62ms | Tokens/s = 172,201.6
2025-01-19 10:16:33.958 | DEBUG    | __main__:<module>:313 - Training step 18600: loss = 2.9672 | 3040.38ms | Tokens/s = 172,441.7
2025-01-19 10:17:04.397 | DEBUG    | __main__:<module>:313 - Training step 18610: loss = 2.9826 | 3044.83ms | Tokens/s = 172,189.3
2025-01-19 10:17:34.850 | DEBUG    | __main__:<module>:313 - Training step 18620: loss = 2.9567 | 3043.41ms | Tokens/s = 172,269.8
2025-01-19 10:18:05.281 | DEBUG    | __main__:<module>:313 - Training step 18630: loss = 2.8411 | 3041.32ms | Tokens/s = 172,388.2
2025-01-19 10:18:35.715 | DEBUG    | __main__:<module>:313 - Training step 18640: loss = 2.9095 | 3043.97ms | Tokens/s = 172,238.0
2025-01-19 10:19:06.171 | DEBUG    | __main__:<module>:313 - Training step 18650: loss = 2.9895 | 3046.38ms | Tokens/s = 172,101.9
2025-01-19 10:19:36.606 | DEBUG    | __main__:<module>:313 - Training step 18660: loss = 2.9435 | 3041.37ms | Tokens/s = 172,385.6
2025-01-19 10:20:07.032 | DEBUG    | __main__:<module>:313 - Training step 18670: loss = 2.9219 | 3043.19ms | Tokens/s = 172,282.6
2025-01-19 10:20:37.479 | DEBUG    | __main__:<module>:313 - Training step 18680: loss = 2.7562 | 3045.68ms | Tokens/s = 172,141.3
2025-01-19 10:21:07.924 | DEBUG    | __main__:<module>:313 - Training step 18690: loss = 3.0244 | 3043.49ms | Tokens/s = 172,265.5
2025-01-19 10:21:38.350 | DEBUG    | __main__:<module>:313 - Training step 18700: loss = 3.0160 | 3040.00ms | Tokens/s = 172,463.3
2025-01-19 10:22:08.763 | DEBUG    | __main__:<module>:313 - Training step 18710: loss = 2.9936 | 3039.78ms | Tokens/s = 172,475.5
2025-01-19 10:22:39.185 | DEBUG    | __main__:<module>:313 - Training step 18720: loss = 3.0597 | 3042.49ms | Tokens/s = 172,321.8
2025-01-19 10:23:09.630 | DEBUG    | __main__:<module>:313 - Training step 18730: loss = 2.8091 | 3046.73ms | Tokens/s = 172,081.9
2025-01-19 10:23:40.080 | DEBUG    | __main__:<module>:313 - Training step 18740: loss = 2.7467 | 3042.71ms | Tokens/s = 172,309.3
2025-01-19 10:24:10.507 | DEBUG    | __main__:<module>:313 - Training step 18750: loss = 3.0603 | 3044.13ms | Tokens/s = 172,228.9
2025-01-19 10:24:40.920 | DEBUG    | __main__:<module>:313 - Training step 18760: loss = 2.9880 | 3040.67ms | Tokens/s = 172,425.1
2025-01-19 10:25:11.358 | DEBUG    | __main__:<module>:313 - Training step 18770: loss = 3.0496 | 3043.61ms | Tokens/s = 172,258.4
2025-01-19 10:25:41.813 | DEBUG    | __main__:<module>:313 - Training step 18780: loss = 2.9696 | 3045.93ms | Tokens/s = 172,127.2
2025-01-19 10:26:12.242 | DEBUG    | __main__:<module>:313 - Training step 18790: loss = 2.9403 | 3042.47ms | Tokens/s = 172,322.9
2025-01-19 10:26:42.651 | DEBUG    | __main__:<module>:313 - Training step 18800: loss = 2.9914 | 3039.54ms | Tokens/s = 172,489.3
2025-01-19 10:27:13.061 | DEBUG    | __main__:<module>:313 - Training step 18810: loss = 3.0157 | 3040.09ms | Tokens/s = 172,458.3
2025-01-19 10:27:43.486 | DEBUG    | __main__:<module>:313 - Training step 18820: loss = 2.8911 | 3042.17ms | Tokens/s = 172,340.2
2025-01-19 10:28:13.901 | DEBUG    | __main__:<module>:313 - Training step 18830: loss = 3.0419 | 3039.36ms | Tokens/s = 172,499.5
2025-01-19 10:28:44.318 | DEBUG    | __main__:<module>:313 - Training step 18840: loss = 3.0339 | 3042.27ms | Tokens/s = 172,334.6
2025-01-19 10:29:14.755 | DEBUG    | __main__:<module>:313 - Training step 18850: loss = 2.8906 | 3044.16ms | Tokens/s = 172,227.4
2025-01-19 10:29:45.213 | DEBUG    | __main__:<module>:313 - Training step 18860: loss = 2.9733 | 3045.39ms | Tokens/s = 172,157.8
2025-01-19 10:30:15.652 | DEBUG    | __main__:<module>:313 - Training step 18870: loss = 2.9899 | 3043.91ms | Tokens/s = 172,241.4
2025-01-19 10:30:46.066 | DEBUG    | __main__:<module>:313 - Training step 18880: loss = 3.0037 | 3043.08ms | Tokens/s = 172,288.6
2025-01-19 10:31:16.482 | DEBUG    | __main__:<module>:313 - Training step 18890: loss = 2.8475 | 3044.40ms | Tokens/s = 172,214.1
2025-01-19 10:31:46.910 | DEBUG    | __main__:<module>:313 - Training step 18900: loss = 2.9903 | 3040.72ms | Tokens/s = 172,422.4
2025-01-19 10:32:17.319 | DEBUG    | __main__:<module>:313 - Training step 18910: loss = 3.0453 | 3038.65ms | Tokens/s = 172,539.6
2025-01-19 10:32:47.710 | DEBUG    | __main__:<module>:313 - Training step 18920: loss = 3.0759 | 3038.81ms | Tokens/s = 172,530.8
2025-01-19 10:33:18.125 | DEBUG    | __main__:<module>:313 - Training step 18930: loss = 2.8997 | 3042.00ms | Tokens/s = 172,349.8
2025-01-19 10:33:48.570 | DEBUG    | __main__:<module>:313 - Training step 18940: loss = 3.0658 | 3043.89ms | Tokens/s = 172,243.0
2025-01-19 10:34:18.988 | DEBUG    | __main__:<module>:313 - Training step 18950: loss = 2.9271 | 3041.27ms | Tokens/s = 172,391.0
2025-01-19 10:34:49.420 | DEBUG    | __main__:<module>:313 - Training step 18960: loss = 2.8041 | 3044.85ms | Tokens/s = 172,188.6
2025-01-19 10:35:19.877 | DEBUG    | __main__:<module>:313 - Training step 18970: loss = 3.0146 | 3045.59ms | Tokens/s = 172,146.7
2025-01-19 10:35:50.338 | DEBUG    | __main__:<module>:313 - Training step 18980: loss = 2.9291 | 3046.79ms | Tokens/s = 172,078.8
2025-01-19 10:36:20.798 | DEBUG    | __main__:<module>:313 - Training step 18990: loss = 3.1208 | 3042.78ms | Tokens/s = 172,305.4
2025-01-19 10:36:54.640 | INFO     | __main__:<module>:265 - Step 19,000/20,000 loss: 2.9788 (T) 2.9853 (V) | lr=7.6e-05
2025-01-19 10:36:54.642 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-19 10:37:08.075 | DEBUG    | __main__:<module>:313 - Training step 19000: loss = 2.9264 | 19888.51ms | Tokens/s = 26,361.3
2025-01-19 10:37:38.356 | DEBUG    | __main__:<module>:313 - Training step 19010: loss = 3.1153 | 3035.63ms | Tokens/s = 172,711.6
2025-01-19 10:38:08.742 | DEBUG    | __main__:<module>:313 - Training step 19020: loss = 2.8418 | 3042.28ms | Tokens/s = 172,334.0
2025-01-19 10:38:39.168 | DEBUG    | __main__:<module>:313 - Training step 19030: loss = 2.9989 | 3042.94ms | Tokens/s = 172,296.4
2025-01-19 10:39:09.619 | DEBUG    | __main__:<module>:313 - Training step 19040: loss = 3.1650 | 3046.75ms | Tokens/s = 172,081.0
2025-01-19 10:39:40.077 | DEBUG    | __main__:<module>:313 - Training step 19050: loss = 2.8334 | 3044.87ms | Tokens/s = 172,187.1
2025-01-19 10:40:10.505 | DEBUG    | __main__:<module>:313 - Training step 19060: loss = 3.0595 | 3042.07ms | Tokens/s = 172,345.6
2025-01-19 10:40:40.921 | DEBUG    | __main__:<module>:313 - Training step 19070: loss = 2.8367 | 3045.86ms | Tokens/s = 172,131.1
2025-01-19 10:41:11.359 | DEBUG    | __main__:<module>:313 - Training step 19080: loss = 3.0318 | 3045.52ms | Tokens/s = 172,150.7
2025-01-19 10:41:41.817 | DEBUG    | __main__:<module>:313 - Training step 19090: loss = 3.0227 | 3045.97ms | Tokens/s = 172,124.9
2025-01-19 10:42:12.243 | DEBUG    | __main__:<module>:313 - Training step 19100: loss = 3.1917 | 3040.43ms | Tokens/s = 172,438.7
2025-01-19 10:42:42.656 | DEBUG    | __main__:<module>:313 - Training step 19110: loss = 3.0936 | 3042.30ms | Tokens/s = 172,333.0
2025-01-19 10:43:13.087 | DEBUG    | __main__:<module>:313 - Training step 19120: loss = 3.0455 | 3044.65ms | Tokens/s = 172,200.0
2025-01-19 10:43:43.548 | DEBUG    | __main__:<module>:313 - Training step 19130: loss = 2.9515 | 3046.88ms | Tokens/s = 172,074.0
2025-01-19 10:44:14.007 | DEBUG    | __main__:<module>:313 - Training step 19140: loss = 2.9426 | 3042.80ms | Tokens/s = 172,304.7
2025-01-19 10:44:44.438 | DEBUG    | __main__:<module>:313 - Training step 19150: loss = 2.9506 | 3042.31ms | Tokens/s = 172,332.1
2025-01-19 10:45:14.872 | DEBUG    | __main__:<module>:313 - Training step 19160: loss = 2.8892 | 3044.56ms | Tokens/s = 172,204.8
2025-01-19 10:45:45.326 | DEBUG    | __main__:<module>:313 - Training step 19170: loss = 2.8604 | 3046.08ms | Tokens/s = 172,118.7
2025-01-19 10:46:15.752 | DEBUG    | __main__:<module>:313 - Training step 19180: loss = 2.9983 | 3042.25ms | Tokens/s = 172,335.7
2025-01-19 10:46:46.177 | DEBUG    | __main__:<module>:313 - Training step 19190: loss = 2.8300 | 3041.25ms | Tokens/s = 172,392.3
2025-01-19 10:47:16.616 | DEBUG    | __main__:<module>:313 - Training step 19200: loss = 2.9732 | 3044.02ms | Tokens/s = 172,235.3
2025-01-19 10:47:47.074 | DEBUG    | __main__:<module>:313 - Training step 19210: loss = 2.9495 | 3045.56ms | Tokens/s = 172,148.4
2025-01-19 10:48:17.533 | DEBUG    | __main__:<module>:313 - Training step 19220: loss = 2.9863 | 3044.91ms | Tokens/s = 172,185.1
2025-01-19 10:48:47.966 | DEBUG    | __main__:<module>:313 - Training step 19230: loss = 2.8571 | 3041.82ms | Tokens/s = 172,359.8
2025-01-19 10:49:18.404 | DEBUG    | __main__:<module>:313 - Training step 19240: loss = 2.8969 | 3047.27ms | Tokens/s = 172,051.5
2025-01-19 10:49:48.863 | DEBUG    | __main__:<module>:313 - Training step 19250: loss = 3.0052 | 3045.71ms | Tokens/s = 172,139.6
2025-01-19 10:50:19.303 | DEBUG    | __main__:<module>:313 - Training step 19260: loss = 2.8119 | 3041.13ms | Tokens/s = 172,399.0
2025-01-19 10:50:49.737 | DEBUG    | __main__:<module>:313 - Training step 19270: loss = 3.0580 | 3045.58ms | Tokens/s = 172,147.0
2025-01-19 10:51:20.196 | DEBUG    | __main__:<module>:313 - Training step 19280: loss = 2.9118 | 3047.77ms | Tokens/s = 172,023.4
2025-01-19 10:51:50.670 | DEBUG    | __main__:<module>:313 - Training step 19290: loss = 3.0296 | 3048.23ms | Tokens/s = 171,997.8
2025-01-19 10:52:21.123 | DEBUG    | __main__:<module>:313 - Training step 19300: loss = 2.9326 | 3041.83ms | Tokens/s = 172,359.5
2025-01-19 10:52:51.545 | DEBUG    | __main__:<module>:313 - Training step 19310: loss = 2.8578 | 3042.67ms | Tokens/s = 172,312.0
2025-01-19 10:53:21.951 | DEBUG    | __main__:<module>:313 - Training step 19320: loss = 2.9227 | 3038.62ms | Tokens/s = 172,541.6
2025-01-19 10:53:52.351 | DEBUG    | __main__:<module>:313 - Training step 19330: loss = 3.0184 | 3040.25ms | Tokens/s = 172,448.8
2025-01-19 10:54:22.785 | DEBUG    | __main__:<module>:313 - Training step 19340: loss = 3.0494 | 3042.64ms | Tokens/s = 172,313.3
2025-01-19 10:54:53.216 | DEBUG    | __main__:<module>:313 - Training step 19350: loss = 2.8810 | 3042.77ms | Tokens/s = 172,306.1
2025-01-19 10:55:23.636 | DEBUG    | __main__:<module>:313 - Training step 19360: loss = 3.0034 | 3040.77ms | Tokens/s = 172,419.4
2025-01-19 10:55:54.067 | DEBUG    | __main__:<module>:313 - Training step 19370: loss = 2.9844 | 3041.64ms | Tokens/s = 172,369.9
2025-01-19 10:56:24.490 | DEBUG    | __main__:<module>:313 - Training step 19380: loss = 2.8396 | 3041.47ms | Tokens/s = 172,379.6
2025-01-19 10:56:54.935 | DEBUG    | __main__:<module>:313 - Training step 19390: loss = 2.8620 | 3047.37ms | Tokens/s = 172,045.8
2025-01-19 10:57:25.403 | DEBUG    | __main__:<module>:313 - Training step 19400: loss = 3.0516 | 3045.07ms | Tokens/s = 172,176.2
2025-01-19 10:57:55.862 | DEBUG    | __main__:<module>:313 - Training step 19410: loss = 2.9851 | 3043.79ms | Tokens/s = 172,248.4
2025-01-19 10:58:26.315 | DEBUG    | __main__:<module>:313 - Training step 19420: loss = 2.9454 | 3044.66ms | Tokens/s = 172,198.9
2025-01-19 10:58:56.782 | DEBUG    | __main__:<module>:313 - Training step 19430: loss = 3.1383 | 3045.81ms | Tokens/s = 172,134.3
2025-01-19 10:59:27.234 | DEBUG    | __main__:<module>:313 - Training step 19440: loss = 2.9342 | 3043.89ms | Tokens/s = 172,242.9
2025-01-19 10:59:57.674 | DEBUG    | __main__:<module>:313 - Training step 19450: loss = 3.0649 | 3042.60ms | Tokens/s = 172,316.0
2025-01-19 11:00:28.116 | DEBUG    | __main__:<module>:313 - Training step 19460: loss = 2.9748 | 3044.31ms | Tokens/s = 172,218.7
2025-01-19 11:00:58.548 | DEBUG    | __main__:<module>:313 - Training step 19470: loss = 2.9840 | 3041.19ms | Tokens/s = 172,395.9
2025-01-19 11:01:28.983 | DEBUG    | __main__:<module>:313 - Training step 19480: loss = 3.0386 | 3042.88ms | Tokens/s = 172,299.9
2025-01-19 11:01:59.420 | DEBUG    | __main__:<module>:313 - Training step 19490: loss = 2.7962 | 3044.92ms | Tokens/s = 172,184.7
2025-01-19 11:02:29.866 | DEBUG    | __main__:<module>:313 - Training step 19500: loss = 3.0192 | 3046.49ms | Tokens/s = 172,095.5
2025-01-19 11:03:00.328 | DEBUG    | __main__:<module>:313 - Training step 19510: loss = 2.9618 | 3043.53ms | Tokens/s = 172,263.4
2025-01-19 11:03:30.787 | DEBUG    | __main__:<module>:313 - Training step 19520: loss = 2.8927 | 3046.52ms | Tokens/s = 172,093.8
2025-01-19 11:04:01.242 | DEBUG    | __main__:<module>:313 - Training step 19530: loss = 3.0538 | 3045.78ms | Tokens/s = 172,135.6
2025-01-19 11:04:31.694 | DEBUG    | __main__:<module>:313 - Training step 19540: loss = 3.0708 | 3045.58ms | Tokens/s = 172,147.4
2025-01-19 11:05:02.145 | DEBUG    | __main__:<module>:313 - Training step 19550: loss = 2.9221 | 3044.33ms | Tokens/s = 172,217.6
2025-01-19 11:05:32.594 | DEBUG    | __main__:<module>:313 - Training step 19560: loss = 2.8029 | 3045.09ms | Tokens/s = 172,174.8
2025-01-19 11:06:03.036 | DEBUG    | __main__:<module>:313 - Training step 19570: loss = 2.8945 | 3044.04ms | Tokens/s = 172,234.3
2025-01-19 11:06:33.474 | DEBUG    | __main__:<module>:313 - Training step 19580: loss = 2.9214 | 3042.37ms | Tokens/s = 172,328.5
2025-01-19 11:07:03.910 | DEBUG    | __main__:<module>:313 - Training step 19590: loss = 2.8758 | 3045.05ms | Tokens/s = 172,177.1
2025-01-19 11:07:34.353 | DEBUG    | __main__:<module>:313 - Training step 19600: loss = 3.0069 | 3044.97ms | Tokens/s = 172,181.6
2025-01-19 11:08:04.790 | DEBUG    | __main__:<module>:313 - Training step 19610: loss = 3.0070 | 3043.20ms | Tokens/s = 172,281.7
2025-01-19 11:08:35.231 | DEBUG    | __main__:<module>:313 - Training step 19620: loss = 3.0081 | 3043.89ms | Tokens/s = 172,242.7
2025-01-19 11:09:05.678 | DEBUG    | __main__:<module>:313 - Training step 19630: loss = 3.0307 | 3044.48ms | Tokens/s = 172,209.5
2025-01-19 11:09:36.122 | DEBUG    | __main__:<module>:313 - Training step 19640: loss = 2.9527 | 3044.16ms | Tokens/s = 172,227.4
2025-01-19 11:10:06.563 | DEBUG    | __main__:<module>:313 - Training step 19650: loss = 2.9684 | 3044.51ms | Tokens/s = 172,207.9
2025-01-19 11:10:37.008 | DEBUG    | __main__:<module>:313 - Training step 19660: loss = 2.9569 | 3045.30ms | Tokens/s = 172,163.3
2025-01-19 11:11:07.452 | DEBUG    | __main__:<module>:313 - Training step 19670: loss = 2.9655 | 3043.68ms | Tokens/s = 172,254.6
2025-01-19 11:11:37.893 | DEBUG    | __main__:<module>:313 - Training step 19680: loss = 2.7846 | 3043.31ms | Tokens/s = 172,275.5
2025-01-19 11:12:08.335 | DEBUG    | __main__:<module>:313 - Training step 19690: loss = 2.9105 | 3046.67ms | Tokens/s = 172,085.4
2025-01-19 11:12:38.780 | DEBUG    | __main__:<module>:313 - Training step 19700: loss = 3.1043 | 3042.62ms | Tokens/s = 172,314.5
2025-01-19 11:13:09.230 | DEBUG    | __main__:<module>:313 - Training step 19710: loss = 2.8738 | 3044.49ms | Tokens/s = 172,208.7
2025-01-19 11:13:39.668 | DEBUG    | __main__:<module>:313 - Training step 19720: loss = 3.0398 | 3044.61ms | Tokens/s = 172,201.9
2025-01-19 11:14:10.105 | DEBUG    | __main__:<module>:313 - Training step 19730: loss = 2.7990 | 3044.85ms | Tokens/s = 172,188.4
2025-01-19 11:14:40.544 | DEBUG    | __main__:<module>:313 - Training step 19740: loss = 2.9646 | 3043.92ms | Tokens/s = 172,241.1
2025-01-19 11:15:10.986 | DEBUG    | __main__:<module>:313 - Training step 19750: loss = 3.0803 | 3044.53ms | Tokens/s = 172,206.6
2025-01-19 11:15:41.425 | DEBUG    | __main__:<module>:313 - Training step 19760: loss = 2.9698 | 3045.70ms | Tokens/s = 172,140.5
2025-01-19 11:16:11.893 | DEBUG    | __main__:<module>:313 - Training step 19770: loss = 3.0279 | 3045.70ms | Tokens/s = 172,140.6
2025-01-19 11:16:42.358 | DEBUG    | __main__:<module>:313 - Training step 19780: loss = 2.8272 | 3045.53ms | Tokens/s = 172,150.3
2025-01-19 11:17:12.817 | DEBUG    | __main__:<module>:313 - Training step 19790: loss = 2.9309 | 3046.45ms | Tokens/s = 172,097.8
2025-01-19 11:17:43.264 | DEBUG    | __main__:<module>:313 - Training step 19800: loss = 3.0437 | 3041.65ms | Tokens/s = 172,369.4
2025-01-19 11:18:13.707 | DEBUG    | __main__:<module>:313 - Training step 19810: loss = 3.1082 | 3042.53ms | Tokens/s = 172,319.9
2025-01-19 11:18:44.152 | DEBUG    | __main__:<module>:313 - Training step 19820: loss = 2.9315 | 3044.71ms | Tokens/s = 172,196.4
2025-01-19 11:19:14.603 | DEBUG    | __main__:<module>:313 - Training step 19830: loss = 2.9935 | 3047.10ms | Tokens/s = 172,061.1
2025-01-19 11:19:45.083 | DEBUG    | __main__:<module>:313 - Training step 19840: loss = 2.9248 | 3047.92ms | Tokens/s = 172,014.8
2025-01-19 11:20:15.547 | DEBUG    | __main__:<module>:313 - Training step 19850: loss = 2.9742 | 3047.50ms | Tokens/s = 172,038.7
2025-01-19 11:20:46.016 | DEBUG    | __main__:<module>:313 - Training step 19860: loss = 3.0039 | 3047.45ms | Tokens/s = 172,041.6
2025-01-19 11:21:16.478 | DEBUG    | __main__:<module>:313 - Training step 19870: loss = 2.8737 | 3045.51ms | Tokens/s = 172,151.3
2025-01-19 11:21:46.924 | DEBUG    | __main__:<module>:313 - Training step 19880: loss = 2.9720 | 3043.64ms | Tokens/s = 172,256.7
2025-01-19 11:22:17.364 | DEBUG    | __main__:<module>:313 - Training step 19890: loss = 2.8879 | 3044.85ms | Tokens/s = 172,188.3
2025-01-19 11:22:47.800 | DEBUG    | __main__:<module>:313 - Training step 19900: loss = 2.9023 | 3042.53ms | Tokens/s = 172,319.9
2025-01-19 11:23:18.246 | DEBUG    | __main__:<module>:313 - Training step 19910: loss = 3.0203 | 3045.22ms | Tokens/s = 172,167.3
2025-01-19 11:23:48.685 | DEBUG    | __main__:<module>:313 - Training step 19920: loss = 3.0035 | 3041.97ms | Tokens/s = 172,351.5
2025-01-19 11:24:19.124 | DEBUG    | __main__:<module>:313 - Training step 19930: loss = 3.0355 | 3042.35ms | Tokens/s = 172,329.7
2025-01-19 11:24:49.558 | DEBUG    | __main__:<module>:313 - Training step 19940: loss = 2.7662 | 3041.38ms | Tokens/s = 172,384.7
2025-01-19 11:25:20.019 | DEBUG    | __main__:<module>:313 - Training step 19950: loss = 3.0558 | 3046.58ms | Tokens/s = 172,091.0
2025-01-19 11:25:50.482 | DEBUG    | __main__:<module>:313 - Training step 19960: loss = 3.0766 | 3044.37ms | Tokens/s = 172,215.9
2025-01-19 11:26:20.937 | DEBUG    | __main__:<module>:313 - Training step 19970: loss = 2.9148 | 3045.01ms | Tokens/s = 172,179.2
2025-01-19 11:26:51.417 | DEBUG    | __main__:<module>:313 - Training step 19980: loss = 2.8878 | 3049.02ms | Tokens/s = 171,953.2
2025-01-19 11:27:21.899 | DEBUG    | __main__:<module>:313 - Training step 19990: loss = 2.9090 | 3047.74ms | Tokens/s = 172,025.0
2025-01-19 11:27:49.218 | INFO     | __main__:<module>:319 - job_name='gpt2-training-124M-2025-01-18-18-27-10' finished in 61232.89s
2025-01-19 11:27:49.219 | INFO     | __main__:<module>:320 - Trained for 20,000 steps total_training_tokens=10,485,760,000 and achieved  best eval loss=2.9852614402770996
2025-01-19 11:27:55.783 | INFO     | __main__:<module>:345 - Loss: 2.9650 (T) 2.9741 (V) | 61242.76953411102s
