2025-01-15 07:42:42.432 | INFO     | __main__:<module>:63 - Got args Namespace(max_steps=40000, eval_interval=1000, eval_steps=100, batch_size=16, gradient_accum=32, model_size='124M', tensorboard=1)
2025-01-15 07:42:42.432 | INFO     | __main__:<module>:88 - Training 124M model with config.n_layer=12, config.n_embd=768config.n_head=12, config.context_size=1024, config.dropout=0.0, config.vocab_size=50304
2025-01-15 07:42:44.679 | DEBUG    | __main__:<module>:138 - ddp_rank=0 ddp_local_rank=0
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:152 - Training data is 9,035,582,489 tokens
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:153 - Evaluation data is 4,434,606 tokens
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:160 - job_name='gpt2-training-124M-2025-01-15-07-42-42'
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:161 - Tokens / step: 524,288
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:162 - Total training tokens: 20,971,520,000
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:163 - Effective batch size with grad accumulation: batch_size * gradient_accumulation_steps=512
2025-01-15 07:42:44.679 | DEBUG    | __main__:<module>:164 - gradient_accumulation_steps_per_gpu=32
2025-01-15 07:42:44.679 | DEBUG    | __main__:<module>:165 - Directories: train_dir='/home/v-youransun/repgpt/input/data/train', eval_dir='/home/v-youransun/repgpt/input/data/eval' model_dir='/home/v-youransun/repgpt/model' log_dir='/home/v-youransun/repgpt/output/tensorboard/nov/gpt2-training-124M-2025-01-15-07-42-42'
2025-01-15 07:42:44.679 | INFO     | __main__:<module>:166 - Loaded dataset 2.2482409477233887
2025-01-15 07:42:47.970 | INFO     | __main__:<module>:200 - OptimizedModule(
  (_orig_mod): DistributedDataParallel(
    (module): GPT2(
      (token_embedding_table): Embedding(50304, 768)
      (position_embedding_table): Embedding(1024, 768)
      (blocks): Sequential(
        (0): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (layer_norm_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lm_head): Linear(in_features=768, out_features=50304, bias=False)
    )
  )
)
2025-01-15 07:42:47.970 | INFO     | __main__:<module>:203 - Training model with 123,587,328 parameters for max_steps=40,000 on total_training_tokens=20,971,520,000
2025-01-15 07:42:47.970 | INFO     | __main__:<module>:206 - Decayed parameter tensors: 74, with 124,354,560 parameters
2025-01-15 07:42:47.970 | INFO     | __main__:<module>:207 - Non-decayed parameter tensors: 25, with 19,200 parameters
[rank0]:W0115 07:42:47.988000 2918284 torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2025-01-15 07:42:58.357 | INFO     | __main__:<module>:265 - Step 0/40,000 loss: 10.9461 (T) 10.9439 (V) | lr=0.0e+00
2025-01-15 07:42:58.358 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 07:43:08.474 | DEBUG    | __main__:<module>:313 - Training step 0: loss = 10.9483 | 20501.38ms | Tokens/s = 25,573.3
2025-01-15 07:43:38.061 | DEBUG    | __main__:<module>:313 - Training step 10: loss = 9.3830 | 2966.31ms | Tokens/s = 176,747.4
2025-01-15 07:44:07.873 | DEBUG    | __main__:<module>:313 - Training step 20: loss = 8.6897 | 2988.26ms | Tokens/s = 175,449.3
2025-01-15 07:44:37.770 | DEBUG    | __main__:<module>:313 - Training step 30: loss = 7.9498 | 2985.76ms | Tokens/s = 175,596.0
2025-01-15 07:45:07.692 | DEBUG    | __main__:<module>:313 - Training step 40: loss = 7.2856 | 2996.59ms | Tokens/s = 174,961.3
2025-01-15 07:45:37.699 | DEBUG    | __main__:<module>:313 - Training step 50: loss = 6.8372 | 3003.10ms | Tokens/s = 174,582.3
2025-01-15 07:46:07.757 | DEBUG    | __main__:<module>:313 - Training step 60: loss = 6.6069 | 3006.55ms | Tokens/s = 174,382.0
2025-01-15 07:46:37.849 | DEBUG    | __main__:<module>:313 - Training step 70: loss = 6.7368 | 3009.00ms | Tokens/s = 174,239.7
2025-01-15 07:47:07.971 | DEBUG    | __main__:<module>:313 - Training step 80: loss = 6.4489 | 3012.07ms | Tokens/s = 174,062.6
2025-01-15 07:47:38.095 | DEBUG    | __main__:<module>:313 - Training step 90: loss = 6.2437 | 3013.71ms | Tokens/s = 173,967.8
2025-01-15 07:48:08.237 | DEBUG    | __main__:<module>:313 - Training step 100: loss = 6.0548 | 3013.14ms | Tokens/s = 174,000.4
2025-01-15 07:48:38.401 | DEBUG    | __main__:<module>:313 - Training step 110: loss = 6.3234 | 3017.03ms | Tokens/s = 173,776.4
2025-01-15 07:49:08.560 | DEBUG    | __main__:<module>:313 - Training step 120: loss = 6.2032 | 3013.42ms | Tokens/s = 173,984.2
2025-01-15 07:49:38.718 | DEBUG    | __main__:<module>:313 - Training step 130: loss = 6.0079 | 3015.99ms | Tokens/s = 173,836.0
2025-01-15 07:50:08.890 | DEBUG    | __main__:<module>:313 - Training step 140: loss = 5.8812 | 3016.00ms | Tokens/s = 173,835.6
2025-01-15 07:50:39.067 | DEBUG    | __main__:<module>:313 - Training step 150: loss = 5.8187 | 3018.22ms | Tokens/s = 173,707.8
2025-01-15 07:51:09.248 | DEBUG    | __main__:<module>:313 - Training step 160: loss = 5.9865 | 3016.69ms | Tokens/s = 173,795.7
2025-01-15 07:51:39.420 | DEBUG    | __main__:<module>:313 - Training step 170: loss = 5.9323 | 3017.09ms | Tokens/s = 173,772.7
2025-01-15 07:52:09.601 | DEBUG    | __main__:<module>:313 - Training step 180: loss = 5.7575 | 3019.68ms | Tokens/s = 173,624.0
2025-01-15 07:52:39.794 | DEBUG    | __main__:<module>:313 - Training step 190: loss = 5.9541 | 3017.47ms | Tokens/s = 173,750.8
2025-01-15 07:53:09.953 | DEBUG    | __main__:<module>:313 - Training step 200: loss = 5.6839 | 3011.83ms | Tokens/s = 174,076.2
2025-01-15 07:53:40.074 | DEBUG    | __main__:<module>:313 - Training step 210: loss = 5.5152 | 3012.37ms | Tokens/s = 174,044.9
2025-01-15 07:54:10.232 | DEBUG    | __main__:<module>:313 - Training step 220: loss = 5.6789 | 3018.72ms | Tokens/s = 173,679.0
2025-01-15 07:54:40.411 | DEBUG    | __main__:<module>:313 - Training step 230: loss = 5.6096 | 3017.41ms | Tokens/s = 173,754.3
2025-01-15 07:55:10.600 | DEBUG    | __main__:<module>:313 - Training step 240: loss = 5.5693 | 3021.46ms | Tokens/s = 173,521.2
2025-01-15 07:55:40.781 | DEBUG    | __main__:<module>:313 - Training step 250: loss = 5.5309 | 3017.93ms | Tokens/s = 173,724.6
2025-01-15 07:56:10.969 | DEBUG    | __main__:<module>:313 - Training step 260: loss = 5.6453 | 3017.29ms | Tokens/s = 173,761.4
2025-01-15 07:56:41.161 | DEBUG    | __main__:<module>:313 - Training step 270: loss = 5.4559 | 3017.55ms | Tokens/s = 173,746.5
2025-01-15 07:57:11.352 | DEBUG    | __main__:<module>:313 - Training step 280: loss = 5.3525 | 3018.59ms | Tokens/s = 173,686.4
2025-01-15 07:57:41.533 | DEBUG    | __main__:<module>:313 - Training step 290: loss = 5.4177 | 3018.39ms | Tokens/s = 173,697.9
2025-01-15 07:58:11.724 | DEBUG    | __main__:<module>:313 - Training step 300: loss = 5.3510 | 3017.01ms | Tokens/s = 173,777.4
2025-01-15 07:58:41.930 | DEBUG    | __main__:<module>:313 - Training step 310: loss = 5.2254 | 3020.59ms | Tokens/s = 173,571.3
2025-01-15 07:59:12.110 | DEBUG    | __main__:<module>:313 - Training step 320: loss = 5.3553 | 3014.41ms | Tokens/s = 173,927.4
2025-01-15 07:59:42.252 | DEBUG    | __main__:<module>:313 - Training step 330: loss = 5.2787 | 3017.85ms | Tokens/s = 173,729.0
2025-01-15 08:00:12.429 | DEBUG    | __main__:<module>:313 - Training step 340: loss = 5.3020 | 3020.75ms | Tokens/s = 173,562.5
2025-01-15 08:00:42.623 | DEBUG    | __main__:<module>:313 - Training step 350: loss = 5.0668 | 3020.66ms | Tokens/s = 173,567.2
2025-01-15 08:01:12.826 | DEBUG    | __main__:<module>:313 - Training step 360: loss = 5.0900 | 3020.35ms | Tokens/s = 173,585.2
2025-01-15 08:01:43.031 | DEBUG    | __main__:<module>:313 - Training step 370: loss = 5.2640 | 3021.41ms | Tokens/s = 173,524.4
2025-01-15 08:02:13.250 | DEBUG    | __main__:<module>:313 - Training step 380: loss = 4.9566 | 3025.11ms | Tokens/s = 173,312.1
2025-01-15 08:02:43.470 | DEBUG    | __main__:<module>:313 - Training step 390: loss = 5.0806 | 3021.87ms | Tokens/s = 173,498.0
2025-01-15 08:03:13.698 | DEBUG    | __main__:<module>:313 - Training step 400: loss = 5.0346 | 3019.97ms | Tokens/s = 173,607.0
2025-01-15 08:03:43.886 | DEBUG    | __main__:<module>:313 - Training step 410: loss = 4.7732 | 3017.15ms | Tokens/s = 173,769.1
2025-01-15 08:04:14.069 | DEBUG    | __main__:<module>:313 - Training step 420: loss = 4.7966 | 3018.87ms | Tokens/s = 173,670.3
2025-01-15 08:04:44.278 | DEBUG    | __main__:<module>:313 - Training step 430: loss = 4.7920 | 3022.14ms | Tokens/s = 173,482.2
2025-01-15 08:05:14.481 | DEBUG    | __main__:<module>:313 - Training step 440: loss = 4.8785 | 3020.54ms | Tokens/s = 173,574.2
2025-01-15 08:05:44.684 | DEBUG    | __main__:<module>:313 - Training step 450: loss = 4.8557 | 3020.49ms | Tokens/s = 173,577.0
2025-01-15 08:06:14.892 | DEBUG    | __main__:<module>:313 - Training step 460: loss = 4.7899 | 3019.61ms | Tokens/s = 173,627.7
2025-01-15 08:06:45.086 | DEBUG    | __main__:<module>:313 - Training step 470: loss = 4.6319 | 3020.25ms | Tokens/s = 173,591.2
2025-01-15 08:07:15.244 | DEBUG    | __main__:<module>:313 - Training step 480: loss = 4.5640 | 3013.93ms | Tokens/s = 173,954.8
2025-01-15 08:07:45.419 | DEBUG    | __main__:<module>:313 - Training step 490: loss = 4.7337 | 3017.51ms | Tokens/s = 173,748.8
2025-01-15 08:08:15.622 | DEBUG    | __main__:<module>:313 - Training step 500: loss = 4.3959 | 3021.60ms | Tokens/s = 173,513.5
2025-01-15 08:08:45.838 | DEBUG    | __main__:<module>:313 - Training step 510: loss = 4.5636 | 3022.42ms | Tokens/s = 173,466.0
2025-01-15 08:09:16.062 | DEBUG    | __main__:<module>:313 - Training step 520: loss = 4.5474 | 3018.77ms | Tokens/s = 173,675.9
2025-01-15 08:09:46.270 | DEBUG    | __main__:<module>:313 - Training step 530: loss = 4.5074 | 3020.13ms | Tokens/s = 173,598.1
2025-01-15 08:10:16.469 | DEBUG    | __main__:<module>:313 - Training step 540: loss = 4.5190 | 3020.20ms | Tokens/s = 173,593.8
2025-01-15 08:10:46.675 | DEBUG    | __main__:<module>:313 - Training step 550: loss = 4.3998 | 3019.61ms | Tokens/s = 173,627.8
2025-01-15 08:11:16.901 | DEBUG    | __main__:<module>:313 - Training step 560: loss = 4.4242 | 3019.98ms | Tokens/s = 173,606.7
2025-01-15 08:11:47.122 | DEBUG    | __main__:<module>:313 - Training step 570: loss = 4.4145 | 3023.66ms | Tokens/s = 173,395.0
2025-01-15 08:12:17.354 | DEBUG    | __main__:<module>:313 - Training step 580: loss = 4.3101 | 3024.38ms | Tokens/s = 173,354.0
2025-01-15 08:12:47.591 | DEBUG    | __main__:<module>:313 - Training step 590: loss = 4.3128 | 3021.68ms | Tokens/s = 173,509.0
2025-01-15 08:13:17.793 | DEBUG    | __main__:<module>:313 - Training step 600: loss = 4.4033 | 3019.69ms | Tokens/s = 173,623.1
2025-01-15 08:13:48.013 | DEBUG    | __main__:<module>:313 - Training step 610: loss = 4.3637 | 3022.65ms | Tokens/s = 173,453.1
2025-01-15 08:14:18.208 | DEBUG    | __main__:<module>:313 - Training step 620: loss = 4.3677 | 3019.84ms | Tokens/s = 173,614.4
2025-01-15 08:14:48.387 | DEBUG    | __main__:<module>:313 - Training step 630: loss = 4.2721 | 3017.00ms | Tokens/s = 173,778.2
2025-01-15 08:15:18.547 | DEBUG    | __main__:<module>:313 - Training step 640: loss = 4.3380 | 3018.27ms | Tokens/s = 173,704.7
2025-01-15 08:15:48.732 | DEBUG    | __main__:<module>:313 - Training step 650: loss = 4.1676 | 3018.29ms | Tokens/s = 173,703.6
2025-01-15 08:16:18.928 | DEBUG    | __main__:<module>:313 - Training step 660: loss = 4.1199 | 3019.64ms | Tokens/s = 173,626.2
2025-01-15 08:16:49.137 | DEBUG    | __main__:<module>:313 - Training step 670: loss = 4.1872 | 3020.87ms | Tokens/s = 173,555.2
2025-01-15 08:17:19.346 | DEBUG    | __main__:<module>:313 - Training step 680: loss = 4.1674 | 3020.39ms | Tokens/s = 173,583.2
2025-01-15 08:17:49.549 | DEBUG    | __main__:<module>:313 - Training step 690: loss = 4.1158 | 3022.72ms | Tokens/s = 173,449.1
2025-01-15 08:18:19.769 | DEBUG    | __main__:<module>:313 - Training step 700: loss = 4.1888 | 3022.60ms | Tokens/s = 173,455.8
2025-01-15 08:18:50.005 | DEBUG    | __main__:<module>:313 - Training step 710: loss = 4.2155 | 3022.82ms | Tokens/s = 173,443.3
2025-01-15 08:19:20.241 | DEBUG    | __main__:<module>:313 - Training step 720: loss = 4.0900 | 3023.40ms | Tokens/s = 173,409.8
2025-01-15 08:19:50.465 | DEBUG    | __main__:<module>:313 - Training step 730: loss = 4.0593 | 3022.59ms | Tokens/s = 173,456.5
2025-01-15 08:20:20.693 | DEBUG    | __main__:<module>:313 - Training step 740: loss = 4.0891 | 3020.76ms | Tokens/s = 173,561.8
2025-01-15 08:20:50.890 | DEBUG    | __main__:<module>:313 - Training step 750: loss = 4.0373 | 3018.03ms | Tokens/s = 173,718.6
2025-01-15 08:21:21.080 | DEBUG    | __main__:<module>:313 - Training step 760: loss = 4.0832 | 3021.45ms | Tokens/s = 173,521.9
2025-01-15 08:21:51.269 | DEBUG    | __main__:<module>:313 - Training step 770: loss = 4.1228 | 3020.04ms | Tokens/s = 173,602.8
2025-01-15 08:22:21.478 | DEBUG    | __main__:<module>:313 - Training step 780: loss = 4.0908 | 3021.86ms | Tokens/s = 173,498.4
2025-01-15 08:22:51.691 | DEBUG    | __main__:<module>:313 - Training step 790: loss = 4.0754 | 3017.72ms | Tokens/s = 173,736.6
2025-01-15 08:23:21.852 | DEBUG    | __main__:<module>:313 - Training step 800: loss = 3.9711 | 3013.70ms | Tokens/s = 173,968.0
2025-01-15 08:23:52.009 | DEBUG    | __main__:<module>:313 - Training step 810: loss = 4.1053 | 3016.69ms | Tokens/s = 173,795.8
2025-01-15 08:24:22.194 | DEBUG    | __main__:<module>:313 - Training step 820: loss = 4.0404 | 3018.86ms | Tokens/s = 173,670.9
2025-01-15 08:24:52.392 | DEBUG    | __main__:<module>:313 - Training step 830: loss = 4.0442 | 3020.61ms | Tokens/s = 173,570.5
2025-01-15 08:25:22.587 | DEBUG    | __main__:<module>:313 - Training step 840: loss = 4.2435 | 3016.75ms | Tokens/s = 173,792.0
2025-01-15 08:25:52.773 | DEBUG    | __main__:<module>:313 - Training step 850: loss = 4.0551 | 3021.06ms | Tokens/s = 173,544.6
2025-01-15 08:26:22.984 | DEBUG    | __main__:<module>:313 - Training step 860: loss = 3.8195 | 3023.80ms | Tokens/s = 173,386.9
2025-01-15 08:26:53.211 | DEBUG    | __main__:<module>:313 - Training step 870: loss = 4.2469 | 3022.68ms | Tokens/s = 173,451.7
2025-01-15 08:27:23.432 | DEBUG    | __main__:<module>:313 - Training step 880: loss = 3.9325 | 3021.65ms | Tokens/s = 173,510.8
2025-01-15 08:27:53.643 | DEBUG    | __main__:<module>:313 - Training step 890: loss = 3.9618 | 3020.10ms | Tokens/s = 173,599.7
2025-01-15 08:28:23.873 | DEBUG    | __main__:<module>:313 - Training step 900: loss = 3.9328 | 3020.30ms | Tokens/s = 173,588.1
2025-01-15 08:28:54.089 | DEBUG    | __main__:<module>:313 - Training step 910: loss = 4.0150 | 3020.70ms | Tokens/s = 173,565.3
2025-01-15 08:29:24.264 | DEBUG    | __main__:<module>:313 - Training step 920: loss = 3.9548 | 3018.57ms | Tokens/s = 173,687.8
2025-01-15 08:29:54.451 | DEBUG    | __main__:<module>:313 - Training step 930: loss = 4.0511 | 3017.72ms | Tokens/s = 173,736.4
2025-01-15 08:30:24.656 | DEBUG    | __main__:<module>:313 - Training step 940: loss = 3.9426 | 3021.39ms | Tokens/s = 173,525.3
2025-01-15 08:30:54.862 | DEBUG    | __main__:<module>:313 - Training step 950: loss = 3.8232 | 3020.11ms | Tokens/s = 173,599.0
2025-01-15 08:31:25.070 | DEBUG    | __main__:<module>:313 - Training step 960: loss = 3.8587 | 3018.42ms | Tokens/s = 173,696.4
2025-01-15 08:31:55.268 | DEBUG    | __main__:<module>:313 - Training step 970: loss = 3.9977 | 3019.92ms | Tokens/s = 173,609.8
2025-01-15 08:32:25.477 | DEBUG    | __main__:<module>:313 - Training step 980: loss = 3.9640 | 3020.91ms | Tokens/s = 173,553.2
2025-01-15 08:32:55.705 | DEBUG    | __main__:<module>:313 - Training step 990: loss = 3.9249 | 3022.36ms | Tokens/s = 173,469.7
2025-01-15 08:33:29.365 | INFO     | __main__:<module>:265 - Step 1,000/40,000 loss: 3.9394 (T) 3.9287 (V) | lr=5.0e-03
2025-01-15 08:33:29.366 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 08:33:43.025 | DEBUG    | __main__:<module>:313 - Training step 1000: loss = 3.9062 | 20126.36ms | Tokens/s = 26,049.8
2025-01-15 08:34:13.120 | DEBUG    | __main__:<module>:313 - Training step 1010: loss = 3.8866 | 3011.67ms | Tokens/s = 174,085.5
2025-01-15 08:34:43.284 | DEBUG    | __main__:<module>:313 - Training step 1020: loss = 3.9614 | 3018.37ms | Tokens/s = 173,699.3
2025-01-15 08:35:13.458 | DEBUG    | __main__:<module>:313 - Training step 1030: loss = 3.8912 | 3017.54ms | Tokens/s = 173,746.6
2025-01-15 08:35:43.645 | DEBUG    | __main__:<module>:313 - Training step 1040: loss = 3.9474 | 3018.36ms | Tokens/s = 173,699.7
2025-01-15 08:36:13.854 | DEBUG    | __main__:<module>:313 - Training step 1050: loss = 3.8366 | 3020.06ms | Tokens/s = 173,601.9
2025-01-15 08:36:44.068 | DEBUG    | __main__:<module>:313 - Training step 1060: loss = 3.9981 | 3020.78ms | Tokens/s = 173,560.7
2025-01-15 08:37:14.293 | DEBUG    | __main__:<module>:313 - Training step 1070: loss = 3.9353 | 3022.03ms | Tokens/s = 173,488.9
2025-01-15 08:37:44.484 | DEBUG    | __main__:<module>:313 - Training step 1080: loss = 3.9428 | 3017.89ms | Tokens/s = 173,726.4
2025-01-15 08:38:14.663 | DEBUG    | __main__:<module>:313 - Training step 1090: loss = 3.7709 | 3018.32ms | Tokens/s = 173,701.7
2025-01-15 08:38:44.826 | DEBUG    | __main__:<module>:313 - Training step 1100: loss = 3.7772 | 3013.76ms | Tokens/s = 173,965.0
2025-01-15 08:39:14.976 | DEBUG    | __main__:<module>:313 - Training step 1110: loss = 3.9213 | 3014.91ms | Tokens/s = 173,898.4
2025-01-15 08:39:45.142 | DEBUG    | __main__:<module>:313 - Training step 1120: loss = 3.6763 | 3018.31ms | Tokens/s = 173,702.6
2025-01-15 08:40:15.349 | DEBUG    | __main__:<module>:313 - Training step 1130: loss = 3.9858 | 3020.47ms | Tokens/s = 173,578.2
2025-01-15 08:40:45.562 | DEBUG    | __main__:<module>:313 - Training step 1140: loss = 3.9765 | 3022.21ms | Tokens/s = 173,478.5
2025-01-15 08:41:15.771 | DEBUG    | __main__:<module>:313 - Training step 1150: loss = 3.9299 | 3018.78ms | Tokens/s = 173,675.4
2025-01-15 08:41:45.955 | DEBUG    | __main__:<module>:313 - Training step 1160: loss = 3.9707 | 3017.99ms | Tokens/s = 173,720.7
2025-01-15 08:42:16.138 | DEBUG    | __main__:<module>:313 - Training step 1170: loss = 3.9985 | 3020.73ms | Tokens/s = 173,563.2
2025-01-15 08:42:46.342 | DEBUG    | __main__:<module>:313 - Training step 1180: loss = 3.9675 | 3020.07ms | Tokens/s = 173,601.1
2025-01-15 08:43:16.558 | DEBUG    | __main__:<module>:313 - Training step 1190: loss = 3.8335 | 3023.70ms | Tokens/s = 173,392.8
2025-01-15 08:43:46.761 | DEBUG    | __main__:<module>:313 - Training step 1200: loss = 3.7944 | 3017.94ms | Tokens/s = 173,723.9
2025-01-15 08:44:16.958 | DEBUG    | __main__:<module>:313 - Training step 1210: loss = 3.8182 | 3020.69ms | Tokens/s = 173,565.7
2025-01-15 08:44:47.158 | DEBUG    | __main__:<module>:313 - Training step 1220: loss = 3.7048 | 3018.33ms | Tokens/s = 173,701.3
2025-01-15 08:45:17.338 | DEBUG    | __main__:<module>:313 - Training step 1230: loss = 3.7637 | 3015.62ms | Tokens/s = 173,857.3
2025-01-15 08:45:47.510 | DEBUG    | __main__:<module>:313 - Training step 1240: loss = 3.8997 | 3020.36ms | Tokens/s = 173,584.8
2025-01-15 08:46:17.714 | DEBUG    | __main__:<module>:313 - Training step 1250: loss = 3.7791 | 3021.52ms | Tokens/s = 173,517.8
2025-01-15 08:46:47.913 | DEBUG    | __main__:<module>:313 - Training step 1260: loss = 3.8968 | 3019.79ms | Tokens/s = 173,617.3
2025-01-15 08:47:18.111 | DEBUG    | __main__:<module>:313 - Training step 1270: loss = 3.7346 | 3020.63ms | Tokens/s = 173,569.0
2025-01-15 08:47:48.319 | DEBUG    | __main__:<module>:313 - Training step 1280: loss = 3.7397 | 3021.54ms | Tokens/s = 173,516.6
2025-01-15 08:48:18.545 | DEBUG    | __main__:<module>:313 - Training step 1290: loss = 3.8679 | 3021.61ms | Tokens/s = 173,512.8
2025-01-15 08:48:48.758 | DEBUG    | __main__:<module>:313 - Training step 1300: loss = 3.8433 | 3019.44ms | Tokens/s = 173,637.7
2025-01-15 08:49:18.973 | DEBUG    | __main__:<module>:313 - Training step 1310: loss = 3.8681 | 3019.75ms | Tokens/s = 173,619.7
2025-01-15 08:49:49.172 | DEBUG    | __main__:<module>:313 - Training step 1320: loss = 3.7878 | 3019.57ms | Tokens/s = 173,629.9
2025-01-15 08:50:19.361 | DEBUG    | __main__:<module>:313 - Training step 1330: loss = 3.7725 | 3020.17ms | Tokens/s = 173,595.5
2025-01-15 08:50:49.560 | DEBUG    | __main__:<module>:313 - Training step 1340: loss = 3.8015 | 3018.85ms | Tokens/s = 173,671.3
2025-01-15 08:51:19.765 | DEBUG    | __main__:<module>:313 - Training step 1350: loss = 3.6379 | 3020.57ms | Tokens/s = 173,572.8
2025-01-15 08:51:49.983 | DEBUG    | __main__:<module>:313 - Training step 1360: loss = 3.6653 | 3018.69ms | Tokens/s = 173,680.7
2025-01-15 08:52:20.209 | DEBUG    | __main__:<module>:313 - Training step 1370: loss = 3.6857 | 3020.59ms | Tokens/s = 173,571.2
2025-01-15 08:52:50.432 | DEBUG    | __main__:<module>:313 - Training step 1380: loss = 3.8279 | 3020.86ms | Tokens/s = 173,556.1
2025-01-15 08:53:20.652 | DEBUG    | __main__:<module>:313 - Training step 1390: loss = 3.9135 | 3023.50ms | Tokens/s = 173,404.3
2025-01-15 08:53:50.880 | DEBUG    | __main__:<module>:313 - Training step 1400: loss = 3.6789 | 3021.50ms | Tokens/s = 173,519.1
2025-01-15 08:54:21.089 | DEBUG    | __main__:<module>:313 - Training step 1410: loss = 3.6904 | 3021.49ms | Tokens/s = 173,519.8
2025-01-15 08:54:51.285 | DEBUG    | __main__:<module>:313 - Training step 1420: loss = 3.7454 | 3019.98ms | Tokens/s = 173,606.6
2025-01-15 08:55:21.490 | DEBUG    | __main__:<module>:313 - Training step 1430: loss = 3.8655 | 3021.92ms | Tokens/s = 173,495.0
2025-01-15 08:55:51.713 | DEBUG    | __main__:<module>:313 - Training step 1440: loss = 3.7181 | 3022.73ms | Tokens/s = 173,448.5
2025-01-15 08:56:21.955 | DEBUG    | __main__:<module>:313 - Training step 1450: loss = 3.7792 | 3025.73ms | Tokens/s = 173,276.5
2025-01-15 08:56:52.181 | DEBUG    | __main__:<module>:313 - Training step 1460: loss = 3.6070 | 3018.85ms | Tokens/s = 173,671.4
2025-01-15 08:57:22.367 | DEBUG    | __main__:<module>:313 - Training step 1470: loss = 3.7024 | 3017.78ms | Tokens/s = 173,732.9
2025-01-15 08:57:52.541 | DEBUG    | __main__:<module>:313 - Training step 1480: loss = 3.8618 | 3018.26ms | Tokens/s = 173,705.2
2025-01-15 08:58:22.706 | DEBUG    | __main__:<module>:313 - Training step 1490: loss = 3.4895 | 3012.72ms | Tokens/s = 174,025.0
2025-01-15 08:58:52.853 | DEBUG    | __main__:<module>:313 - Training step 1500: loss = 3.6670 | 3013.84ms | Tokens/s = 173,959.9
2025-01-15 08:59:23.032 | DEBUG    | __main__:<module>:313 - Training step 1510: loss = 3.8955 | 3019.45ms | Tokens/s = 173,637.1
2025-01-15 08:59:53.229 | DEBUG    | __main__:<module>:313 - Training step 1520: loss = 3.7626 | 3020.98ms | Tokens/s = 173,548.9
2025-01-15 09:00:23.438 | DEBUG    | __main__:<module>:313 - Training step 1530: loss = 3.7397 | 3022.09ms | Tokens/s = 173,485.3
2025-01-15 09:00:53.649 | DEBUG    | __main__:<module>:313 - Training step 1540: loss = 3.7897 | 3020.64ms | Tokens/s = 173,568.3
2025-01-15 09:01:23.861 | DEBUG    | __main__:<module>:313 - Training step 1550: loss = 3.9315 | 3023.39ms | Tokens/s = 173,410.8
2025-01-15 09:01:54.086 | DEBUG    | __main__:<module>:313 - Training step 1560: loss = 3.5841 | 3023.11ms | Tokens/s = 173,426.9
2025-01-15 09:02:24.284 | DEBUG    | __main__:<module>:313 - Training step 1570: loss = 3.5279 | 3017.32ms | Tokens/s = 173,759.6
2025-01-15 09:02:54.458 | DEBUG    | __main__:<module>:313 - Training step 1580: loss = 3.6547 | 3018.03ms | Tokens/s = 173,718.8
2025-01-15 09:03:24.661 | DEBUG    | __main__:<module>:313 - Training step 1590: loss = 3.6729 | 3021.38ms | Tokens/s = 173,526.3
2025-01-15 09:03:54.876 | DEBUG    | __main__:<module>:313 - Training step 1600: loss = 3.7685 | 3021.75ms | Tokens/s = 173,504.6
2025-01-15 09:04:25.092 | DEBUG    | __main__:<module>:313 - Training step 1610: loss = 3.8248 | 3020.16ms | Tokens/s = 173,596.2
2025-01-15 09:04:55.278 | DEBUG    | __main__:<module>:313 - Training step 1620: loss = 3.7478 | 3017.75ms | Tokens/s = 173,734.8
2025-01-15 09:05:25.464 | DEBUG    | __main__:<module>:313 - Training step 1630: loss = 3.5654 | 3019.47ms | Tokens/s = 173,635.6
2025-01-15 09:05:55.673 | DEBUG    | __main__:<module>:313 - Training step 1640: loss = 3.7841 | 3022.06ms | Tokens/s = 173,486.9
2025-01-15 09:06:25.889 | DEBUG    | __main__:<module>:313 - Training step 1650: loss = 3.6596 | 3022.02ms | Tokens/s = 173,489.3
2025-01-15 09:06:56.101 | DEBUG    | __main__:<module>:313 - Training step 1660: loss = 3.7835 | 3019.50ms | Tokens/s = 173,634.3
2025-01-15 09:07:26.312 | DEBUG    | __main__:<module>:313 - Training step 1670: loss = 3.5758 | 3021.08ms | Tokens/s = 173,543.2
2025-01-15 09:07:56.535 | DEBUG    | __main__:<module>:313 - Training step 1680: loss = 3.5340 | 3020.44ms | Tokens/s = 173,580.0
2025-01-15 09:08:26.735 | DEBUG    | __main__:<module>:313 - Training step 1690: loss = 3.6560 | 3015.10ms | Tokens/s = 173,887.4
2025-01-15 09:08:56.917 | DEBUG    | __main__:<module>:313 - Training step 1700: loss = 3.6833 | 3021.25ms | Tokens/s = 173,533.4
2025-01-15 09:09:27.114 | DEBUG    | __main__:<module>:313 - Training step 1710: loss = 3.3381 | 3019.68ms | Tokens/s = 173,623.9
2025-01-15 09:09:57.331 | DEBUG    | __main__:<module>:313 - Training step 1720: loss = 3.7622 | 3023.10ms | Tokens/s = 173,427.5
2025-01-15 09:10:27.546 | DEBUG    | __main__:<module>:313 - Training step 1730: loss = 3.7555 | 3020.99ms | Tokens/s = 173,548.5
2025-01-15 09:10:57.716 | DEBUG    | __main__:<module>:313 - Training step 1740: loss = 3.7025 | 3018.06ms | Tokens/s = 173,716.9
2025-01-15 09:11:27.902 | DEBUG    | __main__:<module>:313 - Training step 1750: loss = 3.5118 | 3019.59ms | Tokens/s = 173,628.9
2025-01-15 09:11:58.108 | DEBUG    | __main__:<module>:313 - Training step 1760: loss = 3.7259 | 3020.53ms | Tokens/s = 173,574.9
2025-01-15 09:12:28.325 | DEBUG    | __main__:<module>:313 - Training step 1770: loss = 3.6959 | 3019.33ms | Tokens/s = 173,643.9
2025-01-15 09:12:58.517 | DEBUG    | __main__:<module>:313 - Training step 1780: loss = 3.6377 | 3020.49ms | Tokens/s = 173,576.9
2025-01-15 09:13:28.696 | DEBUG    | __main__:<module>:313 - Training step 1790: loss = 3.6764 | 3018.48ms | Tokens/s = 173,693.0
2025-01-15 09:13:58.902 | DEBUG    | __main__:<module>:313 - Training step 1800: loss = 3.7334 | 3020.66ms | Tokens/s = 173,567.3
2025-01-15 09:14:29.117 | DEBUG    | __main__:<module>:313 - Training step 1810: loss = 3.6784 | 3023.81ms | Tokens/s = 173,386.7
2025-01-15 09:14:59.322 | DEBUG    | __main__:<module>:313 - Training step 1820: loss = 3.5994 | 3018.33ms | Tokens/s = 173,701.6
2025-01-15 09:15:29.539 | DEBUG    | __main__:<module>:313 - Training step 1830: loss = 3.5840 | 3022.25ms | Tokens/s = 173,475.9
2025-01-15 09:15:59.764 | DEBUG    | __main__:<module>:313 - Training step 1840: loss = 3.5142 | 3020.00ms | Tokens/s = 173,605.4
2025-01-15 09:16:29.957 | DEBUG    | __main__:<module>:313 - Training step 1850: loss = 3.6396 | 3018.78ms | Tokens/s = 173,675.2
2025-01-15 09:17:00.134 | DEBUG    | __main__:<module>:313 - Training step 1860: loss = 3.4476 | 3018.02ms | Tokens/s = 173,719.3
2025-01-15 09:17:30.329 | DEBUG    | __main__:<module>:313 - Training step 1870: loss = 3.7594 | 3020.20ms | Tokens/s = 173,593.6
2025-01-15 09:18:00.541 | DEBUG    | __main__:<module>:313 - Training step 1880: loss = 3.5975 | 3021.49ms | Tokens/s = 173,519.5
2025-01-15 09:18:30.761 | DEBUG    | __main__:<module>:313 - Training step 1890: loss = 3.7196 | 3021.98ms | Tokens/s = 173,491.4
2025-01-15 09:19:00.980 | DEBUG    | __main__:<module>:313 - Training step 1900: loss = 3.5772 | 3020.97ms | Tokens/s = 173,549.6
2025-01-15 09:19:31.185 | DEBUG    | __main__:<module>:313 - Training step 1910: loss = 3.6217 | 3019.31ms | Tokens/s = 173,645.1
2025-01-15 09:20:01.377 | DEBUG    | __main__:<module>:313 - Training step 1920: loss = 3.8059 | 3019.49ms | Tokens/s = 173,634.9
2025-01-15 09:20:31.586 | DEBUG    | __main__:<module>:313 - Training step 1930: loss = 3.7475 | 3020.89ms | Tokens/s = 173,554.1
2025-01-15 09:21:01.805 | DEBUG    | __main__:<module>:313 - Training step 1940: loss = 3.6390 | 3021.75ms | Tokens/s = 173,504.8
2025-01-15 09:21:32.030 | DEBUG    | __main__:<module>:313 - Training step 1950: loss = 3.6621 | 3022.88ms | Tokens/s = 173,440.0
2025-01-15 09:22:02.270 | DEBUG    | __main__:<module>:313 - Training step 1960: loss = 3.7567 | 3024.29ms | Tokens/s = 173,358.9
2025-01-15 09:22:32.489 | DEBUG    | __main__:<module>:313 - Training step 1970: loss = 3.6024 | 3018.79ms | Tokens/s = 173,674.8
2025-01-15 09:23:02.678 | DEBUG    | __main__:<module>:313 - Training step 1980: loss = 3.6864 | 3020.38ms | Tokens/s = 173,583.5
2025-01-15 09:23:32.890 | DEBUG    | __main__:<module>:313 - Training step 1990: loss = 3.5709 | 3021.02ms | Tokens/s = 173,546.5
2025-01-15 09:24:06.566 | INFO     | __main__:<module>:265 - Step 2,000/40,000 loss: 3.6725 (T) 3.6591 (V) | lr=1.0e-02
2025-01-15 09:24:06.567 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 09:24:20.037 | DEBUG    | __main__:<module>:313 - Training step 2000: loss = 3.6503 | 19939.14ms | Tokens/s = 26,294.4
2025-01-15 09:24:50.146 | DEBUG    | __main__:<module>:313 - Training step 2010: loss = 3.6815 | 3010.27ms | Tokens/s = 174,166.4
2025-01-15 09:25:20.305 | DEBUG    | __main__:<module>:313 - Training step 2020: loss = 3.6494 | 3018.31ms | Tokens/s = 173,702.7
2025-01-15 09:25:50.494 | DEBUG    | __main__:<module>:313 - Training step 2030: loss = 3.4722 | 3020.98ms | Tokens/s = 173,548.8
2025-01-15 09:26:20.712 | DEBUG    | __main__:<module>:313 - Training step 2040: loss = 3.3982 | 3023.80ms | Tokens/s = 173,387.1
2025-01-15 09:26:50.899 | DEBUG    | __main__:<module>:313 - Training step 2050: loss = 3.5966 | 3017.24ms | Tokens/s = 173,764.3
2025-01-15 09:27:21.079 | DEBUG    | __main__:<module>:313 - Training step 2060: loss = 3.4871 | 3018.95ms | Tokens/s = 173,665.6
2025-01-15 09:27:51.295 | DEBUG    | __main__:<module>:313 - Training step 2070: loss = 3.3830 | 3021.16ms | Tokens/s = 173,538.7
2025-01-15 09:28:21.522 | DEBUG    | __main__:<module>:313 - Training step 2080: loss = 3.6125 | 3022.58ms | Tokens/s = 173,457.1
2025-01-15 09:28:51.715 | DEBUG    | __main__:<module>:313 - Training step 2090: loss = 3.7412 | 3014.84ms | Tokens/s = 173,902.5
2025-01-15 09:29:21.896 | DEBUG    | __main__:<module>:313 - Training step 2100: loss = 3.7533 | 3018.22ms | Tokens/s = 173,707.8
2025-01-15 09:29:52.101 | DEBUG    | __main__:<module>:313 - Training step 2110: loss = 3.6593 | 3022.49ms | Tokens/s = 173,462.1
2025-01-15 09:30:22.320 | DEBUG    | __main__:<module>:313 - Training step 2120: loss = 3.6017 | 3020.72ms | Tokens/s = 173,563.8
2025-01-15 09:30:52.502 | DEBUG    | __main__:<module>:313 - Training step 2130: loss = 3.6036 | 3015.82ms | Tokens/s = 173,845.7
2025-01-15 09:31:22.693 | DEBUG    | __main__:<module>:313 - Training step 2140: loss = 3.6827 | 3022.26ms | Tokens/s = 173,475.3
2025-01-15 09:31:52.890 | DEBUG    | __main__:<module>:313 - Training step 2150: loss = 3.6224 | 3019.24ms | Tokens/s = 173,649.1
2025-01-15 09:32:23.099 | DEBUG    | __main__:<module>:313 - Training step 2160: loss = 3.5219 | 3021.69ms | Tokens/s = 173,508.2
2025-01-15 09:32:53.309 | DEBUG    | __main__:<module>:313 - Training step 2170: loss = 3.6813 | 3019.05ms | Tokens/s = 173,659.9
2025-01-15 09:33:23.521 | DEBUG    | __main__:<module>:313 - Training step 2180: loss = 3.5647 | 3020.59ms | Tokens/s = 173,571.7
2025-01-15 09:33:53.735 | DEBUG    | __main__:<module>:313 - Training step 2190: loss = 3.8098 | 3020.48ms | Tokens/s = 173,577.9
2025-01-15 09:34:23.922 | DEBUG    | __main__:<module>:313 - Training step 2200: loss = 3.6734 | 3019.46ms | Tokens/s = 173,636.1
2025-01-15 09:34:54.114 | DEBUG    | __main__:<module>:313 - Training step 2210: loss = 3.7497 | 3017.57ms | Tokens/s = 173,745.1
2025-01-15 09:35:24.314 | DEBUG    | __main__:<module>:313 - Training step 2220: loss = 3.6258 | 3020.11ms | Tokens/s = 173,599.2
2025-01-15 09:35:54.534 | DEBUG    | __main__:<module>:313 - Training step 2230: loss = 3.6313 | 3022.57ms | Tokens/s = 173,457.9
2025-01-15 09:36:24.742 | DEBUG    | __main__:<module>:313 - Training step 2240: loss = 3.5905 | 3019.51ms | Tokens/s = 173,633.6
2025-01-15 09:36:54.911 | DEBUG    | __main__:<module>:313 - Training step 2250: loss = 3.6791 | 3014.40ms | Tokens/s = 173,927.9
2025-01-15 09:37:25.076 | DEBUG    | __main__:<module>:313 - Training step 2260: loss = 3.6501 | 3017.55ms | Tokens/s = 173,746.0
2025-01-15 09:37:55.268 | DEBUG    | __main__:<module>:313 - Training step 2270: loss = 3.6618 | 3019.01ms | Tokens/s = 173,662.4
2025-01-15 09:38:25.473 | DEBUG    | __main__:<module>:313 - Training step 2280: loss = 3.5672 | 3021.36ms | Tokens/s = 173,527.0
2025-01-15 09:38:55.678 | DEBUG    | __main__:<module>:313 - Training step 2290: loss = 3.5224 | 3020.83ms | Tokens/s = 173,557.8
2025-01-15 09:39:25.881 | DEBUG    | __main__:<module>:313 - Training step 2300: loss = 3.5866 | 3017.95ms | Tokens/s = 173,723.4
2025-01-15 09:39:56.093 | DEBUG    | __main__:<module>:313 - Training step 2310: loss = 3.5739 | 3022.10ms | Tokens/s = 173,484.5
2025-01-15 09:40:26.285 | DEBUG    | __main__:<module>:313 - Training step 2320: loss = 3.6476 | 3018.67ms | Tokens/s = 173,681.9
2025-01-15 09:40:56.478 | DEBUG    | __main__:<module>:313 - Training step 2330: loss = 3.6513 | 3018.25ms | Tokens/s = 173,705.8
2025-01-15 09:41:26.679 | DEBUG    | __main__:<module>:313 - Training step 2340: loss = 3.4236 | 3018.64ms | Tokens/s = 173,683.2
2025-01-15 09:41:56.893 | DEBUG    | __main__:<module>:313 - Training step 2350: loss = 3.6323 | 3021.86ms | Tokens/s = 173,498.4
2025-01-15 09:42:27.127 | DEBUG    | __main__:<module>:313 - Training step 2360: loss = 3.3995 | 3025.08ms | Tokens/s = 173,313.5
2025-01-15 09:42:57.328 | DEBUG    | __main__:<module>:313 - Training step 2370: loss = 3.5133 | 3015.59ms | Tokens/s = 173,859.4
2025-01-15 09:43:27.503 | DEBUG    | __main__:<module>:313 - Training step 2380: loss = 3.5157 | 3017.50ms | Tokens/s = 173,749.3
2025-01-15 09:43:57.695 | DEBUG    | __main__:<module>:313 - Training step 2390: loss = 3.6396 | 3019.43ms | Tokens/s = 173,638.1
2025-01-15 09:44:27.903 | DEBUG    | __main__:<module>:313 - Training step 2400: loss = 3.5125 | 3017.51ms | Tokens/s = 173,748.4
2025-01-15 09:44:58.100 | DEBUG    | __main__:<module>:313 - Training step 2410: loss = 3.6061 | 3019.33ms | Tokens/s = 173,644.1
2025-01-15 09:45:28.287 | DEBUG    | __main__:<module>:313 - Training step 2420: loss = 6.1111 | 3011.34ms | Tokens/s = 174,104.6
2025-01-15 09:45:58.481 | DEBUG    | __main__:<module>:313 - Training step 2430: loss = 4.3332 | 3019.89ms | Tokens/s = 173,611.4
2025-01-15 09:46:28.661 | DEBUG    | __main__:<module>:313 - Training step 2440: loss = 4.0100 | 3015.76ms | Tokens/s = 173,849.7
2025-01-15 09:46:58.813 | DEBUG    | __main__:<module>:313 - Training step 2450: loss = 3.6824 | 3015.32ms | Tokens/s = 173,874.9
2025-01-15 09:47:28.967 | DEBUG    | __main__:<module>:313 - Training step 2460: loss = 3.7108 | 3018.76ms | Tokens/s = 173,676.8
2025-01-15 09:47:59.154 | DEBUG    | __main__:<module>:313 - Training step 2470: loss = 3.7050 | 3019.38ms | Tokens/s = 173,641.0
2025-01-15 09:48:29.359 | DEBUG    | __main__:<module>:313 - Training step 2480: loss = 3.5757 | 3020.76ms | Tokens/s = 173,561.4
2025-01-15 09:48:59.571 | DEBUG    | __main__:<module>:313 - Training step 2490: loss = 3.5878 | 3020.01ms | Tokens/s = 173,604.6
2025-01-15 09:49:29.789 | DEBUG    | __main__:<module>:313 - Training step 2500: loss = 3.6839 | 3022.09ms | Tokens/s = 173,485.0
2025-01-15 09:50:00.015 | DEBUG    | __main__:<module>:313 - Training step 2510: loss = 3.6325 | 3023.04ms | Tokens/s = 173,431.0
2025-01-15 09:50:30.226 | DEBUG    | __main__:<module>:313 - Training step 2520: loss = 3.4974 | 3021.37ms | Tokens/s = 173,526.3
2025-01-15 09:51:00.398 | DEBUG    | __main__:<module>:313 - Training step 2530: loss = 3.5785 | 3017.04ms | Tokens/s = 173,775.9
2025-01-15 09:51:30.575 | DEBUG    | __main__:<module>:313 - Training step 2540: loss = 3.5390 | 3018.85ms | Tokens/s = 173,671.5
2025-01-15 09:52:00.766 | DEBUG    | __main__:<module>:313 - Training step 2550: loss = 3.5336 | 3019.43ms | Tokens/s = 173,638.0
2025-01-15 09:52:30.968 | DEBUG    | __main__:<module>:313 - Training step 2560: loss = 3.6836 | 3019.45ms | Tokens/s = 173,637.0
2025-01-15 09:53:01.146 | DEBUG    | __main__:<module>:313 - Training step 2570: loss = 3.5937 | 3018.28ms | Tokens/s = 173,704.0
2025-01-15 09:53:31.338 | DEBUG    | __main__:<module>:313 - Training step 2580: loss = 3.4785 | 3019.96ms | Tokens/s = 173,607.7
2025-01-15 09:54:01.544 | DEBUG    | __main__:<module>:313 - Training step 2590: loss = 3.3353 | 3019.53ms | Tokens/s = 173,632.1
2025-01-15 09:54:31.728 | DEBUG    | __main__:<module>:313 - Training step 2600: loss = 3.5382 | 3014.79ms | Tokens/s = 173,905.0
2025-01-15 09:55:01.919 | DEBUG    | __main__:<module>:313 - Training step 2610: loss = 3.4884 | 3019.54ms | Tokens/s = 173,631.5
2025-01-15 09:55:32.123 | DEBUG    | __main__:<module>:313 - Training step 2620: loss = 3.6127 | 3020.07ms | Tokens/s = 173,601.3
2025-01-15 09:56:02.347 | DEBUG    | __main__:<module>:313 - Training step 2630: loss = 3.6322 | 3021.70ms | Tokens/s = 173,507.5
2025-01-15 09:56:32.562 | DEBUG    | __main__:<module>:313 - Training step 2640: loss = 3.4879 | 3020.72ms | Tokens/s = 173,564.0
2025-01-15 09:57:02.725 | DEBUG    | __main__:<module>:313 - Training step 2650: loss = 3.5491 | 3013.35ms | Tokens/s = 173,988.7
2025-01-15 09:57:32.883 | DEBUG    | __main__:<module>:313 - Training step 2660: loss = 3.5638 | 3018.43ms | Tokens/s = 173,695.5
2025-01-15 09:58:03.072 | DEBUG    | __main__:<module>:313 - Training step 2670: loss = 3.7833 | 3020.45ms | Tokens/s = 173,579.4
2025-01-15 09:58:33.274 | DEBUG    | __main__:<module>:313 - Training step 2680: loss = 3.6647 | 3020.27ms | Tokens/s = 173,589.8
2025-01-15 09:59:03.482 | DEBUG    | __main__:<module>:313 - Training step 2690: loss = 3.6878 | 3022.94ms | Tokens/s = 173,436.5
2025-01-15 09:59:33.694 | DEBUG    | __main__:<module>:313 - Training step 2700: loss = 3.4650 | 3021.08ms | Tokens/s = 173,543.3
2025-01-15 10:00:03.915 | DEBUG    | __main__:<module>:313 - Training step 2710: loss = 3.6430 | 3022.21ms | Tokens/s = 173,478.2
2025-01-15 10:00:34.116 | DEBUG    | __main__:<module>:313 - Training step 2720: loss = 3.4910 | 3018.94ms | Tokens/s = 173,666.0
2025-01-15 10:01:04.285 | DEBUG    | __main__:<module>:313 - Training step 2730: loss = 3.5427 | 3016.13ms | Tokens/s = 173,828.3
2025-01-15 10:01:34.464 | DEBUG    | __main__:<module>:313 - Training step 2740: loss = 3.7801 | 3018.08ms | Tokens/s = 173,715.6
2025-01-15 10:02:04.661 | DEBUG    | __main__:<module>:313 - Training step 2750: loss = 3.6716 | 3020.28ms | Tokens/s = 173,589.0
2025-01-15 10:02:34.868 | DEBUG    | __main__:<module>:313 - Training step 2760: loss = 3.5416 | 3020.84ms | Tokens/s = 173,557.2
2025-01-15 10:03:05.057 | DEBUG    | __main__:<module>:313 - Training step 2770: loss = 3.7074 | 3017.37ms | Tokens/s = 173,756.8
2025-01-15 10:03:35.247 | DEBUG    | __main__:<module>:313 - Training step 2780: loss = 3.4020 | 3019.24ms | Tokens/s = 173,649.2
2025-01-15 10:04:05.460 | DEBUG    | __main__:<module>:313 - Training step 2790: loss = 3.5343 | 3021.40ms | Tokens/s = 173,524.7
2025-01-15 10:04:35.690 | DEBUG    | __main__:<module>:313 - Training step 2800: loss = 3.6309 | 3024.04ms | Tokens/s = 173,373.2
2025-01-15 10:05:05.910 | DEBUG    | __main__:<module>:313 - Training step 2810: loss = 3.7370 | 3019.29ms | Tokens/s = 173,646.0
2025-01-15 10:05:36.129 | DEBUG    | __main__:<module>:313 - Training step 2820: loss = 3.5216 | 3022.06ms | Tokens/s = 173,487.0
2025-01-15 10:06:06.348 | DEBUG    | __main__:<module>:313 - Training step 2830: loss = 3.5144 | 3021.85ms | Tokens/s = 173,499.1
2025-01-15 10:06:36.569 | DEBUG    | __main__:<module>:313 - Training step 2840: loss = 3.5098 | 3024.57ms | Tokens/s = 173,342.8
2025-01-15 10:07:06.786 | DEBUG    | __main__:<module>:313 - Training step 2850: loss = 3.6966 | 3020.91ms | Tokens/s = 173,553.2
2025-01-15 10:07:37.002 | DEBUG    | __main__:<module>:313 - Training step 2860: loss = 3.4939 | 3021.50ms | Tokens/s = 173,519.1
2025-01-15 10:08:07.214 | DEBUG    | __main__:<module>:313 - Training step 2870: loss = 3.5191 | 3021.22ms | Tokens/s = 173,535.2
2025-01-15 10:08:37.399 | DEBUG    | __main__:<module>:313 - Training step 2880: loss = 3.3535 | 3019.47ms | Tokens/s = 173,635.7
2025-01-15 10:09:07.593 | DEBUG    | __main__:<module>:313 - Training step 2890: loss = 3.5224 | 3019.05ms | Tokens/s = 173,660.1
2025-01-15 10:09:37.792 | DEBUG    | __main__:<module>:313 - Training step 2900: loss = 3.4177 | 3019.57ms | Tokens/s = 173,629.7
2025-01-15 10:10:08.009 | DEBUG    | __main__:<module>:313 - Training step 2910: loss = 3.2830 | 3023.05ms | Tokens/s = 173,430.2
2025-01-15 10:10:38.222 | DEBUG    | __main__:<module>:313 - Training step 2920: loss = 3.6129 | 3021.64ms | Tokens/s = 173,511.0
2025-01-15 10:11:08.386 | DEBUG    | __main__:<module>:313 - Training step 2930: loss = 3.6727 | 3012.76ms | Tokens/s = 174,022.4
2025-01-15 10:11:38.545 | DEBUG    | __main__:<module>:313 - Training step 2940: loss = 3.5263 | 3016.40ms | Tokens/s = 173,812.6
2025-01-15 10:12:08.722 | DEBUG    | __main__:<module>:313 - Training step 2950: loss = 3.6551 | 3016.43ms | Tokens/s = 173,811.0
2025-01-15 10:12:38.926 | DEBUG    | __main__:<module>:313 - Training step 2960: loss = 3.3625 | 3019.64ms | Tokens/s = 173,625.9
2025-01-15 10:13:09.084 | DEBUG    | __main__:<module>:313 - Training step 2970: loss = 3.2789 | 3016.12ms | Tokens/s = 173,828.4
2025-01-15 10:13:39.257 | DEBUG    | __main__:<module>:313 - Training step 2980: loss = 3.6957 | 3017.62ms | Tokens/s = 173,741.9
2025-01-15 10:14:09.453 | DEBUG    | __main__:<module>:313 - Training step 2990: loss = 3.6033 | 3021.97ms | Tokens/s = 173,492.2
2025-01-15 10:14:43.113 | INFO     | __main__:<module>:265 - Step 3,000/40,000 loss: 3.5459 (T) 3.5269 (V) | lr=1.0e-02
2025-01-15 10:14:43.114 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 10:14:58.444 | DEBUG    | __main__:<module>:313 - Training step 3000: loss = 3.4585 | 21794.16ms | Tokens/s = 24,056.3
2025-01-15 10:15:28.523 | DEBUG    | __main__:<module>:313 - Training step 3010: loss = 3.5424 | 3011.33ms | Tokens/s = 174,105.0
2025-01-15 10:15:58.669 | DEBUG    | __main__:<module>:313 - Training step 3020: loss = 3.6989 | 3016.67ms | Tokens/s = 173,797.1
2025-01-15 10:16:28.848 | DEBUG    | __main__:<module>:313 - Training step 3030: loss = 3.6510 | 3020.10ms | Tokens/s = 173,599.5
2025-01-15 10:16:59.052 | DEBUG    | __main__:<module>:313 - Training step 3040: loss = 3.4326 | 3018.86ms | Tokens/s = 173,671.1
2025-01-15 10:17:29.251 | DEBUG    | __main__:<module>:313 - Training step 3050: loss = 3.6165 | 3020.28ms | Tokens/s = 173,589.0
2025-01-15 10:17:59.468 | DEBUG    | __main__:<module>:313 - Training step 3060: loss = 3.5813 | 3022.45ms | Tokens/s = 173,464.8
2025-01-15 10:18:29.672 | DEBUG    | __main__:<module>:313 - Training step 3070: loss = 3.4999 | 3016.60ms | Tokens/s = 173,801.2
2025-01-15 10:18:59.844 | DEBUG    | __main__:<module>:313 - Training step 3080: loss = 3.6743 | 3017.60ms | Tokens/s = 173,743.3
2025-01-15 10:19:30.037 | DEBUG    | __main__:<module>:313 - Training step 3090: loss = 3.5719 | 3018.17ms | Tokens/s = 173,710.7
2025-01-15 10:20:00.250 | DEBUG    | __main__:<module>:313 - Training step 3100: loss = 3.5053 | 3023.28ms | Tokens/s = 173,417.0
2025-01-15 10:20:30.491 | DEBUG    | __main__:<module>:313 - Training step 3110: loss = 3.4805 | 3026.04ms | Tokens/s = 173,258.6
2025-01-15 10:21:00.706 | DEBUG    | __main__:<module>:313 - Training step 3120: loss = 3.5345 | 3021.24ms | Tokens/s = 173,534.1
2025-01-15 10:21:30.903 | DEBUG    | __main__:<module>:313 - Training step 3130: loss = 3.4398 | 3021.10ms | Tokens/s = 173,541.8
2025-01-15 10:22:01.116 | DEBUG    | __main__:<module>:313 - Training step 3140: loss = 3.6399 | 3020.40ms | Tokens/s = 173,582.4
2025-01-15 10:22:31.324 | DEBUG    | __main__:<module>:313 - Training step 3150: loss = 3.5124 | 3020.03ms | Tokens/s = 173,603.5
2025-01-15 10:23:01.497 | DEBUG    | __main__:<module>:313 - Training step 3160: loss = 3.6038 | 3015.69ms | Tokens/s = 173,853.3
2025-01-15 10:23:31.659 | DEBUG    | __main__:<module>:313 - Training step 3170: loss = 3.5074 | 3018.24ms | Tokens/s = 173,706.7
2025-01-15 10:24:01.854 | DEBUG    | __main__:<module>:313 - Training step 3180: loss = 3.6429 | 3018.32ms | Tokens/s = 173,701.7
2025-01-15 10:24:32.059 | DEBUG    | __main__:<module>:313 - Training step 3190: loss = 3.4445 | 3021.87ms | Tokens/s = 173,497.7
2025-01-15 10:25:02.251 | DEBUG    | __main__:<module>:313 - Training step 3200: loss = 3.4524 | 3015.37ms | Tokens/s = 173,871.6
2025-01-15 10:25:32.427 | DEBUG    | __main__:<module>:313 - Training step 3210: loss = 3.3467 | 3019.18ms | Tokens/s = 173,652.6
2025-01-15 10:26:02.630 | DEBUG    | __main__:<module>:313 - Training step 3220: loss = 3.5032 | 3019.41ms | Tokens/s = 173,639.1
2025-01-15 10:26:32.840 | DEBUG    | __main__:<module>:313 - Training step 3230: loss = 3.5531 | 3019.29ms | Tokens/s = 173,645.8
2025-01-15 10:27:03.034 | DEBUG    | __main__:<module>:313 - Training step 3240: loss = 3.4400 | 3015.24ms | Tokens/s = 173,879.6
2025-01-15 10:27:33.193 | DEBUG    | __main__:<module>:313 - Training step 3250: loss = 3.3483 | 3015.93ms | Tokens/s = 173,839.5
2025-01-15 10:28:03.380 | DEBUG    | __main__:<module>:313 - Training step 3260: loss = 3.5559 | 3020.67ms | Tokens/s = 173,566.8
2025-01-15 10:28:33.588 | DEBUG    | __main__:<module>:313 - Training step 3270: loss = 3.5088 | 3020.32ms | Tokens/s = 173,586.9
2025-01-15 10:29:03.797 | DEBUG    | __main__:<module>:313 - Training step 3280: loss = 3.5962 | 3019.00ms | Tokens/s = 173,662.7
2025-01-15 10:29:33.998 | DEBUG    | __main__:<module>:313 - Training step 3290: loss = 3.6507 | 3019.08ms | Tokens/s = 173,658.0
2025-01-15 10:30:04.214 | DEBUG    | __main__:<module>:313 - Training step 3300: loss = 3.5787 | 3021.61ms | Tokens/s = 173,512.9
2025-01-15 10:30:34.431 | DEBUG    | __main__:<module>:313 - Training step 3310: loss = 3.5268 | 3020.52ms | Tokens/s = 173,575.6
2025-01-15 10:31:04.646 | DEBUG    | __main__:<module>:313 - Training step 3320: loss = 3.6260 | 3020.69ms | Tokens/s = 173,565.8
2025-01-15 10:31:34.819 | DEBUG    | __main__:<module>:313 - Training step 3330: loss = 3.5347 | 3014.49ms | Tokens/s = 173,922.5
2025-01-15 10:32:04.997 | DEBUG    | __main__:<module>:313 - Training step 3340: loss = 3.5370 | 3020.79ms | Tokens/s = 173,559.7
2025-01-15 10:32:35.214 | DEBUG    | __main__:<module>:313 - Training step 3350: loss = 3.3734 | 3020.11ms | Tokens/s = 173,598.7
2025-01-15 10:33:05.429 | DEBUG    | __main__:<module>:313 - Training step 3360: loss = 3.4583 | 3020.45ms | Tokens/s = 173,579.2
2025-01-15 10:33:35.645 | DEBUG    | __main__:<module>:313 - Training step 3370: loss = 3.3379 | 3024.59ms | Tokens/s = 173,342.0
2025-01-15 10:34:05.871 | DEBUG    | __main__:<module>:313 - Training step 3380: loss = 3.4292 | 3021.71ms | Tokens/s = 173,507.0
2025-01-15 10:34:36.067 | DEBUG    | __main__:<module>:313 - Training step 3390: loss = 3.7106 | 3020.19ms | Tokens/s = 173,594.2
2025-01-15 10:35:06.241 | DEBUG    | __main__:<module>:313 - Training step 3400: loss = 3.4422 | 3016.33ms | Tokens/s = 173,816.8
2025-01-15 10:35:36.425 | DEBUG    | __main__:<module>:313 - Training step 3410: loss = 3.5206 | 3019.47ms | Tokens/s = 173,635.5
2025-01-15 10:36:06.628 | DEBUG    | __main__:<module>:313 - Training step 3420: loss = 3.6047 | 3020.81ms | Tokens/s = 173,558.7
2025-01-15 10:36:36.844 | DEBUG    | __main__:<module>:313 - Training step 3430: loss = 3.6374 | 3020.40ms | Tokens/s = 173,582.2
2025-01-15 10:37:07.034 | DEBUG    | __main__:<module>:313 - Training step 3440: loss = 3.2985 | 3018.76ms | Tokens/s = 173,676.7
2025-01-15 10:37:37.200 | DEBUG    | __main__:<module>:313 - Training step 3450: loss = 3.6021 | 3015.04ms | Tokens/s = 173,890.7
2025-01-15 10:38:07.379 | DEBUG    | __main__:<module>:313 - Training step 3460: loss = 3.3299 | 3020.64ms | Tokens/s = 173,568.3
2025-01-15 10:38:37.592 | DEBUG    | __main__:<module>:313 - Training step 3470: loss = 3.4998 | 3020.26ms | Tokens/s = 173,590.2
2025-01-15 10:39:07.807 | DEBUG    | __main__:<module>:313 - Training step 3480: loss = 3.3470 | 3024.60ms | Tokens/s = 173,341.3
2025-01-15 10:39:38.035 | DEBUG    | __main__:<module>:313 - Training step 3490: loss = 3.2998 | 3024.71ms | Tokens/s = 173,334.7
2025-01-15 10:40:08.245 | DEBUG    | __main__:<module>:313 - Training step 3500: loss = 3.6240 | 3020.02ms | Tokens/s = 173,604.3
2025-01-15 10:40:38.447 | DEBUG    | __main__:<module>:313 - Training step 3510: loss = 3.5131 | 3022.22ms | Tokens/s = 173,478.0
2025-01-15 10:41:08.648 | DEBUG    | __main__:<module>:313 - Training step 3520: loss = 3.3896 | 3019.49ms | Tokens/s = 173,634.7
2025-01-15 10:41:38.856 | DEBUG    | __main__:<module>:313 - Training step 3530: loss = 3.4473 | 3019.82ms | Tokens/s = 173,615.4
2025-01-15 10:42:09.071 | DEBUG    | __main__:<module>:313 - Training step 3540: loss = 3.3034 | 3019.10ms | Tokens/s = 173,656.9
2025-01-15 10:42:39.254 | DEBUG    | __main__:<module>:313 - Training step 3550: loss = 3.4251 | 3016.05ms | Tokens/s = 173,832.5
2025-01-15 10:43:09.432 | DEBUG    | __main__:<module>:313 - Training step 3560: loss = 3.5248 | 3020.15ms | Tokens/s = 173,596.9
2025-01-15 10:43:39.632 | DEBUG    | __main__:<module>:313 - Training step 3570: loss = 3.4286 | 3020.73ms | Tokens/s = 173,563.2
2025-01-15 10:44:09.851 | DEBUG    | __main__:<module>:313 - Training step 3580: loss = 3.3514 | 3023.04ms | Tokens/s = 173,430.5
2025-01-15 10:44:40.072 | DEBUG    | __main__:<module>:313 - Training step 3590: loss = 3.5534 | 3022.42ms | Tokens/s = 173,466.2
2025-01-15 10:45:10.276 | DEBUG    | __main__:<module>:313 - Training step 3600: loss = 3.6207 | 3018.23ms | Tokens/s = 173,707.2
2025-01-15 10:45:40.448 | DEBUG    | __main__:<module>:313 - Training step 3610: loss = 3.3678 | 3015.71ms | Tokens/s = 173,852.1
2025-01-15 10:46:10.627 | DEBUG    | __main__:<module>:313 - Training step 3620: loss = 3.4718 | 3020.42ms | Tokens/s = 173,581.3
2025-01-15 10:46:40.823 | DEBUG    | __main__:<module>:313 - Training step 3630: loss = 3.6291 | 3021.13ms | Tokens/s = 173,540.3
2025-01-15 10:47:11.034 | DEBUG    | __main__:<module>:313 - Training step 3640: loss = 3.4494 | 3020.28ms | Tokens/s = 173,589.4
2025-01-15 10:47:41.249 | DEBUG    | __main__:<module>:313 - Training step 3650: loss = 3.4985 | 3023.83ms | Tokens/s = 173,385.5
2025-01-15 10:48:11.472 | DEBUG    | __main__:<module>:313 - Training step 3660: loss = 3.4121 | 3022.89ms | Tokens/s = 173,439.4
2025-01-15 10:48:41.690 | DEBUG    | __main__:<module>:313 - Training step 3670: loss = 3.4593 | 3021.84ms | Tokens/s = 173,499.8
2025-01-15 10:49:11.867 | DEBUG    | __main__:<module>:313 - Training step 3680: loss = 3.5280 | 3014.65ms | Tokens/s = 173,913.5
2025-01-15 10:49:42.041 | DEBUG    | __main__:<module>:313 - Training step 3690: loss = 3.3710 | 3019.10ms | Tokens/s = 173,656.8
2025-01-15 10:50:12.235 | DEBUG    | __main__:<module>:313 - Training step 3700: loss = 3.4098 | 3019.08ms | Tokens/s = 173,658.3
2025-01-15 10:50:42.458 | DEBUG    | __main__:<module>:313 - Training step 3710: loss = 3.6075 | 3021.67ms | Tokens/s = 173,509.4
2025-01-15 10:51:12.686 | DEBUG    | __main__:<module>:313 - Training step 3720: loss = 3.3293 | 3022.01ms | Tokens/s = 173,489.7
2025-01-15 10:51:42.911 | DEBUG    | __main__:<module>:313 - Training step 3730: loss = 3.3736 | 3023.53ms | Tokens/s = 173,402.7
2025-01-15 10:52:13.143 | DEBUG    | __main__:<module>:313 - Training step 3740: loss = 3.4698 | 3023.05ms | Tokens/s = 173,430.4
2025-01-15 10:52:43.353 | DEBUG    | __main__:<module>:313 - Training step 3750: loss = 3.6360 | 3020.14ms | Tokens/s = 173,597.0
2025-01-15 10:53:13.530 | DEBUG    | __main__:<module>:313 - Training step 3760: loss = 3.5271 | 3018.27ms | Tokens/s = 173,704.6
2025-01-15 10:53:43.718 | DEBUG    | __main__:<module>:313 - Training step 3770: loss = 3.5153 | 3020.33ms | Tokens/s = 173,586.2
2025-01-15 10:54:13.913 | DEBUG    | __main__:<module>:313 - Training step 3780: loss = 3.5759 | 3018.28ms | Tokens/s = 173,704.3
2025-01-15 10:54:44.096 | DEBUG    | __main__:<module>:313 - Training step 3790: loss = 3.4969 | 3018.53ms | Tokens/s = 173,689.7
2025-01-15 10:55:14.293 | DEBUG    | __main__:<module>:313 - Training step 3800: loss = 3.5635 | 3019.11ms | Tokens/s = 173,656.7
2025-01-15 10:55:44.480 | DEBUG    | __main__:<module>:313 - Training step 3810: loss = 3.5278 | 3015.61ms | Tokens/s = 173,857.8
2025-01-15 10:56:14.646 | DEBUG    | __main__:<module>:313 - Training step 3820: loss = 3.4583 | 3018.02ms | Tokens/s = 173,719.0
2025-01-15 10:56:44.815 | DEBUG    | __main__:<module>:313 - Training step 3830: loss = 3.3447 | 3016.22ms | Tokens/s = 173,823.0
2025-01-15 10:57:15.006 | DEBUG    | __main__:<module>:313 - Training step 3840: loss = 3.3720 | 3019.49ms | Tokens/s = 173,634.8
2025-01-15 10:57:45.203 | DEBUG    | __main__:<module>:313 - Training step 3850: loss = 3.4776 | 3018.80ms | Tokens/s = 173,674.1
2025-01-15 10:58:15.412 | DEBUG    | __main__:<module>:313 - Training step 3860: loss = 3.6144 | 3023.56ms | Tokens/s = 173,401.1
2025-01-15 10:58:45.626 | DEBUG    | __main__:<module>:313 - Training step 3870: loss = 3.4869 | 3022.10ms | Tokens/s = 173,484.6
2025-01-15 10:59:15.816 | DEBUG    | __main__:<module>:313 - Training step 3880: loss = 3.5629 | 3016.35ms | Tokens/s = 173,815.4
2025-01-15 10:59:45.977 | DEBUG    | __main__:<module>:313 - Training step 3890: loss = 3.4999 | 3015.12ms | Tokens/s = 173,886.5
2025-01-15 11:00:16.142 | DEBUG    | __main__:<module>:313 - Training step 3900: loss = 3.6603 | 3018.18ms | Tokens/s = 173,710.2
2025-01-15 11:00:46.329 | DEBUG    | __main__:<module>:313 - Training step 3910: loss = 3.3141 | 3020.07ms | Tokens/s = 173,601.1
2025-01-15 11:01:16.525 | DEBUG    | __main__:<module>:313 - Training step 3920: loss = 3.5134 | 3018.63ms | Tokens/s = 173,683.9
2025-01-15 11:01:46.734 | DEBUG    | __main__:<module>:313 - Training step 3930: loss = 3.3227 | 3020.64ms | Tokens/s = 173,568.4
2025-01-15 11:02:16.945 | DEBUG    | __main__:<module>:313 - Training step 3940: loss = 3.4990 | 3016.70ms | Tokens/s = 173,795.0
2025-01-15 11:02:47.138 | DEBUG    | __main__:<module>:313 - Training step 3950: loss = 3.5219 | 3019.73ms | Tokens/s = 173,621.1
2025-01-15 11:03:17.338 | DEBUG    | __main__:<module>:313 - Training step 3960: loss = 3.4802 | 3018.27ms | Tokens/s = 173,705.0
2025-01-15 11:03:47.557 | DEBUG    | __main__:<module>:313 - Training step 3970: loss = 3.4412 | 3021.69ms | Tokens/s = 173,508.4
2025-01-15 11:04:17.749 | DEBUG    | __main__:<module>:313 - Training step 3980: loss = 3.3994 | 3017.92ms | Tokens/s = 173,724.9
2025-01-15 11:04:47.918 | DEBUG    | __main__:<module>:313 - Training step 3990: loss = 3.4484 | 3014.10ms | Tokens/s = 173,945.4
2025-01-15 11:05:21.543 | INFO     | __main__:<module>:265 - Step 4,000/40,000 loss: 3.4619 (T) 3.4976 (V) | lr=9.9e-03
2025-01-15 11:05:21.544 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 11:05:35.961 | DEBUG    | __main__:<module>:313 - Training step 4000: loss = 3.3818 | 20874.53ms | Tokens/s = 25,116.2
2025-01-15 11:06:06.043 | DEBUG    | __main__:<module>:313 - Training step 4010: loss = 3.4842 | 3010.26ms | Tokens/s = 174,167.0
2025-01-15 11:06:36.175 | DEBUG    | __main__:<module>:313 - Training step 4020: loss = 3.5660 | 3016.90ms | Tokens/s = 173,783.9
2025-01-15 11:07:06.329 | DEBUG    | __main__:<module>:313 - Training step 4030: loss = 3.4419 | 3019.34ms | Tokens/s = 173,643.0
2025-01-15 11:07:36.511 | DEBUG    | __main__:<module>:313 - Training step 4040: loss = 3.3552 | 3019.91ms | Tokens/s = 173,610.6
2025-01-15 11:08:06.700 | DEBUG    | __main__:<module>:313 - Training step 4050: loss = 3.3914 | 3018.96ms | Tokens/s = 173,665.0
2025-01-15 11:08:36.871 | DEBUG    | __main__:<module>:313 - Training step 4060: loss = 3.4416 | 3015.58ms | Tokens/s = 173,860.0
2025-01-15 11:09:07.045 | DEBUG    | __main__:<module>:313 - Training step 4070: loss = 3.4577 | 3020.15ms | Tokens/s = 173,596.5
2025-01-15 11:09:37.244 | DEBUG    | __main__:<module>:313 - Training step 4080: loss = 3.3284 | 3021.46ms | Tokens/s = 173,521.5
2025-01-15 11:10:07.447 | DEBUG    | __main__:<module>:313 - Training step 4090: loss = 3.4311 | 3019.23ms | Tokens/s = 173,649.6
2025-01-15 11:10:37.666 | DEBUG    | __main__:<module>:313 - Training step 4100: loss = 3.4720 | 3023.44ms | Tokens/s = 173,407.6
2025-01-15 11:11:07.857 | DEBUG    | __main__:<module>:313 - Training step 4110: loss = 3.6075 | 3015.94ms | Tokens/s = 173,839.3
2025-01-15 11:11:38.042 | DEBUG    | __main__:<module>:313 - Training step 4120: loss = 3.6130 | 3018.38ms | Tokens/s = 173,698.7
2025-01-15 11:12:08.238 | DEBUG    | __main__:<module>:313 - Training step 4130: loss = 3.3770 | 3020.42ms | Tokens/s = 173,581.1
2025-01-15 11:12:38.447 | DEBUG    | __main__:<module>:313 - Training step 4140: loss = 3.5528 | 3021.65ms | Tokens/s = 173,510.6
2025-01-15 11:13:08.654 | DEBUG    | __main__:<module>:313 - Training step 4150: loss = 3.4196 | 3020.20ms | Tokens/s = 173,593.9
2025-01-15 11:13:38.871 | DEBUG    | __main__:<module>:313 - Training step 4160: loss = 3.5724 | 3024.14ms | Tokens/s = 173,367.5
2025-01-15 11:14:09.068 | DEBUG    | __main__:<module>:313 - Training step 4170: loss = 3.5733 | 3017.50ms | Tokens/s = 173,748.8
2025-01-15 11:14:39.262 | DEBUG    | __main__:<module>:313 - Training step 4180: loss = 3.3644 | 3019.04ms | Tokens/s = 173,660.4
2025-01-15 11:15:09.469 | DEBUG    | __main__:<module>:313 - Training step 4190: loss = 3.4674 | 3023.00ms | Tokens/s = 173,433.1
2025-01-15 11:15:39.686 | DEBUG    | __main__:<module>:313 - Training step 4200: loss = 3.5114 | 3020.86ms | Tokens/s = 173,555.9
2025-01-15 11:16:09.878 | DEBUG    | __main__:<module>:313 - Training step 4210: loss = 3.3074 | 3019.28ms | Tokens/s = 173,646.6
2025-01-15 11:16:40.071 | DEBUG    | __main__:<module>:313 - Training step 4220: loss = 3.5176 | 3018.11ms | Tokens/s = 173,713.8
2025-01-15 11:17:10.260 | DEBUG    | __main__:<module>:313 - Training step 4230: loss = 3.3942 | 3019.14ms | Tokens/s = 173,654.7
2025-01-15 11:17:40.461 | DEBUG    | __main__:<module>:313 - Training step 4240: loss = 3.5380 | 3020.15ms | Tokens/s = 173,596.5
2025-01-15 11:18:10.682 | DEBUG    | __main__:<module>:313 - Training step 4250: loss = 3.5213 | 3021.38ms | Tokens/s = 173,526.2
2025-01-15 11:18:40.905 | DEBUG    | __main__:<module>:313 - Training step 4260: loss = 3.3522 | 3021.57ms | Tokens/s = 173,514.8
2025-01-15 11:19:11.094 | DEBUG    | __main__:<module>:313 - Training step 4270: loss = 3.6578 | 3015.12ms | Tokens/s = 173,886.3
2025-01-15 11:19:41.267 | DEBUG    | __main__:<module>:313 - Training step 4280: loss = 3.6174 | 3019.10ms | Tokens/s = 173,656.9
2025-01-15 11:20:11.443 | DEBUG    | __main__:<module>:313 - Training step 4290: loss = 3.5049 | 3017.43ms | Tokens/s = 173,752.9
2025-01-15 11:20:41.647 | DEBUG    | __main__:<module>:313 - Training step 4300: loss = 3.3723 | 3019.38ms | Tokens/s = 173,640.8
2025-01-15 11:21:11.855 | DEBUG    | __main__:<module>:313 - Training step 4310: loss = 3.6263 | 3024.09ms | Tokens/s = 173,370.7
2025-01-15 11:21:42.061 | DEBUG    | __main__:<module>:313 - Training step 4320: loss = 3.6177 | 3018.57ms | Tokens/s = 173,687.3
2025-01-15 11:22:12.237 | DEBUG    | __main__:<module>:313 - Training step 4330: loss = 3.3923 | 3016.65ms | Tokens/s = 173,798.3
2025-01-15 11:22:42.399 | DEBUG    | __main__:<module>:313 - Training step 4340: loss = 3.3779 | 3016.94ms | Tokens/s = 173,781.3
2025-01-15 11:23:12.587 | DEBUG    | __main__:<module>:313 - Training step 4350: loss = 3.5131 | 3019.07ms | Tokens/s = 173,658.5
2025-01-15 11:23:42.786 | DEBUG    | __main__:<module>:313 - Training step 4360: loss = 3.4664 | 3018.35ms | Tokens/s = 173,700.2
2025-01-15 11:24:12.995 | DEBUG    | __main__:<module>:313 - Training step 4370: loss = 3.4260 | 3021.46ms | Tokens/s = 173,521.6
2025-01-15 11:24:43.201 | DEBUG    | __main__:<module>:313 - Training step 4380: loss = 3.6048 | 3018.39ms | Tokens/s = 173,698.1
2025-01-15 11:25:13.405 | DEBUG    | __main__:<module>:313 - Training step 4390: loss = 3.4494 | 3020.49ms | Tokens/s = 173,577.0
2025-01-15 11:25:43.589 | DEBUG    | __main__:<module>:313 - Training step 4400: loss = 3.4626 | 3013.70ms | Tokens/s = 173,968.1
2025-01-15 11:26:13.759 | DEBUG    | __main__:<module>:313 - Training step 4410: loss = 3.4104 | 3016.84ms | Tokens/s = 173,787.2
2025-01-15 11:26:43.945 | DEBUG    | __main__:<module>:313 - Training step 4420: loss = 3.6268 | 3018.75ms | Tokens/s = 173,677.0
2025-01-15 11:27:14.146 | DEBUG    | __main__:<module>:313 - Training step 4430: loss = 3.4103 | 3019.88ms | Tokens/s = 173,612.1
2025-01-15 11:27:44.359 | DEBUG    | __main__:<module>:313 - Training step 4440: loss = 3.4748 | 3021.25ms | Tokens/s = 173,533.5
2025-01-15 11:28:14.571 | DEBUG    | __main__:<module>:313 - Training step 4450: loss = 3.4776 | 3018.46ms | Tokens/s = 173,693.8
2025-01-15 11:28:44.755 | DEBUG    | __main__:<module>:313 - Training step 4460: loss = 3.4292 | 3016.27ms | Tokens/s = 173,820.2
2025-01-15 11:29:14.938 | DEBUG    | __main__:<module>:313 - Training step 4470: loss = 3.4171 | 3020.33ms | Tokens/s = 173,586.1
2025-01-15 11:29:45.146 | DEBUG    | __main__:<module>:313 - Training step 4480: loss = 3.5539 | 3020.47ms | Tokens/s = 173,578.3
2025-01-15 11:30:15.364 | DEBUG    | __main__:<module>:313 - Training step 4490: loss = 3.4827 | 3021.72ms | Tokens/s = 173,506.6
2025-01-15 11:30:45.584 | DEBUG    | __main__:<module>:313 - Training step 4500: loss = 3.6299 | 3021.89ms | Tokens/s = 173,496.7
2025-01-15 11:31:15.809 | DEBUG    | __main__:<module>:313 - Training step 4510: loss = 3.3405 | 3024.06ms | Tokens/s = 173,372.0
2025-01-15 11:31:46.043 | DEBUG    | __main__:<module>:313 - Training step 4520: loss = 3.5220 | 3026.33ms | Tokens/s = 173,242.1
2025-01-15 11:32:16.251 | DEBUG    | __main__:<module>:313 - Training step 4530: loss = 3.5652 | 3021.50ms | Tokens/s = 173,518.9
2025-01-15 11:32:46.423 | DEBUG    | __main__:<module>:313 - Training step 4540: loss = 3.2698 | 3015.50ms | Tokens/s = 173,864.1
2025-01-15 11:33:16.581 | DEBUG    | __main__:<module>:313 - Training step 4550: loss = 3.5244 | 3015.16ms | Tokens/s = 173,883.7
2025-01-15 11:33:46.776 | DEBUG    | __main__:<module>:313 - Training step 4560: loss = 3.3335 | 3021.78ms | Tokens/s = 173,503.0
2025-01-15 11:34:16.978 | DEBUG    | __main__:<module>:313 - Training step 4570: loss = 3.3924 | 3020.99ms | Tokens/s = 173,548.3
2025-01-15 11:34:47.180 | DEBUG    | __main__:<module>:313 - Training step 4580: loss = 3.3884 | 3017.87ms | Tokens/s = 173,727.6
2025-01-15 11:35:17.383 | DEBUG    | __main__:<module>:313 - Training step 4590: loss = 3.3559 | 3019.72ms | Tokens/s = 173,621.1
2025-01-15 11:35:47.606 | DEBUG    | __main__:<module>:313 - Training step 4600: loss = 3.3400 | 3022.76ms | Tokens/s = 173,447.0
2025-01-15 11:36:17.826 | DEBUG    | __main__:<module>:313 - Training step 4610: loss = 3.5414 | 3020.64ms | Tokens/s = 173,568.4
2025-01-15 11:36:48.028 | DEBUG    | __main__:<module>:313 - Training step 4620: loss = 3.4289 | 3019.62ms | Tokens/s = 173,627.4
2025-01-15 11:37:18.206 | DEBUG    | __main__:<module>:313 - Training step 4630: loss = 3.5720 | 3016.85ms | Tokens/s = 173,786.5
2025-01-15 11:37:48.376 | DEBUG    | __main__:<module>:313 - Training step 4640: loss = 3.5712 | 3014.43ms | Tokens/s = 173,926.0
2025-01-15 11:38:18.535 | DEBUG    | __main__:<module>:313 - Training step 4650: loss = 3.3909 | 3014.29ms | Tokens/s = 173,934.0
2025-01-15 11:38:48.716 | DEBUG    | __main__:<module>:313 - Training step 4660: loss = 3.4003 | 3017.91ms | Tokens/s = 173,725.4
2025-01-15 11:39:18.920 | DEBUG    | __main__:<module>:313 - Training step 4670: loss = 3.4182 | 3020.98ms | Tokens/s = 173,549.1
2025-01-15 11:39:49.115 | DEBUG    | __main__:<module>:313 - Training step 4680: loss = 3.4652 | 3018.07ms | Tokens/s = 173,716.3
2025-01-15 11:40:19.280 | DEBUG    | __main__:<module>:313 - Training step 4690: loss = 3.6410 | 3015.50ms | Tokens/s = 173,864.1
2025-01-15 11:40:49.444 | DEBUG    | __main__:<module>:313 - Training step 4700: loss = 3.4856 | 3016.63ms | Tokens/s = 173,799.1
2025-01-15 11:41:19.636 | DEBUG    | __main__:<module>:313 - Training step 4710: loss = 3.2168 | 3019.58ms | Tokens/s = 173,629.5
2025-01-15 11:41:49.846 | DEBUG    | __main__:<module>:313 - Training step 4720: loss = 3.5092 | 3020.72ms | Tokens/s = 173,564.2
2025-01-15 11:42:20.052 | DEBUG    | __main__:<module>:313 - Training step 4730: loss = 3.2609 | 3018.59ms | Tokens/s = 173,686.4
2025-01-15 11:42:50.220 | DEBUG    | __main__:<module>:313 - Training step 4740: loss = 3.4045 | 3017.49ms | Tokens/s = 173,749.7
2025-01-15 11:43:20.380 | DEBUG    | __main__:<module>:313 - Training step 4750: loss = 3.5765 | 3016.80ms | Tokens/s = 173,789.5
2025-01-15 11:43:50.538 | DEBUG    | __main__:<module>:313 - Training step 4760: loss = 3.4185 | 3014.36ms | Tokens/s = 173,930.3
2025-01-15 11:44:20.713 | DEBUG    | __main__:<module>:313 - Training step 4770: loss = 3.6265 | 3016.97ms | Tokens/s = 173,779.5
2025-01-15 11:44:50.906 | DEBUG    | __main__:<module>:313 - Training step 4780: loss = 3.7102 | 3019.00ms | Tokens/s = 173,663.0
2025-01-15 11:45:21.106 | DEBUG    | __main__:<module>:313 - Training step 4790: loss = 3.3945 | 3020.68ms | Tokens/s = 173,566.2
2025-01-15 11:45:51.324 | DEBUG    | __main__:<module>:313 - Training step 4800: loss = 3.5165 | 3019.52ms | Tokens/s = 173,632.8
2025-01-15 11:46:21.509 | DEBUG    | __main__:<module>:313 - Training step 4810: loss = 3.5590 | 3017.14ms | Tokens/s = 173,769.6
2025-01-15 11:46:51.667 | DEBUG    | __main__:<module>:313 - Training step 4820: loss = 3.4828 | 3013.50ms | Tokens/s = 173,979.9
2025-01-15 11:47:21.831 | DEBUG    | __main__:<module>:313 - Training step 4830: loss = 3.4255 | 3018.00ms | Tokens/s = 173,720.1
2025-01-15 11:47:52.029 | DEBUG    | __main__:<module>:313 - Training step 4840: loss = 3.4595 | 3018.98ms | Tokens/s = 173,663.8
2025-01-15 11:48:22.225 | DEBUG    | __main__:<module>:313 - Training step 4850: loss = 3.3864 | 3021.01ms | Tokens/s = 173,547.4
2025-01-15 11:48:52.427 | DEBUG    | __main__:<module>:313 - Training step 4860: loss = 3.3102 | 3020.41ms | Tokens/s = 173,581.5
2025-01-15 11:49:22.657 | DEBUG    | __main__:<module>:313 - Training step 4870: loss = 3.5005 | 3022.75ms | Tokens/s = 173,447.3
2025-01-15 11:49:52.855 | DEBUG    | __main__:<module>:313 - Training step 4880: loss = 3.3747 | 3017.81ms | Tokens/s = 173,731.1
2025-01-15 11:50:23.016 | DEBUG    | __main__:<module>:313 - Training step 4890: loss = 3.3323 | 3013.83ms | Tokens/s = 173,961.0
2025-01-15 11:50:53.198 | DEBUG    | __main__:<module>:313 - Training step 4900: loss = 3.5446 | 3018.76ms | Tokens/s = 173,676.6
2025-01-15 11:51:23.405 | DEBUG    | __main__:<module>:313 - Training step 4910: loss = 3.5986 | 3021.83ms | Tokens/s = 173,499.9
2025-01-15 11:51:53.607 | DEBUG    | __main__:<module>:313 - Training step 4920: loss = 3.4460 | 3018.00ms | Tokens/s = 173,720.1
2025-01-15 11:52:23.763 | DEBUG    | __main__:<module>:313 - Training step 4930: loss = 3.4930 | 3014.09ms | Tokens/s = 173,945.7
2025-01-15 11:52:53.928 | DEBUG    | __main__:<module>:313 - Training step 4940: loss = 3.3527 | 3018.73ms | Tokens/s = 173,678.2
2025-01-15 11:53:24.112 | DEBUG    | __main__:<module>:313 - Training step 4950: loss = 3.3835 | 3020.50ms | Tokens/s = 173,576.8
2025-01-15 11:53:54.312 | DEBUG    | __main__:<module>:313 - Training step 4960: loss = 3.5137 | 3020.00ms | Tokens/s = 173,605.4
2025-01-15 11:54:24.507 | DEBUG    | __main__:<module>:313 - Training step 4970: loss = 3.4076 | 3019.17ms | Tokens/s = 173,653.0
2025-01-15 11:54:54.716 | DEBUG    | __main__:<module>:313 - Training step 4980: loss = 3.5548 | 3022.49ms | Tokens/s = 173,462.5
2025-01-15 11:55:24.936 | DEBUG    | __main__:<module>:313 - Training step 4990: loss = 3.4707 | 3025.85ms | Tokens/s = 173,269.4
2025-01-15 11:55:58.605 | INFO     | __main__:<module>:265 - Step 5,000/40,000 loss: 3.4421 (T) 3.4658 (V) | lr=9.8e-03
2025-01-15 11:55:58.606 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 11:56:13.187 | DEBUG    | __main__:<module>:313 - Training step 5000: loss = 3.3196 | 21046.15ms | Tokens/s = 24,911.4
2025-01-15 11:56:43.269 | DEBUG    | __main__:<module>:313 - Training step 5010: loss = 3.3698 | 3009.01ms | Tokens/s = 174,239.7
2025-01-15 11:57:13.407 | DEBUG    | __main__:<module>:313 - Training step 5020: loss = 3.5433 | 3019.17ms | Tokens/s = 173,653.1
2025-01-15 11:57:43.582 | DEBUG    | __main__:<module>:313 - Training step 5030: loss = 3.4423 | 3018.23ms | Tokens/s = 173,706.9
2025-01-15 11:58:13.771 | DEBUG    | __main__:<module>:313 - Training step 5040: loss = 3.4811 | 3018.79ms | Tokens/s = 173,674.6
2025-01-15 11:58:43.952 | DEBUG    | __main__:<module>:313 - Training step 5050: loss = 3.5503 | 3021.56ms | Tokens/s = 173,515.9
2025-01-15 11:59:14.159 | DEBUG    | __main__:<module>:313 - Training step 5060: loss = 3.3973 | 3018.98ms | Tokens/s = 173,664.1
2025-01-15 11:59:44.357 | DEBUG    | __main__:<module>:313 - Training step 5070: loss = 3.4462 | 3018.86ms | Tokens/s = 173,670.9
2025-01-15 12:00:14.514 | DEBUG    | __main__:<module>:313 - Training step 5080: loss = 3.5708 | 3015.79ms | Tokens/s = 173,847.5
2025-01-15 12:00:44.663 | DEBUG    | __main__:<module>:313 - Training step 5090: loss = 3.4774 | 3016.85ms | Tokens/s = 173,786.7
2025-01-15 12:01:14.844 | DEBUG    | __main__:<module>:313 - Training step 5100: loss = 3.6759 | 3019.63ms | Tokens/s = 173,626.5
2025-01-15 12:01:45.043 | DEBUG    | __main__:<module>:313 - Training step 5110: loss = 3.3974 | 3019.86ms | Tokens/s = 173,613.3
2025-01-15 12:02:15.244 | DEBUG    | __main__:<module>:313 - Training step 5120: loss = 3.5890 | 3020.49ms | Tokens/s = 173,577.3
2025-01-15 12:02:45.450 | DEBUG    | __main__:<module>:313 - Training step 5130: loss = 3.3999 | 3020.28ms | Tokens/s = 173,589.4
2025-01-15 12:03:15.661 | DEBUG    | __main__:<module>:313 - Training step 5140: loss = 3.6405 | 3019.51ms | Tokens/s = 173,633.5
2025-01-15 12:03:45.841 | DEBUG    | __main__:<module>:313 - Training step 5150: loss = 3.4187 | 3015.54ms | Tokens/s = 173,862.2
2025-01-15 12:04:15.995 | DEBUG    | __main__:<module>:313 - Training step 5160: loss = 3.4926 | 3015.63ms | Tokens/s = 173,857.1
2025-01-15 12:04:46.178 | DEBUG    | __main__:<module>:313 - Training step 5170: loss = 3.4358 | 3021.36ms | Tokens/s = 173,526.9
2025-01-15 12:05:16.385 | DEBUG    | __main__:<module>:313 - Training step 5180: loss = 3.5320 | 3023.17ms | Tokens/s = 173,423.3
2025-01-15 12:05:46.604 | DEBUG    | __main__:<module>:313 - Training step 5190: loss = 3.4844 | 3021.45ms | Tokens/s = 173,522.1
2025-01-15 12:06:16.783 | DEBUG    | __main__:<module>:313 - Training step 5200: loss = 3.4020 | 3013.83ms | Tokens/s = 173,960.9
2025-01-15 12:06:46.972 | DEBUG    | __main__:<module>:313 - Training step 5210: loss = 3.4805 | 3020.61ms | Tokens/s = 173,570.5
2025-01-15 12:07:17.186 | DEBUG    | __main__:<module>:313 - Training step 5220: loss = 3.5249 | 3023.37ms | Tokens/s = 173,411.6
2025-01-15 12:07:47.405 | DEBUG    | __main__:<module>:313 - Training step 5230: loss = 3.2882 | 3023.54ms | Tokens/s = 173,402.0
2025-01-15 12:08:17.589 | DEBUG    | __main__:<module>:313 - Training step 5240: loss = 3.5842 | 3014.68ms | Tokens/s = 173,911.7
2025-01-15 12:08:47.745 | DEBUG    | __main__:<module>:313 - Training step 5250: loss = 3.4217 | 3015.83ms | Tokens/s = 173,845.6
2025-01-15 12:09:17.893 | DEBUG    | __main__:<module>:313 - Training step 5260: loss = 3.6047 | 3016.06ms | Tokens/s = 173,832.2
2025-01-15 12:09:48.060 | DEBUG    | __main__:<module>:313 - Training step 5270: loss = 3.4116 | 3018.97ms | Tokens/s = 173,664.5
2025-01-15 12:10:18.220 | DEBUG    | __main__:<module>:313 - Training step 5280: loss = 3.4558 | 3015.00ms | Tokens/s = 173,893.1
2025-01-15 12:10:48.374 | DEBUG    | __main__:<module>:313 - Training step 5290: loss = 3.4241 | 3016.30ms | Tokens/s = 173,818.0
2025-01-15 12:11:18.561 | DEBUG    | __main__:<module>:313 - Training step 5300: loss = 3.4218 | 3018.10ms | Tokens/s = 173,714.6
2025-01-15 12:11:48.774 | DEBUG    | __main__:<module>:313 - Training step 5310: loss = 3.6149 | 3021.97ms | Tokens/s = 173,492.4
2025-01-15 12:12:18.983 | DEBUG    | __main__:<module>:313 - Training step 5320: loss = 3.4357 | 3019.14ms | Tokens/s = 173,654.7
2025-01-15 12:12:49.191 | DEBUG    | __main__:<module>:313 - Training step 5330: loss = 3.4804 | 3022.00ms | Tokens/s = 173,490.5
2025-01-15 12:13:19.412 | DEBUG    | __main__:<module>:313 - Training step 5340: loss = 3.4841 | 3022.70ms | Tokens/s = 173,450.3
2025-01-15 12:13:49.626 | DEBUG    | __main__:<module>:313 - Training step 5350: loss = 3.3541 | 3019.16ms | Tokens/s = 173,653.5
2025-01-15 12:14:19.787 | DEBUG    | __main__:<module>:313 - Training step 5360: loss = 3.4533 | 3012.18ms | Tokens/s = 174,056.2
2025-01-15 12:14:49.969 | DEBUG    | __main__:<module>:313 - Training step 5370: loss = 3.5213 | 3018.91ms | Tokens/s = 173,667.9
2025-01-15 12:15:20.159 | DEBUG    | __main__:<module>:313 - Training step 5380: loss = 3.4388 | 3016.04ms | Tokens/s = 173,833.2
2025-01-15 12:15:50.332 | DEBUG    | __main__:<module>:313 - Training step 5390: loss = 3.5034 | 3017.45ms | Tokens/s = 173,752.1
2025-01-15 12:16:20.491 | DEBUG    | __main__:<module>:313 - Training step 5400: loss = 3.4781 | 3015.00ms | Tokens/s = 173,893.4
2025-01-15 12:16:50.679 | DEBUG    | __main__:<module>:313 - Training step 5410: loss = 3.6307 | 3017.67ms | Tokens/s = 173,739.3
2025-01-15 12:17:20.849 | DEBUG    | __main__:<module>:313 - Training step 5420: loss = 3.5116 | 3013.80ms | Tokens/s = 173,962.6
2025-01-15 12:17:51.021 | DEBUG    | __main__:<module>:313 - Training step 5430: loss = 3.3232 | 3018.95ms | Tokens/s = 173,665.5
2025-01-15 12:18:21.206 | DEBUG    | __main__:<module>:313 - Training step 5440: loss = 3.5045 | 3019.75ms | Tokens/s = 173,619.6
2025-01-15 12:18:51.411 | DEBUG    | __main__:<module>:313 - Training step 5450: loss = 3.4837 | 3022.12ms | Tokens/s = 173,483.5
2025-01-15 12:19:21.613 | DEBUG    | __main__:<module>:313 - Training step 5460: loss = 3.5058 | 3017.48ms | Tokens/s = 173,750.2
2025-01-15 12:19:51.779 | DEBUG    | __main__:<module>:313 - Training step 5470: loss = 3.5756 | 3011.87ms | Tokens/s = 174,074.2
2025-01-15 12:20:21.935 | DEBUG    | __main__:<module>:313 - Training step 5480: loss = 3.4305 | 3017.21ms | Tokens/s = 173,765.9
2025-01-15 12:20:52.120 | DEBUG    | __main__:<module>:313 - Training step 5490: loss = 3.4795 | 3018.80ms | Tokens/s = 173,674.2
2025-01-15 12:21:22.320 | DEBUG    | __main__:<module>:313 - Training step 5500: loss = 3.4018 | 3019.55ms | Tokens/s = 173,631.4
2025-01-15 12:21:52.525 | DEBUG    | __main__:<module>:313 - Training step 5510: loss = 3.4384 | 3018.66ms | Tokens/s = 173,682.3
2025-01-15 12:22:22.689 | DEBUG    | __main__:<module>:313 - Training step 5520: loss = 3.5826 | 3016.11ms | Tokens/s = 173,829.2
2025-01-15 12:22:52.853 | DEBUG    | __main__:<module>:313 - Training step 5530: loss = 3.5148 | 3019.81ms | Tokens/s = 173,616.1
2025-01-15 12:23:23.045 | DEBUG    | __main__:<module>:313 - Training step 5540: loss = 3.4683 | 3019.63ms | Tokens/s = 173,626.7
2025-01-15 12:23:53.248 | DEBUG    | __main__:<module>:313 - Training step 5550: loss = 3.3908 | 3020.95ms | Tokens/s = 173,551.0
2025-01-15 12:24:23.448 | DEBUG    | __main__:<module>:313 - Training step 5560: loss = 3.2643 | 3019.67ms | Tokens/s = 173,624.5
2025-01-15 12:24:53.652 | DEBUG    | __main__:<module>:313 - Training step 5570: loss = 3.1579 | 3019.66ms | Tokens/s = 173,624.9
2025-01-15 12:25:23.856 | DEBUG    | __main__:<module>:313 - Training step 5580: loss = 3.4563 | 3020.48ms | Tokens/s = 173,577.4
2025-01-15 12:25:54.038 | DEBUG    | __main__:<module>:313 - Training step 5590: loss = 3.4105 | 3017.76ms | Tokens/s = 173,734.1
2025-01-15 12:26:24.219 | DEBUG    | __main__:<module>:313 - Training step 5600: loss = 3.3862 | 3017.79ms | Tokens/s = 173,732.2
2025-01-15 12:26:54.418 | DEBUG    | __main__:<module>:313 - Training step 5610: loss = 3.4575 | 3021.76ms | Tokens/s = 173,504.2
2025-01-15 12:27:24.627 | DEBUG    | __main__:<module>:313 - Training step 5620: loss = 3.5300 | 3021.63ms | Tokens/s = 173,511.4
2025-01-15 12:27:54.825 | DEBUG    | __main__:<module>:313 - Training step 5630: loss = 3.3770 | 3016.77ms | Tokens/s = 173,791.2
2025-01-15 12:28:24.980 | DEBUG    | __main__:<module>:313 - Training step 5640: loss = 3.4778 | 3016.16ms | Tokens/s = 173,826.1
2025-01-15 12:28:55.150 | DEBUG    | __main__:<module>:313 - Training step 5650: loss = 3.4352 | 3019.26ms | Tokens/s = 173,647.9
2025-01-15 12:29:25.339 | DEBUG    | __main__:<module>:313 - Training step 5660: loss = 3.4281 | 3019.67ms | Tokens/s = 173,624.2
2025-01-15 12:29:55.528 | DEBUG    | __main__:<module>:313 - Training step 5670: loss = 3.4311 | 3018.49ms | Tokens/s = 173,691.9
2025-01-15 12:30:25.721 | DEBUG    | __main__:<module>:313 - Training step 5680: loss = 3.3778 | 3019.84ms | Tokens/s = 173,614.7
2025-01-15 12:30:55.927 | DEBUG    | __main__:<module>:313 - Training step 5690: loss = 3.3890 | 3022.37ms | Tokens/s = 173,469.4
2025-01-15 12:31:26.118 | DEBUG    | __main__:<module>:313 - Training step 5700: loss = 3.4903 | 3018.87ms | Tokens/s = 173,670.2
2025-01-15 12:31:56.292 | DEBUG    | __main__:<module>:313 - Training step 5710: loss = 3.5001 | 3014.23ms | Tokens/s = 173,937.8
2025-01-15 12:32:26.441 | DEBUG    | __main__:<module>:313 - Training step 5720: loss = 3.4726 | 3015.77ms | Tokens/s = 173,849.1
2025-01-15 12:32:56.624 | DEBUG    | __main__:<module>:313 - Training step 5730: loss = 3.5889 | 3019.23ms | Tokens/s = 173,649.8
2025-01-15 12:33:26.824 | DEBUG    | __main__:<module>:313 - Training step 5740: loss = 3.5060 | 3019.52ms | Tokens/s = 173,632.6
2025-01-15 12:33:57.034 | DEBUG    | __main__:<module>:313 - Training step 5750: loss = 3.5520 | 3022.44ms | Tokens/s = 173,465.3
2025-01-15 12:34:27.232 | DEBUG    | __main__:<module>:313 - Training step 5760: loss = 3.4406 | 3017.81ms | Tokens/s = 173,731.6
2025-01-15 12:34:57.406 | DEBUG    | __main__:<module>:313 - Training step 5770: loss = 3.4282 | 3015.06ms | Tokens/s = 173,889.6
2025-01-15 12:35:27.571 | DEBUG    | __main__:<module>:313 - Training step 5780: loss = 3.6731 | 3018.07ms | Tokens/s = 173,716.4
2025-01-15 12:35:57.762 | DEBUG    | __main__:<module>:313 - Training step 5790: loss = 3.6470 | 3020.62ms | Tokens/s = 173,569.6
2025-01-15 12:36:27.954 | DEBUG    | __main__:<module>:313 - Training step 5800: loss = 3.4234 | 3021.03ms | Tokens/s = 173,546.0
2025-01-15 12:36:58.149 | DEBUG    | __main__:<module>:313 - Training step 5810: loss = 3.5345 | 3018.48ms | Tokens/s = 173,692.5
2025-01-15 12:37:28.310 | DEBUG    | __main__:<module>:313 - Training step 5820: loss = 3.4372 | 3017.38ms | Tokens/s = 173,756.1
2025-01-15 12:37:58.455 | DEBUG    | __main__:<module>:313 - Training step 5830: loss = 3.4967 | 3014.12ms | Tokens/s = 173,943.7
2025-01-15 12:38:28.577 | DEBUG    | __main__:<module>:313 - Training step 5840: loss = 3.5614 | 3012.86ms | Tokens/s = 174,017.0
2025-01-15 12:38:58.738 | DEBUG    | __main__:<module>:313 - Training step 5850: loss = 3.3257 | 3017.87ms | Tokens/s = 173,728.0
2025-01-15 12:39:28.924 | DEBUG    | __main__:<module>:313 - Training step 5860: loss = 3.3586 | 3019.12ms | Tokens/s = 173,655.7
2025-01-15 12:39:59.116 | DEBUG    | __main__:<module>:313 - Training step 5870: loss = 3.5097 | 3016.10ms | Tokens/s = 173,829.9
2025-01-15 12:40:29.318 | DEBUG    | __main__:<module>:313 - Training step 5880: loss = 3.4619 | 3020.11ms | Tokens/s = 173,598.8
2025-01-15 12:40:59.529 | DEBUG    | __main__:<module>:313 - Training step 5890: loss = 3.3679 | 3021.92ms | Tokens/s = 173,495.0
2025-01-15 12:41:29.722 | DEBUG    | __main__:<module>:313 - Training step 5900: loss = 3.2592 | 3016.11ms | Tokens/s = 173,829.0
2025-01-15 12:41:59.886 | DEBUG    | __main__:<module>:313 - Training step 5910: loss = 3.5371 | 3015.37ms | Tokens/s = 173,872.0
2025-01-15 12:42:30.062 | DEBUG    | __main__:<module>:313 - Training step 5920: loss = 3.6752 | 3018.99ms | Tokens/s = 173,663.3
2025-01-15 12:43:00.260 | DEBUG    | __main__:<module>:313 - Training step 5930: loss = 3.6262 | 3022.72ms | Tokens/s = 173,449.0
2025-01-15 12:43:30.464 | DEBUG    | __main__:<module>:313 - Training step 5940: loss = 3.1501 | 3019.81ms | Tokens/s = 173,616.0
2025-01-15 12:44:00.675 | DEBUG    | __main__:<module>:313 - Training step 5950: loss = 3.4877 | 3021.72ms | Tokens/s = 173,506.6
2025-01-15 12:44:30.858 | DEBUG    | __main__:<module>:313 - Training step 5960: loss = 3.4969 | 3019.35ms | Tokens/s = 173,642.4
2025-01-15 12:45:01.022 | DEBUG    | __main__:<module>:313 - Training step 5970: loss = 3.3661 | 3017.90ms | Tokens/s = 173,726.2
2025-01-15 12:45:31.210 | DEBUG    | __main__:<module>:313 - Training step 5980: loss = 3.5146 | 3016.70ms | Tokens/s = 173,795.2
2025-01-15 12:46:01.393 | DEBUG    | __main__:<module>:313 - Training step 5990: loss = 3.4315 | 3018.73ms | Tokens/s = 173,678.2
2025-01-15 12:46:35.023 | INFO     | __main__:<module>:265 - Step 6,000/40,000 loss: 3.4400 (T) 3.4588 (V) | lr=9.7e-03
2025-01-15 12:46:35.024 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 12:46:49.207 | DEBUG    | __main__:<module>:313 - Training step 6000: loss = 3.3864 | 20643.43ms | Tokens/s = 25,397.3
2025-01-15 12:47:19.283 | DEBUG    | __main__:<module>:313 - Training step 6010: loss = 3.3150 | 3009.18ms | Tokens/s = 174,229.6
2025-01-15 12:47:49.405 | DEBUG    | __main__:<module>:313 - Training step 6020: loss = 3.5543 | 3012.55ms | Tokens/s = 174,034.7
2025-01-15 12:48:19.574 | DEBUG    | __main__:<module>:313 - Training step 6030: loss = 3.4674 | 3017.21ms | Tokens/s = 173,765.8
2025-01-15 12:48:49.758 | DEBUG    | __main__:<module>:313 - Training step 6040: loss = 3.4408 | 3020.11ms | Tokens/s = 173,598.7
2025-01-15 12:49:19.966 | DEBUG    | __main__:<module>:313 - Training step 6050: loss = 3.3898 | 3020.90ms | Tokens/s = 173,553.6
2025-01-15 12:49:50.180 | DEBUG    | __main__:<module>:313 - Training step 6060: loss = 3.2118 | 3019.91ms | Tokens/s = 173,610.6
2025-01-15 12:50:20.394 | DEBUG    | __main__:<module>:313 - Training step 6070: loss = 3.4120 | 3020.09ms | Tokens/s = 173,600.4
2025-01-15 12:50:50.611 | DEBUG    | __main__:<module>:313 - Training step 6080: loss = 3.4560 | 3021.67ms | Tokens/s = 173,509.6
2025-01-15 12:51:20.828 | DEBUG    | __main__:<module>:313 - Training step 6090: loss = 3.4559 | 3024.33ms | Tokens/s = 173,356.5
2025-01-15 12:51:51.023 | DEBUG    | __main__:<module>:313 - Training step 6100: loss = 3.4005 | 3016.55ms | Tokens/s = 173,803.6
2025-01-15 12:52:21.211 | DEBUG    | __main__:<module>:313 - Training step 6110: loss = 3.5911 | 3020.50ms | Tokens/s = 173,576.3
2025-01-15 12:52:51.420 | DEBUG    | __main__:<module>:313 - Training step 6120: loss = 3.3990 | 3020.34ms | Tokens/s = 173,585.6
2025-01-15 12:53:21.619 | DEBUG    | __main__:<module>:313 - Training step 6130: loss = 3.5440 | 3019.53ms | Tokens/s = 173,632.5
2025-01-15 12:53:51.780 | DEBUG    | __main__:<module>:313 - Training step 6140: loss = 3.4696 | 3013.96ms | Tokens/s = 173,953.3
2025-01-15 12:54:21.911 | DEBUG    | __main__:<module>:313 - Training step 6150: loss = 3.5594 | 3013.23ms | Tokens/s = 173,995.5
2025-01-15 12:54:52.061 | DEBUG    | __main__:<module>:313 - Training step 6160: loss = 3.5329 | 3015.54ms | Tokens/s = 173,862.0
2025-01-15 12:55:22.241 | DEBUG    | __main__:<module>:313 - Training step 6170: loss = 3.4613 | 3018.58ms | Tokens/s = 173,687.2
2025-01-15 12:55:52.429 | DEBUG    | __main__:<module>:313 - Training step 6180: loss = 3.4228 | 3019.38ms | Tokens/s = 173,641.1
2025-01-15 12:56:22.621 | DEBUG    | __main__:<module>:313 - Training step 6190: loss = 3.5046 | 3019.57ms | Tokens/s = 173,629.9
2025-01-15 12:56:52.825 | DEBUG    | __main__:<module>:313 - Training step 6200: loss = 3.3769 | 3021.68ms | Tokens/s = 173,509.0
2025-01-15 12:57:23.008 | DEBUG    | __main__:<module>:313 - Training step 6210: loss = 3.5271 | 3015.35ms | Tokens/s = 173,872.7
2025-01-15 12:57:53.166 | DEBUG    | __main__:<module>:313 - Training step 6220: loss = 3.4458 | 3017.49ms | Tokens/s = 173,749.5
2025-01-15 12:58:23.339 | DEBUG    | __main__:<module>:313 - Training step 6230: loss = 3.4236 | 3021.08ms | Tokens/s = 173,543.0
2025-01-15 12:58:53.527 | DEBUG    | __main__:<module>:313 - Training step 6240: loss = 3.5763 | 3017.24ms | Tokens/s = 173,764.2
2025-01-15 12:59:23.724 | DEBUG    | __main__:<module>:313 - Training step 6250: loss = 3.5224 | 3020.90ms | Tokens/s = 173,553.8
2025-01-15 12:59:53.914 | DEBUG    | __main__:<module>:313 - Training step 6260: loss = 3.4644 | 3020.73ms | Tokens/s = 173,563.4
2025-01-15 13:00:24.108 | DEBUG    | __main__:<module>:313 - Training step 6270: loss = 3.3831 | 3017.81ms | Tokens/s = 173,731.4
2025-01-15 13:00:54.306 | DEBUG    | __main__:<module>:313 - Training step 6280: loss = 3.6026 | 3020.12ms | Tokens/s = 173,598.5
2025-01-15 13:01:24.511 | DEBUG    | __main__:<module>:313 - Training step 6290: loss = 3.4109 | 3021.86ms | Tokens/s = 173,498.2
2025-01-15 13:01:54.718 | DEBUG    | __main__:<module>:313 - Training step 6300: loss = 3.3505 | 3021.23ms | Tokens/s = 173,534.6
2025-01-15 13:02:24.915 | DEBUG    | __main__:<module>:313 - Training step 6310: loss = 3.4771 | 3017.54ms | Tokens/s = 173,746.9
2025-01-15 13:02:55.123 | DEBUG    | __main__:<module>:313 - Training step 6320: loss = 3.3930 | 3023.16ms | Tokens/s = 173,423.6
2025-01-15 13:03:25.303 | DEBUG    | __main__:<module>:313 - Training step 6330: loss = 3.4514 | 3014.78ms | Tokens/s = 173,905.7
2025-01-15 13:03:55.454 | DEBUG    | __main__:<module>:313 - Training step 6340: loss = 3.4633 | 3013.30ms | Tokens/s = 173,991.3
2025-01-15 13:04:25.609 | DEBUG    | __main__:<module>:313 - Training step 6350: loss = 3.4586 | 3016.36ms | Tokens/s = 173,815.0
2025-01-15 13:04:55.794 | DEBUG    | __main__:<module>:313 - Training step 6360: loss = 3.3657 | 3020.69ms | Tokens/s = 173,565.5
2025-01-15 13:05:26.000 | DEBUG    | __main__:<module>:313 - Training step 6370: loss = 3.5849 | 3018.79ms | Tokens/s = 173,674.6
2025-01-15 13:05:56.172 | DEBUG    | __main__:<module>:313 - Training step 6380: loss = 3.5557 | 3016.04ms | Tokens/s = 173,833.5
2025-01-15 13:06:26.336 | DEBUG    | __main__:<module>:313 - Training step 6390: loss = 3.4427 | 3018.28ms | Tokens/s = 173,704.0
2025-01-15 13:06:56.524 | DEBUG    | __main__:<module>:313 - Training step 6400: loss = 3.3989 | 3016.56ms | Tokens/s = 173,803.0
2025-01-15 13:07:26.725 | DEBUG    | __main__:<module>:313 - Training step 6410: loss = 3.3102 | 3019.29ms | Tokens/s = 173,645.9
2025-01-15 13:07:56.919 | DEBUG    | __main__:<module>:313 - Training step 6420: loss = 3.4484 | 3020.79ms | Tokens/s = 173,559.8
2025-01-15 13:08:27.115 | DEBUG    | __main__:<module>:313 - Training step 6430: loss = 3.2843 | 3019.05ms | Tokens/s = 173,659.7
2025-01-15 13:08:57.332 | DEBUG    | __main__:<module>:313 - Training step 6440: loss = 3.5494 | 3022.94ms | Tokens/s = 173,436.3
2025-01-15 13:09:27.516 | DEBUG    | __main__:<module>:313 - Training step 6450: loss = 3.3088 | 3015.73ms | Tokens/s = 173,851.3
2025-01-15 13:09:57.682 | DEBUG    | __main__:<module>:313 - Training step 6460: loss = 3.3753 | 3014.94ms | Tokens/s = 173,896.8
2025-01-15 13:10:27.864 | DEBUG    | __main__:<module>:313 - Training step 6470: loss = 3.5765 | 3015.07ms | Tokens/s = 173,889.3
2025-01-15 13:10:58.071 | DEBUG    | __main__:<module>:313 - Training step 6480: loss = 3.4137 | 3019.79ms | Tokens/s = 173,617.4
2025-01-15 13:11:28.283 | DEBUG    | __main__:<module>:313 - Training step 6490: loss = 3.2818 | 3022.07ms | Tokens/s = 173,486.1
2025-01-15 13:11:58.488 | DEBUG    | __main__:<module>:313 - Training step 6500: loss = 3.5205 | 3017.66ms | Tokens/s = 173,739.8
2025-01-15 13:12:28.701 | DEBUG    | __main__:<module>:313 - Training step 6510: loss = 3.1951 | 3021.38ms | Tokens/s = 173,525.8
2025-01-15 13:12:58.898 | DEBUG    | __main__:<module>:313 - Training step 6520: loss = 3.2749 | 3018.62ms | Tokens/s = 173,684.5
2025-01-15 13:13:29.071 | DEBUG    | __main__:<module>:313 - Training step 6530: loss = 3.2359 | 3015.51ms | Tokens/s = 173,863.9
2025-01-15 13:13:59.206 | DEBUG    | __main__:<module>:313 - Training step 6540: loss = 3.3382 | 3012.26ms | Tokens/s = 174,051.3
2025-01-15 13:14:29.360 | DEBUG    | __main__:<module>:313 - Training step 6550: loss = 3.3298 | 3015.64ms | Tokens/s = 173,856.1
2025-01-15 13:14:59.530 | DEBUG    | __main__:<module>:313 - Training step 6560: loss = 3.6557 | 3015.38ms | Tokens/s = 173,871.4
2025-01-15 13:15:29.694 | DEBUG    | __main__:<module>:313 - Training step 6570: loss = 3.3498 | 3016.13ms | Tokens/s = 173,828.2
2025-01-15 13:15:59.872 | DEBUG    | __main__:<module>:313 - Training step 6580: loss = 3.3034 | 3016.03ms | Tokens/s = 173,833.7
2025-01-15 13:16:30.065 | DEBUG    | __main__:<module>:313 - Training step 6590: loss = 3.5317 | 3016.54ms | Tokens/s = 173,804.3
2025-01-15 13:17:00.274 | DEBUG    | __main__:<module>:313 - Training step 6600: loss = 3.4870 | 3020.24ms | Tokens/s = 173,591.2
2025-01-15 13:17:30.472 | DEBUG    | __main__:<module>:313 - Training step 6610: loss = 3.5236 | 3019.53ms | Tokens/s = 173,632.5
2025-01-15 13:18:00.631 | DEBUG    | __main__:<module>:313 - Training step 6620: loss = 3.4505 | 3014.40ms | Tokens/s = 173,927.7
2025-01-15 13:18:30.815 | DEBUG    | __main__:<module>:313 - Training step 6630: loss = 3.2997 | 3020.80ms | Tokens/s = 173,559.2
2025-01-15 13:19:01.018 | DEBUG    | __main__:<module>:313 - Training step 6640: loss = 3.3964 | 3018.03ms | Tokens/s = 173,718.3
2025-01-15 13:19:31.190 | DEBUG    | __main__:<module>:313 - Training step 6650: loss = 3.5896 | 3014.42ms | Tokens/s = 173,926.4
2025-01-15 13:20:01.330 | DEBUG    | __main__:<module>:313 - Training step 6660: loss = 3.4852 | 3014.99ms | Tokens/s = 173,893.8
2025-01-15 13:20:31.494 | DEBUG    | __main__:<module>:313 - Training step 6670: loss = 3.4992 | 3017.01ms | Tokens/s = 173,777.5
2025-01-15 13:21:01.677 | DEBUG    | __main__:<module>:313 - Training step 6680: loss = 3.3602 | 3014.93ms | Tokens/s = 173,897.0
2025-01-15 13:21:31.829 | DEBUG    | __main__:<module>:313 - Training step 6690: loss = 3.3792 | 3012.62ms | Tokens/s = 174,030.6
2025-01-15 13:22:01.981 | DEBUG    | __main__:<module>:313 - Training step 6700: loss = 3.5755 | 3016.51ms | Tokens/s = 173,806.4
2025-01-15 13:22:32.166 | DEBUG    | __main__:<module>:313 - Training step 6710: loss = 3.3569 | 3018.78ms | Tokens/s = 173,675.7
2025-01-15 13:23:02.360 | DEBUG    | __main__:<module>:313 - Training step 6720: loss = 3.3532 | 3019.65ms | Tokens/s = 173,625.4
2025-01-15 13:23:32.557 | DEBUG    | __main__:<module>:313 - Training step 6730: loss = 3.5460 | 3018.85ms | Tokens/s = 173,671.7
2025-01-15 13:24:02.725 | DEBUG    | __main__:<module>:313 - Training step 6740: loss = 3.4783 | 3016.22ms | Tokens/s = 173,822.7
2025-01-15 13:24:32.884 | DEBUG    | __main__:<module>:313 - Training step 6750: loss = 3.4916 | 3018.55ms | Tokens/s = 173,688.8
2025-01-15 13:25:03.070 | DEBUG    | __main__:<module>:313 - Training step 6760: loss = 3.5202 | 3018.75ms | Tokens/s = 173,677.2
2025-01-15 13:25:33.262 | DEBUG    | __main__:<module>:313 - Training step 6770: loss = 3.3308 | 3018.16ms | Tokens/s = 173,711.0
2025-01-15 13:26:03.447 | DEBUG    | __main__:<module>:313 - Training step 6780: loss = 3.3946 | 3016.85ms | Tokens/s = 173,786.5
2025-01-15 13:26:33.601 | DEBUG    | __main__:<module>:313 - Training step 6790: loss = 3.4773 | 3015.00ms | Tokens/s = 173,893.0
2025-01-15 13:27:03.748 | DEBUG    | __main__:<module>:313 - Training step 6800: loss = 3.4234 | 3015.44ms | Tokens/s = 173,867.7
2025-01-15 13:27:33.912 | DEBUG    | __main__:<module>:313 - Training step 6810: loss = 3.4853 | 3017.66ms | Tokens/s = 173,739.8
2025-01-15 13:28:04.082 | DEBUG    | __main__:<module>:313 - Training step 6820: loss = 3.4323 | 3018.87ms | Tokens/s = 173,670.0
2025-01-15 13:28:34.260 | DEBUG    | __main__:<module>:313 - Training step 6830: loss = 3.5755 | 3017.66ms | Tokens/s = 173,739.6
2025-01-15 13:29:04.445 | DEBUG    | __main__:<module>:313 - Training step 6840: loss = 3.6296 | 3016.65ms | Tokens/s = 173,797.9
2025-01-15 13:29:34.616 | DEBUG    | __main__:<module>:313 - Training step 6850: loss = 3.4241 | 3018.21ms | Tokens/s = 173,708.4
2025-01-15 13:30:04.785 | DEBUG    | __main__:<module>:313 - Training step 6860: loss = 3.3707 | 3016.55ms | Tokens/s = 173,803.7
2025-01-15 13:30:34.965 | DEBUG    | __main__:<module>:313 - Training step 6870: loss = 3.3600 | 3018.43ms | Tokens/s = 173,695.6
2025-01-15 13:31:05.142 | DEBUG    | __main__:<module>:313 - Training step 6880: loss = 3.3853 | 3014.49ms | Tokens/s = 173,922.5
2025-01-15 13:31:35.305 | DEBUG    | __main__:<module>:313 - Training step 6890: loss = 3.3620 | 3019.41ms | Tokens/s = 173,639.5
2025-01-15 13:32:05.458 | DEBUG    | __main__:<module>:313 - Training step 6900: loss = 3.4869 | 3014.48ms | Tokens/s = 173,923.2
2025-01-15 13:32:35.592 | DEBUG    | __main__:<module>:313 - Training step 6910: loss = 3.3940 | 3013.69ms | Tokens/s = 173,968.7
2025-01-15 13:33:05.762 | DEBUG    | __main__:<module>:313 - Training step 6920: loss = 3.4378 | 3017.23ms | Tokens/s = 173,764.5
2025-01-15 13:33:35.928 | DEBUG    | __main__:<module>:313 - Training step 6930: loss = 3.4390 | 3015.24ms | Tokens/s = 173,879.6
2025-01-15 13:34:06.096 | DEBUG    | __main__:<module>:313 - Training step 6940: loss = 3.4536 | 3019.22ms | Tokens/s = 173,650.4
2025-01-15 13:34:36.280 | DEBUG    | __main__:<module>:313 - Training step 6950: loss = 3.5345 | 3018.66ms | Tokens/s = 173,682.3
2025-01-15 13:35:06.471 | DEBUG    | __main__:<module>:313 - Training step 6960: loss = 3.3926 | 3018.98ms | Tokens/s = 173,664.2
2025-01-15 13:35:36.633 | DEBUG    | __main__:<module>:313 - Training step 6970: loss = 3.3638 | 3018.18ms | Tokens/s = 173,710.2
2025-01-15 13:36:06.806 | DEBUG    | __main__:<module>:313 - Training step 6980: loss = 3.5763 | 3015.59ms | Tokens/s = 173,859.3
2025-01-15 13:36:37.003 | DEBUG    | __main__:<module>:313 - Training step 6990: loss = 3.3783 | 3020.36ms | Tokens/s = 173,584.7
2025-01-15 13:37:10.645 | INFO     | __main__:<module>:265 - Step 7,000/40,000 loss: 3.4328 (T) 3.4259 (V) | lr=9.6e-03
2025-01-15 13:37:10.646 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 13:37:24.077 | DEBUG    | __main__:<module>:313 - Training step 7000: loss = 3.4568 | 19890.63ms | Tokens/s = 26,358.5
2025-01-15 13:37:54.165 | DEBUG    | __main__:<module>:313 - Training step 7010: loss = 3.3336 | 3010.00ms | Tokens/s = 174,182.2
2025-01-15 13:38:24.297 | DEBUG    | __main__:<module>:313 - Training step 7020: loss = 3.6484 | 3015.85ms | Tokens/s = 173,844.1
2025-01-15 13:38:54.460 | DEBUG    | __main__:<module>:313 - Training step 7030: loss = 3.5129 | 3015.48ms | Tokens/s = 173,865.6
2025-01-15 13:39:24.651 | DEBUG    | __main__:<module>:313 - Training step 7040: loss = 3.5701 | 3021.21ms | Tokens/s = 173,536.0
2025-01-15 13:39:54.838 | DEBUG    | __main__:<module>:313 - Training step 7050: loss = 3.4418 | 3017.06ms | Tokens/s = 173,774.5
2025-01-15 13:40:25.033 | DEBUG    | __main__:<module>:313 - Training step 7060: loss = 3.2957 | 3021.16ms | Tokens/s = 173,538.5
2025-01-15 13:40:55.243 | DEBUG    | __main__:<module>:313 - Training step 7070: loss = 3.4540 | 3022.29ms | Tokens/s = 173,473.6
2025-01-15 13:41:25.449 | DEBUG    | __main__:<module>:313 - Training step 7080: loss = 3.4999 | 3020.47ms | Tokens/s = 173,578.4
2025-01-15 13:41:55.631 | DEBUG    | __main__:<module>:313 - Training step 7090: loss = 3.5563 | 3018.06ms | Tokens/s = 173,716.9
2025-01-15 13:42:25.804 | DEBUG    | __main__:<module>:313 - Training step 7100: loss = 3.5538 | 3017.95ms | Tokens/s = 173,723.4
2025-01-15 13:42:55.991 | DEBUG    | __main__:<module>:313 - Training step 7110: loss = 3.5225 | 3018.20ms | Tokens/s = 173,708.7
2025-01-15 13:43:26.157 | DEBUG    | __main__:<module>:313 - Training step 7120: loss = 3.5376 | 3013.92ms | Tokens/s = 173,955.6
2025-01-15 13:43:56.340 | DEBUG    | __main__:<module>:313 - Training step 7130: loss = 3.4114 | 3019.40ms | Tokens/s = 173,639.8
2025-01-15 13:44:26.533 | DEBUG    | __main__:<module>:313 - Training step 7140: loss = 3.4576 | 3020.60ms | Tokens/s = 173,571.1
2025-01-15 13:44:56.712 | DEBUG    | __main__:<module>:313 - Training step 7150: loss = 3.4995 | 3016.02ms | Tokens/s = 173,834.4
2025-01-15 13:45:26.882 | DEBUG    | __main__:<module>:313 - Training step 7160: loss = 3.5989 | 3018.82ms | Tokens/s = 173,672.9
2025-01-15 13:45:57.063 | DEBUG    | __main__:<module>:313 - Training step 7170: loss = 3.4076 | 3018.19ms | Tokens/s = 173,709.7
2025-01-15 13:46:27.258 | DEBUG    | __main__:<module>:313 - Training step 7180: loss = 3.4520 | 3020.61ms | Tokens/s = 173,570.4
2025-01-15 13:46:57.456 | DEBUG    | __main__:<module>:313 - Training step 7190: loss = 3.5156 | 3019.85ms | Tokens/s = 173,614.0
2025-01-15 13:47:27.616 | DEBUG    | __main__:<module>:313 - Training step 7200: loss = 3.5352 | 3015.36ms | Tokens/s = 173,872.2
2025-01-15 13:47:57.742 | DEBUG    | __main__:<module>:313 - Training step 7210: loss = 3.4987 | 3012.25ms | Tokens/s = 174,052.1
2025-01-15 13:48:27.891 | DEBUG    | __main__:<module>:313 - Training step 7220: loss = 3.5972 | 3016.83ms | Tokens/s = 173,787.9
2025-01-15 13:48:58.065 | DEBUG    | __main__:<module>:313 - Training step 7230: loss = 3.2362 | 3020.55ms | Tokens/s = 173,573.5
2025-01-15 13:49:28.235 | DEBUG    | __main__:<module>:313 - Training step 7240: loss = 3.5584 | 3014.19ms | Tokens/s = 173,940.2
2025-01-15 13:49:58.391 | DEBUG    | __main__:<module>:313 - Training step 7250: loss = 3.4719 | 3015.76ms | Tokens/s = 173,849.2
2025-01-15 13:50:28.560 | DEBUG    | __main__:<module>:313 - Training step 7260: loss = 3.4265 | 3017.77ms | Tokens/s = 173,733.5
2025-01-15 13:50:58.752 | DEBUG    | __main__:<module>:313 - Training step 7270: loss = 3.4857 | 3020.92ms | Tokens/s = 173,552.6
2025-01-15 13:51:28.945 | DEBUG    | __main__:<module>:313 - Training step 7280: loss = 3.3410 | 3017.41ms | Tokens/s = 173,754.1
2025-01-15 13:51:59.099 | DEBUG    | __main__:<module>:313 - Training step 7290: loss = 3.2811 | 3012.32ms | Tokens/s = 174,047.6
2025-01-15 13:52:29.257 | DEBUG    | __main__:<module>:313 - Training step 7300: loss = 3.3354 | 3014.67ms | Tokens/s = 173,912.4
2025-01-15 13:52:59.451 | DEBUG    | __main__:<module>:313 - Training step 7310: loss = 3.3635 | 3019.35ms | Tokens/s = 173,642.6
2025-01-15 13:53:29.632 | DEBUG    | __main__:<module>:313 - Training step 7320: loss = 3.4046 | 3015.26ms | Tokens/s = 173,878.2
2025-01-15 13:53:59.776 | DEBUG    | __main__:<module>:313 - Training step 7330: loss = 3.7516 | 3016.10ms | Tokens/s = 173,829.5
2025-01-15 13:54:29.943 | DEBUG    | __main__:<module>:313 - Training step 7340: loss = 3.4435 | 3021.52ms | Tokens/s = 173,517.9
2025-01-15 13:55:00.132 | DEBUG    | __main__:<module>:313 - Training step 7350: loss = 3.5285 | 3018.83ms | Tokens/s = 173,672.7
2025-01-15 13:55:30.306 | DEBUG    | __main__:<module>:313 - Training step 7360: loss = 3.3557 | 3016.97ms | Tokens/s = 173,779.5
2025-01-15 13:56:00.458 | DEBUG    | __main__:<module>:313 - Training step 7370: loss = 3.4770 | 3015.95ms | Tokens/s = 173,838.3
2025-01-15 13:56:30.627 | DEBUG    | __main__:<module>:313 - Training step 7380: loss = 3.4549 | 3019.29ms | Tokens/s = 173,645.9
2025-01-15 13:57:00.817 | DEBUG    | __main__:<module>:313 - Training step 7390: loss = 3.3290 | 3021.05ms | Tokens/s = 173,544.9
2025-01-15 13:57:30.996 | DEBUG    | __main__:<module>:313 - Training step 7400: loss = 3.3138 | 3015.71ms | Tokens/s = 173,852.3
2025-01-15 13:58:01.165 | DEBUG    | __main__:<module>:313 - Training step 7410: loss = 3.4329 | 3018.47ms | Tokens/s = 173,693.5
2025-01-15 13:58:31.338 | DEBUG    | __main__:<module>:313 - Training step 7420: loss = 3.4286 | 3019.79ms | Tokens/s = 173,617.6
2025-01-15 13:59:01.534 | DEBUG    | __main__:<module>:313 - Training step 7430: loss = 3.5487 | 3020.78ms | Tokens/s = 173,560.2
2025-01-15 13:59:31.718 | DEBUG    | __main__:<module>:313 - Training step 7440: loss = 3.5694 | 3016.41ms | Tokens/s = 173,811.7
2025-01-15 14:00:01.855 | DEBUG    | __main__:<module>:313 - Training step 7450: loss = 3.3723 | 3014.16ms | Tokens/s = 173,941.7
2025-01-15 14:00:32.003 | DEBUG    | __main__:<module>:313 - Training step 7460: loss = 3.4459 | 3016.93ms | Tokens/s = 173,781.9
2025-01-15 14:01:02.165 | DEBUG    | __main__:<module>:313 - Training step 7470: loss = 3.3656 | 3016.45ms | Tokens/s = 173,809.4
2025-01-15 14:01:32.351 | DEBUG    | __main__:<module>:313 - Training step 7480: loss = 3.6190 | 3019.20ms | Tokens/s = 173,651.1
2025-01-15 14:02:02.532 | DEBUG    | __main__:<module>:313 - Training step 7490: loss = 3.6523 | 3018.93ms | Tokens/s = 173,666.7
2025-01-15 14:02:32.724 | DEBUG    | __main__:<module>:313 - Training step 7500: loss = 3.5535 | 3017.68ms | Tokens/s = 173,738.8
2025-01-15 14:03:02.922 | DEBUG    | __main__:<module>:313 - Training step 7510: loss = 3.5560 | 3020.71ms | Tokens/s = 173,564.3
2025-01-15 14:03:33.099 | DEBUG    | __main__:<module>:313 - Training step 7520: loss = 3.5289 | 3017.46ms | Tokens/s = 173,751.5
2025-01-15 14:04:03.235 | DEBUG    | __main__:<module>:313 - Training step 7530: loss = 3.4611 | 3011.72ms | Tokens/s = 174,082.6
2025-01-15 14:04:33.381 | DEBUG    | __main__:<module>:313 - Training step 7540: loss = 3.4947 | 3015.97ms | Tokens/s = 173,837.1
2025-01-15 14:05:03.562 | DEBUG    | __main__:<module>:313 - Training step 7550: loss = 3.3653 | 3017.81ms | Tokens/s = 173,731.4
2025-01-15 14:05:33.744 | DEBUG    | __main__:<module>:313 - Training step 7560: loss = 3.3894 | 3018.46ms | Tokens/s = 173,694.0
2025-01-15 14:06:03.941 | DEBUG    | __main__:<module>:313 - Training step 7570: loss = 3.4677 | 3021.53ms | Tokens/s = 173,517.2
2025-01-15 14:06:34.147 | DEBUG    | __main__:<module>:313 - Training step 7580: loss = 3.2578 | 3020.01ms | Tokens/s = 173,604.7
2025-01-15 14:07:04.343 | DEBUG    | __main__:<module>:313 - Training step 7590: loss = 3.5309 | 3020.19ms | Tokens/s = 173,594.2
2025-01-15 14:07:34.514 | DEBUG    | __main__:<module>:313 - Training step 7600: loss = 3.3304 | 3014.31ms | Tokens/s = 173,932.8
2025-01-15 14:08:04.681 | DEBUG    | __main__:<module>:313 - Training step 7610: loss = 3.4221 | 3018.69ms | Tokens/s = 173,680.9
2025-01-15 14:08:34.843 | DEBUG    | __main__:<module>:313 - Training step 7620: loss = 3.4259 | 3014.30ms | Tokens/s = 173,933.4
2025-01-15 14:09:05.010 | DEBUG    | __main__:<module>:313 - Training step 7630: loss = 3.6287 | 3015.78ms | Tokens/s = 173,848.0
2025-01-15 14:09:35.164 | DEBUG    | __main__:<module>:313 - Training step 7640: loss = 3.3933 | 3014.07ms | Tokens/s = 173,946.7
2025-01-15 14:10:05.315 | DEBUG    | __main__:<module>:313 - Training step 7650: loss = 3.4894 | 3016.75ms | Tokens/s = 173,792.6
2025-01-15 14:10:35.498 | DEBUG    | __main__:<module>:313 - Training step 7660: loss = 3.4665 | 3017.31ms | Tokens/s = 173,760.1
2025-01-15 14:11:05.661 | DEBUG    | __main__:<module>:313 - Training step 7670: loss = 3.3898 | 3014.32ms | Tokens/s = 173,932.4
2025-01-15 14:11:35.810 | DEBUG    | __main__:<module>:313 - Training step 7680: loss = 3.2385 | 3014.72ms | Tokens/s = 173,909.3
2025-01-15 14:12:05.951 | DEBUG    | __main__:<module>:313 - Training step 7690: loss = 3.5242 | 3014.54ms | Tokens/s = 173,919.9
2025-01-15 14:12:36.140 | DEBUG    | __main__:<module>:313 - Training step 7700: loss = 3.2321 | 3020.79ms | Tokens/s = 173,559.9
2025-01-15 14:13:06.343 | DEBUG    | __main__:<module>:313 - Training step 7710: loss = 3.4923 | 3018.45ms | Tokens/s = 173,694.3
2025-01-15 14:13:36.521 | DEBUG    | __main__:<module>:313 - Training step 7720: loss = 3.4266 | 3015.55ms | Tokens/s = 173,861.6
2025-01-15 14:14:06.707 | DEBUG    | __main__:<module>:313 - Training step 7730: loss = 3.5461 | 3020.15ms | Tokens/s = 173,596.7
2025-01-15 14:14:36.899 | DEBUG    | __main__:<module>:313 - Training step 7740: loss = 3.4653 | 3018.86ms | Tokens/s = 173,670.9
2025-01-15 14:15:07.091 | DEBUG    | __main__:<module>:313 - Training step 7750: loss = 3.3744 | 3016.13ms | Tokens/s = 173,828.0
2025-01-15 14:15:37.263 | DEBUG    | __main__:<module>:313 - Training step 7760: loss = 3.3732 | 3015.73ms | Tokens/s = 173,851.2
2025-01-15 14:16:07.429 | DEBUG    | __main__:<module>:313 - Training step 7770: loss = 3.4450 | 3015.26ms | Tokens/s = 173,878.4
2025-01-15 14:16:37.585 | DEBUG    | __main__:<module>:313 - Training step 7780: loss = 3.5346 | 3015.55ms | Tokens/s = 173,861.2
2025-01-15 14:17:07.747 | DEBUG    | __main__:<module>:313 - Training step 7790: loss = 3.3579 | 3015.48ms | Tokens/s = 173,865.7
2025-01-15 14:17:37.930 | DEBUG    | __main__:<module>:313 - Training step 7800: loss = 3.4215 | 3018.98ms | Tokens/s = 173,664.1
2025-01-15 14:18:08.136 | DEBUG    | __main__:<module>:313 - Training step 7810: loss = 3.2819 | 3019.62ms | Tokens/s = 173,627.0
2025-01-15 14:18:38.329 | DEBUG    | __main__:<module>:313 - Training step 7820: loss = 3.2877 | 3017.50ms | Tokens/s = 173,748.9
2025-01-15 14:19:08.511 | DEBUG    | __main__:<module>:313 - Training step 7830: loss = 3.5436 | 3015.65ms | Tokens/s = 173,855.8
2025-01-15 14:19:38.678 | DEBUG    | __main__:<module>:313 - Training step 7840: loss = 3.3682 | 3016.25ms | Tokens/s = 173,821.4
2025-01-15 14:20:08.830 | DEBUG    | __main__:<module>:313 - Training step 7850: loss = 3.3668 | 3015.38ms | Tokens/s = 173,871.5
2025-01-15 14:20:38.984 | DEBUG    | __main__:<module>:313 - Training step 7860: loss = 3.4304 | 3017.73ms | Tokens/s = 173,736.0
2025-01-15 14:21:09.143 | DEBUG    | __main__:<module>:313 - Training step 7870: loss = 3.4891 | 3016.91ms | Tokens/s = 173,783.0
2025-01-15 14:21:39.296 | DEBUG    | __main__:<module>:313 - Training step 7880: loss = 3.5610 | 3013.59ms | Tokens/s = 173,974.8
2025-01-15 14:22:09.429 | DEBUG    | __main__:<module>:313 - Training step 7890: loss = 3.3191 | 3010.75ms | Tokens/s = 174,138.7
2025-01-15 14:22:39.605 | DEBUG    | __main__:<module>:313 - Training step 7900: loss = 3.4281 | 3019.28ms | Tokens/s = 173,646.9
2025-01-15 14:23:09.815 | DEBUG    | __main__:<module>:313 - Training step 7910: loss = 3.4784 | 3022.64ms | Tokens/s = 173,453.9
2025-01-15 14:23:40.025 | DEBUG    | __main__:<module>:313 - Training step 7920: loss = 3.4226 | 3019.79ms | Tokens/s = 173,617.6
2025-01-15 14:24:10.212 | DEBUG    | __main__:<module>:313 - Training step 7930: loss = 3.3566 | 3020.94ms | Tokens/s = 173,551.2
2025-01-15 14:24:40.393 | DEBUG    | __main__:<module>:313 - Training step 7940: loss = 3.5307 | 3018.66ms | Tokens/s = 173,682.2
2025-01-15 14:25:10.570 | DEBUG    | __main__:<module>:313 - Training step 7950: loss = 3.5022 | 3014.68ms | Tokens/s = 173,911.5
2025-01-15 14:25:40.741 | DEBUG    | __main__:<module>:313 - Training step 7960: loss = 3.5565 | 3014.71ms | Tokens/s = 173,910.1
2025-01-15 14:26:10.903 | DEBUG    | __main__:<module>:313 - Training step 7970: loss = 3.2723 | 3018.04ms | Tokens/s = 173,717.9
2025-01-15 14:26:41.074 | DEBUG    | __main__:<module>:313 - Training step 7980: loss = 3.4153 | 3018.34ms | Tokens/s = 173,700.7
2025-01-15 14:27:11.244 | DEBUG    | __main__:<module>:313 - Training step 7990: loss = 3.5520 | 3017.28ms | Tokens/s = 173,761.7
2025-01-15 14:27:44.811 | INFO     | __main__:<module>:265 - Step 8,000/40,000 loss: 3.4081 (T) 3.4209 (V) | lr=9.4e-03
2025-01-15 14:27:44.812 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 14:27:59.433 | DEBUG    | __main__:<module>:313 - Training step 8000: loss = 3.4011 | 21062.33ms | Tokens/s = 24,892.2
2025-01-15 14:28:29.484 | DEBUG    | __main__:<module>:313 - Training step 8010: loss = 3.3400 | 3008.97ms | Tokens/s = 174,241.9
2025-01-15 14:28:59.614 | DEBUG    | __main__:<module>:313 - Training step 8020: loss = 3.3092 | 3016.21ms | Tokens/s = 173,823.3
2025-01-15 14:29:29.787 | DEBUG    | __main__:<module>:313 - Training step 8030: loss = 3.3627 | 3016.75ms | Tokens/s = 173,792.1
2025-01-15 14:29:59.982 | DEBUG    | __main__:<module>:313 - Training step 8040: loss = 3.4337 | 3020.36ms | Tokens/s = 173,584.9
2025-01-15 14:30:30.173 | DEBUG    | __main__:<module>:313 - Training step 8050: loss = 3.6095 | 3021.21ms | Tokens/s = 173,535.5
2025-01-15 14:31:00.360 | DEBUG    | __main__:<module>:313 - Training step 8060: loss = 3.4274 | 3016.53ms | Tokens/s = 173,805.0
2025-01-15 14:31:30.545 | DEBUG    | __main__:<module>:313 - Training step 8070: loss = 3.6571 | 3020.03ms | Tokens/s = 173,603.8
2025-01-15 14:32:00.729 | DEBUG    | __main__:<module>:313 - Training step 8080: loss = 3.4460 | 3018.90ms | Tokens/s = 173,668.5
2025-01-15 14:32:30.945 | DEBUG    | __main__:<module>:313 - Training step 8090: loss = 3.4757 | 3021.18ms | Tokens/s = 173,537.6
2025-01-15 14:33:01.143 | DEBUG    | __main__:<module>:313 - Training step 8100: loss = 3.1706 | 3017.40ms | Tokens/s = 173,755.0
2025-01-15 14:33:31.311 | DEBUG    | __main__:<module>:313 - Training step 8110: loss = 3.2996 | 3013.50ms | Tokens/s = 173,979.8
2025-01-15 14:34:01.462 | DEBUG    | __main__:<module>:313 - Training step 8120: loss = 3.3390 | 3014.06ms | Tokens/s = 173,947.4
2025-01-15 14:34:31.645 | DEBUG    | __main__:<module>:313 - Training step 8130: loss = 3.4946 | 3021.96ms | Tokens/s = 173,492.9
2025-01-15 14:35:01.844 | DEBUG    | __main__:<module>:313 - Training step 8140: loss = 3.3224 | 3018.94ms | Tokens/s = 173,666.1
2025-01-15 14:35:32.033 | DEBUG    | __main__:<module>:313 - Training step 8150: loss = 3.4246 | 3020.68ms | Tokens/s = 173,566.4
2025-01-15 14:36:02.211 | DEBUG    | __main__:<module>:313 - Training step 8160: loss = 3.3996 | 3015.48ms | Tokens/s = 173,865.3
2025-01-15 14:36:32.387 | DEBUG    | __main__:<module>:313 - Training step 8170: loss = 3.3784 | 3017.35ms | Tokens/s = 173,757.8
2025-01-15 14:37:02.575 | DEBUG    | __main__:<module>:313 - Training step 8180: loss = 3.5283 | 3018.92ms | Tokens/s = 173,667.4
2025-01-15 14:37:32.749 | DEBUG    | __main__:<module>:313 - Training step 8190: loss = 3.2537 | 3014.39ms | Tokens/s = 173,928.1
2025-01-15 14:38:02.922 | DEBUG    | __main__:<module>:313 - Training step 8200: loss = 3.4231 | 3015.78ms | Tokens/s = 173,848.4
2025-01-15 14:38:33.093 | DEBUG    | __main__:<module>:313 - Training step 8210: loss = 3.3196 | 3015.95ms | Tokens/s = 173,838.3
2025-01-15 14:39:03.261 | DEBUG    | __main__:<module>:313 - Training step 8220: loss = 3.3938 | 3015.84ms | Tokens/s = 173,844.5
2025-01-15 14:39:33.420 | DEBUG    | __main__:<module>:313 - Training step 8230: loss = 3.3909 | 3017.26ms | Tokens/s = 173,762.8
2025-01-15 14:40:03.612 | DEBUG    | __main__:<module>:313 - Training step 8240: loss = 3.3937 | 3019.41ms | Tokens/s = 173,638.9
2025-01-15 14:40:33.832 | DEBUG    | __main__:<module>:313 - Training step 8250: loss = 3.5019 | 3020.88ms | Tokens/s = 173,554.6
2025-01-15 14:41:04.038 | DEBUG    | __main__:<module>:313 - Training step 8260: loss = 3.2352 | 3018.04ms | Tokens/s = 173,718.2
2025-01-15 14:41:34.216 | DEBUG    | __main__:<module>:313 - Training step 8270: loss = 3.4565 | 3017.78ms | Tokens/s = 173,733.2
2025-01-15 14:42:04.385 | DEBUG    | __main__:<module>:313 - Training step 8280: loss = 3.4882 | 3015.65ms | Tokens/s = 173,855.5
2025-01-15 14:42:34.562 | DEBUG    | __main__:<module>:313 - Training step 8290: loss = 3.3951 | 3017.16ms | Tokens/s = 173,768.6
2025-01-15 14:43:04.738 | DEBUG    | __main__:<module>:313 - Training step 8300: loss = 3.4828 | 3016.70ms | Tokens/s = 173,795.4
2025-01-15 14:43:34.907 | DEBUG    | __main__:<module>:313 - Training step 8310: loss = 3.4823 | 3015.38ms | Tokens/s = 173,871.3
2025-01-15 14:44:05.075 | DEBUG    | __main__:<module>:313 - Training step 8320: loss = 3.4519 | 3015.97ms | Tokens/s = 173,837.1
2025-01-15 14:44:35.253 | DEBUG    | __main__:<module>:313 - Training step 8330: loss = 3.4172 | 3017.54ms | Tokens/s = 173,746.8
2025-01-15 14:45:05.432 | DEBUG    | __main__:<module>:313 - Training step 8340: loss = 3.2554 | 3018.55ms | Tokens/s = 173,688.6
2025-01-15 14:45:35.633 | DEBUG    | __main__:<module>:313 - Training step 8350: loss = 3.4779 | 3022.06ms | Tokens/s = 173,486.7
2025-01-15 14:46:05.833 | DEBUG    | __main__:<module>:313 - Training step 8360: loss = 3.4286 | 3017.57ms | Tokens/s = 173,745.2
2025-01-15 14:46:36.028 | DEBUG    | __main__:<module>:313 - Training step 8370: loss = 3.5339 | 3020.61ms | Tokens/s = 173,570.3
2025-01-15 14:47:06.214 | DEBUG    | __main__:<module>:313 - Training step 8380: loss = 3.2410 | 3017.46ms | Tokens/s = 173,751.6
2025-01-15 14:47:36.390 | DEBUG    | __main__:<module>:313 - Training step 8390: loss = 3.5247 | 3016.79ms | Tokens/s = 173,790.1
2025-01-15 14:48:06.571 | DEBUG    | __main__:<module>:313 - Training step 8400: loss = 3.2662 | 3019.07ms | Tokens/s = 173,658.5
2025-01-15 14:48:36.752 | DEBUG    | __main__:<module>:313 - Training step 8410: loss = 3.5016 | 3018.02ms | Tokens/s = 173,719.4
2025-01-15 14:49:06.924 | DEBUG    | __main__:<module>:313 - Training step 8420: loss = 3.1094 | 3013.48ms | Tokens/s = 173,980.7
2025-01-15 14:49:37.089 | DEBUG    | __main__:<module>:313 - Training step 8430: loss = 3.5338 | 3017.56ms | Tokens/s = 173,745.5
2025-01-15 14:50:07.259 | DEBUG    | __main__:<module>:313 - Training step 8440: loss = 3.1103 | 3019.20ms | Tokens/s = 173,651.5
2025-01-15 14:50:37.450 | DEBUG    | __main__:<module>:313 - Training step 8450: loss = 3.4994 | 3018.17ms | Tokens/s = 173,710.7
2025-01-15 14:51:07.650 | DEBUG    | __main__:<module>:313 - Training step 8460: loss = 3.3434 | 3020.21ms | Tokens/s = 173,593.0
2025-01-15 14:51:37.844 | DEBUG    | __main__:<module>:313 - Training step 8470: loss = 3.5059 | 3017.82ms | Tokens/s = 173,730.6
2025-01-15 14:52:08.030 | DEBUG    | __main__:<module>:313 - Training step 8480: loss = 3.3216 | 3016.40ms | Tokens/s = 173,812.5
2025-01-15 14:52:38.218 | DEBUG    | __main__:<module>:313 - Training step 8490: loss = 3.2029 | 3020.35ms | Tokens/s = 173,585.0
2025-01-15 14:53:08.391 | DEBUG    | __main__:<module>:313 - Training step 8500: loss = 3.2946 | 3015.00ms | Tokens/s = 173,893.1
2025-01-15 14:53:38.549 | DEBUG    | __main__:<module>:313 - Training step 8510: loss = 3.2622 | 3016.28ms | Tokens/s = 173,819.5
2025-01-15 14:54:08.730 | DEBUG    | __main__:<module>:313 - Training step 8520: loss = 3.3820 | 3020.06ms | Tokens/s = 173,601.6
2025-01-15 14:54:38.944 | DEBUG    | __main__:<module>:313 - Training step 8530: loss = 3.3633 | 3021.37ms | Tokens/s = 173,526.5
2025-01-15 14:55:09.138 | DEBUG    | __main__:<module>:313 - Training step 8540: loss = 3.3697 | 3017.52ms | Tokens/s = 173,747.9
2025-01-15 14:55:39.319 | DEBUG    | __main__:<module>:313 - Training step 8550: loss = 3.5547 | 3016.93ms | Tokens/s = 173,781.9
2025-01-15 14:56:09.489 | DEBUG    | __main__:<module>:313 - Training step 8560: loss = 3.1773 | 3018.33ms | Tokens/s = 173,701.5
2025-01-15 14:56:39.652 | DEBUG    | __main__:<module>:313 - Training step 8570: loss = 3.3453 | 3015.24ms | Tokens/s = 173,879.5
2025-01-15 14:57:09.815 | DEBUG    | __main__:<module>:313 - Training step 8580: loss = 3.2839 | 3017.06ms | Tokens/s = 173,774.3
2025-01-15 14:57:40.012 | DEBUG    | __main__:<module>:313 - Training step 8590: loss = 3.3767 | 3020.74ms | Tokens/s = 173,563.0
2025-01-15 14:58:10.205 | DEBUG    | __main__:<module>:313 - Training step 8600: loss = 3.4519 | 3018.34ms | Tokens/s = 173,700.5
2025-01-15 14:58:40.406 | DEBUG    | __main__:<module>:313 - Training step 8610: loss = 3.3457 | 3018.55ms | Tokens/s = 173,688.7
2025-01-15 14:59:10.605 | DEBUG    | __main__:<module>:313 - Training step 8620: loss = 3.4755 | 3020.41ms | Tokens/s = 173,581.9
2025-01-15 14:59:40.826 | DEBUG    | __main__:<module>:313 - Training step 8630: loss = 3.4728 | 3020.43ms | Tokens/s = 173,580.4
2025-01-15 15:00:11.037 | DEBUG    | __main__:<module>:313 - Training step 8640: loss = 3.3572 | 3020.73ms | Tokens/s = 173,563.2
2025-01-15 15:00:41.235 | DEBUG    | __main__:<module>:313 - Training step 8650: loss = 3.3923 | 3020.14ms | Tokens/s = 173,597.2
2025-01-15 15:01:11.425 | DEBUG    | __main__:<module>:313 - Training step 8660: loss = 3.2580 | 3018.31ms | Tokens/s = 173,702.7
2025-01-15 15:01:41.607 | DEBUG    | __main__:<module>:313 - Training step 8670: loss = 3.3172 | 3018.16ms | Tokens/s = 173,711.4
2025-01-15 15:02:11.770 | DEBUG    | __main__:<module>:313 - Training step 8680: loss = 3.3326 | 3016.14ms | Tokens/s = 173,827.5
2025-01-15 15:02:41.926 | DEBUG    | __main__:<module>:313 - Training step 8690: loss = 3.2439 | 3015.78ms | Tokens/s = 173,848.2
2025-01-15 15:03:12.106 | DEBUG    | __main__:<module>:313 - Training step 8700: loss = 3.5442 | 3017.64ms | Tokens/s = 173,740.9
2025-01-15 15:03:42.289 | DEBUG    | __main__:<module>:313 - Training step 8710: loss = 3.4055 | 3017.66ms | Tokens/s = 173,739.6
2025-01-15 15:04:12.487 | DEBUG    | __main__:<module>:313 - Training step 8720: loss = 3.5243 | 3020.32ms | Tokens/s = 173,586.9
2025-01-15 15:04:42.663 | DEBUG    | __main__:<module>:313 - Training step 8730: loss = 3.3629 | 3014.39ms | Tokens/s = 173,928.7
2025-01-15 15:05:12.816 | DEBUG    | __main__:<module>:313 - Training step 8740: loss = 3.1396 | 3014.29ms | Tokens/s = 173,934.4
2025-01-15 15:05:42.945 | DEBUG    | __main__:<module>:313 - Training step 8750: loss = 3.3928 | 3014.72ms | Tokens/s = 173,909.3
2025-01-15 15:06:13.106 | DEBUG    | __main__:<module>:313 - Training step 8760: loss = 3.2457 | 3019.05ms | Tokens/s = 173,659.7
2025-01-15 15:06:43.278 | DEBUG    | __main__:<module>:313 - Training step 8770: loss = 3.4859 | 3015.69ms | Tokens/s = 173,853.3
2025-01-15 15:07:13.432 | DEBUG    | __main__:<module>:313 - Training step 8780: loss = 3.4438 | 3014.44ms | Tokens/s = 173,925.4
2025-01-15 15:07:43.600 | DEBUG    | __main__:<module>:313 - Training step 8790: loss = 3.2777 | 3016.73ms | Tokens/s = 173,793.7
2025-01-15 15:08:13.784 | DEBUG    | __main__:<module>:313 - Training step 8800: loss = 3.4307 | 3019.83ms | Tokens/s = 173,614.9
2025-01-15 15:08:43.955 | DEBUG    | __main__:<module>:313 - Training step 8810: loss = 3.3928 | 3015.51ms | Tokens/s = 173,863.6
2025-01-15 15:09:14.120 | DEBUG    | __main__:<module>:313 - Training step 8820: loss = 3.3633 | 3019.36ms | Tokens/s = 173,641.9
2025-01-15 15:09:44.297 | DEBUG    | __main__:<module>:313 - Training step 8830: loss = 3.3394 | 3020.04ms | Tokens/s = 173,602.9
2025-01-15 15:10:14.479 | DEBUG    | __main__:<module>:313 - Training step 8840: loss = 3.5753 | 3017.67ms | Tokens/s = 173,739.4
2025-01-15 15:10:44.634 | DEBUG    | __main__:<module>:313 - Training step 8850: loss = 3.3579 | 3015.06ms | Tokens/s = 173,890.0
2025-01-15 15:11:14.760 | DEBUG    | __main__:<module>:313 - Training step 8860: loss = 3.4754 | 3011.35ms | Tokens/s = 174,104.1
2025-01-15 15:11:44.903 | DEBUG    | __main__:<module>:313 - Training step 8870: loss = 3.2973 | 3014.39ms | Tokens/s = 173,928.5
2025-01-15 15:12:15.074 | DEBUG    | __main__:<module>:313 - Training step 8880: loss = 3.4161 | 3015.80ms | Tokens/s = 173,846.9
2025-01-15 15:12:45.247 | DEBUG    | __main__:<module>:313 - Training step 8890: loss = 3.5360 | 3014.73ms | Tokens/s = 173,908.9
2025-01-15 15:13:15.406 | DEBUG    | __main__:<module>:313 - Training step 8900: loss = 3.2534 | 3017.68ms | Tokens/s = 173,738.6
2025-01-15 15:13:45.569 | DEBUG    | __main__:<module>:313 - Training step 8910: loss = 3.3262 | 3015.90ms | Tokens/s = 173,841.5
2025-01-15 15:14:15.749 | DEBUG    | __main__:<module>:313 - Training step 8920: loss = 3.5027 | 3018.10ms | Tokens/s = 173,714.5
2025-01-15 15:14:45.915 | DEBUG    | __main__:<module>:313 - Training step 8930: loss = 3.3897 | 3014.93ms | Tokens/s = 173,897.2
2025-01-15 15:15:16.050 | DEBUG    | __main__:<module>:313 - Training step 8940: loss = 3.3623 | 3010.47ms | Tokens/s = 174,154.6
2025-01-15 15:15:46.184 | DEBUG    | __main__:<module>:313 - Training step 8950: loss = 3.4870 | 3016.13ms | Tokens/s = 173,828.3
2025-01-15 15:16:16.346 | DEBUG    | __main__:<module>:313 - Training step 8960: loss = 3.4538 | 3018.89ms | Tokens/s = 173,669.3
2025-01-15 15:16:46.535 | DEBUG    | __main__:<module>:313 - Training step 8970: loss = 3.4024 | 3021.08ms | Tokens/s = 173,543.4
2025-01-15 15:17:16.706 | DEBUG    | __main__:<module>:313 - Training step 8980: loss = 3.5494 | 3018.61ms | Tokens/s = 173,685.4
2025-01-15 15:17:46.841 | DEBUG    | __main__:<module>:313 - Training step 8990: loss = 3.2824 | 3012.00ms | Tokens/s = 174,066.4
2025-01-15 15:18:20.419 | INFO     | __main__:<module>:265 - Step 9,000/40,000 loss: 3.4019 (T) 3.4322 (V) | lr=9.2e-03
2025-01-15 15:18:23.436 | DEBUG    | __main__:<module>:313 - Training step 9000: loss = 3.2791 | 9463.58ms | Tokens/s = 55,400.6
2025-01-15 15:18:53.612 | DEBUG    | __main__:<module>:313 - Training step 9010: loss = 3.6319 | 3016.77ms | Tokens/s = 173,790.9
2025-01-15 15:19:23.766 | DEBUG    | __main__:<module>:313 - Training step 9020: loss = 3.1935 | 3013.62ms | Tokens/s = 173,973.0
2025-01-15 15:19:53.922 | DEBUG    | __main__:<module>:313 - Training step 9030: loss = 3.4770 | 3015.53ms | Tokens/s = 173,862.5
2025-01-15 15:20:24.098 | DEBUG    | __main__:<module>:313 - Training step 9040: loss = 3.1475 | 3019.73ms | Tokens/s = 173,620.7
2025-01-15 15:20:54.291 | DEBUG    | __main__:<module>:313 - Training step 9050: loss = 3.3490 | 3019.22ms | Tokens/s = 173,650.1
2025-01-15 15:21:24.460 | DEBUG    | __main__:<module>:313 - Training step 9060: loss = 3.4490 | 3014.03ms | Tokens/s = 173,949.1
2025-01-15 15:21:54.600 | DEBUG    | __main__:<module>:313 - Training step 9070: loss = 3.2954 | 3014.71ms | Tokens/s = 173,909.9
2025-01-15 15:22:24.773 | DEBUG    | __main__:<module>:313 - Training step 9080: loss = 3.2943 | 3018.24ms | Tokens/s = 173,706.6
2025-01-15 15:22:54.948 | DEBUG    | __main__:<module>:313 - Training step 9090: loss = 3.3043 | 3017.16ms | Tokens/s = 173,768.9
2025-01-15 15:23:25.110 | DEBUG    | __main__:<module>:313 - Training step 9100: loss = 3.5656 | 3017.64ms | Tokens/s = 173,740.9
2025-01-15 15:23:55.292 | DEBUG    | __main__:<module>:313 - Training step 9110: loss = 3.5264 | 3021.16ms | Tokens/s = 173,538.5
2025-01-15 15:24:25.494 | DEBUG    | __main__:<module>:313 - Training step 9120: loss = 3.6156 | 3018.53ms | Tokens/s = 173,689.7
2025-01-15 15:24:55.659 | DEBUG    | __main__:<module>:313 - Training step 9130: loss = 3.3380 | 3014.37ms | Tokens/s = 173,929.4
2025-01-15 15:25:25.799 | DEBUG    | __main__:<module>:313 - Training step 9140: loss = 3.2493 | 3012.28ms | Tokens/s = 174,050.1
2025-01-15 15:25:55.959 | DEBUG    | __main__:<module>:313 - Training step 9150: loss = 3.3947 | 3016.08ms | Tokens/s = 173,831.0
2025-01-15 15:26:26.119 | DEBUG    | __main__:<module>:313 - Training step 9160: loss = 3.4335 | 3015.73ms | Tokens/s = 173,851.0
2025-01-15 15:26:56.252 | DEBUG    | __main__:<module>:313 - Training step 9170: loss = 3.3799 | 3011.09ms | Tokens/s = 174,118.8
2025-01-15 15:27:26.375 | DEBUG    | __main__:<module>:313 - Training step 9180: loss = 3.3374 | 3014.64ms | Tokens/s = 173,914.1
2025-01-15 15:27:56.530 | DEBUG    | __main__:<module>:313 - Training step 9190: loss = 3.5135 | 3015.74ms | Tokens/s = 173,850.5
2025-01-15 15:28:26.704 | DEBUG    | __main__:<module>:313 - Training step 9200: loss = 3.3852 | 3017.43ms | Tokens/s = 173,753.4
2025-01-15 15:28:56.880 | DEBUG    | __main__:<module>:313 - Training step 9210: loss = 3.4305 | 3018.67ms | Tokens/s = 173,681.5
2025-01-15 15:29:27.052 | DEBUG    | __main__:<module>:313 - Training step 9220: loss = 3.5087 | 3018.68ms | Tokens/s = 173,681.1
2025-01-15 15:29:57.236 | DEBUG    | __main__:<module>:313 - Training step 9230: loss = 3.5051 | 3016.46ms | Tokens/s = 173,809.0
2025-01-15 15:30:27.397 | DEBUG    | __main__:<module>:313 - Training step 9240: loss = 3.3481 | 3016.01ms | Tokens/s = 173,835.0
2025-01-15 15:30:57.525 | DEBUG    | __main__:<module>:313 - Training step 9250: loss = 3.3699 | 3012.14ms | Tokens/s = 174,058.5
2025-01-15 15:31:27.670 | DEBUG    | __main__:<module>:313 - Training step 9260: loss = 3.4989 | 3014.74ms | Tokens/s = 173,908.3
2025-01-15 15:31:57.827 | DEBUG    | __main__:<module>:313 - Training step 9270: loss = 3.3080 | 3017.33ms | Tokens/s = 173,758.7
2025-01-15 15:32:28.002 | DEBUG    | __main__:<module>:313 - Training step 9280: loss = 3.2491 | 3017.41ms | Tokens/s = 173,754.3
2025-01-15 15:32:58.179 | DEBUG    | __main__:<module>:313 - Training step 9290: loss = 3.4818 | 3017.36ms | Tokens/s = 173,757.5
2025-01-15 15:33:28.357 | DEBUG    | __main__:<module>:313 - Training step 9300: loss = 3.4966 | 3015.51ms | Tokens/s = 173,864.0
2025-01-15 15:33:58.503 | DEBUG    | __main__:<module>:313 - Training step 9310: loss = 3.3324 | 3013.93ms | Tokens/s = 173,955.2
2025-01-15 15:34:28.634 | DEBUG    | __main__:<module>:313 - Training step 9320: loss = 3.4063 | 3013.41ms | Tokens/s = 173,985.1
2025-01-15 15:34:58.768 | DEBUG    | __main__:<module>:313 - Training step 9330: loss = 3.2921 | 3013.26ms | Tokens/s = 173,993.3
2025-01-15 15:35:28.946 | DEBUG    | __main__:<module>:313 - Training step 9340: loss = 3.1665 | 3015.45ms | Tokens/s = 173,867.0
2025-01-15 15:35:59.109 | DEBUG    | __main__:<module>:313 - Training step 9350: loss = 3.2460 | 3016.48ms | Tokens/s = 173,807.6
2025-01-15 15:36:29.267 | DEBUG    | __main__:<module>:313 - Training step 9360: loss = 3.3652 | 3014.42ms | Tokens/s = 173,926.7
2025-01-15 15:36:59.421 | DEBUG    | __main__:<module>:313 - Training step 9370: loss = 3.2057 | 3014.89ms | Tokens/s = 173,899.6
2025-01-15 15:37:29.560 | DEBUG    | __main__:<module>:313 - Training step 9380: loss = 3.4447 | 3014.56ms | Tokens/s = 173,918.4
2025-01-15 15:37:59.705 | DEBUG    | __main__:<module>:313 - Training step 9390: loss = 3.3566 | 3013.64ms | Tokens/s = 173,971.6
2025-01-15 15:38:29.861 | DEBUG    | __main__:<module>:313 - Training step 9400: loss = 3.4654 | 3016.16ms | Tokens/s = 173,826.2
2025-01-15 15:39:00.010 | DEBUG    | __main__:<module>:313 - Training step 9410: loss = 3.3270 | 3013.75ms | Tokens/s = 173,965.1
2025-01-15 15:39:30.160 | DEBUG    | __main__:<module>:313 - Training step 9420: loss = 3.7782 | 3015.16ms | Tokens/s = 173,883.8
2025-01-15 15:40:00.342 | DEBUG    | __main__:<module>:313 - Training step 9430: loss = 3.3921 | 3019.98ms | Tokens/s = 173,606.2
2025-01-15 15:40:30.554 | DEBUG    | __main__:<module>:313 - Training step 9440: loss = 3.4325 | 3020.79ms | Tokens/s = 173,560.1
2025-01-15 15:41:00.738 | DEBUG    | __main__:<module>:313 - Training step 9450: loss = 3.4745 | 3017.66ms | Tokens/s = 173,739.9
2025-01-15 15:41:30.908 | DEBUG    | __main__:<module>:313 - Training step 9460: loss = 3.4772 | 3014.56ms | Tokens/s = 173,918.4
2025-01-15 15:42:01.076 | DEBUG    | __main__:<module>:313 - Training step 9470: loss = 3.2150 | 3018.18ms | Tokens/s = 173,710.1
2025-01-15 15:42:31.254 | DEBUG    | __main__:<module>:313 - Training step 9480: loss = 3.2936 | 3019.72ms | Tokens/s = 173,621.4
2025-01-15 15:43:01.419 | DEBUG    | __main__:<module>:313 - Training step 9490: loss = 3.4715 | 3014.04ms | Tokens/s = 173,948.6
2025-01-15 15:43:31.580 | DEBUG    | __main__:<module>:313 - Training step 9500: loss = 3.4203 | 3015.12ms | Tokens/s = 173,886.5
2025-01-15 15:44:01.733 | DEBUG    | __main__:<module>:313 - Training step 9510: loss = 3.3217 | 3013.96ms | Tokens/s = 173,953.5
2025-01-15 15:44:31.887 | DEBUG    | __main__:<module>:313 - Training step 9520: loss = 3.4011 | 3014.12ms | Tokens/s = 173,944.1
2025-01-15 15:45:02.027 | DEBUG    | __main__:<module>:313 - Training step 9530: loss = 3.3248 | 3012.93ms | Tokens/s = 174,012.7
2025-01-15 15:45:32.171 | DEBUG    | __main__:<module>:313 - Training step 9540: loss = 3.3524 | 3016.96ms | Tokens/s = 173,780.4
2025-01-15 15:46:02.346 | DEBUG    | __main__:<module>:313 - Training step 9550: loss = 3.5123 | 3018.74ms | Tokens/s = 173,677.9
2025-01-15 15:46:32.546 | DEBUG    | __main__:<module>:313 - Training step 9560: loss = 3.2363 | 3020.83ms | Tokens/s = 173,557.5
2025-01-15 15:47:02.736 | DEBUG    | __main__:<module>:313 - Training step 9570: loss = 3.3957 | 3018.43ms | Tokens/s = 173,695.7
2025-01-15 15:47:32.907 | DEBUG    | __main__:<module>:313 - Training step 9580: loss = 3.3555 | 3017.78ms | Tokens/s = 173,732.8
2025-01-15 15:48:03.074 | DEBUG    | __main__:<module>:313 - Training step 9590: loss = 3.4011 | 3016.78ms | Tokens/s = 173,790.5
2025-01-15 15:48:33.243 | DEBUG    | __main__:<module>:313 - Training step 9600: loss = 3.4428 | 3014.51ms | Tokens/s = 173,921.6
2025-01-15 15:49:03.406 | DEBUG    | __main__:<module>:313 - Training step 9610: loss = 3.4578 | 3017.98ms | Tokens/s = 173,721.5
2025-01-15 15:49:33.587 | DEBUG    | __main__:<module>:313 - Training step 9620: loss = 3.2974 | 3020.27ms | Tokens/s = 173,589.6
2025-01-15 15:50:03.778 | DEBUG    | __main__:<module>:313 - Training step 9630: loss = 3.4762 | 3018.15ms | Tokens/s = 173,711.5
2025-01-15 15:50:33.966 | DEBUG    | __main__:<module>:313 - Training step 9640: loss = 3.2695 | 3018.11ms | Tokens/s = 173,714.1
2025-01-15 15:51:04.138 | DEBUG    | __main__:<module>:313 - Training step 9650: loss = 3.4877 | 3016.55ms | Tokens/s = 173,804.1
2025-01-15 15:51:34.294 | DEBUG    | __main__:<module>:313 - Training step 9660: loss = 3.4427 | 3014.45ms | Tokens/s = 173,924.7
2025-01-15 15:52:04.450 | DEBUG    | __main__:<module>:313 - Training step 9670: loss = 3.3192 | 3013.62ms | Tokens/s = 173,972.6
2025-01-15 15:52:34.620 | DEBUG    | __main__:<module>:313 - Training step 9680: loss = 3.3871 | 3017.35ms | Tokens/s = 173,757.9
2025-01-15 15:53:04.785 | DEBUG    | __main__:<module>:313 - Training step 9690: loss = 3.2038 | 3016.67ms | Tokens/s = 173,797.2
2025-01-15 15:53:34.929 | DEBUG    | __main__:<module>:313 - Training step 9700: loss = 3.5011 | 3013.37ms | Tokens/s = 173,987.2
2025-01-15 15:54:05.099 | DEBUG    | __main__:<module>:313 - Training step 9710: loss = 3.6209 | 3018.82ms | Tokens/s = 173,673.2
2025-01-15 15:54:35.291 | DEBUG    | __main__:<module>:313 - Training step 9720: loss = 3.3046 | 3019.37ms | Tokens/s = 173,641.3
2025-01-15 15:55:05.456 | DEBUG    | __main__:<module>:313 - Training step 9730: loss = 3.3931 | 3014.57ms | Tokens/s = 173,917.8
2025-01-15 15:55:35.621 | DEBUG    | __main__:<module>:313 - Training step 9740: loss = 3.3777 | 3015.95ms | Tokens/s = 173,838.6
2025-01-15 15:56:05.787 | DEBUG    | __main__:<module>:313 - Training step 9750: loss = 3.5263 | 3015.82ms | Tokens/s = 173,846.2
2025-01-15 15:56:35.960 | DEBUG    | __main__:<module>:313 - Training step 9760: loss = 3.3170 | 3017.82ms | Tokens/s = 173,730.8
2025-01-15 15:57:06.115 | DEBUG    | __main__:<module>:313 - Training step 9770: loss = 3.5386 | 3013.72ms | Tokens/s = 173,966.9
2025-01-15 15:57:36.262 | DEBUG    | __main__:<module>:313 - Training step 9780: loss = 3.4194 | 3015.10ms | Tokens/s = 173,887.2
2025-01-15 15:58:06.409 | DEBUG    | __main__:<module>:313 - Training step 9790: loss = 3.4431 | 3016.36ms | Tokens/s = 173,815.0
2025-01-15 15:58:36.567 | DEBUG    | __main__:<module>:313 - Training step 9800: loss = 3.3267 | 3018.60ms | Tokens/s = 173,685.8
2025-01-15 15:59:06.758 | DEBUG    | __main__:<module>:313 - Training step 9810: loss = 3.5989 | 3017.79ms | Tokens/s = 173,732.5
2025-01-15 15:59:36.935 | DEBUG    | __main__:<module>:313 - Training step 9820: loss = 3.4153 | 3015.91ms | Tokens/s = 173,840.5
2025-01-15 16:00:07.109 | DEBUG    | __main__:<module>:313 - Training step 9830: loss = 3.4226 | 3019.90ms | Tokens/s = 173,611.0
2025-01-15 16:00:37.280 | DEBUG    | __main__:<module>:313 - Training step 9840: loss = 3.4245 | 3015.56ms | Tokens/s = 173,861.0
2025-01-15 16:01:07.462 | DEBUG    | __main__:<module>:313 - Training step 9850: loss = 3.4235 | 3019.26ms | Tokens/s = 173,647.7
2025-01-15 16:01:37.656 | DEBUG    | __main__:<module>:313 - Training step 9860: loss = 3.4385 | 3019.15ms | Tokens/s = 173,654.4
2025-01-15 16:02:07.853 | DEBUG    | __main__:<module>:313 - Training step 9870: loss = 3.4255 | 3021.07ms | Tokens/s = 173,543.6
2025-01-15 16:02:38.041 | DEBUG    | __main__:<module>:313 - Training step 9880: loss = 3.4749 | 3017.48ms | Tokens/s = 173,750.3
2025-01-15 16:03:08.200 | DEBUG    | __main__:<module>:313 - Training step 9890: loss = 3.3671 | 3015.76ms | Tokens/s = 173,849.6
2025-01-15 16:03:38.376 | DEBUG    | __main__:<module>:313 - Training step 9900: loss = 3.2028 | 3017.90ms | Tokens/s = 173,726.0
2025-01-15 16:04:08.576 | DEBUG    | __main__:<module>:313 - Training step 9910: loss = 3.5462 | 3020.72ms | Tokens/s = 173,563.6
2025-01-15 16:04:38.777 | DEBUG    | __main__:<module>:313 - Training step 9920: loss = 3.4558 | 3019.37ms | Tokens/s = 173,641.6
2025-01-15 16:05:08.965 | DEBUG    | __main__:<module>:313 - Training step 9930: loss = 3.2903 | 3017.02ms | Tokens/s = 173,776.8
2025-01-15 16:05:39.144 | DEBUG    | __main__:<module>:313 - Training step 9940: loss = 3.3557 | 3019.74ms | Tokens/s = 173,620.2
2025-01-15 16:06:09.321 | DEBUG    | __main__:<module>:313 - Training step 9950: loss = 3.4500 | 3018.46ms | Tokens/s = 173,693.7
2025-01-15 16:06:39.498 | DEBUG    | __main__:<module>:313 - Training step 9960: loss = 3.2854 | 3017.13ms | Tokens/s = 173,770.6
2025-01-15 16:07:09.669 | DEBUG    | __main__:<module>:313 - Training step 9970: loss = 3.6239 | 3016.75ms | Tokens/s = 173,792.4
2025-01-15 16:07:39.818 | DEBUG    | __main__:<module>:313 - Training step 9980: loss = 3.3295 | 3016.06ms | Tokens/s = 173,832.3
2025-01-15 16:08:09.967 | DEBUG    | __main__:<module>:313 - Training step 9990: loss = 3.3778 | 3014.42ms | Tokens/s = 173,926.6
2025-01-15 16:08:43.570 | INFO     | __main__:<module>:265 - Step 10,000/40,000 loss: 3.3880 (T) 3.3974 (V) | lr=8.9e-03
2025-01-15 16:08:43.572 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 16:08:56.853 | DEBUG    | __main__:<module>:313 - Training step 10000: loss = 3.4840 | 19732.07ms | Tokens/s = 26,570.4
2025-01-15 16:09:26.942 | DEBUG    | __main__:<module>:313 - Training step 10010: loss = 3.3408 | 3010.46ms | Tokens/s = 174,155.2
2025-01-15 16:09:57.091 | DEBUG    | __main__:<module>:313 - Training step 10020: loss = 3.5108 | 3018.00ms | Tokens/s = 173,720.3
2025-01-15 16:10:27.280 | DEBUG    | __main__:<module>:313 - Training step 10030: loss = 3.4945 | 3018.62ms | Tokens/s = 173,684.8
2025-01-15 16:10:57.457 | DEBUG    | __main__:<module>:313 - Training step 10040: loss = 3.2352 | 3016.01ms | Tokens/s = 173,834.9
2025-01-15 16:11:27.628 | DEBUG    | __main__:<module>:313 - Training step 10050: loss = 3.5860 | 3017.35ms | Tokens/s = 173,757.8
2025-01-15 16:11:57.797 | DEBUG    | __main__:<module>:313 - Training step 10060: loss = 3.4251 | 3019.29ms | Tokens/s = 173,646.1
2025-01-15 16:12:27.960 | DEBUG    | __main__:<module>:313 - Training step 10070: loss = 3.4576 | 3016.89ms | Tokens/s = 173,784.4
2025-01-15 16:12:58.107 | DEBUG    | __main__:<module>:313 - Training step 10080: loss = 3.3474 | 3016.71ms | Tokens/s = 173,794.8
2025-01-15 16:13:28.284 | DEBUG    | __main__:<module>:313 - Training step 10090: loss = 3.4305 | 3020.16ms | Tokens/s = 173,596.0
2025-01-15 16:13:58.479 | DEBUG    | __main__:<module>:313 - Training step 10100: loss = 3.2451 | 3019.27ms | Tokens/s = 173,647.5
2025-01-15 16:14:28.671 | DEBUG    | __main__:<module>:313 - Training step 10110: loss = 3.2626 | 3018.88ms | Tokens/s = 173,669.5
2025-01-15 16:14:58.854 | DEBUG    | __main__:<module>:313 - Training step 10120: loss = 3.4175 | 3019.30ms | Tokens/s = 173,645.8
2025-01-15 16:15:29.025 | DEBUG    | __main__:<module>:313 - Training step 10130: loss = 3.3089 | 3017.16ms | Tokens/s = 173,768.8
2025-01-15 16:15:59.204 | DEBUG    | __main__:<module>:313 - Training step 10140: loss = 3.2673 | 3019.34ms | Tokens/s = 173,643.2
2025-01-15 16:16:29.386 | DEBUG    | __main__:<module>:313 - Training step 10150: loss = 3.4854 | 3017.32ms | Tokens/s = 173,759.3
2025-01-15 16:16:59.555 | DEBUG    | __main__:<module>:313 - Training step 10160: loss = 3.3641 | 3014.01ms | Tokens/s = 173,950.2
2025-01-15 16:17:29.723 | DEBUG    | __main__:<module>:313 - Training step 10170: loss = 3.4276 | 3019.38ms | Tokens/s = 173,640.7
2025-01-15 16:17:59.921 | DEBUG    | __main__:<module>:313 - Training step 10180: loss = 3.1979 | 3020.18ms | Tokens/s = 173,594.9
2025-01-15 16:18:30.118 | DEBUG    | __main__:<module>:313 - Training step 10190: loss = 3.3987 | 3018.04ms | Tokens/s = 173,718.1
2025-01-15 16:19:00.296 | DEBUG    | __main__:<module>:313 - Training step 10200: loss = 3.4912 | 3015.55ms | Tokens/s = 173,861.5
2025-01-15 16:19:30.470 | DEBUG    | __main__:<module>:313 - Training step 10210: loss = 3.2938 | 3016.27ms | Tokens/s = 173,820.2
2025-01-15 16:20:00.643 | DEBUG    | __main__:<module>:313 - Training step 10220: loss = 3.4510 | 3017.00ms | Tokens/s = 173,778.0
2025-01-15 16:20:30.822 | DEBUG    | __main__:<module>:313 - Training step 10230: loss = 3.5866 | 3018.39ms | Tokens/s = 173,697.8
2025-01-15 16:21:00.981 | DEBUG    | __main__:<module>:313 - Training step 10240: loss = 3.2691 | 3015.85ms | Tokens/s = 173,844.2
2025-01-15 16:21:31.135 | DEBUG    | __main__:<module>:313 - Training step 10250: loss = 3.4400 | 3015.37ms | Tokens/s = 173,871.6
2025-01-15 16:22:01.285 | DEBUG    | __main__:<module>:313 - Training step 10260: loss = 3.3038 | 3016.33ms | Tokens/s = 173,816.3
2025-01-15 16:22:31.417 | DEBUG    | __main__:<module>:313 - Training step 10270: loss = 3.5258 | 3013.02ms | Tokens/s = 174,007.3
2025-01-15 16:23:01.567 | DEBUG    | __main__:<module>:313 - Training step 10280: loss = 3.5529 | 3014.93ms | Tokens/s = 173,897.5
2025-01-15 16:23:31.745 | DEBUG    | __main__:<module>:313 - Training step 10290: loss = 3.4930 | 3019.63ms | Tokens/s = 173,626.8
2025-01-15 16:24:01.912 | DEBUG    | __main__:<module>:313 - Training step 10300: loss = 3.4973 | 3014.11ms | Tokens/s = 173,944.7
2025-01-15 16:24:32.063 | DEBUG    | __main__:<module>:313 - Training step 10310: loss = 3.4099 | 3013.75ms | Tokens/s = 173,965.1
2025-01-15 16:25:02.192 | DEBUG    | __main__:<module>:313 - Training step 10320: loss = 3.4209 | 3011.75ms | Tokens/s = 174,081.1
2025-01-15 16:25:32.315 | DEBUG    | __main__:<module>:313 - Training step 10330: loss = 3.3646 | 3011.97ms | Tokens/s = 174,068.1
2025-01-15 16:26:02.474 | DEBUG    | __main__:<module>:313 - Training step 10340: loss = 3.1785 | 3016.04ms | Tokens/s = 173,833.2
2025-01-15 16:26:32.638 | DEBUG    | __main__:<module>:313 - Training step 10350: loss = 3.3158 | 3015.91ms | Tokens/s = 173,840.8
2025-01-15 16:27:02.778 | DEBUG    | __main__:<module>:313 - Training step 10360: loss = 3.5819 | 3015.38ms | Tokens/s = 173,871.2
2025-01-15 16:27:32.941 | DEBUG    | __main__:<module>:313 - Training step 10370: loss = 3.5118 | 3016.52ms | Tokens/s = 173,805.8
2025-01-15 16:28:03.131 | DEBUG    | __main__:<module>:313 - Training step 10380: loss = 3.3120 | 3022.08ms | Tokens/s = 173,485.9
2025-01-15 16:28:33.321 | DEBUG    | __main__:<module>:313 - Training step 10390: loss = 3.3155 | 3015.73ms | Tokens/s = 173,850.9
2025-01-15 16:29:03.461 | DEBUG    | __main__:<module>:313 - Training step 10400: loss = 3.4136 | 3012.72ms | Tokens/s = 174,024.5
2025-01-15 16:29:33.579 | DEBUG    | __main__:<module>:313 - Training step 10410: loss = 3.3558 | 3010.44ms | Tokens/s = 174,156.4
2025-01-15 16:30:03.711 | DEBUG    | __main__:<module>:313 - Training step 10420: loss = 3.4758 | 3015.07ms | Tokens/s = 173,889.1
2025-01-15 16:30:33.876 | DEBUG    | __main__:<module>:313 - Training step 10430: loss = 3.5010 | 3017.38ms | Tokens/s = 173,755.8
2025-01-15 16:31:04.044 | DEBUG    | __main__:<module>:313 - Training step 10440: loss = 3.5200 | 3016.78ms | Tokens/s = 173,790.7
2025-01-15 16:31:34.208 | DEBUG    | __main__:<module>:313 - Training step 10450: loss = 3.3430 | 3016.11ms | Tokens/s = 173,829.1
2025-01-15 16:32:04.349 | DEBUG    | __main__:<module>:313 - Training step 10460: loss = 3.1274 | 3015.13ms | Tokens/s = 173,885.6
2025-01-15 16:32:34.516 | DEBUG    | __main__:<module>:313 - Training step 10470: loss = 3.5640 | 3015.00ms | Tokens/s = 173,893.0
2025-01-15 16:33:04.653 | DEBUG    | __main__:<module>:313 - Training step 10480: loss = 3.3633 | 3012.81ms | Tokens/s = 174,019.6
2025-01-15 16:33:34.788 | DEBUG    | __main__:<module>:313 - Training step 10490: loss = 3.4174 | 3018.33ms | Tokens/s = 173,701.2
2025-01-15 16:34:04.961 | DEBUG    | __main__:<module>:313 - Training step 10500: loss = 3.5872 | 3015.78ms | Tokens/s = 173,848.4
2025-01-15 16:34:35.146 | DEBUG    | __main__:<module>:313 - Training step 10510: loss = 3.3758 | 3020.68ms | Tokens/s = 173,566.3
2025-01-15 16:35:05.282 | DEBUG    | __main__:<module>:313 - Training step 10520: loss = 3.4149 | 3009.65ms | Tokens/s = 174,202.6
2025-01-15 16:35:35.406 | DEBUG    | __main__:<module>:313 - Training step 10530: loss = 3.3147 | 3012.75ms | Tokens/s = 174,022.8
2025-01-15 16:36:05.568 | DEBUG    | __main__:<module>:313 - Training step 10540: loss = 3.5598 | 3014.13ms | Tokens/s = 173,943.2
2025-01-15 16:36:35.737 | DEBUG    | __main__:<module>:313 - Training step 10550: loss = 3.3807 | 3015.89ms | Tokens/s = 173,841.7
2025-01-15 16:37:05.865 | DEBUG    | __main__:<module>:313 - Training step 10560: loss = 3.5159 | 3011.87ms | Tokens/s = 174,074.1
2025-01-15 16:37:36.008 | DEBUG    | __main__:<module>:313 - Training step 10570: loss = 3.1982 | 3014.21ms | Tokens/s = 173,938.7
2025-01-15 16:38:06.177 | DEBUG    | __main__:<module>:313 - Training step 10580: loss = 3.2880 | 3018.37ms | Tokens/s = 173,699.3
2025-01-15 16:38:36.355 | DEBUG    | __main__:<module>:313 - Training step 10590: loss = 3.3259 | 3015.88ms | Tokens/s = 173,842.5
2025-01-15 16:39:06.491 | DEBUG    | __main__:<module>:313 - Training step 10600: loss = 3.5514 | 3010.34ms | Tokens/s = 174,162.4
2025-01-15 16:39:36.611 | DEBUG    | __main__:<module>:313 - Training step 10610: loss = 3.4501 | 3011.64ms | Tokens/s = 174,087.3
2025-01-15 16:40:06.771 | DEBUG    | __main__:<module>:313 - Training step 10620: loss = 3.3007 | 3020.23ms | Tokens/s = 173,592.1
2025-01-15 16:40:36.938 | DEBUG    | __main__:<module>:313 - Training step 10630: loss = 3.5170 | 3017.36ms | Tokens/s = 173,757.0
2025-01-15 16:41:07.083 | DEBUG    | __main__:<module>:313 - Training step 10640: loss = 3.2737 | 3013.92ms | Tokens/s = 173,955.5
2025-01-15 16:41:37.245 | DEBUG    | __main__:<module>:313 - Training step 10650: loss = 3.4049 | 3018.20ms | Tokens/s = 173,709.0
2025-01-15 16:42:07.426 | DEBUG    | __main__:<module>:313 - Training step 10660: loss = 3.4996 | 3020.15ms | Tokens/s = 173,596.9
2025-01-15 16:42:37.602 | DEBUG    | __main__:<module>:313 - Training step 10670: loss = 3.5368 | 3015.14ms | Tokens/s = 173,885.3
2025-01-15 16:43:07.748 | DEBUG    | __main__:<module>:313 - Training step 10680: loss = 3.3982 | 3013.93ms | Tokens/s = 173,955.0
2025-01-15 16:43:37.906 | DEBUG    | __main__:<module>:313 - Training step 10690: loss = 3.4284 | 3016.63ms | Tokens/s = 173,799.4
2025-01-15 16:44:08.072 | DEBUG    | __main__:<module>:313 - Training step 10700: loss = 3.3770 | 3012.33ms | Tokens/s = 174,047.2
2025-01-15 16:44:38.232 | DEBUG    | __main__:<module>:313 - Training step 10710: loss = 3.3874 | 3015.18ms | Tokens/s = 173,882.6
2025-01-15 16:45:08.399 | DEBUG    | __main__:<module>:313 - Training step 10720: loss = 3.2675 | 3014.24ms | Tokens/s = 173,936.9
2025-01-15 16:45:38.551 | DEBUG    | __main__:<module>:313 - Training step 10730: loss = 3.3026 | 3013.38ms | Tokens/s = 173,986.6
2025-01-15 16:46:08.704 | DEBUG    | __main__:<module>:313 - Training step 10740: loss = 3.3373 | 3015.92ms | Tokens/s = 173,840.3
2025-01-15 16:46:38.854 | DEBUG    | __main__:<module>:313 - Training step 10750: loss = 3.2782 | 3013.33ms | Tokens/s = 173,989.3
2025-01-15 16:47:08.983 | DEBUG    | __main__:<module>:313 - Training step 10760: loss = 3.3735 | 3014.89ms | Tokens/s = 173,899.7
2025-01-15 16:47:39.157 | DEBUG    | __main__:<module>:313 - Training step 10770: loss = 3.2622 | 3017.30ms | Tokens/s = 173,760.9
2025-01-15 16:48:09.320 | DEBUG    | __main__:<module>:313 - Training step 10780: loss = 3.2261 | 3016.90ms | Tokens/s = 173,783.6
2025-01-15 16:48:39.449 | DEBUG    | __main__:<module>:313 - Training step 10790: loss = 3.3000 | 3012.65ms | Tokens/s = 174,029.1
2025-01-15 16:49:09.590 | DEBUG    | __main__:<module>:313 - Training step 10800: loss = 3.3725 | 3017.04ms | Tokens/s = 173,775.5
2025-01-15 16:49:39.757 | DEBUG    | __main__:<module>:313 - Training step 10810: loss = 3.3151 | 3017.02ms | Tokens/s = 173,777.0
2025-01-15 16:50:09.934 | DEBUG    | __main__:<module>:313 - Training step 10820: loss = 3.3720 | 3016.34ms | Tokens/s = 173,816.1
2025-01-15 16:50:40.120 | DEBUG    | __main__:<module>:313 - Training step 10830: loss = 3.4290 | 3020.06ms | Tokens/s = 173,602.1
2025-01-15 16:51:10.306 | DEBUG    | __main__:<module>:313 - Training step 10840: loss = 3.3036 | 3016.58ms | Tokens/s = 173,802.3
2025-01-15 16:51:40.484 | DEBUG    | __main__:<module>:313 - Training step 10850: loss = 3.3806 | 3017.98ms | Tokens/s = 173,721.7
2025-01-15 16:52:10.641 | DEBUG    | __main__:<module>:313 - Training step 10860: loss = 3.2973 | 3016.66ms | Tokens/s = 173,797.7
2025-01-15 16:52:40.819 | DEBUG    | __main__:<module>:313 - Training step 10870: loss = 3.2970 | 3015.81ms | Tokens/s = 173,846.2
2025-01-15 16:53:11.001 | DEBUG    | __main__:<module>:313 - Training step 10880: loss = 3.3758 | 3017.79ms | Tokens/s = 173,732.5
2025-01-15 16:53:41.182 | DEBUG    | __main__:<module>:313 - Training step 10890: loss = 3.3764 | 3018.28ms | Tokens/s = 173,704.1
2025-01-15 16:54:11.328 | DEBUG    | __main__:<module>:313 - Training step 10900: loss = 3.1855 | 3011.17ms | Tokens/s = 174,114.6
2025-01-15 16:54:41.463 | DEBUG    | __main__:<module>:313 - Training step 10910: loss = 3.3731 | 3016.37ms | Tokens/s = 173,814.1
2025-01-15 16:55:11.622 | DEBUG    | __main__:<module>:313 - Training step 10920: loss = 3.2873 | 3015.60ms | Tokens/s = 173,858.5
2025-01-15 16:55:41.804 | DEBUG    | __main__:<module>:313 - Training step 10930: loss = 3.2836 | 3018.96ms | Tokens/s = 173,665.1
2025-01-15 16:56:11.991 | DEBUG    | __main__:<module>:313 - Training step 10940: loss = 3.4228 | 3020.75ms | Tokens/s = 173,562.0
2025-01-15 16:56:42.175 | DEBUG    | __main__:<module>:313 - Training step 10950: loss = 3.3167 | 3017.24ms | Tokens/s = 173,764.0
2025-01-15 16:57:12.366 | DEBUG    | __main__:<module>:313 - Training step 10960: loss = 3.4043 | 3018.98ms | Tokens/s = 173,663.9
2025-01-15 16:57:42.546 | DEBUG    | __main__:<module>:313 - Training step 10970: loss = 3.3839 | 3016.39ms | Tokens/s = 173,813.0
2025-01-15 16:58:12.687 | DEBUG    | __main__:<module>:313 - Training step 10980: loss = 3.3311 | 3011.70ms | Tokens/s = 174,084.0
2025-01-15 16:58:42.814 | DEBUG    | __main__:<module>:313 - Training step 10990: loss = 3.3185 | 3015.88ms | Tokens/s = 173,842.3
2025-01-15 16:59:16.402 | INFO     | __main__:<module>:265 - Step 11,000/40,000 loss: 3.3749 (T) 3.3587 (V) | lr=8.7e-03
2025-01-15 16:59:16.403 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 16:59:29.818 | DEBUG    | __main__:<module>:313 - Training step 11000: loss = 3.2973 | 19863.47ms | Tokens/s = 26,394.6
2025-01-15 16:59:59.894 | DEBUG    | __main__:<module>:313 - Training step 11010: loss = 3.3479 | 3010.13ms | Tokens/s = 174,174.7
2025-01-15 17:00:30.013 | DEBUG    | __main__:<module>:313 - Training step 11020: loss = 3.4135 | 3012.87ms | Tokens/s = 174,015.9
2025-01-15 17:01:00.159 | DEBUG    | __main__:<module>:313 - Training step 11030: loss = 3.4582 | 3015.71ms | Tokens/s = 173,852.1
2025-01-15 17:01:30.333 | DEBUG    | __main__:<module>:313 - Training step 11040: loss = 3.3953 | 3019.13ms | Tokens/s = 173,655.4
2025-01-15 17:02:00.505 | DEBUG    | __main__:<module>:313 - Training step 11050: loss = 3.5140 | 3017.89ms | Tokens/s = 173,726.6
2025-01-15 17:02:30.634 | DEBUG    | __main__:<module>:313 - Training step 11060: loss = 3.3735 | 3012.76ms | Tokens/s = 174,022.7
2025-01-15 17:03:00.767 | DEBUG    | __main__:<module>:313 - Training step 11070: loss = 3.3492 | 3016.93ms | Tokens/s = 173,782.1
2025-01-15 17:03:30.937 | DEBUG    | __main__:<module>:313 - Training step 11080: loss = 3.2956 | 3019.88ms | Tokens/s = 173,612.3
2025-01-15 17:04:01.117 | DEBUG    | __main__:<module>:313 - Training step 11090: loss = 3.5421 | 3018.06ms | Tokens/s = 173,716.6
2025-01-15 17:04:31.295 | DEBUG    | __main__:<module>:313 - Training step 11100: loss = 3.2812 | 3018.55ms | Tokens/s = 173,688.8
2025-01-15 17:05:01.472 | DEBUG    | __main__:<module>:313 - Training step 11110: loss = 3.3391 | 3015.09ms | Tokens/s = 173,888.0
2025-01-15 17:05:31.625 | DEBUG    | __main__:<module>:313 - Training step 11120: loss = 3.4547 | 3012.59ms | Tokens/s = 174,032.3
2025-01-15 17:06:01.743 | DEBUG    | __main__:<module>:313 - Training step 11130: loss = 3.3934 | 3012.55ms | Tokens/s = 174,034.7
2025-01-15 17:06:31.886 | DEBUG    | __main__:<module>:313 - Training step 11140: loss = 3.2605 | 3018.40ms | Tokens/s = 173,697.2
2025-01-15 17:07:02.045 | DEBUG    | __main__:<module>:313 - Training step 11150: loss = 3.4672 | 3016.51ms | Tokens/s = 173,806.1
2025-01-15 17:07:32.217 | DEBUG    | __main__:<module>:313 - Training step 11160: loss = 3.5069 | 3012.72ms | Tokens/s = 174,024.7
2025-01-15 17:08:02.361 | DEBUG    | __main__:<module>:313 - Training step 11170: loss = 3.3362 | 3015.40ms | Tokens/s = 173,870.0
2025-01-15 17:08:32.534 | DEBUG    | __main__:<module>:313 - Training step 11180: loss = 3.2634 | 3018.19ms | Tokens/s = 173,709.3
2025-01-15 17:09:02.713 | DEBUG    | __main__:<module>:313 - Training step 11190: loss = 3.5335 | 3015.84ms | Tokens/s = 173,844.7
2025-01-15 17:09:32.869 | DEBUG    | __main__:<module>:313 - Training step 11200: loss = 3.4103 | 3013.78ms | Tokens/s = 173,963.5
2025-01-15 17:10:03.003 | DEBUG    | __main__:<module>:313 - Training step 11210: loss = 3.4074 | 3013.00ms | Tokens/s = 174,008.8
2025-01-15 17:10:33.145 | DEBUG    | __main__:<module>:313 - Training step 11220: loss = 3.3176 | 3012.00ms | Tokens/s = 174,066.5
2025-01-15 17:11:03.295 | DEBUG    | __main__:<module>:313 - Training step 11230: loss = 3.3995 | 3013.52ms | Tokens/s = 173,978.3
2025-01-15 17:11:33.416 | DEBUG    | __main__:<module>:313 - Training step 11240: loss = 3.2681 | 3010.09ms | Tokens/s = 174,176.7
2025-01-15 17:12:03.543 | DEBUG    | __main__:<module>:313 - Training step 11250: loss = 3.3376 | 3017.24ms | Tokens/s = 173,764.1
2025-01-15 17:12:33.694 | DEBUG    | __main__:<module>:313 - Training step 11260: loss = 3.2593 | 3013.41ms | Tokens/s = 173,984.7
2025-01-15 17:13:03.827 | DEBUG    | __main__:<module>:313 - Training step 11270: loss = 3.4204 | 3011.11ms | Tokens/s = 174,117.6
2025-01-15 17:13:33.943 | DEBUG    | __main__:<module>:313 - Training step 11280: loss = 3.4841 | 3012.75ms | Tokens/s = 174,022.8
2025-01-15 17:14:04.094 | DEBUG    | __main__:<module>:313 - Training step 11290: loss = 3.0990 | 3015.05ms | Tokens/s = 173,890.2
2025-01-15 17:14:34.266 | DEBUG    | __main__:<module>:313 - Training step 11300: loss = 3.3175 | 3015.66ms | Tokens/s = 173,855.0
2025-01-15 17:15:04.401 | DEBUG    | __main__:<module>:313 - Training step 11310: loss = 3.4490 | 3010.88ms | Tokens/s = 174,131.0
2025-01-15 17:15:34.528 | DEBUG    | __main__:<module>:313 - Training step 11320: loss = 3.0581 | 3016.37ms | Tokens/s = 173,814.1
2025-01-15 17:16:04.685 | DEBUG    | __main__:<module>:313 - Training step 11330: loss = 3.4541 | 3019.01ms | Tokens/s = 173,662.1
2025-01-15 17:16:34.863 | DEBUG    | __main__:<module>:313 - Training step 11340: loss = 3.2649 | 3018.24ms | Tokens/s = 173,706.7
2025-01-15 17:17:05.039 | DEBUG    | __main__:<module>:313 - Training step 11350: loss = 3.2539 | 3016.21ms | Tokens/s = 173,823.6
2025-01-15 17:17:35.184 | DEBUG    | __main__:<module>:313 - Training step 11360: loss = 3.3913 | 3012.32ms | Tokens/s = 174,047.8
2025-01-15 17:18:05.317 | DEBUG    | __main__:<module>:313 - Training step 11370: loss = 3.4338 | 3015.70ms | Tokens/s = 173,852.7
2025-01-15 17:18:35.442 | DEBUG    | __main__:<module>:313 - Training step 11380: loss = 3.3673 | 3011.46ms | Tokens/s = 174,097.7
2025-01-15 17:19:05.544 | DEBUG    | __main__:<module>:313 - Training step 11390: loss = 3.2440 | 3009.73ms | Tokens/s = 174,197.7
2025-01-15 17:19:35.663 | DEBUG    | __main__:<module>:313 - Training step 11400: loss = 3.3347 | 3013.71ms | Tokens/s = 173,967.8
2025-01-15 17:20:05.820 | DEBUG    | __main__:<module>:313 - Training step 11410: loss = 3.3944 | 3016.52ms | Tokens/s = 173,805.7
2025-01-15 17:20:35.991 | DEBUG    | __main__:<module>:313 - Training step 11420: loss = 3.3230 | 3017.08ms | Tokens/s = 173,773.6
2025-01-15 17:21:06.167 | DEBUG    | __main__:<module>:313 - Training step 11430: loss = 3.5160 | 3019.71ms | Tokens/s = 173,621.8
2025-01-15 17:21:36.343 | DEBUG    | __main__:<module>:313 - Training step 11440: loss = 3.2861 | 3017.09ms | Tokens/s = 173,772.6
2025-01-15 17:22:06.516 | DEBUG    | __main__:<module>:313 - Training step 11450: loss = 3.3172 | 3016.96ms | Tokens/s = 173,780.2
2025-01-15 17:22:36.655 | DEBUG    | __main__:<module>:313 - Training step 11460: loss = 3.3151 | 3012.78ms | Tokens/s = 174,021.5
2025-01-15 17:23:06.762 | DEBUG    | __main__:<module>:313 - Training step 11470: loss = 3.2368 | 3009.83ms | Tokens/s = 174,192.1
2025-01-15 17:23:36.878 | DEBUG    | __main__:<module>:313 - Training step 11480: loss = 3.5793 | 3015.35ms | Tokens/s = 173,873.0
2025-01-15 17:24:07.028 | DEBUG    | __main__:<module>:313 - Training step 11490: loss = 3.5434 | 3015.18ms | Tokens/s = 173,882.6
2025-01-15 17:24:37.202 | DEBUG    | __main__:<module>:313 - Training step 11500: loss = 3.3866 | 3019.64ms | Tokens/s = 173,626.1
2025-01-15 17:25:07.342 | DEBUG    | __main__:<module>:313 - Training step 11510: loss = 3.4116 | 3014.78ms | Tokens/s = 173,905.9
2025-01-15 17:25:37.460 | DEBUG    | __main__:<module>:313 - Training step 11520: loss = 3.2387 | 3009.23ms | Tokens/s = 174,226.9
2025-01-15 17:26:07.609 | DEBUG    | __main__:<module>:313 - Training step 11530: loss = 3.2578 | 3018.06ms | Tokens/s = 173,717.0
2025-01-15 17:26:37.771 | DEBUG    | __main__:<module>:313 - Training step 11540: loss = 3.2651 | 3014.31ms | Tokens/s = 173,932.7
2025-01-15 17:27:07.946 | DEBUG    | __main__:<module>:313 - Training step 11550: loss = 3.4279 | 3018.30ms | Tokens/s = 173,703.1
2025-01-15 17:27:38.128 | DEBUG    | __main__:<module>:313 - Training step 11560: loss = 3.4313 | 3016.94ms | Tokens/s = 173,781.1
2025-01-15 17:28:08.284 | DEBUG    | __main__:<module>:313 - Training step 11570: loss = 3.3630 | 3013.14ms | Tokens/s = 174,000.4
2025-01-15 17:28:38.415 | DEBUG    | __main__:<module>:313 - Training step 11580: loss = 3.3380 | 3015.23ms | Tokens/s = 173,880.0
2025-01-15 17:29:08.576 | DEBUG    | __main__:<module>:313 - Training step 11590: loss = 3.3519 | 3015.83ms | Tokens/s = 173,845.1
2025-01-15 17:29:38.759 | DEBUG    | __main__:<module>:313 - Training step 11600: loss = 3.3111 | 3017.98ms | Tokens/s = 173,721.4
2025-01-15 17:30:08.959 | DEBUG    | __main__:<module>:313 - Training step 11610: loss = 3.5004 | 3018.41ms | Tokens/s = 173,696.5
2025-01-15 17:30:39.133 | DEBUG    | __main__:<module>:313 - Training step 11620: loss = 3.3634 | 3013.39ms | Tokens/s = 173,985.9
2025-01-15 17:31:09.278 | DEBUG    | __main__:<module>:313 - Training step 11630: loss = 3.3692 | 3013.68ms | Tokens/s = 173,969.3
2025-01-15 17:31:39.439 | DEBUG    | __main__:<module>:313 - Training step 11640: loss = 3.3900 | 3016.52ms | Tokens/s = 173,805.9
2025-01-15 17:32:09.600 | DEBUG    | __main__:<module>:313 - Training step 11650: loss = 3.3879 | 3014.86ms | Tokens/s = 173,901.3
2025-01-15 17:32:39.770 | DEBUG    | __main__:<module>:313 - Training step 11660: loss = 3.3358 | 3015.44ms | Tokens/s = 173,867.6
2025-01-15 17:33:09.944 | DEBUG    | __main__:<module>:313 - Training step 11670: loss = 3.4281 | 3015.70ms | Tokens/s = 173,853.0
2025-01-15 17:33:40.098 | DEBUG    | __main__:<module>:313 - Training step 11680: loss = 3.4128 | 3014.16ms | Tokens/s = 173,941.4
2025-01-15 17:34:10.231 | DEBUG    | __main__:<module>:313 - Training step 11690: loss = 3.4461 | 3013.64ms | Tokens/s = 173,971.9
2025-01-15 17:34:40.382 | DEBUG    | __main__:<module>:313 - Training step 11700: loss = 3.2347 | 3012.37ms | Tokens/s = 174,045.0
2025-01-15 17:35:10.511 | DEBUG    | __main__:<module>:313 - Training step 11710: loss = 3.3848 | 3010.04ms | Tokens/s = 174,179.9
2025-01-15 17:35:40.655 | DEBUG    | __main__:<module>:313 - Training step 11720: loss = 3.4633 | 3017.07ms | Tokens/s = 173,773.9
2025-01-15 17:36:10.800 | DEBUG    | __main__:<module>:313 - Training step 11730: loss = 3.4579 | 3012.66ms | Tokens/s = 174,028.0
2025-01-15 17:36:40.958 | DEBUG    | __main__:<module>:313 - Training step 11740: loss = 3.3466 | 3014.73ms | Tokens/s = 173,908.7
2025-01-15 17:37:11.110 | DEBUG    | __main__:<module>:313 - Training step 11750: loss = 3.5365 | 3013.00ms | Tokens/s = 174,008.7
2025-01-15 17:37:41.240 | DEBUG    | __main__:<module>:313 - Training step 11760: loss = 3.3187 | 3012.95ms | Tokens/s = 174,011.7
2025-01-15 17:38:11.354 | DEBUG    | __main__:<module>:313 - Training step 11770: loss = 3.4094 | 3009.90ms | Tokens/s = 174,187.7
2025-01-15 17:38:41.485 | DEBUG    | __main__:<module>:313 - Training step 11780: loss = 3.6405 | 3014.51ms | Tokens/s = 173,921.4
2025-01-15 17:39:11.647 | DEBUG    | __main__:<module>:313 - Training step 11790: loss = 3.4228 | 3018.23ms | Tokens/s = 173,707.1
2025-01-15 17:39:41.801 | DEBUG    | __main__:<module>:313 - Training step 11800: loss = 3.4092 | 3014.11ms | Tokens/s = 173,944.5
2025-01-15 17:40:11.936 | DEBUG    | __main__:<module>:313 - Training step 11810: loss = 3.3197 | 3012.20ms | Tokens/s = 174,054.9
2025-01-15 17:40:42.090 | DEBUG    | __main__:<module>:313 - Training step 11820: loss = 3.6530 | 3016.39ms | Tokens/s = 173,813.3
2025-01-15 17:41:12.268 | DEBUG    | __main__:<module>:313 - Training step 11830: loss = 3.2667 | 3021.43ms | Tokens/s = 173,523.1
2025-01-15 17:41:42.434 | DEBUG    | __main__:<module>:313 - Training step 11840: loss = 3.3137 | 3014.96ms | Tokens/s = 173,895.8
2025-01-15 17:42:12.565 | DEBUG    | __main__:<module>:313 - Training step 11850: loss = 3.3320 | 3014.59ms | Tokens/s = 173,916.6
2025-01-15 17:42:42.708 | DEBUG    | __main__:<module>:313 - Training step 11860: loss = 3.2770 | 3013.34ms | Tokens/s = 173,988.8
2025-01-15 17:43:12.876 | DEBUG    | __main__:<module>:313 - Training step 11870: loss = 3.3934 | 3017.51ms | Tokens/s = 173,748.7
2025-01-15 17:43:43.063 | DEBUG    | __main__:<module>:313 - Training step 11880: loss = 3.3876 | 3016.73ms | Tokens/s = 173,793.4
2025-01-15 17:44:13.239 | DEBUG    | __main__:<module>:313 - Training step 11890: loss = 3.2559 | 3015.00ms | Tokens/s = 173,893.0
2025-01-15 17:44:43.392 | DEBUG    | __main__:<module>:313 - Training step 11900: loss = 3.4975 | 3014.48ms | Tokens/s = 173,923.2
2025-01-15 17:45:13.521 | DEBUG    | __main__:<module>:313 - Training step 11910: loss = 3.4345 | 3013.21ms | Tokens/s = 173,996.4
2025-01-15 17:45:43.669 | DEBUG    | __main__:<module>:313 - Training step 11920: loss = 3.0189 | 3017.65ms | Tokens/s = 173,740.5
2025-01-15 17:46:13.823 | DEBUG    | __main__:<module>:313 - Training step 11930: loss = 3.3688 | 3014.46ms | Tokens/s = 173,924.2
2025-01-15 17:46:43.989 | DEBUG    | __main__:<module>:313 - Training step 11940: loss = 3.2960 | 3015.64ms | Tokens/s = 173,856.3
2025-01-15 17:47:14.145 | DEBUG    | __main__:<module>:313 - Training step 11950: loss = 3.3772 | 3015.35ms | Tokens/s = 173,873.2
2025-01-15 17:47:44.271 | DEBUG    | __main__:<module>:313 - Training step 11960: loss = 3.3063 | 3011.69ms | Tokens/s = 174,084.4
2025-01-15 17:48:14.386 | DEBUG    | __main__:<module>:313 - Training step 11970: loss = 3.3493 | 3010.77ms | Tokens/s = 174,137.6
2025-01-15 17:48:44.495 | DEBUG    | __main__:<module>:313 - Training step 11980: loss = 3.5964 | 3009.94ms | Tokens/s = 174,185.5
2025-01-15 17:49:14.605 | DEBUG    | __main__:<module>:313 - Training step 11990: loss = 3.2679 | 3011.84ms | Tokens/s = 174,075.9
2025-01-15 17:49:48.159 | INFO     | __main__:<module>:265 - Step 12,000/40,000 loss: 3.3652 (T) 3.3736 (V) | lr=8.4e-03
2025-01-15 17:49:51.174 | DEBUG    | __main__:<module>:313 - Training step 12000: loss = 3.3989 | 9451.99ms | Tokens/s = 55,468.5
2025-01-15 17:50:21.333 | DEBUG    | __main__:<module>:313 - Training step 12010: loss = 3.2109 | 3015.54ms | Tokens/s = 173,861.8
2025-01-15 17:50:51.513 | DEBUG    | __main__:<module>:313 - Training step 12020: loss = 3.3052 | 3018.85ms | Tokens/s = 173,671.7
2025-01-15 17:51:21.679 | DEBUG    | __main__:<module>:313 - Training step 12030: loss = 3.3720 | 3017.55ms | Tokens/s = 173,746.5
2025-01-15 17:51:51.817 | DEBUG    | __main__:<module>:313 - Training step 12040: loss = 3.4497 | 3012.83ms | Tokens/s = 174,018.5
2025-01-15 17:52:21.960 | DEBUG    | __main__:<module>:313 - Training step 12050: loss = 3.4533 | 3015.26ms | Tokens/s = 173,878.4
2025-01-15 17:52:52.133 | DEBUG    | __main__:<module>:313 - Training step 12060: loss = 3.2505 | 3017.94ms | Tokens/s = 173,723.9
2025-01-15 17:53:22.301 | DEBUG    | __main__:<module>:313 - Training step 12070: loss = 3.2765 | 3015.51ms | Tokens/s = 173,863.9
2025-01-15 17:53:52.446 | DEBUG    | __main__:<module>:313 - Training step 12080: loss = 3.3980 | 3012.23ms | Tokens/s = 174,052.9
2025-01-15 17:54:22.577 | DEBUG    | __main__:<module>:313 - Training step 12090: loss = 3.2773 | 3012.22ms | Tokens/s = 174,053.8
2025-01-15 17:54:52.725 | DEBUG    | __main__:<module>:313 - Training step 12100: loss = 3.2480 | 3013.78ms | Tokens/s = 173,963.7
2025-01-15 17:55:22.875 | DEBUG    | __main__:<module>:313 - Training step 12110: loss = 3.3431 | 3017.57ms | Tokens/s = 173,745.3
2025-01-15 17:55:53.041 | DEBUG    | __main__:<module>:313 - Training step 12120: loss = 3.3841 | 3018.08ms | Tokens/s = 173,715.6
2025-01-15 17:56:23.193 | DEBUG    | __main__:<module>:313 - Training step 12130: loss = 3.4125 | 3015.77ms | Tokens/s = 173,848.9
2025-01-15 17:56:53.315 | DEBUG    | __main__:<module>:313 - Training step 12140: loss = 3.3386 | 3012.40ms | Tokens/s = 174,043.5
2025-01-15 17:57:23.415 | DEBUG    | __main__:<module>:313 - Training step 12150: loss = 3.5080 | 3009.50ms | Tokens/s = 174,211.3
2025-01-15 17:57:53.550 | DEBUG    | __main__:<module>:313 - Training step 12160: loss = 3.5574 | 3014.75ms | Tokens/s = 173,907.9
2025-01-15 17:58:23.690 | DEBUG    | __main__:<module>:313 - Training step 12170: loss = 3.2613 | 3011.81ms | Tokens/s = 174,077.5
2025-01-15 17:58:53.806 | DEBUG    | __main__:<module>:313 - Training step 12180: loss = 3.6186 | 3011.04ms | Tokens/s = 174,122.1
2025-01-15 17:59:23.919 | DEBUG    | __main__:<module>:313 - Training step 12190: loss = 3.4768 | 3013.39ms | Tokens/s = 173,986.4
2025-01-15 17:59:54.062 | DEBUG    | __main__:<module>:313 - Training step 12200: loss = 3.3806 | 3012.84ms | Tokens/s = 174,017.8
2025-01-15 18:00:24.235 | DEBUG    | __main__:<module>:313 - Training step 12210: loss = 3.3597 | 3014.71ms | Tokens/s = 173,910.1
2025-01-15 18:00:54.412 | DEBUG    | __main__:<module>:313 - Training step 12220: loss = 3.4344 | 3015.59ms | Tokens/s = 173,858.9
2025-01-15 18:01:24.561 | DEBUG    | __main__:<module>:313 - Training step 12230: loss = 3.3132 | 3013.07ms | Tokens/s = 174,004.6
2025-01-15 18:01:54.689 | DEBUG    | __main__:<module>:313 - Training step 12240: loss = 3.1782 | 3012.58ms | Tokens/s = 174,032.7
2025-01-15 18:02:24.804 | DEBUG    | __main__:<module>:313 - Training step 12250: loss = 3.2356 | 3013.37ms | Tokens/s = 173,987.2
2025-01-15 18:02:54.928 | DEBUG    | __main__:<module>:313 - Training step 12260: loss = 3.4274 | 3012.31ms | Tokens/s = 174,048.3
2025-01-15 18:03:25.073 | DEBUG    | __main__:<module>:313 - Training step 12270: loss = 3.2220 | 3015.89ms | Tokens/s = 173,841.7
2025-01-15 18:03:55.238 | DEBUG    | __main__:<module>:313 - Training step 12280: loss = 3.4468 | 3014.85ms | Tokens/s = 173,901.9
2025-01-15 18:04:25.394 | DEBUG    | __main__:<module>:313 - Training step 12290: loss = 3.3499 | 3014.57ms | Tokens/s = 173,918.2
2025-01-15 18:04:55.553 | DEBUG    | __main__:<module>:313 - Training step 12300: loss = 3.3996 | 3016.05ms | Tokens/s = 173,832.6
2025-01-15 18:05:25.714 | DEBUG    | __main__:<module>:313 - Training step 12310: loss = 3.3900 | 3016.82ms | Tokens/s = 173,788.4
2025-01-15 18:05:55.896 | DEBUG    | __main__:<module>:313 - Training step 12320: loss = 3.4873 | 3019.36ms | Tokens/s = 173,642.4
2025-01-15 18:06:26.065 | DEBUG    | __main__:<module>:313 - Training step 12330: loss = 3.3557 | 3016.02ms | Tokens/s = 173,834.2
2025-01-15 18:06:56.202 | DEBUG    | __main__:<module>:313 - Training step 12340: loss = 3.1946 | 3011.18ms | Tokens/s = 174,113.6
2025-01-15 18:07:26.320 | DEBUG    | __main__:<module>:313 - Training step 12350: loss = 3.4670 | 3011.78ms | Tokens/s = 174,079.2
2025-01-15 18:07:56.467 | DEBUG    | __main__:<module>:313 - Training step 12360: loss = 3.3485 | 3015.57ms | Tokens/s = 173,860.5
2025-01-15 18:08:26.615 | DEBUG    | __main__:<module>:313 - Training step 12370: loss = 3.3004 | 3012.60ms | Tokens/s = 174,031.9
2025-01-15 18:08:56.735 | DEBUG    | __main__:<module>:313 - Training step 12380: loss = 3.5018 | 3012.51ms | Tokens/s = 174,036.7
2025-01-15 18:09:26.883 | DEBUG    | __main__:<module>:313 - Training step 12390: loss = 3.4952 | 3017.23ms | Tokens/s = 173,764.4
2025-01-15 18:09:57.049 | DEBUG    | __main__:<module>:313 - Training step 12400: loss = 3.4110 | 3013.58ms | Tokens/s = 173,974.9
2025-01-15 18:10:27.209 | DEBUG    | __main__:<module>:313 - Training step 12410: loss = 3.2847 | 3013.84ms | Tokens/s = 173,959.8
2025-01-15 18:10:57.342 | DEBUG    | __main__:<module>:313 - Training step 12420: loss = 3.2806 | 3011.26ms | Tokens/s = 174,109.0
2025-01-15 18:11:27.475 | DEBUG    | __main__:<module>:313 - Training step 12430: loss = 3.3860 | 3012.42ms | Tokens/s = 174,042.4
2025-01-15 18:11:57.630 | DEBUG    | __main__:<module>:313 - Training step 12440: loss = 3.3195 | 3015.08ms | Tokens/s = 173,888.3
2025-01-15 18:12:27.796 | DEBUG    | __main__:<module>:313 - Training step 12450: loss = 3.2627 | 3015.74ms | Tokens/s = 173,850.5
2025-01-15 18:12:57.959 | DEBUG    | __main__:<module>:313 - Training step 12460: loss = 3.3535 | 3014.61ms | Tokens/s = 173,915.8
2025-01-15 18:13:28.089 | DEBUG    | __main__:<module>:313 - Training step 12470: loss = 3.1085 | 3014.49ms | Tokens/s = 173,922.6
2025-01-15 18:13:58.226 | DEBUG    | __main__:<module>:313 - Training step 12480: loss = 3.4201 | 3015.21ms | Tokens/s = 173,881.2
2025-01-15 18:14:28.377 | DEBUG    | __main__:<module>:313 - Training step 12490: loss = 3.4434 | 3017.64ms | Tokens/s = 173,741.0
2025-01-15 18:14:58.547 | DEBUG    | __main__:<module>:313 - Training step 12500: loss = 3.3468 | 3018.38ms | Tokens/s = 173,698.3
2025-01-15 18:15:28.723 | DEBUG    | __main__:<module>:313 - Training step 12510: loss = 3.3206 | 3017.53ms | Tokens/s = 173,747.4
2025-01-15 18:15:58.904 | DEBUG    | __main__:<module>:313 - Training step 12520: loss = 3.3059 | 3019.10ms | Tokens/s = 173,657.1
2025-01-15 18:16:29.064 | DEBUG    | __main__:<module>:313 - Training step 12530: loss = 3.3621 | 3017.95ms | Tokens/s = 173,722.9
2025-01-15 18:16:59.231 | DEBUG    | __main__:<module>:313 - Training step 12540: loss = 3.3449 | 3017.01ms | Tokens/s = 173,777.1
2025-01-15 18:17:29.402 | DEBUG    | __main__:<module>:313 - Training step 12550: loss = 3.5006 | 3019.35ms | Tokens/s = 173,642.6
2025-01-15 18:17:59.548 | DEBUG    | __main__:<module>:313 - Training step 12560: loss = 3.3686 | 3014.66ms | Tokens/s = 173,912.9
2025-01-15 18:18:29.674 | DEBUG    | __main__:<module>:313 - Training step 12570: loss = 3.3561 | 3012.38ms | Tokens/s = 174,044.2
2025-01-15 18:18:59.797 | DEBUG    | __main__:<module>:313 - Training step 12580: loss = 3.3902 | 3014.04ms | Tokens/s = 173,948.5
2025-01-15 18:19:29.947 | DEBUG    | __main__:<module>:313 - Training step 12590: loss = 3.3670 | 3013.07ms | Tokens/s = 174,004.9
2025-01-15 18:20:00.115 | DEBUG    | __main__:<module>:313 - Training step 12600: loss = 3.3087 | 3014.82ms | Tokens/s = 173,903.5
2025-01-15 18:20:30.260 | DEBUG    | __main__:<module>:313 - Training step 12610: loss = 3.3222 | 3013.13ms | Tokens/s = 174,000.8
2025-01-15 18:21:00.385 | DEBUG    | __main__:<module>:313 - Training step 12620: loss = 3.3323 | 3010.77ms | Tokens/s = 174,137.3
2025-01-15 18:21:30.507 | DEBUG    | __main__:<module>:313 - Training step 12630: loss = 3.4606 | 3014.38ms | Tokens/s = 173,928.9
2025-01-15 18:22:00.660 | DEBUG    | __main__:<module>:313 - Training step 12640: loss = 3.0855 | 3015.68ms | Tokens/s = 173,854.0
2025-01-15 18:22:30.829 | DEBUG    | __main__:<module>:313 - Training step 12650: loss = 3.3709 | 3015.71ms | Tokens/s = 173,852.4
2025-01-15 18:23:00.994 | DEBUG    | __main__:<module>:313 - Training step 12660: loss = 3.3876 | 3016.39ms | Tokens/s = 173,813.3
2025-01-15 18:23:31.166 | DEBUG    | __main__:<module>:313 - Training step 12670: loss = 3.3897 | 3016.80ms | Tokens/s = 173,789.3
2025-01-15 18:24:01.299 | DEBUG    | __main__:<module>:313 - Training step 12680: loss = 3.2477 | 3012.18ms | Tokens/s = 174,056.3
2025-01-15 18:24:31.421 | DEBUG    | __main__:<module>:313 - Training step 12690: loss = 3.3526 | 3013.18ms | Tokens/s = 173,998.4
2025-01-15 18:25:01.583 | DEBUG    | __main__:<module>:313 - Training step 12700: loss = 3.4298 | 3019.65ms | Tokens/s = 173,625.7
2025-01-15 18:25:31.741 | DEBUG    | __main__:<module>:313 - Training step 12710: loss = 3.4826 | 3012.99ms | Tokens/s = 174,008.9
2025-01-15 18:26:01.885 | DEBUG    | __main__:<module>:313 - Training step 12720: loss = 3.3715 | 3014.94ms | Tokens/s = 173,896.5
2025-01-15 18:26:32.031 | DEBUG    | __main__:<module>:313 - Training step 12730: loss = 3.3241 | 3013.54ms | Tokens/s = 173,977.6
2025-01-15 18:27:02.161 | DEBUG    | __main__:<module>:313 - Training step 12740: loss = 3.3873 | 3014.17ms | Tokens/s = 173,941.3
2025-01-15 18:27:32.311 | DEBUG    | __main__:<module>:313 - Training step 12750: loss = 3.4916 | 3016.15ms | Tokens/s = 173,827.1
2025-01-15 18:28:02.475 | DEBUG    | __main__:<module>:313 - Training step 12760: loss = 3.3643 | 3016.55ms | Tokens/s = 173,803.6
2025-01-15 18:28:32.650 | DEBUG    | __main__:<module>:313 - Training step 12770: loss = 3.3312 | 3015.90ms | Tokens/s = 173,841.3
2025-01-15 18:29:02.827 | DEBUG    | __main__:<module>:313 - Training step 12780: loss = 3.3873 | 3019.72ms | Tokens/s = 173,621.5
2025-01-15 18:29:32.986 | DEBUG    | __main__:<module>:313 - Training step 12790: loss = 3.3536 | 3011.75ms | Tokens/s = 174,081.0
2025-01-15 18:30:03.120 | DEBUG    | __main__:<module>:313 - Training step 12800: loss = 3.2643 | 3011.07ms | Tokens/s = 174,120.2
2025-01-15 18:30:33.235 | DEBUG    | __main__:<module>:313 - Training step 12810: loss = 3.3649 | 3012.25ms | Tokens/s = 174,052.1
2025-01-15 18:31:03.368 | DEBUG    | __main__:<module>:313 - Training step 12820: loss = 3.3597 | 3014.74ms | Tokens/s = 173,908.1
2025-01-15 18:31:33.528 | DEBUG    | __main__:<module>:313 - Training step 12830: loss = 3.2512 | 3015.66ms | Tokens/s = 173,854.9
2025-01-15 18:32:03.696 | DEBUG    | __main__:<module>:313 - Training step 12840: loss = 3.4164 | 3016.11ms | Tokens/s = 173,829.3
2025-01-15 18:32:33.838 | DEBUG    | __main__:<module>:313 - Training step 12850: loss = 3.2780 | 3013.25ms | Tokens/s = 173,994.3
2025-01-15 18:33:03.960 | DEBUG    | __main__:<module>:313 - Training step 12860: loss = 3.3910 | 3011.61ms | Tokens/s = 174,089.1
2025-01-15 18:33:34.086 | DEBUG    | __main__:<module>:313 - Training step 12870: loss = 3.4180 | 3013.63ms | Tokens/s = 173,972.0
2025-01-15 18:34:04.232 | DEBUG    | __main__:<module>:313 - Training step 12880: loss = 3.4019 | 3012.68ms | Tokens/s = 174,027.0
2025-01-15 18:34:34.399 | DEBUG    | __main__:<module>:313 - Training step 12890: loss = 3.3981 | 3016.43ms | Tokens/s = 173,810.7
2025-01-15 18:35:04.574 | DEBUG    | __main__:<module>:313 - Training step 12900: loss = 3.2919 | 3017.83ms | Tokens/s = 173,730.2
2025-01-15 18:35:34.753 | DEBUG    | __main__:<module>:313 - Training step 12910: loss = 3.3370 | 3017.55ms | Tokens/s = 173,746.2
2025-01-15 18:36:04.907 | DEBUG    | __main__:<module>:313 - Training step 12920: loss = 3.2995 | 3015.11ms | Tokens/s = 173,886.8
2025-01-15 18:36:35.064 | DEBUG    | __main__:<module>:313 - Training step 12930: loss = 3.4598 | 3014.87ms | Tokens/s = 173,900.8
2025-01-15 18:37:05.243 | DEBUG    | __main__:<module>:313 - Training step 12940: loss = 3.2160 | 3019.17ms | Tokens/s = 173,653.1
2025-01-15 18:37:35.431 | DEBUG    | __main__:<module>:313 - Training step 12950: loss = 3.3845 | 3019.56ms | Tokens/s = 173,630.8
2025-01-15 18:38:05.588 | DEBUG    | __main__:<module>:313 - Training step 12960: loss = 3.2801 | 3014.14ms | Tokens/s = 173,942.5
2025-01-15 18:38:35.758 | DEBUG    | __main__:<module>:313 - Training step 12970: loss = 3.3831 | 3017.25ms | Tokens/s = 173,763.3
2025-01-15 18:39:05.928 | DEBUG    | __main__:<module>:313 - Training step 12980: loss = 3.3327 | 3013.51ms | Tokens/s = 173,979.1
2025-01-15 18:39:36.073 | DEBUG    | __main__:<module>:313 - Training step 12990: loss = 3.5172 | 3011.52ms | Tokens/s = 174,094.2
2025-01-15 18:40:09.620 | INFO     | __main__:<module>:265 - Step 13,000/40,000 loss: 3.3618 (T) 3.3577 (V) | lr=8.1e-03
2025-01-15 18:40:09.622 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 18:40:23.000 | DEBUG    | __main__:<module>:313 - Training step 13000: loss = 3.4407 | 19813.12ms | Tokens/s = 26,461.7
2025-01-15 18:40:53.055 | DEBUG    | __main__:<module>:313 - Training step 13010: loss = 3.2634 | 3002.46ms | Tokens/s = 174,619.8
2025-01-15 18:41:23.147 | DEBUG    | __main__:<module>:313 - Training step 13020: loss = 3.4669 | 3012.31ms | Tokens/s = 174,048.8
2025-01-15 18:41:53.289 | DEBUG    | __main__:<module>:313 - Training step 13030: loss = 3.2294 | 3016.00ms | Tokens/s = 173,835.5
2025-01-15 18:42:23.433 | DEBUG    | __main__:<module>:313 - Training step 13040: loss = 3.4388 | 3017.73ms | Tokens/s = 173,736.1
2025-01-15 18:42:53.564 | DEBUG    | __main__:<module>:313 - Training step 13050: loss = 3.3579 | 3013.81ms | Tokens/s = 173,962.1
2025-01-15 18:43:23.719 | DEBUG    | __main__:<module>:313 - Training step 13060: loss = 3.4818 | 3014.20ms | Tokens/s = 173,939.2
2025-01-15 18:43:53.881 | DEBUG    | __main__:<module>:313 - Training step 13070: loss = 3.1444 | 3015.85ms | Tokens/s = 173,844.3
2025-01-15 18:44:24.045 | DEBUG    | __main__:<module>:313 - Training step 13080: loss = 3.4205 | 3017.35ms | Tokens/s = 173,758.0
2025-01-15 18:44:54.218 | DEBUG    | __main__:<module>:313 - Training step 13090: loss = 3.3103 | 3017.89ms | Tokens/s = 173,726.7
2025-01-15 18:45:24.391 | DEBUG    | __main__:<module>:313 - Training step 13100: loss = 3.3532 | 3017.23ms | Tokens/s = 173,764.5
2025-01-15 18:45:54.532 | DEBUG    | __main__:<module>:313 - Training step 13110: loss = 3.4286 | 3012.68ms | Tokens/s = 174,027.0
2025-01-15 18:46:24.653 | DEBUG    | __main__:<module>:313 - Training step 13120: loss = 3.4802 | 3010.09ms | Tokens/s = 174,176.9
2025-01-15 18:46:54.792 | DEBUG    | __main__:<module>:313 - Training step 13130: loss = 3.1888 | 3018.85ms | Tokens/s = 173,671.5
2025-01-15 18:47:24.953 | DEBUG    | __main__:<module>:313 - Training step 13140: loss = 3.5391 | 3018.82ms | Tokens/s = 173,672.9
2025-01-15 18:47:55.137 | DEBUG    | __main__:<module>:313 - Training step 13150: loss = 3.4439 | 3019.43ms | Tokens/s = 173,638.2
2025-01-15 18:48:25.329 | DEBUG    | __main__:<module>:313 - Training step 13160: loss = 3.3541 | 3016.64ms | Tokens/s = 173,798.8
2025-01-15 18:48:55.493 | DEBUG    | __main__:<module>:313 - Training step 13170: loss = 3.5129 | 3014.28ms | Tokens/s = 173,934.5
2025-01-15 18:49:25.647 | DEBUG    | __main__:<module>:313 - Training step 13180: loss = 3.3938 | 3013.82ms | Tokens/s = 173,961.5
2025-01-15 18:49:55.774 | DEBUG    | __main__:<module>:313 - Training step 13190: loss = 3.4312 | 3011.22ms | Tokens/s = 174,111.3
2025-01-15 18:50:25.896 | DEBUG    | __main__:<module>:313 - Training step 13200: loss = 3.4313 | 3011.39ms | Tokens/s = 174,101.7
2025-01-15 18:50:56.053 | DEBUG    | __main__:<module>:313 - Training step 13210: loss = 3.3961 | 3015.27ms | Tokens/s = 173,877.8
2025-01-15 18:51:26.227 | DEBUG    | __main__:<module>:313 - Training step 13220: loss = 3.3205 | 3018.17ms | Tokens/s = 173,710.3
2025-01-15 18:51:56.398 | DEBUG    | __main__:<module>:313 - Training step 13230: loss = 3.3565 | 3015.50ms | Tokens/s = 173,864.3
2025-01-15 18:52:26.539 | DEBUG    | __main__:<module>:313 - Training step 13240: loss = 3.4644 | 3015.43ms | Tokens/s = 173,868.2
2025-01-15 18:52:56.670 | DEBUG    | __main__:<module>:313 - Training step 13250: loss = 3.4400 | 3013.59ms | Tokens/s = 173,974.4
2025-01-15 18:53:26.820 | DEBUG    | __main__:<module>:313 - Training step 13260: loss = 3.3372 | 3014.03ms | Tokens/s = 173,949.3
2025-01-15 18:53:57.000 | DEBUG    | __main__:<module>:313 - Training step 13270: loss = 3.3513 | 3018.96ms | Tokens/s = 173,665.3
2025-01-15 18:54:27.161 | DEBUG    | __main__:<module>:313 - Training step 13280: loss = 3.4236 | 3014.15ms | Tokens/s = 173,942.1
2025-01-15 18:54:57.301 | DEBUG    | __main__:<module>:313 - Training step 13290: loss = 3.3735 | 3014.79ms | Tokens/s = 173,905.4
2025-01-15 18:55:27.454 | DEBUG    | __main__:<module>:313 - Training step 13300: loss = 3.3448 | 3013.92ms | Tokens/s = 173,955.4
2025-01-15 18:55:57.620 | DEBUG    | __main__:<module>:313 - Training step 13310: loss = 3.3995 | 3018.27ms | Tokens/s = 173,705.0
2025-01-15 18:56:27.792 | DEBUG    | __main__:<module>:313 - Training step 13320: loss = 3.4551 | 3012.72ms | Tokens/s = 174,024.8
2025-01-15 18:56:57.931 | DEBUG    | __main__:<module>:313 - Training step 13330: loss = 3.1276 | 3013.26ms | Tokens/s = 173,993.6
2025-01-15 18:57:28.060 | DEBUG    | __main__:<module>:313 - Training step 13340: loss = 3.5441 | 3011.81ms | Tokens/s = 174,077.6
2025-01-15 18:57:58.199 | DEBUG    | __main__:<module>:313 - Training step 13350: loss = 3.4327 | 3014.79ms | Tokens/s = 173,905.2
2025-01-15 18:58:28.363 | DEBUG    | __main__:<module>:313 - Training step 13360: loss = 3.3720 | 3015.61ms | Tokens/s = 173,857.9
2025-01-15 18:58:58.546 | DEBUG    | __main__:<module>:313 - Training step 13370: loss = 3.4414 | 3018.27ms | Tokens/s = 173,704.8
2025-01-15 18:59:28.701 | DEBUG    | __main__:<module>:313 - Training step 13380: loss = 3.3797 | 3014.41ms | Tokens/s = 173,927.1
2025-01-15 18:59:58.859 | DEBUG    | __main__:<module>:313 - Training step 13390: loss = 3.2832 | 3015.94ms | Tokens/s = 173,838.7
2025-01-15 19:00:29.029 | DEBUG    | __main__:<module>:313 - Training step 13400: loss = 3.4577 | 3015.92ms | Tokens/s = 173,839.9
2025-01-15 19:00:59.201 | DEBUG    | __main__:<module>:313 - Training step 13410: loss = 3.3426 | 3014.00ms | Tokens/s = 173,950.7
2025-01-15 19:01:29.341 | DEBUG    | __main__:<module>:313 - Training step 13420: loss = 3.2952 | 3013.33ms | Tokens/s = 173,989.7
2025-01-15 19:01:59.475 | DEBUG    | __main__:<module>:313 - Training step 13430: loss = 3.4064 | 3016.35ms | Tokens/s = 173,815.4
2025-01-15 19:02:29.634 | DEBUG    | __main__:<module>:313 - Training step 13440: loss = 3.2095 | 3019.43ms | Tokens/s = 173,637.9
2025-01-15 19:02:59.821 | DEBUG    | __main__:<module>:313 - Training step 13450: loss = 3.4680 | 3017.20ms | Tokens/s = 173,766.3
2025-01-15 19:03:29.997 | DEBUG    | __main__:<module>:313 - Training step 13460: loss = 3.5560 | 3016.01ms | Tokens/s = 173,835.1
2025-01-15 19:04:00.144 | DEBUG    | __main__:<module>:313 - Training step 13470: loss = 3.2928 | 3012.74ms | Tokens/s = 174,023.7
2025-01-15 19:04:30.271 | DEBUG    | __main__:<module>:313 - Training step 13480: loss = 3.1216 | 3010.93ms | Tokens/s = 174,128.4
2025-01-15 19:05:00.402 | DEBUG    | __main__:<module>:313 - Training step 13490: loss = 3.3575 | 3013.44ms | Tokens/s = 173,983.1
2025-01-15 19:05:30.578 | DEBUG    | __main__:<module>:313 - Training step 13500: loss = 3.1982 | 3014.99ms | Tokens/s = 173,893.6
2025-01-15 19:06:00.774 | DEBUG    | __main__:<module>:313 - Training step 13510: loss = 3.4286 | 3022.67ms | Tokens/s = 173,451.9
2025-01-15 19:06:30.946 | DEBUG    | __main__:<module>:313 - Training step 13520: loss = 3.4150 | 3017.84ms | Tokens/s = 173,729.7
2025-01-15 19:07:01.092 | DEBUG    | __main__:<module>:313 - Training step 13530: loss = 3.3534 | 3013.81ms | Tokens/s = 173,961.9
2025-01-15 19:07:31.225 | DEBUG    | __main__:<module>:313 - Training step 13540: loss = 3.3630 | 3013.75ms | Tokens/s = 173,965.2
2025-01-15 19:08:01.390 | DEBUG    | __main__:<module>:313 - Training step 13550: loss = 3.1844 | 3017.61ms | Tokens/s = 173,743.0
2025-01-15 19:08:31.576 | DEBUG    | __main__:<module>:313 - Training step 13560: loss = 3.3467 | 3019.52ms | Tokens/s = 173,632.8
2025-01-15 19:09:01.757 | DEBUG    | __main__:<module>:313 - Training step 13570: loss = 3.2956 | 3016.27ms | Tokens/s = 173,820.1
2025-01-15 19:09:31.901 | DEBUG    | __main__:<module>:313 - Training step 13580: loss = 3.5114 | 3013.34ms | Tokens/s = 173,989.2
2025-01-15 19:10:02.056 | DEBUG    | __main__:<module>:313 - Training step 13590: loss = 3.3401 | 3016.86ms | Tokens/s = 173,786.2
2025-01-15 19:10:32.230 | DEBUG    | __main__:<module>:313 - Training step 13600: loss = 3.3983 | 3017.02ms | Tokens/s = 173,776.8
2025-01-15 19:11:02.415 | DEBUG    | __main__:<module>:313 - Training step 13610: loss = 3.1326 | 3017.03ms | Tokens/s = 173,775.9
2025-01-15 19:11:32.567 | DEBUG    | __main__:<module>:313 - Training step 13620: loss = 3.3530 | 3014.26ms | Tokens/s = 173,936.1
2025-01-15 19:12:02.699 | DEBUG    | __main__:<module>:313 - Training step 13630: loss = 3.4090 | 3013.98ms | Tokens/s = 173,952.2
2025-01-15 19:12:32.819 | DEBUG    | __main__:<module>:313 - Training step 13640: loss = 3.2311 | 3012.44ms | Tokens/s = 174,040.9
2025-01-15 19:13:02.965 | DEBUG    | __main__:<module>:313 - Training step 13650: loss = 3.3657 | 3015.64ms | Tokens/s = 173,856.3
2025-01-15 19:13:33.133 | DEBUG    | __main__:<module>:313 - Training step 13660: loss = 3.3876 | 3019.22ms | Tokens/s = 173,650.3
2025-01-15 19:14:03.313 | DEBUG    | __main__:<module>:313 - Training step 13670: loss = 3.4181 | 3017.99ms | Tokens/s = 173,720.7
2025-01-15 19:14:33.465 | DEBUG    | __main__:<module>:313 - Training step 13680: loss = 3.3814 | 3014.31ms | Tokens/s = 173,933.2
2025-01-15 19:15:03.616 | DEBUG    | __main__:<module>:313 - Training step 13690: loss = 3.4813 | 3018.92ms | Tokens/s = 173,667.5
2025-01-15 19:15:33.783 | DEBUG    | __main__:<module>:313 - Training step 13700: loss = 3.2947 | 3014.36ms | Tokens/s = 173,930.3
2025-01-15 19:16:03.915 | DEBUG    | __main__:<module>:313 - Training step 13710: loss = 3.2733 | 3011.22ms | Tokens/s = 174,111.2
2025-01-15 19:16:34.046 | DEBUG    | __main__:<module>:313 - Training step 13720: loss = 3.2654 | 3011.93ms | Tokens/s = 174,070.3
2025-01-15 19:17:04.193 | DEBUG    | __main__:<module>:313 - Training step 13730: loss = 3.4678 | 3017.64ms | Tokens/s = 173,741.2
2025-01-15 19:17:34.368 | DEBUG    | __main__:<module>:313 - Training step 13740: loss = 3.3932 | 3021.31ms | Tokens/s = 173,530.0
2025-01-15 19:18:04.527 | DEBUG    | __main__:<module>:313 - Training step 13750: loss = 3.2930 | 3012.94ms | Tokens/s = 174,012.2
2025-01-15 19:18:34.678 | DEBUG    | __main__:<module>:313 - Training step 13760: loss = 3.4337 | 3016.99ms | Tokens/s = 173,778.3
2025-01-15 19:19:04.852 | DEBUG    | __main__:<module>:313 - Training step 13770: loss = 3.4746 | 3015.19ms | Tokens/s = 173,882.1
2025-01-15 19:19:34.993 | DEBUG    | __main__:<module>:313 - Training step 13780: loss = 3.4340 | 3013.72ms | Tokens/s = 173,967.0
2025-01-15 19:20:05.125 | DEBUG    | __main__:<module>:313 - Training step 13790: loss = 3.3439 | 3015.09ms | Tokens/s = 173,887.8
2025-01-15 19:20:35.281 | DEBUG    | __main__:<module>:313 - Training step 13800: loss = 3.3892 | 3013.01ms | Tokens/s = 174,008.2
2025-01-15 19:21:05.444 | DEBUG    | __main__:<module>:313 - Training step 13810: loss = 3.3856 | 3014.70ms | Tokens/s = 173,910.8
2025-01-15 19:21:35.577 | DEBUG    | __main__:<module>:313 - Training step 13820: loss = 3.4090 | 3013.48ms | Tokens/s = 173,980.8
2025-01-15 19:22:05.706 | DEBUG    | __main__:<module>:313 - Training step 13830: loss = 3.1847 | 3015.22ms | Tokens/s = 173,880.4
2025-01-15 19:22:35.851 | DEBUG    | __main__:<module>:313 - Training step 13840: loss = 3.2879 | 3015.86ms | Tokens/s = 173,843.4
2025-01-15 19:23:06.024 | DEBUG    | __main__:<module>:313 - Training step 13850: loss = 3.3336 | 3017.27ms | Tokens/s = 173,762.5
2025-01-15 19:23:36.218 | DEBUG    | __main__:<module>:313 - Training step 13860: loss = 3.2331 | 3021.89ms | Tokens/s = 173,496.9
2025-01-15 19:24:06.395 | DEBUG    | __main__:<module>:313 - Training step 13870: loss = 3.2940 | 3017.08ms | Tokens/s = 173,773.1
2025-01-15 19:24:36.545 | DEBUG    | __main__:<module>:313 - Training step 13880: loss = 3.3370 | 3012.43ms | Tokens/s = 174,041.8
2025-01-15 19:25:06.674 | DEBUG    | __main__:<module>:313 - Training step 13890: loss = 3.3430 | 3011.33ms | Tokens/s = 174,105.0
2025-01-15 19:25:36.823 | DEBUG    | __main__:<module>:313 - Training step 13900: loss = 3.3026 | 3016.58ms | Tokens/s = 173,802.1
2025-01-15 19:26:06.990 | DEBUG    | __main__:<module>:313 - Training step 13910: loss = 3.4379 | 3017.68ms | Tokens/s = 173,738.5
2025-01-15 19:26:37.173 | DEBUG    | __main__:<module>:313 - Training step 13920: loss = 3.1757 | 3018.24ms | Tokens/s = 173,706.2
2025-01-15 19:27:07.340 | DEBUG    | __main__:<module>:313 - Training step 13930: loss = 3.4252 | 3014.45ms | Tokens/s = 173,925.2
2025-01-15 19:27:37.475 | DEBUG    | __main__:<module>:313 - Training step 13940: loss = 3.3903 | 3012.88ms | Tokens/s = 174,015.4
2025-01-15 19:28:07.595 | DEBUG    | __main__:<module>:313 - Training step 13950: loss = 3.2421 | 3012.18ms | Tokens/s = 174,056.1
2025-01-15 19:28:37.707 | DEBUG    | __main__:<module>:313 - Training step 13960: loss = 3.4612 | 3013.72ms | Tokens/s = 173,966.9
2025-01-15 19:29:07.857 | DEBUG    | __main__:<module>:313 - Training step 13970: loss = 3.3820 | 3015.20ms | Tokens/s = 173,881.4
2025-01-15 19:29:38.022 | DEBUG    | __main__:<module>:313 - Training step 13980: loss = 3.2593 | 3017.28ms | Tokens/s = 173,761.9
2025-01-15 19:30:08.185 | DEBUG    | __main__:<module>:313 - Training step 13990: loss = 3.2265 | 3014.06ms | Tokens/s = 173,947.7
2025-01-15 19:30:41.737 | INFO     | __main__:<module>:265 - Step 14,000/40,000 loss: 3.3442 (T) 3.3725 (V) | lr=7.7e-03
2025-01-15 19:30:44.752 | DEBUG    | __main__:<module>:313 - Training step 14000: loss = 3.0989 | 9451.13ms | Tokens/s = 55,473.6
2025-01-15 19:31:14.901 | DEBUG    | __main__:<module>:313 - Training step 14010: loss = 3.4378 | 3016.97ms | Tokens/s = 173,779.5
2025-01-15 19:31:45.072 | DEBUG    | __main__:<module>:313 - Training step 14020: loss = 3.2566 | 3016.36ms | Tokens/s = 173,814.8
2025-01-15 19:32:15.261 | DEBUG    | __main__:<module>:313 - Training step 14030: loss = 3.2774 | 3018.78ms | Tokens/s = 173,675.6
2025-01-15 19:32:45.429 | DEBUG    | __main__:<module>:313 - Training step 14040: loss = 3.1702 | 3017.44ms | Tokens/s = 173,752.4
2025-01-15 19:33:15.594 | DEBUG    | __main__:<module>:313 - Training step 14050: loss = 3.3963 | 3012.76ms | Tokens/s = 174,022.6
2025-01-15 19:33:45.736 | DEBUG    | __main__:<module>:313 - Training step 14060: loss = 3.4554 | 3015.09ms | Tokens/s = 173,887.8
2025-01-15 19:34:15.866 | DEBUG    | __main__:<module>:313 - Training step 14070: loss = 3.3765 | 3012.64ms | Tokens/s = 174,029.6
2025-01-15 19:34:46.005 | DEBUG    | __main__:<module>:313 - Training step 14080: loss = 3.3577 | 3014.91ms | Tokens/s = 173,898.7
2025-01-15 19:35:16.160 | DEBUG    | __main__:<module>:313 - Training step 14090: loss = 3.2780 | 3019.08ms | Tokens/s = 173,658.3
2025-01-15 19:35:46.341 | DEBUG    | __main__:<module>:313 - Training step 14100: loss = 3.1916 | 3017.36ms | Tokens/s = 173,757.1
2025-01-15 19:36:16.492 | DEBUG    | __main__:<module>:313 - Training step 14110: loss = 3.3968 | 3015.63ms | Tokens/s = 173,856.8
2025-01-15 19:36:46.628 | DEBUG    | __main__:<module>:313 - Training step 14120: loss = 3.2060 | 3013.42ms | Tokens/s = 173,984.4
2025-01-15 19:37:16.782 | DEBUG    | __main__:<module>:313 - Training step 14130: loss = 3.3722 | 3015.55ms | Tokens/s = 173,861.5
2025-01-15 19:37:46.952 | DEBUG    | __main__:<module>:313 - Training step 14140: loss = 3.3305 | 3014.52ms | Tokens/s = 173,921.1
2025-01-15 19:38:17.092 | DEBUG    | __main__:<module>:313 - Training step 14150: loss = 3.6095 | 3014.25ms | Tokens/s = 173,936.3
2025-01-15 19:38:47.230 | DEBUG    | __main__:<module>:313 - Training step 14160: loss = 3.4725 | 3014.33ms | Tokens/s = 173,931.9
2025-01-15 19:39:17.373 | DEBUG    | __main__:<module>:313 - Training step 14170: loss = 3.4682 | 3010.83ms | Tokens/s = 174,134.0
2025-01-15 19:39:47.520 | DEBUG    | __main__:<module>:313 - Training step 14180: loss = 3.3353 | 3019.56ms | Tokens/s = 173,630.5
2025-01-15 19:40:17.688 | DEBUG    | __main__:<module>:313 - Training step 14190: loss = 3.4314 | 3019.86ms | Tokens/s = 173,613.3
2025-01-15 19:40:47.865 | DEBUG    | __main__:<module>:313 - Training step 14200: loss = 3.4219 | 3017.06ms | Tokens/s = 173,774.7
2025-01-15 19:41:18.019 | DEBUG    | __main__:<module>:313 - Training step 14210: loss = 3.2396 | 3014.90ms | Tokens/s = 173,898.7
2025-01-15 19:41:48.144 | DEBUG    | __main__:<module>:313 - Training step 14220: loss = 3.2829 | 3012.76ms | Tokens/s = 174,022.4
2025-01-15 19:42:18.296 | DEBUG    | __main__:<module>:313 - Training step 14230: loss = 3.2867 | 3020.19ms | Tokens/s = 173,594.2
2025-01-15 19:42:48.463 | DEBUG    | __main__:<module>:313 - Training step 14240: loss = 3.2869 | 3017.11ms | Tokens/s = 173,771.6
2025-01-15 19:43:18.606 | DEBUG    | __main__:<module>:313 - Training step 14250: loss = 3.2507 | 3013.10ms | Tokens/s = 174,003.1
2025-01-15 19:43:48.728 | DEBUG    | __main__:<module>:313 - Training step 14260: loss = 3.2410 | 3010.23ms | Tokens/s = 174,168.5
2025-01-15 19:44:18.852 | DEBUG    | __main__:<module>:313 - Training step 14270: loss = 3.3577 | 3014.15ms | Tokens/s = 173,942.4
2025-01-15 19:44:49.006 | DEBUG    | __main__:<module>:313 - Training step 14280: loss = 3.4587 | 3013.77ms | Tokens/s = 173,964.1
2025-01-15 19:45:19.177 | DEBUG    | __main__:<module>:313 - Training step 14290: loss = 3.3576 | 3018.87ms | Tokens/s = 173,670.3
2025-01-15 19:45:49.340 | DEBUG    | __main__:<module>:313 - Training step 14300: loss = 3.4724 | 3016.69ms | Tokens/s = 173,795.6
2025-01-15 19:46:19.480 | DEBUG    | __main__:<module>:313 - Training step 14310: loss = 3.2751 | 3012.79ms | Tokens/s = 174,021.0
2025-01-15 19:46:49.656 | DEBUG    | __main__:<module>:313 - Training step 14320: loss = 3.4131 | 3017.91ms | Tokens/s = 173,725.6
2025-01-15 19:47:19.839 | DEBUG    | __main__:<module>:313 - Training step 14330: loss = 3.2810 | 3020.90ms | Tokens/s = 173,553.7
2025-01-15 19:47:50.012 | DEBUG    | __main__:<module>:313 - Training step 14340: loss = 3.3727 | 3018.18ms | Tokens/s = 173,709.8
2025-01-15 19:48:20.171 | DEBUG    | __main__:<module>:313 - Training step 14350: loss = 3.4056 | 3017.30ms | Tokens/s = 173,760.5
2025-01-15 19:48:50.331 | DEBUG    | __main__:<module>:313 - Training step 14360: loss = 3.2773 | 3017.76ms | Tokens/s = 173,734.2
2025-01-15 19:49:20.496 | DEBUG    | __main__:<module>:313 - Training step 14370: loss = 3.1665 | 3016.22ms | Tokens/s = 173,822.7
2025-01-15 19:49:50.626 | DEBUG    | __main__:<module>:313 - Training step 14380: loss = 3.2604 | 3010.82ms | Tokens/s = 174,134.5
2025-01-15 19:50:20.764 | DEBUG    | __main__:<module>:313 - Training step 14390: loss = 3.7185 | 3014.07ms | Tokens/s = 173,947.1
2025-01-15 19:50:50.922 | DEBUG    | __main__:<module>:313 - Training step 14400: loss = 3.3960 | 3016.98ms | Tokens/s = 173,778.9
2025-01-15 19:51:21.091 | DEBUG    | __main__:<module>:313 - Training step 14410: loss = 3.5258 | 3017.44ms | Tokens/s = 173,752.4
2025-01-15 19:51:51.253 | DEBUG    | __main__:<module>:313 - Training step 14420: loss = 3.3762 | 3012.66ms | Tokens/s = 174,028.3
2025-01-15 19:52:21.393 | DEBUG    | __main__:<module>:313 - Training step 14430: loss = 3.3684 | 3012.61ms | Tokens/s = 174,031.4
2025-01-15 19:52:51.519 | DEBUG    | __main__:<module>:313 - Training step 14440: loss = 3.2788 | 3011.31ms | Tokens/s = 174,106.0
2025-01-15 19:53:21.663 | DEBUG    | __main__:<module>:313 - Training step 14450: loss = 3.3072 | 3016.58ms | Tokens/s = 173,802.0
2025-01-15 19:53:51.833 | DEBUG    | __main__:<module>:313 - Training step 14460: loss = 3.4352 | 3013.64ms | Tokens/s = 173,971.9
2025-01-15 19:54:21.987 | DEBUG    | __main__:<module>:313 - Training step 14470: loss = 3.3048 | 3015.17ms | Tokens/s = 173,883.5
2025-01-15 19:54:52.161 | DEBUG    | __main__:<module>:313 - Training step 14480: loss = 3.4682 | 3018.60ms | Tokens/s = 173,685.7
2025-01-15 19:55:22.356 | DEBUG    | __main__:<module>:313 - Training step 14490: loss = 3.2517 | 3020.87ms | Tokens/s = 173,555.4
2025-01-15 19:55:52.535 | DEBUG    | __main__:<module>:313 - Training step 14500: loss = 3.3493 | 3012.16ms | Tokens/s = 174,057.2
2025-01-15 19:56:22.685 | DEBUG    | __main__:<module>:313 - Training step 14510: loss = 3.4227 | 3016.47ms | Tokens/s = 173,808.5
2025-01-15 19:56:52.850 | DEBUG    | __main__:<module>:313 - Training step 14520: loss = 3.5551 | 3015.07ms | Tokens/s = 173,889.0
2025-01-15 19:57:22.988 | DEBUG    | __main__:<module>:313 - Training step 14530: loss = 3.3120 | 3012.69ms | Tokens/s = 174,026.8
2025-01-15 19:57:53.116 | DEBUG    | __main__:<module>:313 - Training step 14540: loss = 3.2576 | 3012.49ms | Tokens/s = 174,038.3
2025-01-15 19:58:23.250 | DEBUG    | __main__:<module>:313 - Training step 14550: loss = 3.3891 | 3014.21ms | Tokens/s = 173,938.5
2025-01-15 19:58:53.415 | DEBUG    | __main__:<module>:313 - Training step 14560: loss = 3.4510 | 3016.08ms | Tokens/s = 173,830.8
2025-01-15 19:59:23.597 | DEBUG    | __main__:<module>:313 - Training step 14570: loss = 3.4157 | 3019.55ms | Tokens/s = 173,631.3
2025-01-15 19:59:53.756 | DEBUG    | __main__:<module>:313 - Training step 14580: loss = 3.3399 | 3016.14ms | Tokens/s = 173,827.3
2025-01-15 20:00:23.895 | DEBUG    | __main__:<module>:313 - Training step 14590: loss = 3.3912 | 3012.04ms | Tokens/s = 174,064.2
2025-01-15 20:00:54.038 | DEBUG    | __main__:<module>:313 - Training step 14600: loss = 3.4083 | 3015.46ms | Tokens/s = 173,866.6
2025-01-15 20:01:24.201 | DEBUG    | __main__:<module>:313 - Training step 14610: loss = 3.3524 | 3016.79ms | Tokens/s = 173,789.7
2025-01-15 20:01:54.367 | DEBUG    | __main__:<module>:313 - Training step 14620: loss = 3.4527 | 3014.93ms | Tokens/s = 173,897.0
2025-01-15 20:02:24.510 | DEBUG    | __main__:<module>:313 - Training step 14630: loss = 3.2301 | 3012.68ms | Tokens/s = 174,026.8
2025-01-15 20:02:54.640 | DEBUG    | __main__:<module>:313 - Training step 14640: loss = 3.3295 | 3013.75ms | Tokens/s = 173,965.5
2025-01-15 20:03:24.795 | DEBUG    | __main__:<module>:313 - Training step 14650: loss = 3.4498 | 3016.67ms | Tokens/s = 173,797.2
2025-01-15 20:03:54.959 | DEBUG    | __main__:<module>:313 - Training step 14660: loss = 3.3645 | 3017.34ms | Tokens/s = 173,758.5
2025-01-15 20:04:25.100 | DEBUG    | __main__:<module>:313 - Training step 14670: loss = 3.2761 | 3013.18ms | Tokens/s = 173,998.0
2025-01-15 20:04:55.262 | DEBUG    | __main__:<module>:313 - Training step 14680: loss = 3.2476 | 3017.55ms | Tokens/s = 173,746.1
2025-01-15 20:05:25.427 | DEBUG    | __main__:<module>:313 - Training step 14690: loss = 3.4459 | 3014.11ms | Tokens/s = 173,944.6
2025-01-15 20:05:55.590 | DEBUG    | __main__:<module>:313 - Training step 14700: loss = 3.3719 | 3013.54ms | Tokens/s = 173,977.5
2025-01-15 20:06:25.728 | DEBUG    | __main__:<module>:313 - Training step 14710: loss = 3.2944 | 3013.04ms | Tokens/s = 174,006.5
2025-01-15 20:06:55.847 | DEBUG    | __main__:<module>:313 - Training step 14720: loss = 3.3089 | 3012.03ms | Tokens/s = 174,064.5
2025-01-15 20:07:25.977 | DEBUG    | __main__:<module>:313 - Training step 14730: loss = 3.2087 | 3014.89ms | Tokens/s = 173,899.5
2025-01-15 20:07:56.144 | DEBUG    | __main__:<module>:313 - Training step 14740: loss = 3.3131 | 3017.26ms | Tokens/s = 173,762.9
2025-01-15 20:08:26.316 | DEBUG    | __main__:<module>:313 - Training step 14750: loss = 3.2520 | 3018.37ms | Tokens/s = 173,699.2
2025-01-15 20:08:56.497 | DEBUG    | __main__:<module>:313 - Training step 14760: loss = 3.4336 | 3019.68ms | Tokens/s = 173,623.9
2025-01-15 20:09:26.668 | DEBUG    | __main__:<module>:313 - Training step 14770: loss = 3.3613 | 3016.22ms | Tokens/s = 173,823.1
2025-01-15 20:09:56.813 | DEBUG    | __main__:<module>:313 - Training step 14780: loss = 3.5035 | 3012.99ms | Tokens/s = 174,009.1
2025-01-15 20:10:26.932 | DEBUG    | __main__:<module>:313 - Training step 14790: loss = 3.3668 | 3008.93ms | Tokens/s = 174,243.8
2025-01-15 20:10:57.052 | DEBUG    | __main__:<module>:313 - Training step 14800: loss = 3.2414 | 3012.88ms | Tokens/s = 174,015.6
2025-01-15 20:11:27.205 | DEBUG    | __main__:<module>:313 - Training step 14810: loss = 3.4376 | 3015.59ms | Tokens/s = 173,859.0
2025-01-15 20:11:57.369 | DEBUG    | __main__:<module>:313 - Training step 14820: loss = 3.2956 | 3018.12ms | Tokens/s = 173,713.4
2025-01-15 20:12:27.529 | DEBUG    | __main__:<module>:313 - Training step 14830: loss = 3.3049 | 3015.95ms | Tokens/s = 173,838.2
2025-01-15 20:12:57.660 | DEBUG    | __main__:<module>:313 - Training step 14840: loss = 3.3623 | 3011.29ms | Tokens/s = 174,107.2
2025-01-15 20:13:27.783 | DEBUG    | __main__:<module>:313 - Training step 14850: loss = 3.4579 | 3012.33ms | Tokens/s = 174,047.2
2025-01-15 20:13:57.936 | DEBUG    | __main__:<module>:313 - Training step 14860: loss = 3.4535 | 3015.16ms | Tokens/s = 173,884.0
2025-01-15 20:14:28.119 | DEBUG    | __main__:<module>:313 - Training step 14870: loss = 3.3351 | 3019.17ms | Tokens/s = 173,652.7
2025-01-15 20:14:58.292 | DEBUG    | __main__:<module>:313 - Training step 14880: loss = 3.3356 | 3016.55ms | Tokens/s = 173,804.0
2025-01-15 20:15:28.449 | DEBUG    | __main__:<module>:313 - Training step 14890: loss = 3.2347 | 3014.90ms | Tokens/s = 173,898.7
2025-01-15 20:15:58.587 | DEBUG    | __main__:<module>:313 - Training step 14900: loss = 3.6067 | 3012.29ms | Tokens/s = 174,049.5
2025-01-15 20:16:28.708 | DEBUG    | __main__:<module>:313 - Training step 14910: loss = 3.2671 | 3009.97ms | Tokens/s = 174,183.8
2025-01-15 20:16:58.829 | DEBUG    | __main__:<module>:313 - Training step 14920: loss = 3.3164 | 3014.19ms | Tokens/s = 173,940.2
2025-01-15 20:17:28.979 | DEBUG    | __main__:<module>:313 - Training step 14930: loss = 3.3159 | 3015.97ms | Tokens/s = 173,837.4
2025-01-15 20:17:59.146 | DEBUG    | __main__:<module>:313 - Training step 14940: loss = 3.3262 | 3018.60ms | Tokens/s = 173,685.9
2025-01-15 20:18:29.319 | DEBUG    | __main__:<module>:313 - Training step 14950: loss = 3.1501 | 3019.00ms | Tokens/s = 173,663.1
2025-01-15 20:18:59.482 | DEBUG    | __main__:<module>:313 - Training step 14960: loss = 3.3607 | 3013.62ms | Tokens/s = 173,973.0
2025-01-15 20:19:29.623 | DEBUG    | __main__:<module>:313 - Training step 14970: loss = 3.3257 | 3012.63ms | Tokens/s = 174,029.9
2025-01-15 20:19:59.741 | DEBUG    | __main__:<module>:313 - Training step 14980: loss = 3.4087 | 3010.63ms | Tokens/s = 174,145.8
2025-01-15 20:20:29.870 | DEBUG    | __main__:<module>:313 - Training step 14990: loss = 3.3733 | 3013.52ms | Tokens/s = 173,978.4
2025-01-15 20:21:03.456 | INFO     | __main__:<module>:265 - Step 15,000/40,000 loss: 3.3457 (T) 3.3397 (V) | lr=7.4e-03
2025-01-15 20:21:03.457 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 20:21:17.029 | DEBUG    | __main__:<module>:313 - Training step 15000: loss = 3.2691 | 20020.46ms | Tokens/s = 26,187.6
2025-01-15 20:21:47.096 | DEBUG    | __main__:<module>:313 - Training step 15010: loss = 3.3721 | 3007.01ms | Tokens/s = 174,355.0
2025-01-15 20:22:17.202 | DEBUG    | __main__:<module>:313 - Training step 15020: loss = 3.3982 | 3012.26ms | Tokens/s = 174,051.2
2025-01-15 20:22:47.340 | DEBUG    | __main__:<module>:313 - Training step 15030: loss = 3.3301 | 3012.46ms | Tokens/s = 174,039.8
2025-01-15 20:23:17.491 | DEBUG    | __main__:<module>:313 - Training step 15040: loss = 3.3502 | 3014.47ms | Tokens/s = 173,923.9
2025-01-15 20:23:47.664 | DEBUG    | __main__:<module>:313 - Training step 15050: loss = 3.2975 | 3018.17ms | Tokens/s = 173,710.4
2025-01-15 20:24:17.855 | DEBUG    | __main__:<module>:313 - Training step 15060: loss = 3.2750 | 3018.61ms | Tokens/s = 173,685.3
2025-01-15 20:24:48.044 | DEBUG    | __main__:<module>:313 - Training step 15070: loss = 3.3153 | 3018.16ms | Tokens/s = 173,711.3
2025-01-15 20:25:18.206 | DEBUG    | __main__:<module>:313 - Training step 15080: loss = 3.3485 | 3014.09ms | Tokens/s = 173,945.6
2025-01-15 20:25:48.343 | DEBUG    | __main__:<module>:313 - Training step 15090: loss = 3.2782 | 3013.50ms | Tokens/s = 173,979.8
2025-01-15 20:26:18.472 | DEBUG    | __main__:<module>:313 - Training step 15100: loss = 3.3319 | 3012.09ms | Tokens/s = 174,061.3
2025-01-15 20:26:48.622 | DEBUG    | __main__:<module>:313 - Training step 15110: loss = 3.1957 | 3016.34ms | Tokens/s = 173,815.9
2025-01-15 20:27:18.763 | DEBUG    | __main__:<module>:313 - Training step 15120: loss = 3.4867 | 3010.91ms | Tokens/s = 174,129.5
2025-01-15 20:27:48.900 | DEBUG    | __main__:<module>:313 - Training step 15130: loss = 3.3349 | 3014.54ms | Tokens/s = 173,919.6
2025-01-15 20:28:19.067 | DEBUG    | __main__:<module>:313 - Training step 15140: loss = 3.2409 | 3017.72ms | Tokens/s = 173,736.5
2025-01-15 20:28:49.236 | DEBUG    | __main__:<module>:313 - Training step 15150: loss = 3.0813 | 3014.43ms | Tokens/s = 173,926.2
2025-01-15 20:29:19.391 | DEBUG    | __main__:<module>:313 - Training step 15160: loss = 3.3136 | 3013.08ms | Tokens/s = 174,003.8
2025-01-15 20:29:49.522 | DEBUG    | __main__:<module>:313 - Training step 15170: loss = 3.1997 | 3010.74ms | Tokens/s = 174,139.5
2025-01-15 20:30:19.636 | DEBUG    | __main__:<module>:313 - Training step 15180: loss = 3.4034 | 3012.86ms | Tokens/s = 174,016.5
2025-01-15 20:30:49.785 | DEBUG    | __main__:<module>:313 - Training step 15190: loss = 3.3697 | 3015.20ms | Tokens/s = 173,881.9
2025-01-15 20:31:19.964 | DEBUG    | __main__:<module>:313 - Training step 15200: loss = 3.4584 | 3016.07ms | Tokens/s = 173,831.4
2025-01-15 20:31:50.149 | DEBUG    | __main__:<module>:313 - Training step 15210: loss = 3.1433 | 3016.24ms | Tokens/s = 173,821.8
2025-01-15 20:32:20.297 | DEBUG    | __main__:<module>:313 - Training step 15220: loss = 3.2768 | 3011.01ms | Tokens/s = 174,123.4
2025-01-15 20:32:50.438 | DEBUG    | __main__:<module>:313 - Training step 15230: loss = 3.3034 | 3015.48ms | Tokens/s = 173,865.7
2025-01-15 20:33:20.611 | DEBUG    | __main__:<module>:313 - Training step 15240: loss = 3.4141 | 3019.60ms | Tokens/s = 173,628.5
2025-01-15 20:33:50.767 | DEBUG    | __main__:<module>:313 - Training step 15250: loss = 3.4615 | 3012.62ms | Tokens/s = 174,030.8
2025-01-15 20:34:20.903 | DEBUG    | __main__:<module>:313 - Training step 15260: loss = 3.3000 | 3015.71ms | Tokens/s = 173,852.0
2025-01-15 20:34:51.073 | DEBUG    | __main__:<module>:313 - Training step 15270: loss = 3.3037 | 3018.69ms | Tokens/s = 173,680.6
2025-01-15 20:35:21.235 | DEBUG    | __main__:<module>:313 - Training step 15280: loss = 3.2539 | 3014.18ms | Tokens/s = 173,940.5
2025-01-15 20:35:51.364 | DEBUG    | __main__:<module>:313 - Training step 15290: loss = 3.2272 | 3011.73ms | Tokens/s = 174,081.8
2025-01-15 20:36:21.482 | DEBUG    | __main__:<module>:313 - Training step 15300: loss = 3.3139 | 3011.14ms | Tokens/s = 174,116.2
2025-01-15 20:36:51.621 | DEBUG    | __main__:<module>:313 - Training step 15310: loss = 3.3340 | 3014.60ms | Tokens/s = 173,916.1
2025-01-15 20:37:21.767 | DEBUG    | __main__:<module>:313 - Training step 15320: loss = 3.2102 | 3012.88ms | Tokens/s = 174,015.6
2025-01-15 20:37:51.903 | DEBUG    | __main__:<module>:313 - Training step 15330: loss = 3.4074 | 3014.96ms | Tokens/s = 173,895.7
2025-01-15 20:38:22.071 | DEBUG    | __main__:<module>:313 - Training step 15340: loss = 3.3675 | 3013.89ms | Tokens/s = 173,957.4
2025-01-15 20:38:52.252 | DEBUG    | __main__:<module>:313 - Training step 15350: loss = 3.3927 | 3016.89ms | Tokens/s = 173,784.4
2025-01-15 20:39:22.390 | DEBUG    | __main__:<module>:313 - Training step 15360: loss = 3.2459 | 3014.22ms | Tokens/s = 173,938.5
2025-01-15 20:39:52.505 | DEBUG    | __main__:<module>:313 - Training step 15370: loss = 3.3537 | 3011.49ms | Tokens/s = 174,096.1
2025-01-15 20:40:22.616 | DEBUG    | __main__:<module>:313 - Training step 15380: loss = 3.2617 | 3013.14ms | Tokens/s = 174,000.4
2025-01-15 20:40:52.766 | DEBUG    | __main__:<module>:313 - Training step 15390: loss = 3.3613 | 3014.13ms | Tokens/s = 173,943.7
2025-01-15 20:41:22.920 | DEBUG    | __main__:<module>:313 - Training step 15400: loss = 3.2785 | 3013.68ms | Tokens/s = 173,969.2
2025-01-15 20:41:53.080 | DEBUG    | __main__:<module>:313 - Training step 15410: loss = 3.2379 | 3015.68ms | Tokens/s = 173,854.2
2025-01-15 20:42:23.259 | DEBUG    | __main__:<module>:313 - Training step 15420: loss = 3.1329 | 3016.49ms | Tokens/s = 173,807.6
2025-01-15 20:42:53.420 | DEBUG    | __main__:<module>:313 - Training step 15430: loss = 3.3244 | 3013.77ms | Tokens/s = 173,964.4
2025-01-15 20:43:23.546 | DEBUG    | __main__:<module>:313 - Training step 15440: loss = 3.4110 | 3010.84ms | Tokens/s = 174,133.3
2025-01-15 20:43:53.662 | DEBUG    | __main__:<module>:313 - Training step 15450: loss = 3.4340 | 3012.56ms | Tokens/s = 174,034.2
2025-01-15 20:44:23.808 | DEBUG    | __main__:<module>:313 - Training step 15460: loss = 3.3256 | 3013.88ms | Tokens/s = 173,958.0
2025-01-15 20:44:53.975 | DEBUG    | __main__:<module>:313 - Training step 15470: loss = 3.3598 | 3015.35ms | Tokens/s = 173,873.0
2025-01-15 20:45:24.154 | DEBUG    | __main__:<module>:313 - Training step 15480: loss = 3.3190 | 3021.00ms | Tokens/s = 173,547.7
2025-01-15 20:45:54.344 | DEBUG    | __main__:<module>:313 - Training step 15490: loss = 3.3674 | 3021.94ms | Tokens/s = 173,493.8
2025-01-15 20:46:24.526 | DEBUG    | __main__:<module>:313 - Training step 15500: loss = 3.3890 | 3016.43ms | Tokens/s = 173,810.7
2025-01-15 20:46:54.675 | DEBUG    | __main__:<module>:313 - Training step 15510: loss = 3.3320 | 3014.23ms | Tokens/s = 173,937.8
2025-01-15 20:47:24.800 | DEBUG    | __main__:<module>:313 - Training step 15520: loss = 3.4064 | 3011.60ms | Tokens/s = 174,089.5
2025-01-15 20:47:54.910 | DEBUG    | __main__:<module>:313 - Training step 15530: loss = 3.1863 | 3010.72ms | Tokens/s = 174,140.4
2025-01-15 20:48:25.037 | DEBUG    | __main__:<module>:313 - Training step 15540: loss = 3.5306 | 3013.12ms | Tokens/s = 174,001.4
2025-01-15 20:48:55.177 | DEBUG    | __main__:<module>:313 - Training step 15550: loss = 3.3452 | 3015.10ms | Tokens/s = 173,887.4
2025-01-15 20:49:25.310 | DEBUG    | __main__:<module>:313 - Training step 15560: loss = 3.4813 | 3012.89ms | Tokens/s = 174,014.8
2025-01-15 20:49:55.436 | DEBUG    | __main__:<module>:313 - Training step 15570: loss = 3.3787 | 3011.03ms | Tokens/s = 174,122.7
2025-01-15 20:50:25.549 | DEBUG    | __main__:<module>:313 - Training step 15580: loss = 3.1842 | 3011.06ms | Tokens/s = 174,120.9
2025-01-15 20:50:55.690 | DEBUG    | __main__:<module>:313 - Training step 15590: loss = 3.2421 | 3013.79ms | Tokens/s = 173,963.2
2025-01-15 20:51:25.851 | DEBUG    | __main__:<module>:313 - Training step 15600: loss = 3.1646 | 3017.62ms | Tokens/s = 173,742.2
2025-01-15 20:51:56.030 | DEBUG    | __main__:<module>:313 - Training step 15610: loss = 3.1241 | 3019.80ms | Tokens/s = 173,616.7
2025-01-15 20:52:26.167 | DEBUG    | __main__:<module>:313 - Training step 15620: loss = 3.3550 | 3011.07ms | Tokens/s = 174,120.0
2025-01-15 20:52:56.302 | DEBUG    | __main__:<module>:313 - Training step 15630: loss = 3.1697 | 3014.27ms | Tokens/s = 173,935.5
2025-01-15 20:53:26.458 | DEBUG    | __main__:<module>:313 - Training step 15640: loss = 3.3240 | 3016.56ms | Tokens/s = 173,803.4
2025-01-15 20:53:56.630 | DEBUG    | __main__:<module>:313 - Training step 15650: loss = 3.3284 | 3016.96ms | Tokens/s = 173,780.1
2025-01-15 20:54:26.772 | DEBUG    | __main__:<module>:313 - Training step 15660: loss = 3.3222 | 3015.37ms | Tokens/s = 173,872.0
2025-01-15 20:54:56.925 | DEBUG    | __main__:<module>:313 - Training step 15670: loss = 3.3243 | 3014.83ms | Tokens/s = 173,902.9
2025-01-15 20:55:27.109 | DEBUG    | __main__:<module>:313 - Training step 15680: loss = 3.1672 | 3017.83ms | Tokens/s = 173,729.9
2025-01-15 20:55:57.270 | DEBUG    | __main__:<module>:313 - Training step 15690: loss = 3.2908 | 3013.17ms | Tokens/s = 173,998.9
2025-01-15 20:56:27.399 | DEBUG    | __main__:<module>:313 - Training step 15700: loss = 3.1983 | 3010.40ms | Tokens/s = 174,159.0
2025-01-15 20:56:57.516 | DEBUG    | __main__:<module>:313 - Training step 15710: loss = 3.2211 | 3010.28ms | Tokens/s = 174,166.0
2025-01-15 20:57:27.666 | DEBUG    | __main__:<module>:313 - Training step 15720: loss = 3.2997 | 3018.21ms | Tokens/s = 173,708.5
2025-01-15 20:57:57.831 | DEBUG    | __main__:<module>:313 - Training step 15730: loss = 3.2549 | 3019.38ms | Tokens/s = 173,640.8
2025-01-15 20:58:27.978 | DEBUG    | __main__:<module>:313 - Training step 15740: loss = 3.3678 | 3011.71ms | Tokens/s = 174,083.2
2025-01-15 20:58:58.112 | DEBUG    | __main__:<module>:313 - Training step 15750: loss = 3.2726 | 3014.65ms | Tokens/s = 173,913.5
2025-01-15 20:59:28.278 | DEBUG    | __main__:<module>:313 - Training step 15760: loss = 3.2448 | 3017.28ms | Tokens/s = 173,761.9
2025-01-15 20:59:58.438 | DEBUG    | __main__:<module>:313 - Training step 15770: loss = 3.3433 | 3015.26ms | Tokens/s = 173,878.3
2025-01-15 21:00:28.562 | DEBUG    | __main__:<module>:313 - Training step 15780: loss = 3.3973 | 3012.44ms | Tokens/s = 174,041.0
2025-01-15 21:00:58.703 | DEBUG    | __main__:<module>:313 - Training step 15790: loss = 3.2813 | 3014.74ms | Tokens/s = 173,908.4
2025-01-15 21:01:28.874 | DEBUG    | __main__:<module>:313 - Training step 15800: loss = 3.2892 | 3015.01ms | Tokens/s = 173,892.8
2025-01-15 21:01:59.049 | DEBUG    | __main__:<module>:313 - Training step 15810: loss = 3.2929 | 3013.08ms | Tokens/s = 174,004.3
2025-01-15 21:02:29.198 | DEBUG    | __main__:<module>:313 - Training step 15820: loss = 3.2351 | 3014.34ms | Tokens/s = 173,931.4
2025-01-15 21:02:59.327 | DEBUG    | __main__:<module>:313 - Training step 15830: loss = 3.3628 | 3012.68ms | Tokens/s = 174,027.2
2025-01-15 21:03:29.452 | DEBUG    | __main__:<module>:313 - Training step 15840: loss = 3.4535 | 3014.30ms | Tokens/s = 173,933.8
2025-01-15 21:03:59.591 | DEBUG    | __main__:<module>:313 - Training step 15850: loss = 3.2746 | 3013.76ms | Tokens/s = 173,964.6
2025-01-15 21:04:29.750 | DEBUG    | __main__:<module>:313 - Training step 15860: loss = 3.5006 | 3014.90ms | Tokens/s = 173,899.1
2025-01-15 21:04:59.923 | DEBUG    | __main__:<module>:313 - Training step 15870: loss = 3.3528 | 3015.92ms | Tokens/s = 173,840.2
2025-01-15 21:05:30.075 | DEBUG    | __main__:<module>:313 - Training step 15880: loss = 3.1919 | 3013.41ms | Tokens/s = 173,984.7
2025-01-15 21:06:00.191 | DEBUG    | __main__:<module>:313 - Training step 15890: loss = 3.1904 | 3009.77ms | Tokens/s = 174,195.4
2025-01-15 21:06:30.296 | DEBUG    | __main__:<module>:313 - Training step 15900: loss = 3.4900 | 3009.38ms | Tokens/s = 174,218.2
2025-01-15 21:07:00.412 | DEBUG    | __main__:<module>:313 - Training step 15910: loss = 3.2538 | 3013.16ms | Tokens/s = 173,999.2
2025-01-15 21:07:30.555 | DEBUG    | __main__:<module>:313 - Training step 15920: loss = 3.2984 | 3014.07ms | Tokens/s = 173,946.9
2025-01-15 21:08:00.730 | DEBUG    | __main__:<module>:313 - Training step 15930: loss = 3.4684 | 3017.74ms | Tokens/s = 173,735.2
2025-01-15 21:08:30.875 | DEBUG    | __main__:<module>:313 - Training step 15940: loss = 3.3628 | 3014.41ms | Tokens/s = 173,927.1
2025-01-15 21:09:01.010 | DEBUG    | __main__:<module>:313 - Training step 15950: loss = 3.1418 | 3015.09ms | Tokens/s = 173,888.0
2025-01-15 21:09:31.160 | DEBUG    | __main__:<module>:313 - Training step 15960: loss = 3.3238 | 3016.95ms | Tokens/s = 173,780.9
2025-01-15 21:10:01.326 | DEBUG    | __main__:<module>:313 - Training step 15970: loss = 3.2163 | 3016.66ms | Tokens/s = 173,797.6
2025-01-15 21:10:31.501 | DEBUG    | __main__:<module>:313 - Training step 15980: loss = 3.3269 | 3017.53ms | Tokens/s = 173,747.2
2025-01-15 21:11:01.652 | DEBUG    | __main__:<module>:313 - Training step 15990: loss = 3.2860 | 3011.94ms | Tokens/s = 174,070.0
2025-01-15 21:11:35.215 | INFO     | __main__:<module>:265 - Step 16,000/40,000 loss: 3.3197 (T) 3.3338 (V) | lr=7.0e-03
2025-01-15 21:11:35.216 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 21:11:48.849 | DEBUG    | __main__:<module>:313 - Training step 16000: loss = 3.4337 | 20076.32ms | Tokens/s = 26,114.7
2025-01-15 21:12:18.912 | DEBUG    | __main__:<module>:313 - Training step 16010: loss = 3.4045 | 3008.03ms | Tokens/s = 174,296.1
2025-01-15 21:12:49.028 | DEBUG    | __main__:<module>:313 - Training step 16020: loss = 3.3529 | 3015.58ms | Tokens/s = 173,859.5
2025-01-15 21:13:19.165 | DEBUG    | __main__:<module>:313 - Training step 16030: loss = 3.2739 | 3012.48ms | Tokens/s = 174,038.6
2025-01-15 21:13:49.321 | DEBUG    | __main__:<module>:313 - Training step 16040: loss = 3.3294 | 3014.79ms | Tokens/s = 173,905.3
2025-01-15 21:14:19.486 | DEBUG    | __main__:<module>:313 - Training step 16050: loss = 3.4128 | 3014.33ms | Tokens/s = 173,931.6
2025-01-15 21:14:49.617 | DEBUG    | __main__:<module>:313 - Training step 16060: loss = 3.3898 | 3012.20ms | Tokens/s = 174,055.1
2025-01-15 21:15:19.739 | DEBUG    | __main__:<module>:313 - Training step 16070: loss = 3.3162 | 3014.89ms | Tokens/s = 173,899.4
2025-01-15 21:15:49.862 | DEBUG    | __main__:<module>:313 - Training step 16080: loss = 3.3813 | 3013.05ms | Tokens/s = 174,005.9
2025-01-15 21:16:20.019 | DEBUG    | __main__:<module>:313 - Training step 16090: loss = 3.4943 | 3016.39ms | Tokens/s = 173,813.1
2025-01-15 21:16:50.171 | DEBUG    | __main__:<module>:313 - Training step 16100: loss = 3.3034 | 3011.57ms | Tokens/s = 174,091.2
2025-01-15 21:17:20.298 | DEBUG    | __main__:<module>:313 - Training step 16110: loss = 3.3680 | 3013.08ms | Tokens/s = 174,004.0
2025-01-15 21:17:50.413 | DEBUG    | __main__:<module>:313 - Training step 16120: loss = 3.3663 | 3011.83ms | Tokens/s = 174,076.4
2025-01-15 21:18:20.520 | DEBUG    | __main__:<module>:313 - Training step 16130: loss = 3.2048 | 3011.48ms | Tokens/s = 174,096.6
2025-01-15 21:18:50.619 | DEBUG    | __main__:<module>:313 - Training step 16140: loss = 3.3855 | 3009.81ms | Tokens/s = 174,193.1
2025-01-15 21:19:20.754 | DEBUG    | __main__:<module>:313 - Training step 16150: loss = 3.1321 | 3014.09ms | Tokens/s = 173,945.9
2025-01-15 21:19:50.917 | DEBUG    | __main__:<module>:313 - Training step 16160: loss = 3.1106 | 3014.93ms | Tokens/s = 173,897.4
2025-01-15 21:20:21.046 | DEBUG    | __main__:<module>:313 - Training step 16170: loss = 3.2581 | 3011.25ms | Tokens/s = 174,110.0
2025-01-15 21:20:51.180 | DEBUG    | __main__:<module>:313 - Training step 16180: loss = 3.1255 | 3014.39ms | Tokens/s = 173,928.3
2025-01-15 21:21:21.352 | DEBUG    | __main__:<module>:313 - Training step 16190: loss = 3.3998 | 3017.91ms | Tokens/s = 173,725.6
2025-01-15 21:21:51.520 | DEBUG    | __main__:<module>:313 - Training step 16200: loss = 3.3863 | 3014.23ms | Tokens/s = 173,937.8
2025-01-15 21:22:21.654 | DEBUG    | __main__:<module>:313 - Training step 16210: loss = 3.3016 | 3011.63ms | Tokens/s = 174,088.0
2025-01-15 21:22:51.793 | DEBUG    | __main__:<module>:313 - Training step 16220: loss = 3.3286 | 3014.13ms | Tokens/s = 173,943.3
2025-01-15 21:23:21.956 | DEBUG    | __main__:<module>:313 - Training step 16230: loss = 3.2930 | 3016.75ms | Tokens/s = 173,792.2
2025-01-15 21:23:52.091 | DEBUG    | __main__:<module>:313 - Training step 16240: loss = 3.2923 | 3012.84ms | Tokens/s = 174,017.8
2025-01-15 21:24:22.218 | DEBUG    | __main__:<module>:313 - Training step 16250: loss = 3.3537 | 3013.25ms | Tokens/s = 173,994.3
2025-01-15 21:24:52.343 | DEBUG    | __main__:<module>:313 - Training step 16260: loss = 3.3792 | 3014.00ms | Tokens/s = 173,950.9
2025-01-15 21:25:22.499 | DEBUG    | __main__:<module>:313 - Training step 16270: loss = 3.2807 | 3017.83ms | Tokens/s = 173,730.1
2025-01-15 21:25:52.650 | DEBUG    | __main__:<module>:313 - Training step 16280: loss = 3.2427 | 3015.19ms | Tokens/s = 173,882.4
2025-01-15 21:26:22.802 | DEBUG    | __main__:<module>:313 - Training step 16290: loss = 3.3766 | 3013.91ms | Tokens/s = 173,956.0
2025-01-15 21:26:52.951 | DEBUG    | __main__:<module>:313 - Training step 16300: loss = 3.2480 | 3012.13ms | Tokens/s = 174,058.7
2025-01-15 21:27:23.080 | DEBUG    | __main__:<module>:313 - Training step 16310: loss = 3.4032 | 3012.53ms | Tokens/s = 174,035.9
2025-01-15 21:27:53.187 | DEBUG    | __main__:<module>:313 - Training step 16320: loss = 3.3296 | 3010.07ms | Tokens/s = 174,177.9
2025-01-15 21:28:23.309 | DEBUG    | __main__:<module>:313 - Training step 16330: loss = 3.3753 | 3016.30ms | Tokens/s = 173,818.2
2025-01-15 21:28:53.461 | DEBUG    | __main__:<module>:313 - Training step 16340: loss = 3.2167 | 3016.38ms | Tokens/s = 173,813.6
2025-01-15 21:29:23.590 | DEBUG    | __main__:<module>:313 - Training step 16350: loss = 3.3070 | 3010.98ms | Tokens/s = 174,125.5
2025-01-15 21:29:53.704 | DEBUG    | __main__:<module>:313 - Training step 16360: loss = 3.4817 | 3010.46ms | Tokens/s = 174,155.6
2025-01-15 21:30:23.827 | DEBUG    | __main__:<module>:313 - Training step 16370: loss = 3.0947 | 3013.79ms | Tokens/s = 173,963.1
2025-01-15 21:30:53.984 | DEBUG    | __main__:<module>:313 - Training step 16380: loss = 3.0706 | 3015.89ms | Tokens/s = 173,841.7
2025-01-15 21:31:24.153 | DEBUG    | __main__:<module>:313 - Training step 16390: loss = 3.3089 | 3014.02ms | Tokens/s = 173,949.7
2025-01-15 21:31:54.294 | DEBUG    | __main__:<module>:313 - Training step 16400: loss = 3.1711 | 3014.00ms | Tokens/s = 173,951.0
2025-01-15 21:32:24.422 | DEBUG    | __main__:<module>:313 - Training step 16410: loss = 3.3376 | 3012.48ms | Tokens/s = 174,038.7
2025-01-15 21:32:54.536 | DEBUG    | __main__:<module>:313 - Training step 16420: loss = 3.2333 | 3010.54ms | Tokens/s = 174,151.0
2025-01-15 21:33:24.635 | DEBUG    | __main__:<module>:313 - Training step 16430: loss = 3.4988 | 3009.34ms | Tokens/s = 174,220.1
2025-01-15 21:33:54.757 | DEBUG    | __main__:<module>:313 - Training step 16440: loss = 3.2469 | 3015.39ms | Tokens/s = 173,870.9
2025-01-15 21:34:24.915 | DEBUG    | __main__:<module>:313 - Training step 16450: loss = 3.3625 | 3014.96ms | Tokens/s = 173,895.3
2025-01-15 21:34:55.052 | DEBUG    | __main__:<module>:313 - Training step 16460: loss = 3.1998 | 3012.74ms | Tokens/s = 174,023.7
2025-01-15 21:35:25.168 | DEBUG    | __main__:<module>:313 - Training step 16470: loss = 3.5011 | 3014.45ms | Tokens/s = 173,925.0
2025-01-15 21:35:55.301 | DEBUG    | __main__:<module>:313 - Training step 16480: loss = 3.2422 | 3013.25ms | Tokens/s = 173,994.1
2025-01-15 21:36:25.428 | DEBUG    | __main__:<module>:313 - Training step 16490: loss = 3.2392 | 3013.45ms | Tokens/s = 173,982.6
2025-01-15 21:36:55.579 | DEBUG    | __main__:<module>:313 - Training step 16500: loss = 3.3852 | 3014.34ms | Tokens/s = 173,931.1
2025-01-15 21:37:25.737 | DEBUG    | __main__:<module>:313 - Training step 16510: loss = 3.2035 | 3014.20ms | Tokens/s = 173,939.3
2025-01-15 21:37:55.917 | DEBUG    | __main__:<module>:313 - Training step 16520: loss = 3.2823 | 3018.16ms | Tokens/s = 173,711.3
2025-01-15 21:38:26.086 | DEBUG    | __main__:<module>:313 - Training step 16530: loss = 3.3499 | 3017.68ms | Tokens/s = 173,738.6
2025-01-15 21:38:56.220 | DEBUG    | __main__:<module>:313 - Training step 16540: loss = 3.2481 | 3011.57ms | Tokens/s = 174,091.3
2025-01-15 21:39:26.334 | DEBUG    | __main__:<module>:313 - Training step 16550: loss = 3.1555 | 3009.20ms | Tokens/s = 174,228.2
2025-01-15 21:39:56.457 | DEBUG    | __main__:<module>:313 - Training step 16560: loss = 3.3338 | 3014.40ms | Tokens/s = 173,927.8
2025-01-15 21:40:26.612 | DEBUG    | __main__:<module>:313 - Training step 16570: loss = 3.2834 | 3015.44ms | Tokens/s = 173,868.1
2025-01-15 21:40:56.750 | DEBUG    | __main__:<module>:313 - Training step 16580: loss = 3.3493 | 3011.59ms | Tokens/s = 174,090.1
2025-01-15 21:41:26.871 | DEBUG    | __main__:<module>:313 - Training step 16590: loss = 3.3195 | 3012.35ms | Tokens/s = 174,046.0
2025-01-15 21:41:56.990 | DEBUG    | __main__:<module>:313 - Training step 16600: loss = 3.2953 | 3010.90ms | Tokens/s = 174,130.1
2025-01-15 21:42:27.132 | DEBUG    | __main__:<module>:313 - Training step 16610: loss = 3.2571 | 3015.69ms | Tokens/s = 173,853.3
2025-01-15 21:42:57.293 | DEBUG    | __main__:<module>:313 - Training step 16620: loss = 3.5521 | 3017.45ms | Tokens/s = 173,751.9
2025-01-15 21:43:27.463 | DEBUG    | __main__:<module>:313 - Training step 16630: loss = 3.3935 | 3017.09ms | Tokens/s = 173,772.6
2025-01-15 21:43:57.609 | DEBUG    | __main__:<module>:313 - Training step 16640: loss = 3.3661 | 3011.97ms | Tokens/s = 174,068.0
2025-01-15 21:44:27.737 | DEBUG    | __main__:<module>:313 - Training step 16650: loss = 3.3029 | 3013.90ms | Tokens/s = 173,956.8
2025-01-15 21:44:57.856 | DEBUG    | __main__:<module>:313 - Training step 16660: loss = 3.3635 | 3012.91ms | Tokens/s = 174,013.8
2025-01-15 21:45:28.001 | DEBUG    | __main__:<module>:313 - Training step 16670: loss = 3.1315 | 3015.42ms | Tokens/s = 173,869.0
2025-01-15 21:45:58.170 | DEBUG    | __main__:<module>:313 - Training step 16680: loss = 3.3192 | 3018.12ms | Tokens/s = 173,713.6
2025-01-15 21:46:28.335 | DEBUG    | __main__:<module>:313 - Training step 16690: loss = 3.3597 | 3014.15ms | Tokens/s = 173,942.4
2025-01-15 21:46:58.476 | DEBUG    | __main__:<module>:313 - Training step 16700: loss = 3.3224 | 3013.00ms | Tokens/s = 174,008.6
2025-01-15 21:47:28.600 | DEBUG    | __main__:<module>:313 - Training step 16710: loss = 3.5207 | 3011.29ms | Tokens/s = 174,107.2
2025-01-15 21:47:58.732 | DEBUG    | __main__:<module>:313 - Training step 16720: loss = 3.2863 | 3014.69ms | Tokens/s = 173,911.1
2025-01-15 21:48:28.902 | DEBUG    | __main__:<module>:313 - Training step 16730: loss = 3.3014 | 3019.06ms | Tokens/s = 173,659.3
2025-01-15 21:48:59.076 | DEBUG    | __main__:<module>:313 - Training step 16740: loss = 3.2367 | 3015.70ms | Tokens/s = 173,852.7
2025-01-15 21:49:29.251 | DEBUG    | __main__:<module>:313 - Training step 16750: loss = 3.2910 | 3013.90ms | Tokens/s = 173,956.6
2025-01-15 21:49:59.412 | DEBUG    | __main__:<module>:313 - Training step 16760: loss = 3.3355 | 3014.68ms | Tokens/s = 173,911.8
2025-01-15 21:50:29.570 | DEBUG    | __main__:<module>:313 - Training step 16770: loss = 3.2522 | 3017.52ms | Tokens/s = 173,747.9
2025-01-15 21:50:59.729 | DEBUG    | __main__:<module>:313 - Training step 16780: loss = 3.1321 | 3014.30ms | Tokens/s = 173,933.6
2025-01-15 21:51:29.872 | DEBUG    | __main__:<module>:313 - Training step 16790: loss = 3.2802 | 3015.90ms | Tokens/s = 173,841.4
2025-01-15 21:52:00.014 | DEBUG    | __main__:<module>:313 - Training step 16800: loss = 3.3173 | 3014.85ms | Tokens/s = 173,901.6
2025-01-15 21:52:30.177 | DEBUG    | __main__:<module>:313 - Training step 16810: loss = 3.1928 | 3014.02ms | Tokens/s = 173,949.6
2025-01-15 21:53:00.338 | DEBUG    | __main__:<module>:313 - Training step 16820: loss = 3.1774 | 3017.68ms | Tokens/s = 173,738.5
2025-01-15 21:53:30.508 | DEBUG    | __main__:<module>:313 - Training step 16830: loss = 3.2222 | 3015.01ms | Tokens/s = 173,892.5
2025-01-15 21:54:00.656 | DEBUG    | __main__:<module>:313 - Training step 16840: loss = 3.4454 | 3013.32ms | Tokens/s = 173,989.9
2025-01-15 21:54:30.787 | DEBUG    | __main__:<module>:313 - Training step 16850: loss = 3.3622 | 3014.37ms | Tokens/s = 173,929.4
2025-01-15 21:55:00.942 | DEBUG    | __main__:<module>:313 - Training step 16860: loss = 3.4410 | 3017.26ms | Tokens/s = 173,762.9
2025-01-15 21:55:31.124 | DEBUG    | __main__:<module>:313 - Training step 16870: loss = 3.2864 | 3019.97ms | Tokens/s = 173,607.0
2025-01-15 21:56:01.288 | DEBUG    | __main__:<module>:313 - Training step 16880: loss = 3.1899 | 3014.35ms | Tokens/s = 173,930.9
2025-01-15 21:56:31.425 | DEBUG    | __main__:<module>:313 - Training step 16890: loss = 3.2722 | 3012.89ms | Tokens/s = 174,014.9
2025-01-15 21:57:01.549 | DEBUG    | __main__:<module>:313 - Training step 16900: loss = 3.1931 | 3012.08ms | Tokens/s = 174,061.6
2025-01-15 21:57:31.709 | DEBUG    | __main__:<module>:313 - Training step 16910: loss = 3.4989 | 3015.14ms | Tokens/s = 173,884.9
2025-01-15 21:58:01.880 | DEBUG    | __main__:<module>:313 - Training step 16920: loss = 3.1572 | 3019.52ms | Tokens/s = 173,633.1
2025-01-15 21:58:32.066 | DEBUG    | __main__:<module>:313 - Training step 16930: loss = 3.4191 | 3018.89ms | Tokens/s = 173,668.9
2025-01-15 21:59:02.257 | DEBUG    | __main__:<module>:313 - Training step 16940: loss = 3.2222 | 3019.94ms | Tokens/s = 173,608.9
2025-01-15 21:59:32.410 | DEBUG    | __main__:<module>:313 - Training step 16950: loss = 3.1568 | 3012.24ms | Tokens/s = 174,052.4
2025-01-15 22:00:02.536 | DEBUG    | __main__:<module>:313 - Training step 16960: loss = 3.2666 | 3013.07ms | Tokens/s = 174,004.5
2025-01-15 22:00:32.667 | DEBUG    | __main__:<module>:313 - Training step 16970: loss = 3.2660 | 3013.04ms | Tokens/s = 174,006.3
2025-01-15 22:01:02.821 | DEBUG    | __main__:<module>:313 - Training step 16980: loss = 3.2617 | 3016.96ms | Tokens/s = 173,780.2
2025-01-15 22:01:32.985 | DEBUG    | __main__:<module>:313 - Training step 16990: loss = 3.3285 | 3015.29ms | Tokens/s = 173,876.4
2025-01-15 22:02:06.555 | INFO     | __main__:<module>:265 - Step 17,000/40,000 loss: 3.3195 (T) 3.3122 (V) | lr=6.6e-03
2025-01-15 22:02:06.556 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 22:02:23.098 | DEBUG    | __main__:<module>:313 - Training step 17000: loss = 3.2596 | 22985.12ms | Tokens/s = 22,809.9
2025-01-15 22:02:53.153 | DEBUG    | __main__:<module>:313 - Training step 17010: loss = 3.4261 | 3003.15ms | Tokens/s = 174,579.2
2025-01-15 22:03:23.238 | DEBUG    | __main__:<module>:313 - Training step 17020: loss = 3.3600 | 3010.57ms | Tokens/s = 174,149.1
2025-01-15 22:03:53.367 | DEBUG    | __main__:<module>:313 - Training step 17030: loss = 3.5373 | 3014.47ms | Tokens/s = 173,923.8
2025-01-15 22:04:23.524 | DEBUG    | __main__:<module>:313 - Training step 17040: loss = 3.2997 | 3016.10ms | Tokens/s = 173,830.0
2025-01-15 22:04:53.699 | DEBUG    | __main__:<module>:313 - Training step 17050: loss = 3.5159 | 3018.23ms | Tokens/s = 173,707.1
2025-01-15 22:05:23.839 | DEBUG    | __main__:<module>:313 - Training step 17060: loss = 3.3675 | 3013.13ms | Tokens/s = 174,001.2
2025-01-15 22:05:53.967 | DEBUG    | __main__:<module>:313 - Training step 17070: loss = 3.1892 | 3013.70ms | Tokens/s = 173,968.2
2025-01-15 22:06:24.097 | DEBUG    | __main__:<module>:313 - Training step 17080: loss = 3.2419 | 3014.86ms | Tokens/s = 173,901.2
2025-01-15 22:06:54.248 | DEBUG    | __main__:<module>:313 - Training step 17090: loss = 3.2910 | 3017.25ms | Tokens/s = 173,763.7
2025-01-15 22:07:24.416 | DEBUG    | __main__:<module>:313 - Training step 17100: loss = 3.2790 | 3016.42ms | Tokens/s = 173,811.5
2025-01-15 22:07:54.573 | DEBUG    | __main__:<module>:313 - Training step 17110: loss = 3.2196 | 3014.32ms | Tokens/s = 173,932.2
2025-01-15 22:08:24.704 | DEBUG    | __main__:<module>:313 - Training step 17120: loss = 3.2282 | 3009.87ms | Tokens/s = 174,189.8
2025-01-15 22:08:54.824 | DEBUG    | __main__:<module>:313 - Training step 17130: loss = 3.2013 | 3014.62ms | Tokens/s = 173,915.2
2025-01-15 22:09:24.973 | DEBUG    | __main__:<module>:313 - Training step 17140: loss = 3.4632 | 3017.96ms | Tokens/s = 173,722.4
2025-01-15 22:09:55.146 | DEBUG    | __main__:<module>:313 - Training step 17150: loss = 3.2399 | 3019.12ms | Tokens/s = 173,656.1
2025-01-15 22:10:25.318 | DEBUG    | __main__:<module>:313 - Training step 17160: loss = 3.2239 | 3014.39ms | Tokens/s = 173,928.1
2025-01-15 22:10:55.452 | DEBUG    | __main__:<module>:313 - Training step 17170: loss = 3.2652 | 3012.38ms | Tokens/s = 174,044.6
2025-01-15 22:11:25.576 | DEBUG    | __main__:<module>:313 - Training step 17180: loss = 3.2610 | 3012.29ms | Tokens/s = 174,049.7
2025-01-15 22:11:55.688 | DEBUG    | __main__:<module>:313 - Training step 17190: loss = 3.2670 | 3011.13ms | Tokens/s = 174,116.6
2025-01-15 22:12:25.811 | DEBUG    | __main__:<module>:313 - Training step 17200: loss = 3.4483 | 3013.79ms | Tokens/s = 173,962.8
2025-01-15 22:12:55.970 | DEBUG    | __main__:<module>:313 - Training step 17210: loss = 3.3618 | 3015.48ms | Tokens/s = 173,865.6
2025-01-15 22:13:26.140 | DEBUG    | __main__:<module>:313 - Training step 17220: loss = 3.3435 | 3018.03ms | Tokens/s = 173,718.7
2025-01-15 22:13:56.296 | DEBUG    | __main__:<module>:313 - Training step 17230: loss = 3.4479 | 3017.55ms | Tokens/s = 173,746.2
2025-01-15 22:14:26.430 | DEBUG    | __main__:<module>:313 - Training step 17240: loss = 3.3009 | 3013.02ms | Tokens/s = 174,007.5
2025-01-15 22:14:56.559 | DEBUG    | __main__:<module>:313 - Training step 17250: loss = 3.2082 | 3010.44ms | Tokens/s = 174,156.9
2025-01-15 22:15:26.671 | DEBUG    | __main__:<module>:313 - Training step 17260: loss = 3.1836 | 3009.36ms | Tokens/s = 174,219.2
2025-01-15 22:15:56.781 | DEBUG    | __main__:<module>:313 - Training step 17270: loss = 3.3722 | 3010.31ms | Tokens/s = 174,164.0
2025-01-15 22:16:26.912 | DEBUG    | __main__:<module>:313 - Training step 17280: loss = 3.2471 | 3013.28ms | Tokens/s = 173,992.4
2025-01-15 22:16:57.063 | DEBUG    | __main__:<module>:313 - Training step 17290: loss = 3.3685 | 3015.92ms | Tokens/s = 173,840.0
2025-01-15 22:17:27.198 | DEBUG    | __main__:<module>:313 - Training step 17300: loss = 3.2443 | 3010.37ms | Tokens/s = 174,160.7
2025-01-15 22:17:57.329 | DEBUG    | __main__:<module>:313 - Training step 17310: loss = 3.2619 | 3013.00ms | Tokens/s = 174,008.4
2025-01-15 22:18:27.475 | DEBUG    | __main__:<module>:313 - Training step 17320: loss = 3.2782 | 3015.86ms | Tokens/s = 173,843.6
2025-01-15 22:18:57.627 | DEBUG    | __main__:<module>:313 - Training step 17330: loss = 3.1618 | 3016.84ms | Tokens/s = 173,787.3
2025-01-15 22:19:27.799 | DEBUG    | __main__:<module>:313 - Training step 17340: loss = 3.2564 | 3017.73ms | Tokens/s = 173,736.2
2025-01-15 22:19:57.945 | DEBUG    | __main__:<module>:313 - Training step 17350: loss = 3.3800 | 3013.70ms | Tokens/s = 173,968.0
2025-01-15 22:20:28.074 | DEBUG    | __main__:<module>:313 - Training step 17360: loss = 3.3195 | 3011.82ms | Tokens/s = 174,077.0
2025-01-15 22:20:58.218 | DEBUG    | __main__:<module>:313 - Training step 17370: loss = 3.3576 | 3016.28ms | Tokens/s = 173,819.5
2025-01-15 22:21:28.390 | DEBUG    | __main__:<module>:313 - Training step 17380: loss = 3.3112 | 3017.98ms | Tokens/s = 173,721.4
2025-01-15 22:21:58.552 | DEBUG    | __main__:<module>:313 - Training step 17390: loss = 3.1957 | 3014.31ms | Tokens/s = 173,932.9
2025-01-15 22:22:28.678 | DEBUG    | __main__:<module>:313 - Training step 17400: loss = 3.1911 | 3011.67ms | Tokens/s = 174,085.5
2025-01-15 22:22:58.795 | DEBUG    | __main__:<module>:313 - Training step 17410: loss = 3.1728 | 3011.82ms | Tokens/s = 174,077.0
2025-01-15 22:23:28.943 | DEBUG    | __main__:<module>:313 - Training step 17420: loss = 3.4134 | 3014.43ms | Tokens/s = 173,926.2
2025-01-15 22:23:59.074 | DEBUG    | __main__:<module>:313 - Training step 17430: loss = 3.2920 | 3010.47ms | Tokens/s = 174,154.8
2025-01-15 22:24:29.182 | DEBUG    | __main__:<module>:313 - Training step 17440: loss = 3.4151 | 3011.13ms | Tokens/s = 174,116.5
2025-01-15 22:24:59.309 | DEBUG    | __main__:<module>:313 - Training step 17450: loss = 3.3127 | 3013.90ms | Tokens/s = 173,956.5
2025-01-15 22:25:29.458 | DEBUG    | __main__:<module>:313 - Training step 17460: loss = 3.2439 | 3015.78ms | Tokens/s = 173,848.0
2025-01-15 22:25:59.625 | DEBUG    | __main__:<module>:313 - Training step 17470: loss = 3.4737 | 3017.08ms | Tokens/s = 173,773.3
2025-01-15 22:26:29.793 | DEBUG    | __main__:<module>:313 - Training step 17480: loss = 3.4064 | 3017.83ms | Tokens/s = 173,730.0
2025-01-15 22:26:59.967 | DEBUG    | __main__:<module>:313 - Training step 17490: loss = 3.1741 | 3017.27ms | Tokens/s = 173,762.3
2025-01-15 22:27:30.107 | DEBUG    | __main__:<module>:313 - Training step 17500: loss = 3.1227 | 3011.55ms | Tokens/s = 174,092.1
2025-01-15 22:28:00.228 | DEBUG    | __main__:<module>:313 - Training step 17510: loss = 2.9793 | 3009.70ms | Tokens/s = 174,199.4
2025-01-15 22:28:30.361 | DEBUG    | __main__:<module>:313 - Training step 17520: loss = 3.2973 | 3013.34ms | Tokens/s = 173,989.2
2025-01-15 22:29:00.519 | DEBUG    | __main__:<module>:313 - Training step 17530: loss = 3.3238 | 3015.68ms | Tokens/s = 173,853.8
2025-01-15 22:29:30.692 | DEBUG    | __main__:<module>:313 - Training step 17540: loss = 3.3639 | 3017.50ms | Tokens/s = 173,748.9
2025-01-15 22:30:00.832 | DEBUG    | __main__:<module>:313 - Training step 17550: loss = 3.3908 | 3010.91ms | Tokens/s = 174,129.7
2025-01-15 22:30:30.975 | DEBUG    | __main__:<module>:313 - Training step 17560: loss = 3.1006 | 3017.79ms | Tokens/s = 173,732.3
2025-01-15 22:31:01.141 | DEBUG    | __main__:<module>:313 - Training step 17570: loss = 3.3452 | 3014.19ms | Tokens/s = 173,939.9
2025-01-15 22:31:31.286 | DEBUG    | __main__:<module>:313 - Training step 17580: loss = 3.5420 | 3013.53ms | Tokens/s = 173,978.3
2025-01-15 22:32:01.417 | DEBUG    | __main__:<module>:313 - Training step 17590: loss = 3.3907 | 3014.36ms | Tokens/s = 173,930.3
2025-01-15 22:32:31.572 | DEBUG    | __main__:<module>:313 - Training step 17600: loss = 3.4369 | 3017.08ms | Tokens/s = 173,773.1
2025-01-15 22:33:01.754 | DEBUG    | __main__:<module>:313 - Training step 17610: loss = 3.2616 | 3018.43ms | Tokens/s = 173,695.8
2025-01-15 22:33:31.923 | DEBUG    | __main__:<module>:313 - Training step 17620: loss = 3.4404 | 3016.52ms | Tokens/s = 173,805.6
2025-01-15 22:34:02.087 | DEBUG    | __main__:<module>:313 - Training step 17630: loss = 3.2238 | 3016.76ms | Tokens/s = 173,791.7
2025-01-15 22:34:32.240 | DEBUG    | __main__:<module>:313 - Training step 17640: loss = 3.5217 | 3013.68ms | Tokens/s = 173,969.6
2025-01-15 22:35:02.375 | DEBUG    | __main__:<module>:313 - Training step 17650: loss = 3.2531 | 3012.24ms | Tokens/s = 174,052.6
2025-01-15 22:35:32.495 | DEBUG    | __main__:<module>:313 - Training step 17660: loss = 3.3927 | 3012.41ms | Tokens/s = 174,042.8
2025-01-15 22:36:02.645 | DEBUG    | __main__:<module>:313 - Training step 17670: loss = 3.3229 | 3015.33ms | Tokens/s = 173,874.2
2025-01-15 22:36:32.784 | DEBUG    | __main__:<module>:313 - Training step 17680: loss = 3.3124 | 3012.71ms | Tokens/s = 174,025.6
2025-01-15 22:37:02.906 | DEBUG    | __main__:<module>:313 - Training step 17690: loss = 3.4099 | 3011.04ms | Tokens/s = 174,122.1
2025-01-15 22:37:33.009 | DEBUG    | __main__:<module>:313 - Training step 17700: loss = 3.2326 | 3009.76ms | Tokens/s = 174,196.0
2025-01-15 22:38:03.145 | DEBUG    | __main__:<module>:313 - Training step 17710: loss = 3.2107 | 3013.17ms | Tokens/s = 173,998.9
2025-01-15 22:38:33.282 | DEBUG    | __main__:<module>:313 - Training step 17720: loss = 3.3704 | 3014.85ms | Tokens/s = 173,901.9
2025-01-15 22:39:03.401 | DEBUG    | __main__:<module>:313 - Training step 17730: loss = 3.3384 | 3010.50ms | Tokens/s = 174,153.0
2025-01-15 22:39:33.524 | DEBUG    | __main__:<module>:313 - Training step 17740: loss = 3.2283 | 3013.23ms | Tokens/s = 173,995.4
2025-01-15 22:40:03.678 | DEBUG    | __main__:<module>:313 - Training step 17750: loss = 3.4502 | 3016.45ms | Tokens/s = 173,809.6
2025-01-15 22:40:33.848 | DEBUG    | __main__:<module>:313 - Training step 17760: loss = 3.4607 | 3016.65ms | Tokens/s = 173,798.1
2025-01-15 22:41:03.990 | DEBUG    | __main__:<module>:313 - Training step 17770: loss = 3.3873 | 3013.63ms | Tokens/s = 173,972.4
2025-01-15 22:41:34.122 | DEBUG    | __main__:<module>:313 - Training step 17780: loss = 3.4498 | 3014.53ms | Tokens/s = 173,920.2
2025-01-15 22:42:04.253 | DEBUG    | __main__:<module>:313 - Training step 17790: loss = 3.4091 | 3012.87ms | Tokens/s = 174,016.3
2025-01-15 22:42:34.401 | DEBUG    | __main__:<module>:313 - Training step 17800: loss = 3.4001 | 3013.96ms | Tokens/s = 173,953.2
2025-01-15 22:43:04.560 | DEBUG    | __main__:<module>:313 - Training step 17810: loss = 3.5816 | 3017.21ms | Tokens/s = 173,766.0
2025-01-15 22:43:34.694 | DEBUG    | __main__:<module>:313 - Training step 17820: loss = 3.3640 | 3012.22ms | Tokens/s = 174,053.5
2025-01-15 22:44:04.819 | DEBUG    | __main__:<module>:313 - Training step 17830: loss = 3.3500 | 3013.96ms | Tokens/s = 173,953.0
2025-01-15 22:44:34.937 | DEBUG    | __main__:<module>:313 - Training step 17840: loss = 3.1648 | 3011.57ms | Tokens/s = 174,091.4
2025-01-15 22:45:05.082 | DEBUG    | __main__:<module>:313 - Training step 17850: loss = 3.4627 | 3017.94ms | Tokens/s = 173,723.6
2025-01-15 22:45:35.248 | DEBUG    | __main__:<module>:313 - Training step 17860: loss = 3.3513 | 3015.25ms | Tokens/s = 173,878.9
2025-01-15 22:46:05.416 | DEBUG    | __main__:<module>:313 - Training step 17870: loss = 3.2826 | 3015.61ms | Tokens/s = 173,857.9
2025-01-15 22:46:35.548 | DEBUG    | __main__:<module>:313 - Training step 17880: loss = 3.2714 | 3013.68ms | Tokens/s = 173,969.2
2025-01-15 22:47:05.678 | DEBUG    | __main__:<module>:313 - Training step 17890: loss = 3.1420 | 3015.77ms | Tokens/s = 173,849.1
2025-01-15 22:47:35.841 | DEBUG    | __main__:<module>:313 - Training step 17900: loss = 3.3744 | 3016.40ms | Tokens/s = 173,812.5
2025-01-15 22:48:06.020 | DEBUG    | __main__:<module>:313 - Training step 17910: loss = 3.3314 | 3019.44ms | Tokens/s = 173,637.6
2025-01-15 22:48:36.197 | DEBUG    | __main__:<module>:313 - Training step 17920: loss = 3.3233 | 3015.68ms | Tokens/s = 173,853.8
2025-01-15 22:49:06.345 | DEBUG    | __main__:<module>:313 - Training step 17930: loss = 3.3609 | 3014.12ms | Tokens/s = 173,943.7
2025-01-15 22:49:36.474 | DEBUG    | __main__:<module>:313 - Training step 17940: loss = 3.2396 | 3011.66ms | Tokens/s = 174,086.3
2025-01-15 22:50:06.614 | DEBUG    | __main__:<module>:313 - Training step 17950: loss = 3.2627 | 3014.56ms | Tokens/s = 173,918.6
2025-01-15 22:50:36.780 | DEBUG    | __main__:<module>:313 - Training step 17960: loss = 3.3487 | 3016.93ms | Tokens/s = 173,781.8
2025-01-15 22:51:06.926 | DEBUG    | __main__:<module>:313 - Training step 17970: loss = 3.4043 | 3015.46ms | Tokens/s = 173,866.9
2025-01-15 22:51:37.055 | DEBUG    | __main__:<module>:313 - Training step 17980: loss = 3.1942 | 3013.95ms | Tokens/s = 173,953.5
2025-01-15 22:52:07.207 | DEBUG    | __main__:<module>:313 - Training step 17990: loss = 3.2109 | 3018.01ms | Tokens/s = 173,719.9
2025-01-15 22:52:40.800 | INFO     | __main__:<module>:265 - Step 18,000/40,000 loss: 3.2871 (T) 3.2883 (V) | lr=6.2e-03
2025-01-15 22:52:40.801 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 22:52:54.325 | DEBUG    | __main__:<module>:313 - Training step 18000: loss = 3.3806 | 19967.75ms | Tokens/s = 26,256.7
2025-01-15 22:53:24.374 | DEBUG    | __main__:<module>:313 - Training step 18010: loss = 3.3391 | 3005.76ms | Tokens/s = 174,427.8
2025-01-15 22:53:54.475 | DEBUG    | __main__:<module>:313 - Training step 18020: loss = 3.1722 | 3013.83ms | Tokens/s = 173,960.9
2025-01-15 22:54:24.624 | DEBUG    | __main__:<module>:313 - Training step 18030: loss = 3.3865 | 3018.96ms | Tokens/s = 173,665.0
2025-01-15 22:54:54.759 | DEBUG    | __main__:<module>:313 - Training step 18040: loss = 3.1669 | 3010.92ms | Tokens/s = 174,128.8
2025-01-15 22:55:24.889 | DEBUG    | __main__:<module>:313 - Training step 18050: loss = 3.3363 | 3011.88ms | Tokens/s = 174,073.3
2025-01-15 22:55:55.046 | DEBUG    | __main__:<module>:313 - Training step 18060: loss = 3.3401 | 3015.29ms | Tokens/s = 173,876.6
2025-01-15 22:56:25.196 | DEBUG    | __main__:<module>:313 - Training step 18070: loss = 3.3110 | 3014.05ms | Tokens/s = 173,948.0
2025-01-15 22:56:55.339 | DEBUG    | __main__:<module>:313 - Training step 18080: loss = 3.2667 | 3014.16ms | Tokens/s = 173,941.8
2025-01-15 22:57:25.473 | DEBUG    | __main__:<module>:313 - Training step 18090: loss = 3.4426 | 3016.96ms | Tokens/s = 173,780.3
2025-01-15 22:57:55.596 | DEBUG    | __main__:<module>:313 - Training step 18100: loss = 3.1863 | 3013.17ms | Tokens/s = 173,998.9
2025-01-15 22:58:25.732 | DEBUG    | __main__:<module>:313 - Training step 18110: loss = 3.2427 | 3013.90ms | Tokens/s = 173,956.5
2025-01-15 22:58:55.844 | DEBUG    | __main__:<module>:313 - Training step 18120: loss = 3.2067 | 3010.41ms | Tokens/s = 174,158.6
2025-01-15 22:59:25.966 | DEBUG    | __main__:<module>:313 - Training step 18130: loss = 3.2277 | 3012.51ms | Tokens/s = 174,037.1
2025-01-15 22:59:56.131 | DEBUG    | __main__:<module>:313 - Training step 18140: loss = 3.4001 | 3018.19ms | Tokens/s = 173,709.1
2025-01-15 23:00:26.290 | DEBUG    | __main__:<module>:313 - Training step 18150: loss = 3.3163 | 3014.95ms | Tokens/s = 173,896.0
2025-01-15 23:00:56.424 | DEBUG    | __main__:<module>:313 - Training step 18160: loss = 3.3854 | 3012.34ms | Tokens/s = 174,046.6
2025-01-15 23:01:26.575 | DEBUG    | __main__:<module>:313 - Training step 18170: loss = 3.1978 | 3018.39ms | Tokens/s = 173,698.2
2025-01-15 23:01:56.750 | DEBUG    | __main__:<module>:313 - Training step 18180: loss = 3.3325 | 3018.72ms | Tokens/s = 173,678.7
2025-01-15 23:02:26.897 | DEBUG    | __main__:<module>:313 - Training step 18190: loss = 3.4542 | 3015.03ms | Tokens/s = 173,891.6
2025-01-15 23:02:57.058 | DEBUG    | __main__:<module>:313 - Training step 18200: loss = 3.2175 | 3014.32ms | Tokens/s = 173,932.4
2025-01-15 23:03:27.237 | DEBUG    | __main__:<module>:313 - Training step 18210: loss = 3.3389 | 3015.84ms | Tokens/s = 173,845.0
2025-01-15 23:03:57.392 | DEBUG    | __main__:<module>:313 - Training step 18220: loss = 3.2091 | 3014.65ms | Tokens/s = 173,913.4
2025-01-15 23:04:27.530 | DEBUG    | __main__:<module>:313 - Training step 18230: loss = 3.3526 | 3012.54ms | Tokens/s = 174,035.2
2025-01-15 23:04:57.648 | DEBUG    | __main__:<module>:313 - Training step 18240: loss = 3.1046 | 3011.37ms | Tokens/s = 174,103.1
2025-01-15 23:05:27.803 | DEBUG    | __main__:<module>:313 - Training step 18250: loss = 3.4775 | 3017.75ms | Tokens/s = 173,735.0
2025-01-15 23:05:57.983 | DEBUG    | __main__:<module>:313 - Training step 18260: loss = 3.2517 | 3018.01ms | Tokens/s = 173,719.6
2025-01-15 23:06:28.142 | DEBUG    | __main__:<module>:313 - Training step 18270: loss = 3.4262 | 3014.12ms | Tokens/s = 173,944.0
2025-01-15 23:06:58.269 | DEBUG    | __main__:<module>:313 - Training step 18280: loss = 3.2213 | 3013.02ms | Tokens/s = 174,007.3
2025-01-15 23:07:28.393 | DEBUG    | __main__:<module>:313 - Training step 18290: loss = 3.2956 | 3012.76ms | Tokens/s = 174,022.3
2025-01-15 23:07:58.543 | DEBUG    | __main__:<module>:313 - Training step 18300: loss = 3.0026 | 3013.20ms | Tokens/s = 173,997.1
2025-01-15 23:08:28.688 | DEBUG    | __main__:<module>:313 - Training step 18310: loss = 3.2345 | 3012.78ms | Tokens/s = 174,021.6
2025-01-15 23:08:58.802 | DEBUG    | __main__:<module>:313 - Training step 18320: loss = 3.4529 | 3009.62ms | Tokens/s = 174,204.0
2025-01-15 23:09:28.919 | DEBUG    | __main__:<module>:313 - Training step 18330: loss = 3.2906 | 3013.32ms | Tokens/s = 173,990.3
2025-01-15 23:09:59.070 | DEBUG    | __main__:<module>:313 - Training step 18340: loss = 3.4733 | 3016.59ms | Tokens/s = 173,801.8
2025-01-15 23:10:29.224 | DEBUG    | __main__:<module>:313 - Training step 18350: loss = 3.3171 | 3013.73ms | Tokens/s = 173,966.3
2025-01-15 23:10:59.354 | DEBUG    | __main__:<module>:313 - Training step 18360: loss = 3.3169 | 3012.13ms | Tokens/s = 174,058.8
2025-01-15 23:11:29.477 | DEBUG    | __main__:<module>:313 - Training step 18370: loss = 3.0781 | 3012.21ms | Tokens/s = 174,054.1
2025-01-15 23:11:59.594 | DEBUG    | __main__:<module>:313 - Training step 18380: loss = 3.2766 | 3009.98ms | Tokens/s = 174,183.1
2025-01-15 23:12:29.704 | DEBUG    | __main__:<module>:313 - Training step 18390: loss = 3.1744 | 3011.94ms | Tokens/s = 174,070.0
2025-01-15 23:12:59.843 | DEBUG    | __main__:<module>:313 - Training step 18400: loss = 3.3848 | 3015.02ms | Tokens/s = 173,892.1
2025-01-15 23:13:29.965 | DEBUG    | __main__:<module>:313 - Training step 18410: loss = 3.4213 | 3012.16ms | Tokens/s = 174,057.1
2025-01-15 23:14:00.088 | DEBUG    | __main__:<module>:313 - Training step 18420: loss = 3.3408 | 3012.96ms | Tokens/s = 174,011.1
2025-01-15 23:14:30.234 | DEBUG    | __main__:<module>:313 - Training step 18430: loss = 3.2105 | 3014.71ms | Tokens/s = 173,909.8
2025-01-15 23:15:00.388 | DEBUG    | __main__:<module>:313 - Training step 18440: loss = 3.3152 | 3015.70ms | Tokens/s = 173,853.0
2025-01-15 23:15:30.556 | DEBUG    | __main__:<module>:313 - Training step 18450: loss = 3.2383 | 3018.22ms | Tokens/s = 173,707.6
2025-01-15 23:16:00.695 | DEBUG    | __main__:<module>:313 - Training step 18460: loss = 3.2357 | 3013.98ms | Tokens/s = 173,952.1
2025-01-15 23:16:30.850 | DEBUG    | __main__:<module>:313 - Training step 18470: loss = 3.3923 | 3020.05ms | Tokens/s = 173,602.4
2025-01-15 23:17:01.024 | DEBUG    | __main__:<module>:313 - Training step 18480: loss = 3.3012 | 3018.59ms | Tokens/s = 173,686.1
2025-01-15 23:17:31.175 | DEBUG    | __main__:<module>:313 - Training step 18490: loss = 3.1403 | 3014.89ms | Tokens/s = 173,899.4
2025-01-15 23:18:01.302 | DEBUG    | __main__:<module>:313 - Training step 18500: loss = 3.2544 | 3010.29ms | Tokens/s = 174,165.6
2025-01-15 23:18:31.423 | DEBUG    | __main__:<module>:313 - Training step 18510: loss = 3.0945 | 3013.62ms | Tokens/s = 173,972.6
2025-01-15 23:19:01.593 | DEBUG    | __main__:<module>:313 - Training step 18520: loss = 3.3898 | 3017.42ms | Tokens/s = 173,753.7
2025-01-15 23:19:31.767 | DEBUG    | __main__:<module>:313 - Training step 18530: loss = 3.2388 | 3018.02ms | Tokens/s = 173,719.2
2025-01-15 23:20:01.914 | DEBUG    | __main__:<module>:313 - Training step 18540: loss = 3.3659 | 3012.14ms | Tokens/s = 174,058.4
2025-01-15 23:20:32.048 | DEBUG    | __main__:<module>:313 - Training step 18550: loss = 3.3374 | 3013.62ms | Tokens/s = 173,972.8
2025-01-15 23:21:02.209 | DEBUG    | __main__:<module>:313 - Training step 18560: loss = 3.2101 | 3016.17ms | Tokens/s = 173,826.0
2025-01-15 23:21:32.359 | DEBUG    | __main__:<module>:313 - Training step 18570: loss = 3.2310 | 3012.75ms | Tokens/s = 174,023.2
2025-01-15 23:22:02.478 | DEBUG    | __main__:<module>:313 - Training step 18580: loss = 3.3934 | 3008.87ms | Tokens/s = 174,247.5
2025-01-15 23:22:32.595 | DEBUG    | __main__:<module>:313 - Training step 18590: loss = 3.2048 | 3011.20ms | Tokens/s = 174,112.4
2025-01-15 23:23:02.747 | DEBUG    | __main__:<module>:313 - Training step 18600: loss = 3.3048 | 3014.63ms | Tokens/s = 173,914.6
2025-01-15 23:23:32.916 | DEBUG    | __main__:<module>:313 - Training step 18610: loss = 3.2884 | 3015.67ms | Tokens/s = 173,854.7
2025-01-15 23:24:03.052 | DEBUG    | __main__:<module>:313 - Training step 18620: loss = 3.2693 | 3011.33ms | Tokens/s = 174,105.2
2025-01-15 23:24:33.173 | DEBUG    | __main__:<module>:313 - Training step 18630: loss = 3.1197 | 3011.59ms | Tokens/s = 174,090.2
2025-01-15 23:25:03.281 | DEBUG    | __main__:<module>:313 - Training step 18640: loss = 3.2076 | 3011.30ms | Tokens/s = 174,107.0
2025-01-15 23:25:33.387 | DEBUG    | __main__:<module>:313 - Training step 18650: loss = 3.2843 | 3009.52ms | Tokens/s = 174,209.6
2025-01-15 23:26:03.488 | DEBUG    | __main__:<module>:313 - Training step 18660: loss = 3.2182 | 3009.25ms | Tokens/s = 174,225.7
2025-01-15 23:26:33.613 | DEBUG    | __main__:<module>:313 - Training step 18670: loss = 3.2256 | 3014.54ms | Tokens/s = 173,920.0
2025-01-15 23:27:03.768 | DEBUG    | __main__:<module>:313 - Training step 18680: loss = 3.0597 | 3015.31ms | Tokens/s = 173,875.5
2025-01-15 23:27:33.931 | DEBUG    | __main__:<module>:313 - Training step 18690: loss = 3.3452 | 3015.64ms | Tokens/s = 173,856.4
2025-01-15 23:28:04.068 | DEBUG    | __main__:<module>:313 - Training step 18700: loss = 3.3544 | 3012.05ms | Tokens/s = 174,063.7
2025-01-15 23:28:34.218 | DEBUG    | __main__:<module>:313 - Training step 18710: loss = 3.3165 | 3014.73ms | Tokens/s = 173,908.5
2025-01-15 23:29:04.360 | DEBUG    | __main__:<module>:313 - Training step 18720: loss = 3.3649 | 3012.01ms | Tokens/s = 174,065.8
2025-01-15 23:29:34.482 | DEBUG    | __main__:<module>:313 - Training step 18730: loss = 3.1002 | 3014.30ms | Tokens/s = 173,933.5
2025-01-15 23:30:04.612 | DEBUG    | __main__:<module>:313 - Training step 18740: loss = 3.0496 | 3013.55ms | Tokens/s = 173,976.8
2025-01-15 23:30:34.766 | DEBUG    | __main__:<module>:313 - Training step 18750: loss = 3.3685 | 3016.21ms | Tokens/s = 173,823.5
2025-01-15 23:31:04.930 | DEBUG    | __main__:<module>:313 - Training step 18760: loss = 3.2607 | 3014.98ms | Tokens/s = 173,894.6
2025-01-15 23:31:35.054 | DEBUG    | __main__:<module>:313 - Training step 18770: loss = 3.3671 | 3010.90ms | Tokens/s = 174,130.0
2025-01-15 23:32:05.181 | DEBUG    | __main__:<module>:313 - Training step 18780: loss = 3.3001 | 3013.23ms | Tokens/s = 173,995.6
2025-01-15 23:32:35.342 | DEBUG    | __main__:<module>:313 - Training step 18790: loss = 3.2299 | 3017.04ms | Tokens/s = 173,775.7
2025-01-15 23:33:05.529 | DEBUG    | __main__:<module>:313 - Training step 18800: loss = 3.3054 | 3018.33ms | Tokens/s = 173,701.6
2025-01-15 23:33:35.696 | DEBUG    | __main__:<module>:313 - Training step 18810: loss = 3.3129 | 3012.55ms | Tokens/s = 174,034.4
2025-01-15 23:34:05.813 | DEBUG    | __main__:<module>:313 - Training step 18820: loss = 3.1776 | 3011.12ms | Tokens/s = 174,117.4
2025-01-15 23:34:35.951 | DEBUG    | __main__:<module>:313 - Training step 18830: loss = 3.3669 | 3014.35ms | Tokens/s = 173,930.5
2025-01-15 23:35:06.119 | DEBUG    | __main__:<module>:313 - Training step 18840: loss = 3.3435 | 3017.03ms | Tokens/s = 173,776.0
2025-01-15 23:35:36.284 | DEBUG    | __main__:<module>:313 - Training step 18850: loss = 3.1940 | 3015.99ms | Tokens/s = 173,836.1
2025-01-15 23:36:06.415 | DEBUG    | __main__:<module>:313 - Training step 18860: loss = 3.3009 | 3011.25ms | Tokens/s = 174,109.9
2025-01-15 23:36:36.550 | DEBUG    | __main__:<module>:313 - Training step 18870: loss = 3.2629 | 3016.85ms | Tokens/s = 173,786.6
2025-01-15 23:37:06.711 | DEBUG    | __main__:<module>:313 - Training step 18880: loss = 3.3110 | 3016.57ms | Tokens/s = 173,802.9
2025-01-15 23:37:36.854 | DEBUG    | __main__:<module>:313 - Training step 18890: loss = 3.1476 | 3012.25ms | Tokens/s = 174,052.1
2025-01-15 23:38:06.976 | DEBUG    | __main__:<module>:313 - Training step 18900: loss = 3.2905 | 3013.18ms | Tokens/s = 173,998.1
2025-01-15 23:38:37.119 | DEBUG    | __main__:<module>:313 - Training step 18910: loss = 3.3290 | 3016.70ms | Tokens/s = 173,795.2
2025-01-15 23:39:07.291 | DEBUG    | __main__:<module>:313 - Training step 18920: loss = 3.3734 | 3017.80ms | Tokens/s = 173,732.1
2025-01-15 23:39:37.454 | DEBUG    | __main__:<module>:313 - Training step 18930: loss = 3.2082 | 3013.25ms | Tokens/s = 173,994.4
2025-01-15 23:40:07.588 | DEBUG    | __main__:<module>:313 - Training step 18940: loss = 3.3630 | 3011.44ms | Tokens/s = 174,099.0
2025-01-15 23:40:37.709 | DEBUG    | __main__:<module>:313 - Training step 18950: loss = 3.2450 | 3011.70ms | Tokens/s = 174,083.8
2025-01-15 23:41:07.828 | DEBUG    | __main__:<module>:313 - Training step 18960: loss = 3.0957 | 3014.09ms | Tokens/s = 173,945.8
2025-01-15 23:41:37.961 | DEBUG    | __main__:<module>:313 - Training step 18970: loss = 3.2981 | 3012.83ms | Tokens/s = 174,018.3
2025-01-15 23:42:08.111 | DEBUG    | __main__:<module>:313 - Training step 18980: loss = 3.2269 | 3016.11ms | Tokens/s = 173,829.5
2025-01-15 23:42:38.287 | DEBUG    | __main__:<module>:313 - Training step 18990: loss = 3.4507 | 3015.59ms | Tokens/s = 173,859.0
2025-01-15 23:43:11.879 | INFO     | __main__:<module>:265 - Step 19,000/40,000 loss: 3.2875 (T) 3.2877 (V) | lr=5.8e-03
2025-01-15 23:43:11.880 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-15 23:43:25.227 | DEBUG    | __main__:<module>:313 - Training step 19000: loss = 3.2374 | 19787.43ms | Tokens/s = 26,496.0
2025-01-15 23:43:55.280 | DEBUG    | __main__:<module>:313 - Training step 19010: loss = 3.4260 | 3005.12ms | Tokens/s = 174,464.8
2025-01-15 23:44:25.371 | DEBUG    | __main__:<module>:313 - Training step 19020: loss = 3.1460 | 3013.43ms | Tokens/s = 173,983.8
2025-01-15 23:44:55.513 | DEBUG    | __main__:<module>:313 - Training step 19030: loss = 3.2899 | 3015.30ms | Tokens/s = 173,875.9
2025-01-15 23:45:25.670 | DEBUG    | __main__:<module>:313 - Training step 19040: loss = 3.4895 | 3018.16ms | Tokens/s = 173,711.3
2025-01-15 23:45:55.799 | DEBUG    | __main__:<module>:313 - Training step 19050: loss = 3.1532 | 3011.29ms | Tokens/s = 174,107.4
2025-01-15 23:46:25.916 | DEBUG    | __main__:<module>:313 - Training step 19060: loss = 3.3644 | 3011.48ms | Tokens/s = 174,096.7
2025-01-15 23:46:56.032 | DEBUG    | __main__:<module>:313 - Training step 19070: loss = 3.1531 | 3010.14ms | Tokens/s = 174,174.2
2025-01-15 23:47:26.124 | DEBUG    | __main__:<module>:313 - Training step 19080: loss = 3.3401 | 3010.58ms | Tokens/s = 174,148.2
2025-01-15 23:47:56.251 | DEBUG    | __main__:<module>:313 - Training step 19090: loss = 3.3370 | 3015.83ms | Tokens/s = 173,845.5
2025-01-15 23:48:26.390 | DEBUG    | __main__:<module>:313 - Training step 19100: loss = 3.5245 | 3013.97ms | Tokens/s = 173,952.8
2025-01-15 23:48:56.532 | DEBUG    | __main__:<module>:313 - Training step 19110: loss = 3.4380 | 3014.69ms | Tokens/s = 173,911.3
2025-01-15 23:49:26.655 | DEBUG    | __main__:<module>:313 - Training step 19120: loss = 3.3325 | 3012.57ms | Tokens/s = 174,033.7
2025-01-15 23:49:56.783 | DEBUG    | __main__:<module>:313 - Training step 19130: loss = 3.2884 | 3013.87ms | Tokens/s = 173,958.3
2025-01-15 23:50:26.942 | DEBUG    | __main__:<module>:313 - Training step 19140: loss = 3.2458 | 3015.99ms | Tokens/s = 173,836.4
2025-01-15 23:50:57.112 | DEBUG    | __main__:<module>:313 - Training step 19150: loss = 3.2495 | 3018.11ms | Tokens/s = 173,714.3
2025-01-15 23:51:27.259 | DEBUG    | __main__:<module>:313 - Training step 19160: loss = 3.2104 | 3013.30ms | Tokens/s = 173,991.3
2025-01-15 23:51:57.377 | DEBUG    | __main__:<module>:313 - Training step 19170: loss = 3.1943 | 3012.43ms | Tokens/s = 174,041.4
2025-01-15 23:52:27.499 | DEBUG    | __main__:<module>:313 - Training step 19180: loss = 3.3222 | 3012.31ms | Tokens/s = 174,048.5
2025-01-15 23:52:57.655 | DEBUG    | __main__:<module>:313 - Training step 19190: loss = 3.1718 | 3016.25ms | Tokens/s = 173,820.9
2025-01-15 23:53:27.821 | DEBUG    | __main__:<module>:313 - Training step 19200: loss = 3.2984 | 3018.94ms | Tokens/s = 173,666.2
2025-01-15 23:53:57.972 | DEBUG    | __main__:<module>:313 - Training step 19210: loss = 3.2840 | 3012.80ms | Tokens/s = 174,020.3
2025-01-15 23:54:28.109 | DEBUG    | __main__:<module>:313 - Training step 19220: loss = 3.3150 | 3013.32ms | Tokens/s = 173,990.3
2025-01-15 23:54:58.228 | DEBUG    | __main__:<module>:313 - Training step 19230: loss = 3.1489 | 3012.38ms | Tokens/s = 174,044.7
2025-01-15 23:55:28.356 | DEBUG    | __main__:<module>:313 - Training step 19240: loss = 3.2497 | 3012.55ms | Tokens/s = 174,034.5
2025-01-15 23:55:58.511 | DEBUG    | __main__:<module>:313 - Training step 19250: loss = 3.3072 | 3012.98ms | Tokens/s = 174,009.7
2025-01-15 23:56:28.680 | DEBUG    | __main__:<module>:313 - Training step 19260: loss = 3.0790 | 3019.12ms | Tokens/s = 173,655.7
2025-01-15 23:56:58.826 | DEBUG    | __main__:<module>:313 - Training step 19270: loss = 3.4102 | 3012.99ms | Tokens/s = 174,009.4
2025-01-15 23:57:28.952 | DEBUG    | __main__:<module>:313 - Training step 19280: loss = 3.2159 | 3013.75ms | Tokens/s = 173,965.1
2025-01-15 23:57:59.082 | DEBUG    | __main__:<module>:313 - Training step 19290: loss = 3.3354 | 3017.67ms | Tokens/s = 173,739.6
2025-01-15 23:58:29.232 | DEBUG    | __main__:<module>:313 - Training step 19300: loss = 3.2245 | 3012.62ms | Tokens/s = 174,030.5
2025-01-15 23:58:59.403 | DEBUG    | __main__:<module>:313 - Training step 19310: loss = 3.1621 | 3018.53ms | Tokens/s = 173,689.6
2025-01-15 23:59:29.573 | DEBUG    | __main__:<module>:313 - Training step 19320: loss = 3.2172 | 3015.17ms | Tokens/s = 173,883.2
2025-01-15 23:59:59.738 | DEBUG    | __main__:<module>:313 - Training step 19330: loss = 3.3126 | 3014.24ms | Tokens/s = 173,937.2
2025-01-16 00:00:29.880 | DEBUG    | __main__:<module>:313 - Training step 19340: loss = 3.3446 | 3011.65ms | Tokens/s = 174,086.8
2025-01-16 00:01:00.005 | DEBUG    | __main__:<module>:313 - Training step 19350: loss = 3.1639 | 3013.85ms | Tokens/s = 173,959.7
2025-01-16 00:01:30.155 | DEBUG    | __main__:<module>:313 - Training step 19360: loss = 3.3523 | 3014.69ms | Tokens/s = 173,910.8
2025-01-16 00:02:00.304 | DEBUG    | __main__:<module>:313 - Training step 19370: loss = 3.3057 | 3013.66ms | Tokens/s = 173,970.6
2025-01-16 00:02:30.421 | DEBUG    | __main__:<module>:313 - Training step 19380: loss = 3.1565 | 3009.07ms | Tokens/s = 174,235.6
2025-01-16 00:03:00.544 | DEBUG    | __main__:<module>:313 - Training step 19390: loss = 3.1581 | 3014.35ms | Tokens/s = 173,930.4
2025-01-16 00:03:30.690 | DEBUG    | __main__:<module>:313 - Training step 19400: loss = 3.3342 | 3015.42ms | Tokens/s = 173,869.0
2025-01-16 00:04:00.863 | DEBUG    | __main__:<module>:313 - Training step 19410: loss = 3.2923 | 3017.99ms | Tokens/s = 173,721.1
2025-01-16 00:04:31.000 | DEBUG    | __main__:<module>:313 - Training step 19420: loss = 3.2498 | 3011.85ms | Tokens/s = 174,075.1
2025-01-16 00:05:01.116 | DEBUG    | __main__:<module>:313 - Training step 19430: loss = 3.4821 | 3010.83ms | Tokens/s = 174,134.0
2025-01-16 00:05:31.263 | DEBUG    | __main__:<module>:313 - Training step 19440: loss = 3.2263 | 3015.29ms | Tokens/s = 173,876.3
2025-01-16 00:06:01.431 | DEBUG    | __main__:<module>:313 - Training step 19450: loss = 3.3735 | 3019.28ms | Tokens/s = 173,646.9
2025-01-16 00:06:31.585 | DEBUG    | __main__:<module>:313 - Training step 19460: loss = 3.2749 | 3011.17ms | Tokens/s = 174,114.4
2025-01-16 00:07:01.710 | DEBUG    | __main__:<module>:313 - Training step 19470: loss = 3.2848 | 3011.42ms | Tokens/s = 174,099.8
2025-01-16 00:07:31.841 | DEBUG    | __main__:<module>:313 - Training step 19480: loss = 3.3634 | 3013.67ms | Tokens/s = 173,970.1
2025-01-16 00:08:01.994 | DEBUG    | __main__:<module>:313 - Training step 19490: loss = 3.1164 | 3017.23ms | Tokens/s = 173,764.9
2025-01-16 00:08:32.158 | DEBUG    | __main__:<module>:313 - Training step 19500: loss = 3.2933 | 3014.14ms | Tokens/s = 173,942.8
2025-01-16 00:09:02.329 | DEBUG    | __main__:<module>:313 - Training step 19510: loss = 3.2645 | 3015.82ms | Tokens/s = 173,845.8
2025-01-16 00:09:32.492 | DEBUG    | __main__:<module>:313 - Training step 19520: loss = 3.1780 | 3014.22ms | Tokens/s = 173,938.4
2025-01-16 00:10:02.638 | DEBUG    | __main__:<module>:313 - Training step 19530: loss = 3.3625 | 3011.80ms | Tokens/s = 174,078.0
2025-01-16 00:10:32.798 | DEBUG    | __main__:<module>:313 - Training step 19540: loss = 3.3643 | 3016.34ms | Tokens/s = 173,816.1
2025-01-16 00:11:02.971 | DEBUG    | __main__:<module>:313 - Training step 19550: loss = 3.2557 | 3015.93ms | Tokens/s = 173,839.7
2025-01-16 00:11:33.162 | DEBUG    | __main__:<module>:313 - Training step 19560: loss = 3.1374 | 3018.49ms | Tokens/s = 173,691.9
2025-01-16 00:12:03.335 | DEBUG    | __main__:<module>:313 - Training step 19570: loss = 3.1792 | 3015.25ms | Tokens/s = 173,878.8
2025-01-16 00:12:33.469 | DEBUG    | __main__:<module>:313 - Training step 19580: loss = 3.2233 | 3013.72ms | Tokens/s = 173,966.8
2025-01-16 00:13:03.600 | DEBUG    | __main__:<module>:313 - Training step 19590: loss = 3.1829 | 3014.14ms | Tokens/s = 173,943.0
2025-01-16 00:13:33.765 | DEBUG    | __main__:<module>:313 - Training step 19600: loss = 3.2953 | 3015.53ms | Tokens/s = 173,862.7
2025-01-16 00:14:03.926 | DEBUG    | __main__:<module>:313 - Training step 19610: loss = 3.3062 | 3016.16ms | Tokens/s = 173,826.4
2025-01-16 00:14:34.056 | DEBUG    | __main__:<module>:313 - Training step 19620: loss = 3.3368 | 3018.39ms | Tokens/s = 173,698.0
2025-01-16 00:15:04.208 | DEBUG    | __main__:<module>:313 - Training step 19630: loss = 3.3304 | 3016.10ms | Tokens/s = 173,829.6
2025-01-16 00:15:34.377 | DEBUG    | __main__:<module>:313 - Training step 19640: loss = 3.2685 | 3015.49ms | Tokens/s = 173,865.0
2025-01-16 00:16:04.531 | DEBUG    | __main__:<module>:313 - Training step 19650: loss = 3.2767 | 3015.77ms | Tokens/s = 173,848.8
2025-01-16 00:16:34.651 | DEBUG    | __main__:<module>:313 - Training step 19660: loss = 3.2541 | 3012.45ms | Tokens/s = 174,040.4
2025-01-16 00:17:04.764 | DEBUG    | __main__:<module>:313 - Training step 19670: loss = 3.2914 | 3012.66ms | Tokens/s = 174,028.3
2025-01-16 00:17:34.899 | DEBUG    | __main__:<module>:313 - Training step 19680: loss = 3.0709 | 3015.03ms | Tokens/s = 173,891.7
2025-01-16 00:18:05.054 | DEBUG    | __main__:<module>:313 - Training step 19690: loss = 3.2194 | 3016.48ms | Tokens/s = 173,808.0
2025-01-16 00:18:35.230 | DEBUG    | __main__:<module>:313 - Training step 19700: loss = 3.4082 | 3017.31ms | Tokens/s = 173,760.1
2025-01-16 00:19:05.409 | DEBUG    | __main__:<module>:313 - Training step 19710: loss = 3.1707 | 3018.66ms | Tokens/s = 173,682.5
2025-01-16 00:19:35.549 | DEBUG    | __main__:<module>:313 - Training step 19720: loss = 3.3402 | 3012.85ms | Tokens/s = 174,017.2
2025-01-16 00:20:05.671 | DEBUG    | __main__:<module>:313 - Training step 19730: loss = 3.0905 | 3012.27ms | Tokens/s = 174,050.6
2025-01-16 00:20:35.820 | DEBUG    | __main__:<module>:313 - Training step 19740: loss = 3.2700 | 3015.57ms | Tokens/s = 173,860.1
2025-01-16 00:21:05.981 | DEBUG    | __main__:<module>:313 - Training step 19750: loss = 3.3805 | 3019.61ms | Tokens/s = 173,627.6
2025-01-16 00:21:36.153 | DEBUG    | __main__:<module>:313 - Training step 19760: loss = 3.2601 | 3019.47ms | Tokens/s = 173,635.9
2025-01-16 00:22:06.287 | DEBUG    | __main__:<module>:313 - Training step 19770: loss = 3.3461 | 3013.22ms | Tokens/s = 173,995.7
2025-01-16 00:22:36.412 | DEBUG    | __main__:<module>:313 - Training step 19780: loss = 3.1423 | 3014.19ms | Tokens/s = 173,940.1
2025-01-16 00:23:06.533 | DEBUG    | __main__:<module>:313 - Training step 19790: loss = 3.2792 | 3013.02ms | Tokens/s = 174,007.5
2025-01-16 00:23:36.672 | DEBUG    | __main__:<module>:313 - Training step 19800: loss = 3.3684 | 3013.92ms | Tokens/s = 173,955.3
2025-01-16 00:24:06.827 | DEBUG    | __main__:<module>:313 - Training step 19810: loss = 3.4391 | 3014.23ms | Tokens/s = 173,937.7
2025-01-16 00:24:36.975 | DEBUG    | __main__:<module>:313 - Training step 19820: loss = 3.2573 | 3012.81ms | Tokens/s = 174,019.7
2025-01-16 00:25:07.110 | DEBUG    | __main__:<module>:313 - Training step 19830: loss = 3.2989 | 3011.00ms | Tokens/s = 174,124.2
2025-01-16 00:25:37.233 | DEBUG    | __main__:<module>:313 - Training step 19840: loss = 3.2189 | 3013.31ms | Tokens/s = 173,990.5
2025-01-16 00:26:07.376 | DEBUG    | __main__:<module>:313 - Training step 19850: loss = 3.2992 | 3013.92ms | Tokens/s = 173,955.2
2025-01-16 00:26:37.536 | DEBUG    | __main__:<module>:313 - Training step 19860: loss = 3.3127 | 3013.97ms | Tokens/s = 173,952.4
2025-01-16 00:27:07.661 | DEBUG    | __main__:<module>:313 - Training step 19870: loss = 3.1883 | 3012.51ms | Tokens/s = 174,036.9
2025-01-16 00:27:37.775 | DEBUG    | __main__:<module>:313 - Training step 19880: loss = 3.2986 | 3009.88ms | Tokens/s = 174,189.1
2025-01-16 00:28:07.898 | DEBUG    | __main__:<module>:313 - Training step 19890: loss = 3.1855 | 3014.36ms | Tokens/s = 173,930.1
2025-01-16 00:28:38.050 | DEBUG    | __main__:<module>:313 - Training step 19900: loss = 3.1914 | 3015.94ms | Tokens/s = 173,839.0
2025-01-16 00:29:08.203 | DEBUG    | __main__:<module>:313 - Training step 19910: loss = 3.3339 | 3012.88ms | Tokens/s = 174,015.7
2025-01-16 00:29:38.331 | DEBUG    | __main__:<module>:313 - Training step 19920: loss = 3.3110 | 3013.15ms | Tokens/s = 173,999.9
2025-01-16 00:30:08.440 | DEBUG    | __main__:<module>:313 - Training step 19930: loss = 3.3709 | 3009.92ms | Tokens/s = 174,186.9
2025-01-16 00:30:38.549 | DEBUG    | __main__:<module>:313 - Training step 19940: loss = 3.0717 | 3009.00ms | Tokens/s = 174,240.0
2025-01-16 00:31:08.667 | DEBUG    | __main__:<module>:313 - Training step 19950: loss = 3.3600 | 3013.05ms | Tokens/s = 174,005.9
2025-01-16 00:31:38.813 | DEBUG    | __main__:<module>:313 - Training step 19960: loss = 3.3732 | 3014.74ms | Tokens/s = 173,908.3
2025-01-16 00:32:08.970 | DEBUG    | __main__:<module>:313 - Training step 19970: loss = 3.2108 | 3015.28ms | Tokens/s = 173,877.1
2025-01-16 00:32:39.109 | DEBUG    | __main__:<module>:313 - Training step 19980: loss = 3.1905 | 3012.71ms | Tokens/s = 174,025.2
2025-01-16 00:33:09.242 | DEBUG    | __main__:<module>:313 - Training step 19990: loss = 3.1691 | 3014.15ms | Tokens/s = 173,942.1
2025-01-16 00:33:42.823 | INFO     | __main__:<module>:265 - Step 20,000/40,000 loss: 3.2645 (T) 3.2716 (V) | lr=5.4e-03
2025-01-16 00:33:42.824 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 00:33:56.514 | DEBUG    | __main__:<module>:313 - Training step 20000: loss = 3.2677 | 20135.29ms | Tokens/s = 26,038.3
2025-01-16 00:34:26.584 | DEBUG    | __main__:<module>:313 - Training step 20010: loss = 3.3378 | 3007.37ms | Tokens/s = 174,334.6
2025-01-16 00:34:56.685 | DEBUG    | __main__:<module>:313 - Training step 20020: loss = 3.2024 | 3012.07ms | Tokens/s = 174,062.2
2025-01-16 00:35:26.818 | DEBUG    | __main__:<module>:313 - Training step 20030: loss = 3.2350 | 3014.54ms | Tokens/s = 173,919.6
2025-01-16 00:35:56.982 | DEBUG    | __main__:<module>:313 - Training step 20040: loss = 3.1910 | 3018.29ms | Tokens/s = 173,703.6
2025-01-16 00:36:27.160 | DEBUG    | __main__:<module>:313 - Training step 20050: loss = 3.1867 | 3016.33ms | Tokens/s = 173,816.6
2025-01-16 00:36:57.321 | DEBUG    | __main__:<module>:313 - Training step 20060: loss = 3.1725 | 3015.73ms | Tokens/s = 173,851.3
2025-01-16 00:37:27.455 | DEBUG    | __main__:<module>:313 - Training step 20070: loss = 3.2305 | 3011.43ms | Tokens/s = 174,099.1
2025-01-16 00:37:57.591 | DEBUG    | __main__:<module>:313 - Training step 20080: loss = 3.1685 | 3017.74ms | Tokens/s = 173,735.1
2025-01-16 00:38:27.759 | DEBUG    | __main__:<module>:313 - Training step 20090: loss = 3.3605 | 3018.94ms | Tokens/s = 173,666.0
2025-01-16 00:38:57.924 | DEBUG    | __main__:<module>:313 - Training step 20100: loss = 3.3769 | 3013.50ms | Tokens/s = 173,979.8
2025-01-16 00:39:28.067 | DEBUG    | __main__:<module>:313 - Training step 20110: loss = 3.2629 | 3012.55ms | Tokens/s = 174,034.8
2025-01-16 00:39:58.206 | DEBUG    | __main__:<module>:313 - Training step 20120: loss = 3.2345 | 3015.00ms | Tokens/s = 173,893.5
2025-01-16 00:40:28.344 | DEBUG    | __main__:<module>:313 - Training step 20130: loss = 3.2528 | 3013.47ms | Tokens/s = 173,981.5
2025-01-16 00:40:58.469 | DEBUG    | __main__:<module>:313 - Training step 20140: loss = 3.2315 | 3012.28ms | Tokens/s = 174,050.0
2025-01-16 00:41:28.611 | DEBUG    | __main__:<module>:313 - Training step 20150: loss = 3.4206 | 3013.80ms | Tokens/s = 173,962.2
2025-01-16 00:41:58.781 | DEBUG    | __main__:<module>:313 - Training step 20160: loss = 3.3489 | 3014.83ms | Tokens/s = 173,902.8
2025-01-16 00:42:28.934 | DEBUG    | __main__:<module>:313 - Training step 20170: loss = 3.3417 | 3014.77ms | Tokens/s = 173,906.7
2025-01-16 00:42:59.063 | DEBUG    | __main__:<module>:313 - Training step 20180: loss = 3.2137 | 3011.54ms | Tokens/s = 174,093.0
2025-01-16 00:43:29.182 | DEBUG    | __main__:<module>:313 - Training step 20190: loss = 3.2655 | 3012.83ms | Tokens/s = 174,018.5
2025-01-16 00:43:59.339 | DEBUG    | __main__:<module>:313 - Training step 20200: loss = 3.2469 | 3014.70ms | Tokens/s = 173,910.3
2025-01-16 00:44:29.497 | DEBUG    | __main__:<module>:313 - Training step 20210: loss = 3.2902 | 3014.29ms | Tokens/s = 173,934.4
2025-01-16 00:44:59.632 | DEBUG    | __main__:<module>:313 - Training step 20220: loss = 3.2297 | 3011.07ms | Tokens/s = 174,120.0
2025-01-16 00:45:29.765 | DEBUG    | __main__:<module>:313 - Training step 20230: loss = 3.2151 | 3014.40ms | Tokens/s = 173,927.9
2025-01-16 00:45:59.927 | DEBUG    | __main__:<module>:313 - Training step 20240: loss = 3.3134 | 3015.58ms | Tokens/s = 173,859.5
2025-01-16 00:46:30.102 | DEBUG    | __main__:<module>:313 - Training step 20250: loss = 3.1286 | 3014.02ms | Tokens/s = 173,949.9
2025-01-16 00:47:00.236 | DEBUG    | __main__:<module>:313 - Training step 20260: loss = 3.3324 | 3013.75ms | Tokens/s = 173,965.3
2025-01-16 00:47:30.355 | DEBUG    | __main__:<module>:313 - Training step 20270: loss = 3.1670 | 3013.63ms | Tokens/s = 173,972.5
2025-01-16 00:48:00.512 | DEBUG    | __main__:<module>:313 - Training step 20280: loss = 3.2149 | 3016.81ms | Tokens/s = 173,789.1
2025-01-16 00:48:30.670 | DEBUG    | __main__:<module>:313 - Training step 20290: loss = 3.2196 | 3016.96ms | Tokens/s = 173,780.3
2025-01-16 00:49:00.806 | DEBUG    | __main__:<module>:313 - Training step 20300: loss = 3.4185 | 3012.76ms | Tokens/s = 174,022.3
2025-01-16 00:49:30.968 | DEBUG    | __main__:<module>:313 - Training step 20310: loss = 3.1819 | 3018.05ms | Tokens/s = 173,717.2
2025-01-16 00:50:01.120 | DEBUG    | __main__:<module>:313 - Training step 20320: loss = 3.1597 | 3014.18ms | Tokens/s = 173,940.4
2025-01-16 00:50:31.256 | DEBUG    | __main__:<module>:313 - Training step 20330: loss = 3.2316 | 3014.23ms | Tokens/s = 173,937.5
2025-01-16 00:51:01.420 | DEBUG    | __main__:<module>:313 - Training step 20340: loss = 3.3118 | 3014.92ms | Tokens/s = 173,898.1
2025-01-16 00:51:31.577 | DEBUG    | __main__:<module>:313 - Training step 20350: loss = 3.2266 | 3015.42ms | Tokens/s = 173,869.2
2025-01-16 00:52:01.748 | DEBUG    | __main__:<module>:313 - Training step 20360: loss = 3.2766 | 3020.13ms | Tokens/s = 173,597.6
2025-01-16 00:52:31.896 | DEBUG    | __main__:<module>:313 - Training step 20370: loss = 3.1347 | 3011.63ms | Tokens/s = 174,087.6
2025-01-16 00:53:02.044 | DEBUG    | __main__:<module>:313 - Training step 20380: loss = 3.3186 | 3016.18ms | Tokens/s = 173,825.0
2025-01-16 00:53:32.206 | DEBUG    | __main__:<module>:313 - Training step 20390: loss = 3.2682 | 3013.78ms | Tokens/s = 173,963.7
2025-01-16 00:54:02.355 | DEBUG    | __main__:<module>:313 - Training step 20400: loss = 3.2406 | 3013.51ms | Tokens/s = 173,979.3
2025-01-16 00:54:32.486 | DEBUG    | __main__:<module>:313 - Training step 20410: loss = 3.2761 | 3014.36ms | Tokens/s = 173,929.9
2025-01-16 00:55:02.609 | DEBUG    | __main__:<module>:313 - Training step 20420: loss = 3.2956 | 3012.95ms | Tokens/s = 174,011.3
2025-01-16 00:55:32.757 | DEBUG    | __main__:<module>:313 - Training step 20430: loss = 3.2950 | 3013.67ms | Tokens/s = 173,969.9
2025-01-16 00:56:02.928 | DEBUG    | __main__:<module>:313 - Training step 20440: loss = 3.3075 | 3016.29ms | Tokens/s = 173,819.0
2025-01-16 00:56:33.075 | DEBUG    | __main__:<module>:313 - Training step 20450: loss = 3.2819 | 3013.14ms | Tokens/s = 174,000.3
2025-01-16 00:57:03.194 | DEBUG    | __main__:<module>:313 - Training step 20460: loss = 3.2154 | 3010.98ms | Tokens/s = 174,125.5
2025-01-16 00:57:33.333 | DEBUG    | __main__:<module>:313 - Training step 20470: loss = 3.0387 | 3016.36ms | Tokens/s = 173,814.8
2025-01-16 00:58:03.510 | DEBUG    | __main__:<module>:313 - Training step 20480: loss = 3.1721 | 3015.96ms | Tokens/s = 173,837.8
2025-01-16 00:58:33.666 | DEBUG    | __main__:<module>:313 - Training step 20490: loss = 3.1651 | 3014.15ms | Tokens/s = 173,942.0
2025-01-16 00:59:03.791 | DEBUG    | __main__:<module>:313 - Training step 20500: loss = 3.1912 | 3013.21ms | Tokens/s = 173,996.5
2025-01-16 00:59:33.950 | DEBUG    | __main__:<module>:313 - Training step 20510: loss = 3.2760 | 3016.91ms | Tokens/s = 173,783.1
2025-01-16 01:00:04.122 | DEBUG    | __main__:<module>:313 - Training step 20520: loss = 3.2369 | 3015.99ms | Tokens/s = 173,836.0
2025-01-16 01:00:34.264 | DEBUG    | __main__:<module>:313 - Training step 20530: loss = 3.2122 | 3011.51ms | Tokens/s = 174,094.5
2025-01-16 01:01:04.379 | DEBUG    | __main__:<module>:313 - Training step 20540: loss = 3.2604 | 3010.47ms | Tokens/s = 174,154.9
2025-01-16 01:01:34.500 | DEBUG    | __main__:<module>:313 - Training step 20550: loss = 3.4485 | 3014.02ms | Tokens/s = 173,949.8
2025-01-16 01:02:04.663 | DEBUG    | __main__:<module>:313 - Training step 20560: loss = 3.3070 | 3017.19ms | Tokens/s = 173,767.0
2025-01-16 01:02:34.821 | DEBUG    | __main__:<module>:313 - Training step 20570: loss = 3.1125 | 3013.55ms | Tokens/s = 173,977.0
2025-01-16 01:03:04.962 | DEBUG    | __main__:<module>:313 - Training step 20580: loss = 3.3459 | 3013.50ms | Tokens/s = 173,979.8
2025-01-16 01:03:35.094 | DEBUG    | __main__:<module>:313 - Training step 20590: loss = 3.4901 | 3012.59ms | Tokens/s = 174,032.3
2025-01-16 01:04:05.214 | DEBUG    | __main__:<module>:313 - Training step 20600: loss = 3.0939 | 3010.70ms | Tokens/s = 174,141.8
2025-01-16 01:04:35.330 | DEBUG    | __main__:<module>:313 - Training step 20610: loss = 3.2744 | 3010.34ms | Tokens/s = 174,162.5
2025-01-16 01:05:05.460 | DEBUG    | __main__:<module>:313 - Training step 20620: loss = 3.3487 | 3013.82ms | Tokens/s = 173,961.4
2025-01-16 01:05:35.620 | DEBUG    | __main__:<module>:313 - Training step 20630: loss = 3.2283 | 3018.16ms | Tokens/s = 173,711.2
2025-01-16 01:06:05.779 | DEBUG    | __main__:<module>:313 - Training step 20640: loss = 3.4422 | 3015.40ms | Tokens/s = 173,870.0
2025-01-16 01:06:35.919 | DEBUG    | __main__:<module>:313 - Training step 20650: loss = 3.2416 | 3012.72ms | Tokens/s = 174,025.0
2025-01-16 01:07:06.033 | DEBUG    | __main__:<module>:313 - Training step 20660: loss = 3.2332 | 3010.67ms | Tokens/s = 174,143.6
2025-01-16 01:07:36.158 | DEBUG    | __main__:<module>:313 - Training step 20670: loss = 3.0749 | 3012.12ms | Tokens/s = 174,059.7
2025-01-16 01:08:06.312 | DEBUG    | __main__:<module>:313 - Training step 20680: loss = 3.2321 | 3015.30ms | Tokens/s = 173,875.7
2025-01-16 01:08:36.478 | DEBUG    | __main__:<module>:313 - Training step 20690: loss = 3.0433 | 3016.29ms | Tokens/s = 173,818.6
2025-01-16 01:09:06.649 | DEBUG    | __main__:<module>:313 - Training step 20700: loss = 3.2394 | 3018.16ms | Tokens/s = 173,711.0
2025-01-16 01:09:36.815 | DEBUG    | __main__:<module>:313 - Training step 20710: loss = 3.4710 | 3015.53ms | Tokens/s = 173,862.4
2025-01-16 01:10:06.960 | DEBUG    | __main__:<module>:313 - Training step 20720: loss = 3.1488 | 3013.22ms | Tokens/s = 173,996.2
2025-01-16 01:10:37.084 | DEBUG    | __main__:<module>:313 - Training step 20730: loss = 3.4216 | 3011.32ms | Tokens/s = 174,105.8
2025-01-16 01:11:07.222 | DEBUG    | __main__:<module>:313 - Training step 20740: loss = 3.2041 | 3017.75ms | Tokens/s = 173,735.0
2025-01-16 01:11:37.389 | DEBUG    | __main__:<module>:313 - Training step 20750: loss = 3.3335 | 3014.58ms | Tokens/s = 173,917.7
2025-01-16 01:12:07.539 | DEBUG    | __main__:<module>:313 - Training step 20760: loss = 3.3079 | 3013.94ms | Tokens/s = 173,954.5
2025-01-16 01:12:37.676 | DEBUG    | __main__:<module>:313 - Training step 20770: loss = 3.2417 | 3013.64ms | Tokens/s = 173,971.7
2025-01-16 01:13:07.842 | DEBUG    | __main__:<module>:313 - Training step 20780: loss = 3.1163 | 3016.18ms | Tokens/s = 173,824.9
2025-01-16 01:13:38.021 | DEBUG    | __main__:<module>:313 - Training step 20790: loss = 3.2198 | 3018.70ms | Tokens/s = 173,680.2
2025-01-16 01:14:08.207 | DEBUG    | __main__:<module>:313 - Training step 20800: loss = 3.1511 | 3017.21ms | Tokens/s = 173,765.8
2025-01-16 01:14:38.380 | DEBUG    | __main__:<module>:313 - Training step 20810: loss = 3.1909 | 3015.74ms | Tokens/s = 173,850.5
2025-01-16 01:15:08.543 | DEBUG    | __main__:<module>:313 - Training step 20820: loss = 3.2906 | 3013.15ms | Tokens/s = 173,999.9
2025-01-16 01:15:38.689 | DEBUG    | __main__:<module>:313 - Training step 20830: loss = 3.0736 | 3012.40ms | Tokens/s = 174,043.2
2025-01-16 01:16:08.816 | DEBUG    | __main__:<module>:313 - Training step 20840: loss = 3.3061 | 3012.35ms | Tokens/s = 174,046.3
2025-01-16 01:16:38.966 | DEBUG    | __main__:<module>:313 - Training step 20850: loss = 3.3081 | 3013.65ms | Tokens/s = 173,971.1
2025-01-16 01:17:09.135 | DEBUG    | __main__:<module>:313 - Training step 20860: loss = 3.1979 | 3016.72ms | Tokens/s = 173,794.1
2025-01-16 01:17:39.295 | DEBUG    | __main__:<module>:313 - Training step 20870: loss = 3.1240 | 3017.20ms | Tokens/s = 173,766.3
2025-01-16 01:18:09.462 | DEBUG    | __main__:<module>:313 - Training step 20880: loss = 3.3813 | 3019.27ms | Tokens/s = 173,647.1
2025-01-16 01:18:39.612 | DEBUG    | __main__:<module>:313 - Training step 20890: loss = 3.3422 | 3014.54ms | Tokens/s = 173,920.0
2025-01-16 01:19:09.788 | DEBUG    | __main__:<module>:313 - Training step 20900: loss = 3.3297 | 3016.64ms | Tokens/s = 173,798.4
2025-01-16 01:19:39.949 | DEBUG    | __main__:<module>:313 - Training step 20910: loss = 3.4285 | 3015.88ms | Tokens/s = 173,842.3
2025-01-16 01:20:10.081 | DEBUG    | __main__:<module>:313 - Training step 20920: loss = 3.2556 | 3012.13ms | Tokens/s = 174,058.9
2025-01-16 01:20:40.223 | DEBUG    | __main__:<module>:313 - Training step 20930: loss = 3.2522 | 3016.78ms | Tokens/s = 173,790.4
2025-01-16 01:21:10.404 | DEBUG    | __main__:<module>:313 - Training step 20940: loss = 3.3050 | 3020.43ms | Tokens/s = 173,580.6
2025-01-16 01:21:40.593 | DEBUG    | __main__:<module>:313 - Training step 20950: loss = 3.0277 | 3018.47ms | Tokens/s = 173,693.2
2025-01-16 01:22:10.757 | DEBUG    | __main__:<module>:313 - Training step 20960: loss = 3.2266 | 3018.10ms | Tokens/s = 173,714.5
2025-01-16 01:22:40.910 | DEBUG    | __main__:<module>:313 - Training step 20970: loss = 3.1025 | 3016.85ms | Tokens/s = 173,786.8
2025-01-16 01:23:11.067 | DEBUG    | __main__:<module>:313 - Training step 20980: loss = 3.0309 | 3015.98ms | Tokens/s = 173,836.8
2025-01-16 01:23:41.192 | DEBUG    | __main__:<module>:313 - Training step 20990: loss = 3.1513 | 3009.28ms | Tokens/s = 174,223.8
2025-01-16 01:24:14.738 | INFO     | __main__:<module>:265 - Step 21,000/40,000 loss: 3.2343 (T) 3.2711 (V) | lr=5.0e-03
2025-01-16 01:24:14.740 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 01:24:28.692 | DEBUG    | __main__:<module>:313 - Training step 21000: loss = 3.1342 | 20392.18ms | Tokens/s = 25,710.3
2025-01-16 01:24:58.741 | DEBUG    | __main__:<module>:313 - Training step 21010: loss = 3.1963 | 3005.01ms | Tokens/s = 174,471.3
2025-01-16 01:25:28.838 | DEBUG    | __main__:<module>:313 - Training step 21020: loss = 3.2744 | 3010.94ms | Tokens/s = 174,127.5
2025-01-16 01:25:58.967 | DEBUG    | __main__:<module>:313 - Training step 21030: loss = 3.0770 | 3012.79ms | Tokens/s = 174,020.8
2025-01-16 01:26:29.130 | DEBUG    | __main__:<module>:313 - Training step 21040: loss = 3.2561 | 3016.46ms | Tokens/s = 173,809.3
2025-01-16 01:26:59.289 | DEBUG    | __main__:<module>:313 - Training step 21050: loss = 3.1791 | 3013.17ms | Tokens/s = 173,998.6
2025-01-16 01:27:29.412 | DEBUG    | __main__:<module>:313 - Training step 21060: loss = 3.4511 | 3011.62ms | Tokens/s = 174,088.2
2025-01-16 01:27:59.554 | DEBUG    | __main__:<module>:313 - Training step 21070: loss = 3.1612 | 3017.25ms | Tokens/s = 173,763.3
2025-01-16 01:28:29.703 | DEBUG    | __main__:<module>:313 - Training step 21080: loss = 3.1789 | 3014.69ms | Tokens/s = 173,910.9
2025-01-16 01:28:59.841 | DEBUG    | __main__:<module>:313 - Training step 21090: loss = 3.1181 | 3011.04ms | Tokens/s = 174,121.9
2025-01-16 01:29:29.952 | DEBUG    | __main__:<module>:313 - Training step 21100: loss = 3.2818 | 3011.63ms | Tokens/s = 174,087.8
2025-01-16 01:30:00.071 | DEBUG    | __main__:<module>:313 - Training step 21110: loss = 3.3135 | 3011.31ms | Tokens/s = 174,106.4
2025-01-16 01:30:30.226 | DEBUG    | __main__:<module>:313 - Training step 21120: loss = 3.2910 | 3018.65ms | Tokens/s = 173,683.1
2025-01-16 01:31:00.374 | DEBUG    | __main__:<module>:313 - Training step 21130: loss = 3.2608 | 3013.86ms | Tokens/s = 173,959.1
2025-01-16 01:31:30.501 | DEBUG    | __main__:<module>:313 - Training step 21140: loss = 3.2786 | 3012.78ms | Tokens/s = 174,021.4
2025-01-16 01:32:00.614 | DEBUG    | __main__:<module>:313 - Training step 21150: loss = 3.2954 | 3012.09ms | Tokens/s = 174,061.4
2025-01-16 01:32:30.755 | DEBUG    | __main__:<module>:313 - Training step 21160: loss = 3.3604 | 3015.92ms | Tokens/s = 173,840.0
2025-01-16 01:33:00.924 | DEBUG    | __main__:<module>:313 - Training step 21170: loss = 3.2638 | 3015.80ms | Tokens/s = 173,846.9
2025-01-16 01:33:31.072 | DEBUG    | __main__:<module>:313 - Training step 21180: loss = 3.1655 | 3015.40ms | Tokens/s = 173,870.2
2025-01-16 01:34:01.191 | DEBUG    | __main__:<module>:313 - Training step 21190: loss = 3.2686 | 3012.25ms | Tokens/s = 174,051.8
2025-01-16 01:34:31.334 | DEBUG    | __main__:<module>:313 - Training step 21200: loss = 3.2135 | 3015.72ms | Tokens/s = 173,851.8
2025-01-16 01:35:01.500 | DEBUG    | __main__:<module>:313 - Training step 21210: loss = 3.1389 | 3014.84ms | Tokens/s = 173,902.4
2025-01-16 01:35:31.682 | DEBUG    | __main__:<module>:313 - Training step 21220: loss = 3.1613 | 3016.12ms | Tokens/s = 173,828.6
2025-01-16 01:36:01.841 | DEBUG    | __main__:<module>:313 - Training step 21230: loss = 3.3587 | 3013.31ms | Tokens/s = 173,990.7
2025-01-16 01:36:31.975 | DEBUG    | __main__:<module>:313 - Training step 21240: loss = 3.3935 | 3012.04ms | Tokens/s = 174,064.2
2025-01-16 01:37:02.123 | DEBUG    | __main__:<module>:313 - Training step 21250: loss = 3.1792 | 3014.41ms | Tokens/s = 173,927.4
2025-01-16 01:37:32.288 | DEBUG    | __main__:<module>:313 - Training step 21260: loss = 3.3477 | 3014.99ms | Tokens/s = 173,894.1
2025-01-16 01:38:02.437 | DEBUG    | __main__:<module>:313 - Training step 21270: loss = 3.2754 | 3013.31ms | Tokens/s = 173,990.8
2025-01-16 01:38:32.571 | DEBUG    | __main__:<module>:313 - Training step 21280: loss = 3.3133 | 3012.88ms | Tokens/s = 174,015.8
2025-01-16 01:39:02.714 | DEBUG    | __main__:<module>:313 - Training step 21290: loss = 3.2795 | 3012.49ms | Tokens/s = 174,038.3
2025-01-16 01:39:32.862 | DEBUG    | __main__:<module>:313 - Training step 21300: loss = 3.2116 | 3012.24ms | Tokens/s = 174,052.4
2025-01-16 01:40:03.002 | DEBUG    | __main__:<module>:313 - Training step 21310: loss = 3.4202 | 3012.96ms | Tokens/s = 174,010.7
2025-01-16 01:40:33.133 | DEBUG    | __main__:<module>:313 - Training step 21320: loss = 3.2609 | 3013.35ms | Tokens/s = 173,988.7
2025-01-16 01:41:03.250 | DEBUG    | __main__:<module>:313 - Training step 21330: loss = 3.2950 | 3012.65ms | Tokens/s = 174,028.7
2025-01-16 01:41:33.394 | DEBUG    | __main__:<module>:313 - Training step 21340: loss = 3.1345 | 3016.23ms | Tokens/s = 173,822.5
2025-01-16 01:42:03.559 | DEBUG    | __main__:<module>:313 - Training step 21350: loss = 3.2797 | 3017.05ms | Tokens/s = 173,775.2
2025-01-16 01:42:33.704 | DEBUG    | __main__:<module>:313 - Training step 21360: loss = 3.3356 | 3011.54ms | Tokens/s = 174,093.1
2025-01-16 01:43:03.822 | DEBUG    | __main__:<module>:313 - Training step 21370: loss = 3.0573 | 3010.41ms | Tokens/s = 174,158.2
2025-01-16 01:43:33.932 | DEBUG    | __main__:<module>:313 - Training step 21380: loss = 3.2963 | 3011.92ms | Tokens/s = 174,070.9
2025-01-16 01:44:04.075 | DEBUG    | __main__:<module>:313 - Training step 21390: loss = 3.2150 | 3017.79ms | Tokens/s = 173,732.4
2025-01-16 01:44:34.237 | DEBUG    | __main__:<module>:313 - Training step 21400: loss = 3.2573 | 3014.96ms | Tokens/s = 173,895.8
2025-01-16 01:45:04.409 | DEBUG    | __main__:<module>:313 - Training step 21410: loss = 3.1532 | 3019.80ms | Tokens/s = 173,616.9
2025-01-16 01:45:34.591 | DEBUG    | __main__:<module>:313 - Training step 21420: loss = 3.1807 | 3018.82ms | Tokens/s = 173,673.3
2025-01-16 01:46:04.771 | DEBUG    | __main__:<module>:313 - Training step 21430: loss = 3.2686 | 3017.72ms | Tokens/s = 173,736.2
2025-01-16 01:46:34.921 | DEBUG    | __main__:<module>:313 - Training step 21440: loss = 3.2209 | 3014.42ms | Tokens/s = 173,926.6
2025-01-16 01:47:05.049 | DEBUG    | __main__:<module>:313 - Training step 21450: loss = 3.1708 | 3011.83ms | Tokens/s = 174,076.2
2025-01-16 01:47:35.208 | DEBUG    | __main__:<module>:313 - Training step 21460: loss = 3.2014 | 3014.81ms | Tokens/s = 173,903.9
2025-01-16 01:48:05.384 | DEBUG    | __main__:<module>:313 - Training step 21470: loss = 3.2364 | 3019.75ms | Tokens/s = 173,619.9
2025-01-16 01:48:35.534 | DEBUG    | __main__:<module>:313 - Training step 21480: loss = 3.2686 | 3013.24ms | Tokens/s = 173,994.8
2025-01-16 01:49:05.655 | DEBUG    | __main__:<module>:313 - Training step 21490: loss = 3.1901 | 3011.51ms | Tokens/s = 174,094.5
2025-01-16 01:49:35.776 | DEBUG    | __main__:<module>:313 - Training step 21500: loss = 3.0948 | 3012.49ms | Tokens/s = 174,038.3
2025-01-16 01:50:05.924 | DEBUG    | __main__:<module>:313 - Training step 21510: loss = 3.2782 | 3018.50ms | Tokens/s = 173,691.5
2025-01-16 01:50:36.091 | DEBUG    | __main__:<module>:313 - Training step 21520: loss = 3.1881 | 3013.08ms | Tokens/s = 174,004.3
2025-01-16 01:51:06.240 | DEBUG    | __main__:<module>:313 - Training step 21530: loss = 3.2056 | 3016.44ms | Tokens/s = 173,810.1
2025-01-16 01:51:36.425 | DEBUG    | __main__:<module>:313 - Training step 21540: loss = 3.4121 | 3019.59ms | Tokens/s = 173,629.1
2025-01-16 01:52:06.603 | DEBUG    | __main__:<module>:313 - Training step 21550: loss = 3.3633 | 3015.21ms | Tokens/s = 173,881.1
2025-01-16 01:52:36.752 | DEBUG    | __main__:<module>:313 - Training step 21560: loss = 3.0073 | 3012.51ms | Tokens/s = 174,036.8
2025-01-16 01:53:06.874 | DEBUG    | __main__:<module>:313 - Training step 21570: loss = 3.3064 | 3010.05ms | Tokens/s = 174,179.1
2025-01-16 01:53:37.003 | DEBUG    | __main__:<module>:313 - Training step 21580: loss = 3.3605 | 3013.23ms | Tokens/s = 173,995.6
2025-01-16 01:54:07.160 | DEBUG    | __main__:<module>:313 - Training step 21590: loss = 3.2811 | 3014.76ms | Tokens/s = 173,907.1
2025-01-16 01:54:37.323 | DEBUG    | __main__:<module>:313 - Training step 21600: loss = 3.2241 | 3015.57ms | Tokens/s = 173,860.4
2025-01-16 01:55:07.458 | DEBUG    | __main__:<module>:313 - Training step 21610: loss = 3.3686 | 3011.39ms | Tokens/s = 174,101.6
2025-01-16 01:55:37.581 | DEBUG    | __main__:<module>:313 - Training step 21620: loss = 3.2783 | 3009.86ms | Tokens/s = 174,190.3
2025-01-16 01:56:07.703 | DEBUG    | __main__:<module>:313 - Training step 21630: loss = 3.3917 | 3013.98ms | Tokens/s = 173,951.8
2025-01-16 01:56:37.840 | DEBUG    | __main__:<module>:313 - Training step 21640: loss = 3.2931 | 3014.06ms | Tokens/s = 173,947.2
2025-01-16 01:57:08.000 | DEBUG    | __main__:<module>:313 - Training step 21650: loss = 3.2656 | 3015.57ms | Tokens/s = 173,860.1
2025-01-16 01:57:38.181 | DEBUG    | __main__:<module>:313 - Training step 21660: loss = 3.2889 | 3018.98ms | Tokens/s = 173,663.9
2025-01-16 01:58:08.352 | DEBUG    | __main__:<module>:313 - Training step 21670: loss = 3.3356 | 3013.10ms | Tokens/s = 174,003.1
2025-01-16 01:58:38.492 | DEBUG    | __main__:<module>:313 - Training step 21680: loss = 3.1082 | 3013.34ms | Tokens/s = 173,988.9
2025-01-16 01:59:08.616 | DEBUG    | __main__:<module>:313 - Training step 21690: loss = 3.2231 | 3014.82ms | Tokens/s = 173,903.5
2025-01-16 01:59:38.744 | DEBUG    | __main__:<module>:313 - Training step 21700: loss = 3.3557 | 3011.25ms | Tokens/s = 174,109.8
2025-01-16 02:00:08.858 | DEBUG    | __main__:<module>:313 - Training step 21710: loss = 3.2215 | 3010.36ms | Tokens/s = 174,161.5
2025-01-16 02:00:38.974 | DEBUG    | __main__:<module>:313 - Training step 21720: loss = 3.3121 | 3011.62ms | Tokens/s = 174,088.1
2025-01-16 02:01:09.112 | DEBUG    | __main__:<module>:313 - Training step 21730: loss = 3.2122 | 3016.27ms | Tokens/s = 173,819.9
2025-01-16 02:01:39.285 | DEBUG    | __main__:<module>:313 - Training step 21740: loss = 3.3764 | 3019.40ms | Tokens/s = 173,639.9
2025-01-16 02:02:09.439 | DEBUG    | __main__:<module>:313 - Training step 21750: loss = 3.2482 | 3012.98ms | Tokens/s = 174,009.6
2025-01-16 02:02:39.566 | DEBUG    | __main__:<module>:313 - Training step 21760: loss = 3.2559 | 3011.37ms | Tokens/s = 174,102.5
2025-01-16 02:03:09.708 | DEBUG    | __main__:<module>:313 - Training step 21770: loss = 3.3161 | 3015.66ms | Tokens/s = 173,855.0
2025-01-16 02:03:39.872 | DEBUG    | __main__:<module>:313 - Training step 21780: loss = 3.0771 | 3016.14ms | Tokens/s = 173,827.4
2025-01-16 02:04:10.010 | DEBUG    | __main__:<module>:313 - Training step 21790: loss = 3.2487 | 3014.28ms | Tokens/s = 173,934.9
2025-01-16 02:04:40.141 | DEBUG    | __main__:<module>:313 - Training step 21800: loss = 3.2674 | 3015.09ms | Tokens/s = 173,888.0
2025-01-16 02:05:10.297 | DEBUG    | __main__:<module>:313 - Training step 21810: loss = 3.1794 | 3017.70ms | Tokens/s = 173,737.7
2025-01-16 02:05:40.455 | DEBUG    | __main__:<module>:313 - Training step 21820: loss = 3.2381 | 3014.27ms | Tokens/s = 173,935.5
2025-01-16 02:06:10.584 | DEBUG    | __main__:<module>:313 - Training step 21830: loss = 3.1961 | 3012.62ms | Tokens/s = 174,030.7
2025-01-16 02:06:40.707 | DEBUG    | __main__:<module>:313 - Training step 21840: loss = 3.2783 | 3013.16ms | Tokens/s = 173,999.2
2025-01-16 02:07:10.858 | DEBUG    | __main__:<module>:313 - Training step 21850: loss = 3.1719 | 3016.04ms | Tokens/s = 173,833.1
2025-01-16 02:07:41.033 | DEBUG    | __main__:<module>:313 - Training step 21860: loss = 3.2205 | 3017.77ms | Tokens/s = 173,733.7
2025-01-16 02:08:11.184 | DEBUG    | __main__:<module>:313 - Training step 21870: loss = 3.1746 | 3015.90ms | Tokens/s = 173,841.2
2025-01-16 02:08:41.350 | DEBUG    | __main__:<module>:313 - Training step 21880: loss = 3.1314 | 3016.29ms | Tokens/s = 173,818.6
2025-01-16 02:09:11.491 | DEBUG    | __main__:<module>:313 - Training step 21890: loss = 3.1033 | 3012.09ms | Tokens/s = 174,060.9
2025-01-16 02:09:41.614 | DEBUG    | __main__:<module>:313 - Training step 21900: loss = 3.4070 | 3013.97ms | Tokens/s = 173,952.8
2025-01-16 02:10:11.729 | DEBUG    | __main__:<module>:313 - Training step 21910: loss = 3.1722 | 3010.81ms | Tokens/s = 174,135.4
2025-01-16 02:10:41.866 | DEBUG    | __main__:<module>:313 - Training step 21920: loss = 3.1552 | 3014.00ms | Tokens/s = 173,951.1
2025-01-16 02:11:12.013 | DEBUG    | __main__:<module>:313 - Training step 21930: loss = 3.2664 | 3013.89ms | Tokens/s = 173,957.1
2025-01-16 02:11:42.173 | DEBUG    | __main__:<module>:313 - Training step 21940: loss = 3.1449 | 3013.25ms | Tokens/s = 173,994.3
2025-01-16 02:12:12.332 | DEBUG    | __main__:<module>:313 - Training step 21950: loss = 3.1999 | 3017.50ms | Tokens/s = 173,748.8
2025-01-16 02:12:42.506 | DEBUG    | __main__:<module>:313 - Training step 21960: loss = 3.3219 | 3017.93ms | Tokens/s = 173,724.2
2025-01-16 02:13:12.691 | DEBUG    | __main__:<module>:313 - Training step 21970: loss = 3.1282 | 3016.45ms | Tokens/s = 173,809.3
2025-01-16 02:13:42.865 | DEBUG    | __main__:<module>:313 - Training step 21980: loss = 3.3563 | 3017.39ms | Tokens/s = 173,755.3
2025-01-16 02:14:13.013 | DEBUG    | __main__:<module>:313 - Training step 21990: loss = 3.3120 | 3014.49ms | Tokens/s = 173,922.6
2025-01-16 02:14:46.560 | INFO     | __main__:<module>:265 - Step 22,000/40,000 loss: 3.2274 (T) 3.2453 (V) | lr=4.6e-03
2025-01-16 02:14:46.561 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 02:15:00.435 | DEBUG    | __main__:<module>:313 - Training step 22000: loss = 3.0933 | 20313.89ms | Tokens/s = 25,809.3
2025-01-16 02:15:30.494 | DEBUG    | __main__:<module>:313 - Training step 22010: loss = 3.3125 | 3007.62ms | Tokens/s = 174,320.1
2025-01-16 02:16:00.621 | DEBUG    | __main__:<module>:313 - Training step 22020: loss = 3.3403 | 3015.02ms | Tokens/s = 173,892.2
2025-01-16 02:16:30.769 | DEBUG    | __main__:<module>:313 - Training step 22030: loss = 3.2192 | 3013.84ms | Tokens/s = 173,960.1
2025-01-16 02:17:00.932 | DEBUG    | __main__:<module>:313 - Training step 22040: loss = 3.2126 | 3017.38ms | Tokens/s = 173,755.9
2025-01-16 02:17:31.092 | DEBUG    | __main__:<module>:313 - Training step 22050: loss = 3.1830 | 3014.46ms | Tokens/s = 173,924.5
2025-01-16 02:18:01.242 | DEBUG    | __main__:<module>:313 - Training step 22060: loss = 3.1620 | 3014.31ms | Tokens/s = 173,933.0
2025-01-16 02:18:31.370 | DEBUG    | __main__:<module>:313 - Training step 22070: loss = 3.2383 | 3012.84ms | Tokens/s = 174,017.7
2025-01-16 02:19:01.498 | DEBUG    | __main__:<module>:313 - Training step 22080: loss = 3.3336 | 3013.90ms | Tokens/s = 173,956.6
2025-01-16 02:19:31.657 | DEBUG    | __main__:<module>:313 - Training step 22090: loss = 3.1263 | 3012.88ms | Tokens/s = 174,015.3
2025-01-16 02:20:01.800 | DEBUG    | __main__:<module>:313 - Training step 22100: loss = 3.1275 | 3010.47ms | Tokens/s = 174,155.0
2025-01-16 02:20:31.927 | DEBUG    | __main__:<module>:313 - Training step 22110: loss = 3.3664 | 3012.49ms | Tokens/s = 174,038.0
2025-01-16 02:21:02.087 | DEBUG    | __main__:<module>:313 - Training step 22120: loss = 3.2186 | 3019.54ms | Tokens/s = 173,631.5
2025-01-16 02:21:32.273 | DEBUG    | __main__:<module>:313 - Training step 22130: loss = 3.2450 | 3018.50ms | Tokens/s = 173,691.7
2025-01-16 02:22:02.447 | DEBUG    | __main__:<module>:313 - Training step 22140: loss = 3.2543 | 3015.11ms | Tokens/s = 173,887.1
2025-01-16 02:22:32.602 | DEBUG    | __main__:<module>:313 - Training step 22150: loss = 3.1092 | 3015.23ms | Tokens/s = 173,879.9
2025-01-16 02:23:02.737 | DEBUG    | __main__:<module>:313 - Training step 22160: loss = 3.1919 | 3012.49ms | Tokens/s = 174,037.9
2025-01-16 02:23:32.865 | DEBUG    | __main__:<module>:313 - Training step 22170: loss = 3.2121 | 3012.79ms | Tokens/s = 174,021.0
2025-01-16 02:24:02.976 | DEBUG    | __main__:<module>:313 - Training step 22180: loss = 3.2781 | 3011.35ms | Tokens/s = 174,104.0
2025-01-16 02:24:33.114 | DEBUG    | __main__:<module>:313 - Training step 22190: loss = 3.1940 | 3017.69ms | Tokens/s = 173,738.3
2025-01-16 02:25:03.277 | DEBUG    | __main__:<module>:313 - Training step 22200: loss = 3.3271 | 3015.89ms | Tokens/s = 173,841.9
2025-01-16 02:25:33.410 | DEBUG    | __main__:<module>:313 - Training step 22210: loss = 3.1536 | 3011.81ms | Tokens/s = 174,077.6
2025-01-16 02:26:03.553 | DEBUG    | __main__:<module>:313 - Training step 22220: loss = 3.2166 | 3014.55ms | Tokens/s = 173,919.1
2025-01-16 02:26:33.723 | DEBUG    | __main__:<module>:313 - Training step 22230: loss = 3.2953 | 3018.34ms | Tokens/s = 173,700.8
2025-01-16 02:27:03.887 | DEBUG    | __main__:<module>:313 - Training step 22240: loss = 3.1549 | 3013.74ms | Tokens/s = 173,965.9
2025-01-16 02:27:34.028 | DEBUG    | __main__:<module>:313 - Training step 22250: loss = 3.2005 | 3014.92ms | Tokens/s = 173,898.0
2025-01-16 02:28:04.183 | DEBUG    | __main__:<module>:313 - Training step 22260: loss = 3.1698 | 3016.44ms | Tokens/s = 173,810.0
2025-01-16 02:28:34.371 | DEBUG    | __main__:<module>:313 - Training step 22270: loss = 3.2914 | 3018.70ms | Tokens/s = 173,680.2
2025-01-16 02:29:04.527 | DEBUG    | __main__:<module>:313 - Training step 22280: loss = 3.2414 | 3012.56ms | Tokens/s = 174,034.1
2025-01-16 02:29:34.656 | DEBUG    | __main__:<module>:313 - Training step 22290: loss = 3.2164 | 3011.97ms | Tokens/s = 174,068.2
2025-01-16 02:30:04.804 | DEBUG    | __main__:<module>:313 - Training step 22300: loss = 3.2204 | 3014.57ms | Tokens/s = 173,917.9
2025-01-16 02:30:34.978 | DEBUG    | __main__:<module>:313 - Training step 22310: loss = 3.4093 | 3017.08ms | Tokens/s = 173,773.3
2025-01-16 02:31:05.126 | DEBUG    | __main__:<module>:313 - Training step 22320: loss = 3.2942 | 3012.71ms | Tokens/s = 174,025.5
2025-01-16 02:31:35.261 | DEBUG    | __main__:<module>:313 - Training step 22330: loss = 3.2709 | 3017.07ms | Tokens/s = 173,773.9
2025-01-16 02:32:05.433 | DEBUG    | __main__:<module>:313 - Training step 22340: loss = 3.3467 | 3019.87ms | Tokens/s = 173,612.5
2025-01-16 02:32:35.618 | DEBUG    | __main__:<module>:313 - Training step 22350: loss = 3.1982 | 3017.19ms | Tokens/s = 173,766.8
2025-01-16 02:33:05.785 | DEBUG    | __main__:<module>:313 - Training step 22360: loss = 3.2409 | 3014.43ms | Tokens/s = 173,926.3
2025-01-16 02:33:35.914 | DEBUG    | __main__:<module>:313 - Training step 22370: loss = 3.1566 | 3012.79ms | Tokens/s = 174,021.0
2025-01-16 02:34:06.034 | DEBUG    | __main__:<module>:313 - Training step 22380: loss = 3.4499 | 3011.49ms | Tokens/s = 174,095.7
2025-01-16 02:34:36.179 | DEBUG    | __main__:<module>:313 - Training step 22390: loss = 3.0568 | 3017.39ms | Tokens/s = 173,755.3
2025-01-16 02:35:06.323 | DEBUG    | __main__:<module>:313 - Training step 22400: loss = 3.2199 | 3011.47ms | Tokens/s = 174,097.1
2025-01-16 02:35:36.448 | DEBUG    | __main__:<module>:313 - Training step 22410: loss = 3.0566 | 3014.05ms | Tokens/s = 173,947.9
2025-01-16 02:36:06.601 | DEBUG    | __main__:<module>:313 - Training step 22420: loss = 3.2198 | 3019.92ms | Tokens/s = 173,610.1
2025-01-16 02:36:36.764 | DEBUG    | __main__:<module>:313 - Training step 22430: loss = 3.1155 | 3015.96ms | Tokens/s = 173,837.7
2025-01-16 02:37:06.899 | DEBUG    | __main__:<module>:313 - Training step 22440: loss = 3.4058 | 3012.64ms | Tokens/s = 174,029.7
2025-01-16 02:37:37.023 | DEBUG    | __main__:<module>:313 - Training step 22450: loss = 3.1031 | 3011.34ms | Tokens/s = 174,104.3
2025-01-16 02:38:07.163 | DEBUG    | __main__:<module>:313 - Training step 22460: loss = 3.3035 | 3015.99ms | Tokens/s = 173,836.1
2025-01-16 02:38:37.321 | DEBUG    | __main__:<module>:313 - Training step 22470: loss = 3.2489 | 3013.70ms | Tokens/s = 173,968.0
2025-01-16 02:39:07.452 | DEBUG    | __main__:<module>:313 - Training step 22480: loss = 3.3227 | 3013.14ms | Tokens/s = 174,000.5
2025-01-16 02:39:37.603 | DEBUG    | __main__:<module>:313 - Training step 22490: loss = 3.3549 | 3016.17ms | Tokens/s = 173,826.0
2025-01-16 02:40:07.744 | DEBUG    | __main__:<module>:313 - Training step 22500: loss = 3.1918 | 3011.46ms | Tokens/s = 174,097.8
2025-01-16 02:40:37.876 | DEBUG    | __main__:<module>:313 - Training step 22510: loss = 3.3296 | 3013.25ms | Tokens/s = 173,994.2
2025-01-16 02:41:08.014 | DEBUG    | __main__:<module>:313 - Training step 22520: loss = 3.1598 | 3013.96ms | Tokens/s = 173,953.4
2025-01-16 02:41:38.182 | DEBUG    | __main__:<module>:313 - Training step 22530: loss = 3.0874 | 3015.83ms | Tokens/s = 173,845.2
2025-01-16 02:42:08.329 | DEBUG    | __main__:<module>:313 - Training step 22540: loss = 3.2480 | 3012.19ms | Tokens/s = 174,055.2
2025-01-16 02:42:38.460 | DEBUG    | __main__:<module>:313 - Training step 22550: loss = 3.1381 | 3011.90ms | Tokens/s = 174,072.4
2025-01-16 02:43:08.608 | DEBUG    | __main__:<module>:313 - Training step 22560: loss = 3.2891 | 3013.33ms | Tokens/s = 173,989.7
2025-01-16 02:43:38.747 | DEBUG    | __main__:<module>:313 - Training step 22570: loss = 3.1874 | 3012.09ms | Tokens/s = 174,061.2
2025-01-16 02:44:08.879 | DEBUG    | __main__:<module>:313 - Training step 22580: loss = 3.2441 | 3012.35ms | Tokens/s = 174,046.3
2025-01-16 02:44:39.035 | DEBUG    | __main__:<module>:313 - Training step 22590: loss = 3.0960 | 3016.80ms | Tokens/s = 173,789.2
2025-01-16 02:45:09.191 | DEBUG    | __main__:<module>:313 - Training step 22600: loss = 3.1604 | 3011.67ms | Tokens/s = 174,085.3
2025-01-16 02:45:39.331 | DEBUG    | __main__:<module>:313 - Training step 22610: loss = 3.1157 | 3013.51ms | Tokens/s = 173,979.1
2025-01-16 02:46:09.453 | DEBUG    | __main__:<module>:313 - Training step 22620: loss = 3.1686 | 3012.02ms | Tokens/s = 174,065.2
2025-01-16 02:46:39.590 | DEBUG    | __main__:<module>:313 - Training step 22630: loss = 3.3490 | 3013.36ms | Tokens/s = 173,988.0
2025-01-16 02:47:09.740 | DEBUG    | __main__:<module>:313 - Training step 22640: loss = 3.1973 | 3013.99ms | Tokens/s = 173,951.5
2025-01-16 02:47:39.886 | DEBUG    | __main__:<module>:313 - Training step 22650: loss = 3.3474 | 3012.02ms | Tokens/s = 174,065.0
2025-01-16 02:48:10.015 | DEBUG    | __main__:<module>:313 - Training step 22660: loss = 3.0221 | 3013.24ms | Tokens/s = 173,994.9
2025-01-16 02:48:40.138 | DEBUG    | __main__:<module>:313 - Training step 22670: loss = 3.3695 | 3011.17ms | Tokens/s = 174,114.2
2025-01-16 02:49:10.248 | DEBUG    | __main__:<module>:313 - Training step 22680: loss = 3.2939 | 3010.92ms | Tokens/s = 174,128.6
2025-01-16 02:49:40.408 | DEBUG    | __main__:<module>:313 - Training step 22690: loss = 3.1380 | 3019.20ms | Tokens/s = 173,651.1
2025-01-16 02:50:10.590 | DEBUG    | __main__:<module>:313 - Training step 22700: loss = 3.2854 | 3015.83ms | Tokens/s = 173,845.6
2025-01-16 02:50:40.750 | DEBUG    | __main__:<module>:313 - Training step 22710: loss = 3.2310 | 3014.23ms | Tokens/s = 173,937.9
2025-01-16 02:51:10.881 | DEBUG    | __main__:<module>:313 - Training step 22720: loss = 3.2292 | 3012.05ms | Tokens/s = 174,063.8
2025-01-16 02:51:41.011 | DEBUG    | __main__:<module>:313 - Training step 22730: loss = 3.0798 | 3014.60ms | Tokens/s = 173,916.4
2025-01-16 02:52:11.177 | DEBUG    | __main__:<module>:313 - Training step 22740: loss = 3.3834 | 3016.02ms | Tokens/s = 173,834.5
2025-01-16 02:52:41.319 | DEBUG    | __main__:<module>:313 - Training step 22750: loss = 3.2252 | 3012.46ms | Tokens/s = 174,039.7
2025-01-16 02:53:11.435 | DEBUG    | __main__:<module>:313 - Training step 22760: loss = 3.2195 | 3012.12ms | Tokens/s = 174,059.3
2025-01-16 02:53:41.567 | DEBUG    | __main__:<module>:313 - Training step 22770: loss = 3.2068 | 3015.77ms | Tokens/s = 173,848.6
2025-01-16 02:54:11.736 | DEBUG    | __main__:<module>:313 - Training step 22780: loss = 3.4239 | 3018.60ms | Tokens/s = 173,686.0
2025-01-16 02:54:41.888 | DEBUG    | __main__:<module>:313 - Training step 22790: loss = 3.1492 | 3013.51ms | Tokens/s = 173,979.2
2025-01-16 02:55:12.046 | DEBUG    | __main__:<module>:313 - Training step 22800: loss = 3.3456 | 3016.08ms | Tokens/s = 173,830.6
2025-01-16 02:55:42.216 | DEBUG    | __main__:<module>:313 - Training step 22810: loss = 3.1847 | 3015.29ms | Tokens/s = 173,876.5
2025-01-16 02:56:12.358 | DEBUG    | __main__:<module>:313 - Training step 22820: loss = 3.2931 | 3014.18ms | Tokens/s = 173,940.3
2025-01-16 02:56:42.487 | DEBUG    | __main__:<module>:313 - Training step 22830: loss = 3.2107 | 3012.56ms | Tokens/s = 174,033.8
2025-01-16 02:57:12.602 | DEBUG    | __main__:<module>:313 - Training step 22840: loss = 3.2221 | 3012.20ms | Tokens/s = 174,054.6
2025-01-16 02:57:42.735 | DEBUG    | __main__:<module>:313 - Training step 22850: loss = 3.2342 | 3012.30ms | Tokens/s = 174,048.8
2025-01-16 02:58:12.861 | DEBUG    | __main__:<module>:313 - Training step 22860: loss = 3.1813 | 3012.15ms | Tokens/s = 174,057.5
2025-01-16 02:58:42.998 | DEBUG    | __main__:<module>:313 - Training step 22870: loss = 3.2732 | 3014.83ms | Tokens/s = 173,903.1
2025-01-16 02:59:13.154 | DEBUG    | __main__:<module>:313 - Training step 22880: loss = 3.3942 | 3017.73ms | Tokens/s = 173,736.1
2025-01-16 02:59:43.327 | DEBUG    | __main__:<module>:313 - Training step 22890: loss = 3.2462 | 3018.12ms | Tokens/s = 173,713.3
2025-01-16 03:00:13.474 | DEBUG    | __main__:<module>:313 - Training step 22900: loss = 3.2028 | 3014.65ms | Tokens/s = 173,913.3
2025-01-16 03:00:43.637 | DEBUG    | __main__:<module>:313 - Training step 22910: loss = 3.4529 | 3017.56ms | Tokens/s = 173,745.7
2025-01-16 03:01:13.811 | DEBUG    | __main__:<module>:313 - Training step 22920: loss = 3.2267 | 3016.08ms | Tokens/s = 173,831.1
2025-01-16 03:01:43.953 | DEBUG    | __main__:<module>:313 - Training step 22930: loss = 3.3520 | 3013.00ms | Tokens/s = 174,008.8
2025-01-16 03:02:14.080 | DEBUG    | __main__:<module>:313 - Training step 22940: loss = 3.1795 | 3011.00ms | Tokens/s = 174,124.3
2025-01-16 03:02:44.220 | DEBUG    | __main__:<module>:313 - Training step 22950: loss = 3.1741 | 3015.20ms | Tokens/s = 173,881.5
2025-01-16 03:03:14.388 | DEBUG    | __main__:<module>:313 - Training step 22960: loss = 3.1729 | 3015.08ms | Tokens/s = 173,888.4
2025-01-16 03:03:44.531 | DEBUG    | __main__:<module>:313 - Training step 22970: loss = 2.9787 | 3010.83ms | Tokens/s = 174,134.0
2025-01-16 03:04:14.661 | DEBUG    | __main__:<module>:313 - Training step 22980: loss = 3.2292 | 3012.73ms | Tokens/s = 174,024.2
2025-01-16 03:04:44.762 | DEBUG    | __main__:<module>:313 - Training step 22990: loss = 3.0713 | 3009.25ms | Tokens/s = 174,225.2
2025-01-16 03:05:18.296 | INFO     | __main__:<module>:265 - Step 23,000/40,000 loss: 3.2037 (T) 3.2215 (V) | lr=4.2e-03
2025-01-16 03:05:18.297 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 03:05:31.412 | DEBUG    | __main__:<module>:313 - Training step 23000: loss = 3.2630 | 19552.84ms | Tokens/s = 26,813.9
2025-01-16 03:06:01.413 | DEBUG    | __main__:<module>:313 - Training step 23010: loss = 3.1436 | 3007.47ms | Tokens/s = 174,328.3
2025-01-16 03:06:31.516 | DEBUG    | __main__:<module>:313 - Training step 23020: loss = 3.2314 | 3011.43ms | Tokens/s = 174,099.4
2025-01-16 03:07:01.655 | DEBUG    | __main__:<module>:313 - Training step 23030: loss = 3.2963 | 3015.24ms | Tokens/s = 173,879.1
2025-01-16 03:07:31.807 | DEBUG    | __main__:<module>:313 - Training step 23040: loss = 3.3560 | 3014.41ms | Tokens/s = 173,927.2
2025-01-16 03:08:01.940 | DEBUG    | __main__:<module>:313 - Training step 23050: loss = 3.1912 | 3011.90ms | Tokens/s = 174,072.1
2025-01-16 03:08:32.095 | DEBUG    | __main__:<module>:313 - Training step 23060: loss = 3.2613 | 3016.59ms | Tokens/s = 173,801.4
2025-01-16 03:09:02.264 | DEBUG    | __main__:<module>:313 - Training step 23070: loss = 3.3962 | 3017.20ms | Tokens/s = 173,766.6
2025-01-16 03:09:32.407 | DEBUG    | __main__:<module>:313 - Training step 23080: loss = 2.9179 | 3013.25ms | Tokens/s = 173,994.2
2025-01-16 03:10:02.533 | DEBUG    | __main__:<module>:313 - Training step 23090: loss = 3.2489 | 3011.97ms | Tokens/s = 174,068.4
2025-01-16 03:10:32.646 | DEBUG    | __main__:<module>:313 - Training step 23100: loss = 3.1416 | 3012.48ms | Tokens/s = 174,038.7
2025-01-16 03:11:02.792 | DEBUG    | __main__:<module>:313 - Training step 23110: loss = 3.1915 | 3015.78ms | Tokens/s = 173,848.3
2025-01-16 03:11:32.944 | DEBUG    | __main__:<module>:313 - Training step 23120: loss = 3.2185 | 3014.30ms | Tokens/s = 173,933.5
2025-01-16 03:12:03.082 | DEBUG    | __main__:<module>:313 - Training step 23130: loss = 3.3013 | 3011.72ms | Tokens/s = 174,082.5
2025-01-16 03:12:33.195 | DEBUG    | __main__:<module>:313 - Training step 23140: loss = 3.3596 | 3008.86ms | Tokens/s = 174,248.2
2025-01-16 03:13:03.334 | DEBUG    | __main__:<module>:313 - Training step 23150: loss = 3.2173 | 3015.31ms | Tokens/s = 173,875.0
2025-01-16 03:13:33.504 | DEBUG    | __main__:<module>:313 - Training step 23160: loss = 3.1314 | 3016.46ms | Tokens/s = 173,809.1
2025-01-16 03:14:03.670 | DEBUG    | __main__:<module>:313 - Training step 23170: loss = 3.2555 | 3014.35ms | Tokens/s = 173,930.6
2025-01-16 03:14:33.806 | DEBUG    | __main__:<module>:313 - Training step 23180: loss = 3.2211 | 3011.11ms | Tokens/s = 174,117.9
2025-01-16 03:15:03.929 | DEBUG    | __main__:<module>:313 - Training step 23190: loss = 3.1308 | 3011.57ms | Tokens/s = 174,091.5
2025-01-16 03:15:34.099 | DEBUG    | __main__:<module>:313 - Training step 23200: loss = 3.2075 | 3017.08ms | Tokens/s = 173,773.2
2025-01-16 03:16:04.281 | DEBUG    | __main__:<module>:313 - Training step 23210: loss = 3.0509 | 3015.77ms | Tokens/s = 173,848.6
2025-01-16 03:16:34.443 | DEBUG    | __main__:<module>:313 - Training step 23220: loss = 3.2306 | 3013.07ms | Tokens/s = 174,004.7
2025-01-16 03:17:04.597 | DEBUG    | __main__:<module>:313 - Training step 23230: loss = 3.1813 | 3016.50ms | Tokens/s = 173,806.8
2025-01-16 03:17:34.747 | DEBUG    | __main__:<module>:313 - Training step 23240: loss = 3.2472 | 3014.08ms | Tokens/s = 173,946.6
2025-01-16 03:18:04.875 | DEBUG    | __main__:<module>:313 - Training step 23250: loss = 3.3238 | 3010.75ms | Tokens/s = 174,138.6
2025-01-16 03:18:35.014 | DEBUG    | __main__:<module>:313 - Training step 23260: loss = 3.0986 | 3014.65ms | Tokens/s = 173,913.2
2025-01-16 03:19:05.189 | DEBUG    | __main__:<module>:313 - Training step 23270: loss = 3.2202 | 3015.76ms | Tokens/s = 173,849.5
2025-01-16 03:19:35.352 | DEBUG    | __main__:<module>:313 - Training step 23280: loss = 3.1516 | 3014.88ms | Tokens/s = 173,900.1
2025-01-16 03:20:05.485 | DEBUG    | __main__:<module>:313 - Training step 23290: loss = 3.2408 | 3011.68ms | Tokens/s = 174,084.9
2025-01-16 03:20:35.595 | DEBUG    | __main__:<module>:313 - Training step 23300: loss = 3.2091 | 3009.47ms | Tokens/s = 174,212.7
2025-01-16 03:21:05.714 | DEBUG    | __main__:<module>:313 - Training step 23310: loss = 3.2226 | 3011.14ms | Tokens/s = 174,116.2
2025-01-16 03:21:35.849 | DEBUG    | __main__:<module>:313 - Training step 23320: loss = 3.2026 | 3014.63ms | Tokens/s = 173,914.4
2025-01-16 03:22:06.009 | DEBUG    | __main__:<module>:313 - Training step 23330: loss = 3.3262 | 3015.85ms | Tokens/s = 173,844.2
2025-01-16 03:22:36.147 | DEBUG    | __main__:<module>:313 - Training step 23340: loss = 3.1700 | 3013.53ms | Tokens/s = 173,978.2
2025-01-16 03:23:06.284 | DEBUG    | __main__:<module>:313 - Training step 23350: loss = 3.2624 | 3016.38ms | Tokens/s = 173,813.7
2025-01-16 03:23:36.459 | DEBUG    | __main__:<module>:313 - Training step 23360: loss = 3.1828 | 3017.82ms | Tokens/s = 173,731.0
2025-01-16 03:24:06.643 | DEBUG    | __main__:<module>:313 - Training step 23370: loss = 3.1741 | 3017.68ms | Tokens/s = 173,738.6
2025-01-16 03:24:36.816 | DEBUG    | __main__:<module>:313 - Training step 23380: loss = 3.2502 | 3017.35ms | Tokens/s = 173,757.7
2025-01-16 03:25:06.956 | DEBUG    | __main__:<module>:313 - Training step 23390: loss = 3.3246 | 3011.37ms | Tokens/s = 174,102.8
2025-01-16 03:25:37.083 | DEBUG    | __main__:<module>:313 - Training step 23400: loss = 3.2487 | 3008.96ms | Tokens/s = 174,242.5
2025-01-16 03:26:07.210 | DEBUG    | __main__:<module>:313 - Training step 23410: loss = 3.3637 | 3013.85ms | Tokens/s = 173,959.3
2025-01-16 03:26:37.369 | DEBUG    | __main__:<module>:313 - Training step 23420: loss = 3.3127 | 3016.89ms | Tokens/s = 173,784.3
2025-01-16 03:27:07.542 | DEBUG    | __main__:<module>:313 - Training step 23430: loss = 3.0430 | 3018.73ms | Tokens/s = 173,678.2
2025-01-16 03:27:37.675 | DEBUG    | __main__:<module>:313 - Training step 23440: loss = 3.2061 | 3012.39ms | Tokens/s = 174,043.6
2025-01-16 03:28:07.796 | DEBUG    | __main__:<module>:313 - Training step 23450: loss = 3.1403 | 3011.71ms | Tokens/s = 174,083.3
2025-01-16 03:28:37.945 | DEBUG    | __main__:<module>:313 - Training step 23460: loss = 3.0933 | 3017.48ms | Tokens/s = 173,750.4
2025-01-16 03:29:08.120 | DEBUG    | __main__:<module>:313 - Training step 23470: loss = 3.2224 | 3016.85ms | Tokens/s = 173,786.6
2025-01-16 03:29:38.291 | DEBUG    | __main__:<module>:313 - Training step 23480: loss = 3.0912 | 3017.82ms | Tokens/s = 173,730.8
2025-01-16 03:30:08.436 | DEBUG    | __main__:<module>:313 - Training step 23490: loss = 3.2191 | 3012.48ms | Tokens/s = 174,038.6
2025-01-16 03:30:38.588 | DEBUG    | __main__:<module>:313 - Training step 23500: loss = 3.1607 | 3017.05ms | Tokens/s = 173,775.3
2025-01-16 03:31:08.753 | DEBUG    | __main__:<module>:313 - Training step 23510: loss = 3.4749 | 3011.60ms | Tokens/s = 174,089.4
2025-01-16 03:31:38.894 | DEBUG    | __main__:<module>:313 - Training step 23520: loss = 3.1922 | 3012.94ms | Tokens/s = 174,012.3
2025-01-16 03:32:09.058 | DEBUG    | __main__:<module>:313 - Training step 23530: loss = 3.1568 | 3018.14ms | Tokens/s = 173,712.0
2025-01-16 03:32:39.237 | DEBUG    | __main__:<module>:313 - Training step 23540: loss = 3.1507 | 3018.97ms | Tokens/s = 173,664.8
2025-01-16 03:33:09.402 | DEBUG    | __main__:<module>:313 - Training step 23550: loss = 3.1845 | 3017.32ms | Tokens/s = 173,759.3
2025-01-16 03:33:39.584 | DEBUG    | __main__:<module>:313 - Training step 23560: loss = 3.0652 | 3015.44ms | Tokens/s = 173,867.7
2025-01-16 03:34:09.745 | DEBUG    | __main__:<module>:313 - Training step 23570: loss = 3.2605 | 3015.34ms | Tokens/s = 173,873.8
2025-01-16 03:34:39.895 | DEBUG    | __main__:<module>:313 - Training step 23580: loss = 3.2042 | 3013.33ms | Tokens/s = 173,989.3
2025-01-16 03:35:10.020 | DEBUG    | __main__:<module>:313 - Training step 23590: loss = 3.0635 | 3011.41ms | Tokens/s = 174,100.4
2025-01-16 03:35:40.152 | DEBUG    | __main__:<module>:313 - Training step 23600: loss = 3.2713 | 3013.31ms | Tokens/s = 173,990.7
2025-01-16 03:36:10.316 | DEBUG    | __main__:<module>:313 - Training step 23610: loss = 3.1538 | 3016.38ms | Tokens/s = 173,813.8
2025-01-16 03:36:40.475 | DEBUG    | __main__:<module>:313 - Training step 23620: loss = 3.1146 | 3014.97ms | Tokens/s = 173,894.7
2025-01-16 03:37:10.614 | DEBUG    | __main__:<module>:313 - Training step 23630: loss = 3.2490 | 3013.60ms | Tokens/s = 173,973.8
2025-01-16 03:37:40.760 | DEBUG    | __main__:<module>:313 - Training step 23640: loss = 3.2932 | 3015.50ms | Tokens/s = 173,864.5
2025-01-16 03:38:10.894 | DEBUG    | __main__:<module>:313 - Training step 23650: loss = 3.2696 | 3015.85ms | Tokens/s = 173,844.0
2025-01-16 03:38:41.061 | DEBUG    | __main__:<module>:313 - Training step 23660: loss = 3.2373 | 3018.16ms | Tokens/s = 173,710.9
2025-01-16 03:39:11.210 | DEBUG    | __main__:<module>:313 - Training step 23670: loss = 3.2572 | 3011.93ms | Tokens/s = 174,070.6
2025-01-16 03:39:41.338 | DEBUG    | __main__:<module>:313 - Training step 23680: loss = 3.3165 | 3011.58ms | Tokens/s = 174,090.4
2025-01-16 03:40:11.471 | DEBUG    | __main__:<module>:313 - Training step 23690: loss = 3.2410 | 3012.08ms | Tokens/s = 174,061.5
2025-01-16 03:40:41.630 | DEBUG    | __main__:<module>:313 - Training step 23700: loss = 3.2187 | 3016.13ms | Tokens/s = 173,828.3
2025-01-16 03:41:11.801 | DEBUG    | __main__:<module>:313 - Training step 23710: loss = 3.1947 | 3017.23ms | Tokens/s = 173,764.8
2025-01-16 03:41:41.973 | DEBUG    | __main__:<module>:313 - Training step 23720: loss = 3.3080 | 3015.18ms | Tokens/s = 173,882.7
2025-01-16 03:42:12.128 | DEBUG    | __main__:<module>:313 - Training step 23730: loss = 3.1194 | 3015.48ms | Tokens/s = 173,865.7
2025-01-16 03:42:42.267 | DEBUG    | __main__:<module>:313 - Training step 23740: loss = 3.1255 | 3011.67ms | Tokens/s = 174,085.7
2025-01-16 03:43:12.389 | DEBUG    | __main__:<module>:313 - Training step 23750: loss = 3.1612 | 3011.38ms | Tokens/s = 174,102.0
2025-01-16 03:43:42.512 | DEBUG    | __main__:<module>:313 - Training step 23760: loss = 3.3418 | 3012.40ms | Tokens/s = 174,043.3
2025-01-16 03:44:12.632 | DEBUG    | __main__:<module>:313 - Training step 23770: loss = 3.1169 | 3011.43ms | Tokens/s = 174,099.1
2025-01-16 03:44:42.767 | DEBUG    | __main__:<module>:313 - Training step 23780: loss = 2.9816 | 3011.84ms | Tokens/s = 174,075.9
2025-01-16 03:45:12.916 | DEBUG    | __main__:<module>:313 - Training step 23790: loss = 3.3057 | 3016.81ms | Tokens/s = 173,788.7
2025-01-16 03:45:43.092 | DEBUG    | __main__:<module>:313 - Training step 23800: loss = 3.3266 | 3018.55ms | Tokens/s = 173,688.9
2025-01-16 03:46:13.251 | DEBUG    | __main__:<module>:313 - Training step 23810: loss = 3.3072 | 3014.68ms | Tokens/s = 173,911.5
2025-01-16 03:46:43.378 | DEBUG    | __main__:<module>:313 - Training step 23820: loss = 3.1674 | 3010.61ms | Tokens/s = 174,146.9
2025-01-16 03:47:13.490 | DEBUG    | __main__:<module>:313 - Training step 23830: loss = 3.1811 | 3009.28ms | Tokens/s = 174,223.5
2025-01-16 03:47:43.599 | DEBUG    | __main__:<module>:313 - Training step 23840: loss = 3.2134 | 3012.52ms | Tokens/s = 174,036.1
2025-01-16 03:48:13.716 | DEBUG    | __main__:<module>:313 - Training step 23850: loss = 3.3095 | 3011.88ms | Tokens/s = 174,073.1
2025-01-16 03:48:43.852 | DEBUG    | __main__:<module>:313 - Training step 23860: loss = 3.1523 | 3012.70ms | Tokens/s = 174,026.1
2025-01-16 03:49:14.006 | DEBUG    | __main__:<module>:313 - Training step 23870: loss = 3.1372 | 3014.58ms | Tokens/s = 173,917.5
2025-01-16 03:49:44.142 | DEBUG    | __main__:<module>:313 - Training step 23880: loss = 2.9336 | 3010.97ms | Tokens/s = 174,126.0
2025-01-16 03:50:14.286 | DEBUG    | __main__:<module>:313 - Training step 23890: loss = 3.3234 | 3015.50ms | Tokens/s = 173,864.1
2025-01-16 03:50:44.454 | DEBUG    | __main__:<module>:313 - Training step 23900: loss = 3.3569 | 3014.78ms | Tokens/s = 173,905.9
2025-01-16 03:51:14.638 | DEBUG    | __main__:<module>:313 - Training step 23910: loss = 3.1382 | 3019.37ms | Tokens/s = 173,641.5
2025-01-16 03:51:44.800 | DEBUG    | __main__:<module>:313 - Training step 23920: loss = 3.1619 | 3015.12ms | Tokens/s = 173,886.3
2025-01-16 03:52:14.933 | DEBUG    | __main__:<module>:313 - Training step 23930: loss = 3.1755 | 3013.64ms | Tokens/s = 173,971.9
2025-01-16 03:52:45.059 | DEBUG    | __main__:<module>:313 - Training step 23940: loss = 3.3213 | 3013.15ms | Tokens/s = 174,000.1
2025-01-16 03:53:15.180 | DEBUG    | __main__:<module>:313 - Training step 23950: loss = 3.2893 | 3012.52ms | Tokens/s = 174,036.3
2025-01-16 03:53:45.345 | DEBUG    | __main__:<module>:313 - Training step 23960: loss = 3.0591 | 3016.22ms | Tokens/s = 173,823.0
2025-01-16 03:54:15.511 | DEBUG    | __main__:<module>:313 - Training step 23970: loss = 3.1916 | 3018.15ms | Tokens/s = 173,712.0
2025-01-16 03:54:45.652 | DEBUG    | __main__:<module>:313 - Training step 23980: loss = 3.2151 | 3011.47ms | Tokens/s = 174,096.8
2025-01-16 03:55:15.783 | DEBUG    | __main__:<module>:313 - Training step 23990: loss = 3.1544 | 3013.23ms | Tokens/s = 173,995.4
2025-01-16 03:55:49.326 | INFO     | __main__:<module>:265 - Step 24,000/40,000 loss: 3.1968 (T) 3.2028 (V) | lr=3.8e-03
2025-01-16 03:55:49.327 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 03:56:02.447 | DEBUG    | __main__:<module>:313 - Training step 24000: loss = 3.2773 | 19556.93ms | Tokens/s = 26,808.3
2025-01-16 03:56:32.491 | DEBUG    | __main__:<module>:313 - Training step 24010: loss = 3.2288 | 3004.86ms | Tokens/s = 174,480.1
2025-01-16 03:57:02.588 | DEBUG    | __main__:<module>:313 - Training step 24020: loss = 3.1462 | 3011.07ms | Tokens/s = 174,119.9
2025-01-16 03:57:32.710 | DEBUG    | __main__:<module>:313 - Training step 24030: loss = 3.0593 | 3013.19ms | Tokens/s = 173,997.7
2025-01-16 03:58:02.853 | DEBUG    | __main__:<module>:313 - Training step 24040: loss = 3.2675 | 3015.03ms | Tokens/s = 173,891.5
2025-01-16 03:58:33.023 | DEBUG    | __main__:<module>:313 - Training step 24050: loss = 3.2079 | 3019.59ms | Tokens/s = 173,628.9
2025-01-16 03:59:03.209 | DEBUG    | __main__:<module>:313 - Training step 24060: loss = 3.2704 | 3020.55ms | Tokens/s = 173,573.5
2025-01-16 03:59:33.391 | DEBUG    | __main__:<module>:313 - Training step 24070: loss = 3.2331 | 3016.77ms | Tokens/s = 173,791.1
2025-01-16 04:00:03.537 | DEBUG    | __main__:<module>:313 - Training step 24080: loss = 3.2097 | 3013.01ms | Tokens/s = 174,008.3
2025-01-16 04:00:33.672 | DEBUG    | __main__:<module>:313 - Training step 24090: loss = 3.1889 | 3012.94ms | Tokens/s = 174,011.9
2025-01-16 04:01:03.808 | DEBUG    | __main__:<module>:313 - Training step 24100: loss = 3.1917 | 3012.36ms | Tokens/s = 174,045.7
2025-01-16 04:01:33.927 | DEBUG    | __main__:<module>:313 - Training step 24110: loss = 3.2732 | 3010.49ms | Tokens/s = 174,153.8
2025-01-16 04:02:04.047 | DEBUG    | __main__:<module>:313 - Training step 24120: loss = 3.1669 | 3014.42ms | Tokens/s = 173,926.4
2025-01-16 04:02:34.189 | DEBUG    | __main__:<module>:313 - Training step 24130: loss = 3.3713 | 3013.87ms | Tokens/s = 173,958.2
2025-01-16 04:03:04.364 | DEBUG    | __main__:<module>:313 - Training step 24140: loss = 3.1710 | 3020.37ms | Tokens/s = 173,584.1
2025-01-16 04:03:34.513 | DEBUG    | __main__:<module>:313 - Training step 24150: loss = 3.3607 | 3012.67ms | Tokens/s = 174,027.8
2025-01-16 04:04:04.636 | DEBUG    | __main__:<module>:313 - Training step 24160: loss = 3.2268 | 3010.68ms | Tokens/s = 174,142.4
2025-01-16 04:04:34.758 | DEBUG    | __main__:<module>:313 - Training step 24170: loss = 3.3303 | 3013.09ms | Tokens/s = 174,003.6
2025-01-16 04:05:04.800 | DEBUG    | __main__:<module>:313 - Training step 24180: loss = 2.9388 | 2979.22ms | Tokens/s = 175,981.9
2025-01-16 04:05:34.597 | DEBUG    | __main__:<module>:313 - Training step 24190: loss = 3.1189 | 2991.03ms | Tokens/s = 175,286.5
2025-01-16 04:06:04.624 | DEBUG    | __main__:<module>:313 - Training step 24200: loss = 2.9924 | 3008.26ms | Tokens/s = 174,282.6
2025-01-16 04:06:34.694 | DEBUG    | __main__:<module>:313 - Training step 24210: loss = 3.1499 | 3010.70ms | Tokens/s = 174,141.7
2025-01-16 04:07:04.815 | DEBUG    | __main__:<module>:313 - Training step 24220: loss = 3.2239 | 3010.39ms | Tokens/s = 174,159.7
2025-01-16 04:07:34.932 | DEBUG    | __main__:<module>:313 - Training step 24230: loss = 3.2871 | 3011.86ms | Tokens/s = 174,074.3
2025-01-16 04:08:05.074 | DEBUG    | __main__:<module>:313 - Training step 24240: loss = 3.2042 | 3014.74ms | Tokens/s = 173,908.1
2025-01-16 04:08:35.245 | DEBUG    | __main__:<module>:313 - Training step 24250: loss = 3.0821 | 3017.27ms | Tokens/s = 173,762.1
2025-01-16 04:09:05.390 | DEBUG    | __main__:<module>:313 - Training step 24260: loss = 3.1430 | 3012.45ms | Tokens/s = 174,040.4
2025-01-16 04:09:35.520 | DEBUG    | __main__:<module>:313 - Training step 24270: loss = 3.1195 | 3011.63ms | Tokens/s = 174,087.7
2025-01-16 04:10:05.657 | DEBUG    | __main__:<module>:313 - Training step 24280: loss = 3.3932 | 3013.23ms | Tokens/s = 173,995.3
2025-01-16 04:10:35.827 | DEBUG    | __main__:<module>:313 - Training step 24290: loss = 3.3145 | 3017.32ms | Tokens/s = 173,759.5
2025-01-16 04:11:06.002 | DEBUG    | __main__:<module>:313 - Training step 24300: loss = 3.3453 | 3016.19ms | Tokens/s = 173,824.7
2025-01-16 04:11:36.175 | DEBUG    | __main__:<module>:313 - Training step 24310: loss = 3.2458 | 3016.41ms | Tokens/s = 173,811.9
2025-01-16 04:12:06.328 | DEBUG    | __main__:<module>:313 - Training step 24320: loss = 3.1595 | 3015.11ms | Tokens/s = 173,887.0
2025-01-16 04:12:36.501 | DEBUG    | __main__:<module>:313 - Training step 24330: loss = 3.2490 | 3017.28ms | Tokens/s = 173,762.1
2025-01-16 04:13:06.654 | DEBUG    | __main__:<module>:313 - Training step 24340: loss = 3.2112 | 3013.93ms | Tokens/s = 173,955.2
2025-01-16 04:13:36.792 | DEBUG    | __main__:<module>:313 - Training step 24350: loss = 3.0984 | 3013.00ms | Tokens/s = 174,008.5
2025-01-16 04:14:06.933 | DEBUG    | __main__:<module>:313 - Training step 24360: loss = 3.3122 | 3015.78ms | Tokens/s = 173,848.3
2025-01-16 04:14:37.050 | DEBUG    | __main__:<module>:313 - Training step 24370: loss = 3.3304 | 3013.23ms | Tokens/s = 173,995.3
2025-01-16 04:15:07.153 | DEBUG    | __main__:<module>:313 - Training step 24380: loss = 3.2932 | 3008.54ms | Tokens/s = 174,266.6
2025-01-16 04:15:37.263 | DEBUG    | __main__:<module>:313 - Training step 24390: loss = 3.3119 | 3011.72ms | Tokens/s = 174,082.4
2025-01-16 04:16:07.417 | DEBUG    | __main__:<module>:313 - Training step 24400: loss = 3.2333 | 3014.34ms | Tokens/s = 173,931.1
2025-01-16 04:16:37.593 | DEBUG    | __main__:<module>:313 - Training step 24410: loss = 3.1387 | 3017.38ms | Tokens/s = 173,756.1
2025-01-16 04:17:07.764 | DEBUG    | __main__:<module>:313 - Training step 24420: loss = 3.3350 | 3016.34ms | Tokens/s = 173,816.0
2025-01-16 04:17:37.911 | DEBUG    | __main__:<module>:313 - Training step 24430: loss = 3.1529 | 3014.65ms | Tokens/s = 173,913.4
2025-01-16 04:18:08.049 | DEBUG    | __main__:<module>:313 - Training step 24440: loss = 3.1430 | 3014.84ms | Tokens/s = 173,902.3
2025-01-16 04:18:38.179 | DEBUG    | __main__:<module>:313 - Training step 24450: loss = 3.2004 | 3012.65ms | Tokens/s = 174,029.1
2025-01-16 04:19:08.328 | DEBUG    | __main__:<module>:313 - Training step 24460: loss = 3.1799 | 3016.16ms | Tokens/s = 173,826.6
2025-01-16 04:19:38.507 | DEBUG    | __main__:<module>:313 - Training step 24470: loss = 2.9166 | 3016.31ms | Tokens/s = 173,817.5
2025-01-16 04:20:08.665 | DEBUG    | __main__:<module>:313 - Training step 24480: loss = 3.3436 | 3014.91ms | Tokens/s = 173,898.3
2025-01-16 04:20:38.804 | DEBUG    | __main__:<module>:313 - Training step 24490: loss = 3.1719 | 3012.78ms | Tokens/s = 174,021.2
2025-01-16 04:21:08.931 | DEBUG    | __main__:<module>:313 - Training step 24500: loss = 3.1966 | 3009.25ms | Tokens/s = 174,225.5
2025-01-16 04:21:39.054 | DEBUG    | __main__:<module>:313 - Training step 24510: loss = 3.1498 | 3011.92ms | Tokens/s = 174,071.2
2025-01-16 04:22:09.184 | DEBUG    | __main__:<module>:313 - Training step 24520: loss = 3.2042 | 3013.59ms | Tokens/s = 173,974.4
2025-01-16 04:22:39.350 | DEBUG    | __main__:<module>:313 - Training step 24530: loss = 3.2639 | 3017.15ms | Tokens/s = 173,769.1
2025-01-16 04:23:09.526 | DEBUG    | __main__:<module>:313 - Training step 24540: loss = 3.2923 | 3013.91ms | Tokens/s = 173,956.0
2025-01-16 04:23:39.685 | DEBUG    | __main__:<module>:313 - Training step 24550: loss = 3.1307 | 3014.86ms | Tokens/s = 173,901.5
2025-01-16 04:24:09.816 | DEBUG    | __main__:<module>:313 - Training step 24560: loss = 3.1806 | 3011.07ms | Tokens/s = 174,120.2
2025-01-16 04:24:39.935 | DEBUG    | __main__:<module>:313 - Training step 24570: loss = 3.2045 | 3012.75ms | Tokens/s = 174,023.2
2025-01-16 04:25:10.071 | DEBUG    | __main__:<module>:313 - Training step 24580: loss = 3.2060 | 3013.66ms | Tokens/s = 173,970.7
2025-01-16 04:25:40.240 | DEBUG    | __main__:<module>:313 - Training step 24590: loss = 3.0437 | 3017.39ms | Tokens/s = 173,755.7
2025-01-16 04:26:10.426 | DEBUG    | __main__:<module>:313 - Training step 24600: loss = 3.0756 | 3016.77ms | Tokens/s = 173,791.1
2025-01-16 04:26:40.577 | DEBUG    | __main__:<module>:313 - Training step 24610: loss = 3.2102 | 3015.01ms | Tokens/s = 173,892.6
2025-01-16 04:27:10.723 | DEBUG    | __main__:<module>:313 - Training step 24620: loss = 3.2727 | 3015.00ms | Tokens/s = 173,893.2
2025-01-16 04:27:40.845 | DEBUG    | __main__:<module>:313 - Training step 24630: loss = 3.0924 | 3011.19ms | Tokens/s = 174,113.1
2025-01-16 04:28:10.972 | DEBUG    | __main__:<module>:313 - Training step 24640: loss = 3.0951 | 3013.67ms | Tokens/s = 173,970.0
2025-01-16 04:28:41.136 | DEBUG    | __main__:<module>:313 - Training step 24650: loss = 3.1764 | 3019.09ms | Tokens/s = 173,657.9
2025-01-16 04:29:11.316 | DEBUG    | __main__:<module>:313 - Training step 24660: loss = 3.2793 | 3018.61ms | Tokens/s = 173,685.2
2025-01-16 04:29:41.478 | DEBUG    | __main__:<module>:313 - Training step 24670: loss = 3.0910 | 3013.61ms | Tokens/s = 173,973.3
2025-01-16 04:30:11.612 | DEBUG    | __main__:<module>:313 - Training step 24680: loss = 3.1447 | 3013.46ms | Tokens/s = 173,981.9
2025-01-16 04:30:41.750 | DEBUG    | __main__:<module>:313 - Training step 24690: loss = 3.2192 | 3015.39ms | Tokens/s = 173,870.8
2025-01-16 04:31:11.875 | DEBUG    | __main__:<module>:313 - Training step 24700: loss = 3.3000 | 3011.18ms | Tokens/s = 174,113.8
2025-01-16 04:31:41.992 | DEBUG    | __main__:<module>:313 - Training step 24710: loss = 3.0465 | 3012.37ms | Tokens/s = 174,044.9
2025-01-16 04:32:12.129 | DEBUG    | __main__:<module>:313 - Training step 24720: loss = 3.3500 | 3014.10ms | Tokens/s = 173,945.1
2025-01-16 04:32:42.306 | DEBUG    | __main__:<module>:313 - Training step 24730: loss = 3.0835 | 3017.22ms | Tokens/s = 173,765.2
2025-01-16 04:33:12.468 | DEBUG    | __main__:<module>:313 - Training step 24740: loss = 3.1338 | 3016.21ms | Tokens/s = 173,823.5
2025-01-16 04:33:42.606 | DEBUG    | __main__:<module>:313 - Training step 24750: loss = 2.9345 | 3015.07ms | Tokens/s = 173,889.3
2025-01-16 04:34:12.744 | DEBUG    | __main__:<module>:313 - Training step 24760: loss = 3.0837 | 3012.37ms | Tokens/s = 174,045.0
2025-01-16 04:34:42.887 | DEBUG    | __main__:<module>:313 - Training step 24770: loss = 3.1149 | 3016.83ms | Tokens/s = 173,787.8
2025-01-16 04:35:13.060 | DEBUG    | __main__:<module>:313 - Training step 24780: loss = 3.1999 | 3019.64ms | Tokens/s = 173,625.8
2025-01-16 04:35:43.249 | DEBUG    | __main__:<module>:313 - Training step 24790: loss = 3.2219 | 3019.18ms | Tokens/s = 173,652.7
2025-01-16 04:36:13.418 | DEBUG    | __main__:<module>:313 - Training step 24800: loss = 3.2231 | 3014.05ms | Tokens/s = 173,948.3
2025-01-16 04:36:43.564 | DEBUG    | __main__:<module>:313 - Training step 24810: loss = 3.2716 | 3013.33ms | Tokens/s = 173,989.7
2025-01-16 04:37:13.695 | DEBUG    | __main__:<module>:313 - Training step 24820: loss = 3.1839 | 3013.17ms | Tokens/s = 173,999.0
2025-01-16 04:37:43.821 | DEBUG    | __main__:<module>:313 - Training step 24830: loss = 3.1522 | 3016.02ms | Tokens/s = 173,834.4
2025-01-16 04:38:13.990 | DEBUG    | __main__:<module>:313 - Training step 24840: loss = 3.1271 | 3018.35ms | Tokens/s = 173,700.0
2025-01-16 04:38:44.149 | DEBUG    | __main__:<module>:313 - Training step 24850: loss = 3.0681 | 3013.53ms | Tokens/s = 173,978.2
2025-01-16 04:39:14.281 | DEBUG    | __main__:<module>:313 - Training step 24860: loss = 3.3351 | 3010.79ms | Tokens/s = 174,136.4
2025-01-16 04:39:44.408 | DEBUG    | __main__:<module>:313 - Training step 24870: loss = 3.1273 | 3012.86ms | Tokens/s = 174,016.6
2025-01-16 04:40:14.538 | DEBUG    | __main__:<module>:313 - Training step 24880: loss = 3.2163 | 3013.75ms | Tokens/s = 173,965.6
2025-01-16 04:40:44.700 | DEBUG    | __main__:<module>:313 - Training step 24890: loss = 3.2309 | 3018.03ms | Tokens/s = 173,718.6
2025-01-16 04:41:14.846 | DEBUG    | __main__:<module>:313 - Training step 24900: loss = 3.1902 | 3012.39ms | Tokens/s = 174,043.7
2025-01-16 04:41:44.987 | DEBUG    | __main__:<module>:313 - Training step 24910: loss = 3.3976 | 3015.38ms | Tokens/s = 173,871.1
2025-01-16 04:42:15.115 | DEBUG    | __main__:<module>:313 - Training step 24920: loss = 3.0442 | 3010.78ms | Tokens/s = 174,137.0
2025-01-16 04:42:45.256 | DEBUG    | __main__:<module>:313 - Training step 24930: loss = 2.9853 | 3016.51ms | Tokens/s = 173,806.4
2025-01-16 04:43:15.433 | DEBUG    | __main__:<module>:313 - Training step 24940: loss = 3.1435 | 3017.78ms | Tokens/s = 173,733.1
2025-01-16 04:43:45.595 | DEBUG    | __main__:<module>:313 - Training step 24950: loss = 3.1850 | 3014.20ms | Tokens/s = 173,939.6
2025-01-16 04:44:15.735 | DEBUG    | __main__:<module>:313 - Training step 24960: loss = 3.1187 | 3012.84ms | Tokens/s = 174,017.7
2025-01-16 04:44:45.871 | DEBUG    | __main__:<module>:313 - Training step 24970: loss = 3.2237 | 3013.39ms | Tokens/s = 173,986.0
2025-01-16 04:45:16.041 | DEBUG    | __main__:<module>:313 - Training step 24980: loss = 3.2453 | 3017.13ms | Tokens/s = 173,770.7
2025-01-16 04:45:46.217 | DEBUG    | __main__:<module>:313 - Training step 24990: loss = 3.0410 | 3016.77ms | Tokens/s = 173,791.2
2025-01-16 04:46:19.774 | INFO     | __main__:<module>:265 - Step 25,000/40,000 loss: 3.1881 (T) 3.2042 (V) | lr=3.4e-03
2025-01-16 04:46:22.788 | DEBUG    | __main__:<module>:313 - Training step 25000: loss = 3.1679 | 9449.19ms | Tokens/s = 55,485.0
2025-01-16 04:46:52.922 | DEBUG    | __main__:<module>:313 - Training step 25010: loss = 3.1300 | 3014.32ms | Tokens/s = 173,932.5
2025-01-16 04:47:23.090 | DEBUG    | __main__:<module>:313 - Training step 25020: loss = 3.2967 | 3021.49ms | Tokens/s = 173,519.6
2025-01-16 04:47:53.270 | DEBUG    | __main__:<module>:313 - Training step 25030: loss = 3.1067 | 3016.56ms | Tokens/s = 173,803.3
2025-01-16 04:48:23.440 | DEBUG    | __main__:<module>:313 - Training step 25040: loss = 3.2458 | 3015.48ms | Tokens/s = 173,865.4
2025-01-16 04:48:53.595 | DEBUG    | __main__:<module>:313 - Training step 25050: loss = 3.2084 | 3014.46ms | Tokens/s = 173,924.2
2025-01-16 04:49:23.741 | DEBUG    | __main__:<module>:313 - Training step 25060: loss = 3.1010 | 3014.67ms | Tokens/s = 173,912.2
2025-01-16 04:49:53.876 | DEBUG    | __main__:<module>:313 - Training step 25070: loss = 3.3010 | 3014.12ms | Tokens/s = 173,943.9
2025-01-16 04:50:24.038 | DEBUG    | __main__:<module>:313 - Training step 25080: loss = 3.1903 | 3020.67ms | Tokens/s = 173,566.9
2025-01-16 04:50:54.226 | DEBUG    | __main__:<module>:313 - Training step 25090: loss = 2.9641 | 3016.72ms | Tokens/s = 173,794.2
2025-01-16 04:51:24.387 | DEBUG    | __main__:<module>:313 - Training step 25100: loss = 3.1915 | 3011.89ms | Tokens/s = 174,072.5
2025-01-16 04:51:54.521 | DEBUG    | __main__:<module>:313 - Training step 25110: loss = 3.2327 | 3012.88ms | Tokens/s = 174,015.8
2025-01-16 04:52:24.644 | DEBUG    | __main__:<module>:313 - Training step 25120: loss = 3.0679 | 3011.84ms | Tokens/s = 174,075.5
2025-01-16 04:52:54.768 | DEBUG    | __main__:<module>:313 - Training step 25130: loss = 3.3330 | 3012.69ms | Tokens/s = 174,026.6
2025-01-16 04:53:24.884 | DEBUG    | __main__:<module>:313 - Training step 25140: loss = 3.1245 | 3011.20ms | Tokens/s = 174,112.8
2025-01-16 04:53:55.006 | DEBUG    | __main__:<module>:313 - Training step 25150: loss = 3.2317 | 3012.78ms | Tokens/s = 174,021.0
2025-01-16 04:54:25.159 | DEBUG    | __main__:<module>:313 - Training step 25160: loss = 3.1322 | 3018.38ms | Tokens/s = 173,698.3
2025-01-16 04:54:55.342 | DEBUG    | __main__:<module>:313 - Training step 25170: loss = 3.0893 | 3018.97ms | Tokens/s = 173,664.6
2025-01-16 04:55:25.520 | DEBUG    | __main__:<module>:313 - Training step 25180: loss = 3.2292 | 3015.92ms | Tokens/s = 173,840.0
2025-01-16 04:55:55.662 | DEBUG    | __main__:<module>:313 - Training step 25190: loss = 3.1344 | 3013.50ms | Tokens/s = 173,979.7
2025-01-16 04:56:25.786 | DEBUG    | __main__:<module>:313 - Training step 25200: loss = 3.1794 | 3009.32ms | Tokens/s = 174,221.4
2025-01-16 04:56:55.918 | DEBUG    | __main__:<module>:313 - Training step 25210: loss = 3.2587 | 3013.18ms | Tokens/s = 173,998.1
2025-01-16 04:57:26.060 | DEBUG    | __main__:<module>:313 - Training step 25220: loss = 3.3207 | 3016.42ms | Tokens/s = 173,811.6
2025-01-16 04:57:56.222 | DEBUG    | __main__:<module>:313 - Training step 25230: loss = 3.3439 | 3017.61ms | Tokens/s = 173,743.0
2025-01-16 04:58:26.395 | DEBUG    | __main__:<module>:313 - Training step 25240: loss = 3.1967 | 3016.72ms | Tokens/s = 173,793.9
2025-01-16 04:58:56.557 | DEBUG    | __main__:<module>:313 - Training step 25250: loss = 3.1901 | 3015.32ms | Tokens/s = 173,874.7
2025-01-16 04:59:26.693 | DEBUG    | __main__:<module>:313 - Training step 25260: loss = 3.1468 | 3011.79ms | Tokens/s = 174,078.8
2025-01-16 04:59:56.814 | DEBUG    | __main__:<module>:313 - Training step 25270: loss = 3.1187 | 3014.59ms | Tokens/s = 173,917.1
2025-01-16 05:00:26.948 | DEBUG    | __main__:<module>:313 - Training step 25280: loss = 3.1908 | 3017.62ms | Tokens/s = 173,742.0
2025-01-16 05:00:57.119 | DEBUG    | __main__:<module>:313 - Training step 25290: loss = 3.2093 | 3018.88ms | Tokens/s = 173,669.5
2025-01-16 05:01:27.285 | DEBUG    | __main__:<module>:313 - Training step 25300: loss = 3.2828 | 3014.71ms | Tokens/s = 173,910.1
2025-01-16 05:01:57.462 | DEBUG    | __main__:<module>:313 - Training step 25310: loss = 3.0987 | 3021.37ms | Tokens/s = 173,526.7
2025-01-16 05:02:27.654 | DEBUG    | __main__:<module>:313 - Training step 25320: loss = 3.1079 | 3018.14ms | Tokens/s = 173,712.0
2025-01-16 05:02:57.846 | DEBUG    | __main__:<module>:313 - Training step 25330: loss = 3.2725 | 3018.74ms | Tokens/s = 173,677.6
2025-01-16 05:03:28.002 | DEBUG    | __main__:<module>:313 - Training step 25340: loss = 2.9212 | 3014.33ms | Tokens/s = 173,931.7
2025-01-16 05:03:58.150 | DEBUG    | __main__:<module>:313 - Training step 25350: loss = 3.1440 | 3016.01ms | Tokens/s = 173,834.7
2025-01-16 05:04:28.317 | DEBUG    | __main__:<module>:313 - Training step 25360: loss = 3.2307 | 3015.94ms | Tokens/s = 173,839.0
2025-01-16 05:04:58.463 | DEBUG    | __main__:<module>:313 - Training step 25370: loss = 3.1762 | 3013.75ms | Tokens/s = 173,965.3
2025-01-16 05:05:28.592 | DEBUG    | __main__:<module>:313 - Training step 25380: loss = 3.2076 | 3013.21ms | Tokens/s = 173,996.5
2025-01-16 05:05:58.711 | DEBUG    | __main__:<module>:313 - Training step 25390: loss = 3.2902 | 3012.91ms | Tokens/s = 174,013.6
2025-01-16 05:06:28.868 | DEBUG    | __main__:<module>:313 - Training step 25400: loss = 3.2466 | 3016.52ms | Tokens/s = 173,805.8
2025-01-16 05:06:59.047 | DEBUG    | __main__:<module>:313 - Training step 25410: loss = 3.2118 | 3014.39ms | Tokens/s = 173,928.1
2025-01-16 05:07:29.202 | DEBUG    | __main__:<module>:313 - Training step 25420: loss = 3.2255 | 3015.55ms | Tokens/s = 173,861.3
2025-01-16 05:07:59.345 | DEBUG    | __main__:<module>:313 - Training step 25430: loss = 3.1540 | 3015.87ms | Tokens/s = 173,842.9
2025-01-16 05:08:29.508 | DEBUG    | __main__:<module>:313 - Training step 25440: loss = 3.0670 | 3018.08ms | Tokens/s = 173,715.6
2025-01-16 05:08:59.689 | DEBUG    | __main__:<module>:313 - Training step 25450: loss = 3.1060 | 3015.75ms | Tokens/s = 173,850.0
2025-01-16 05:09:29.841 | DEBUG    | __main__:<module>:313 - Training step 25460: loss = 3.0919 | 3017.04ms | Tokens/s = 173,775.6
2025-01-16 05:09:59.980 | DEBUG    | __main__:<module>:313 - Training step 25470: loss = 3.2200 | 3011.20ms | Tokens/s = 174,112.8
2025-01-16 05:10:30.106 | DEBUG    | __main__:<module>:313 - Training step 25480: loss = 3.0914 | 3012.41ms | Tokens/s = 174,042.7
2025-01-16 05:11:00.233 | DEBUG    | __main__:<module>:313 - Training step 25490: loss = 3.1148 | 3013.43ms | Tokens/s = 173,983.6
2025-01-16 05:11:30.370 | DEBUG    | __main__:<module>:313 - Training step 25500: loss = 3.2182 | 3013.28ms | Tokens/s = 173,992.7
2025-01-16 05:12:00.499 | DEBUG    | __main__:<module>:313 - Training step 25510: loss = 3.0419 | 3012.58ms | Tokens/s = 174,032.7
2025-01-16 05:12:30.620 | DEBUG    | __main__:<module>:313 - Training step 25520: loss = 3.0062 | 3012.93ms | Tokens/s = 174,012.7
2025-01-16 05:13:00.749 | DEBUG    | __main__:<module>:313 - Training step 25530: loss = 3.1179 | 3015.14ms | Tokens/s = 173,885.3
2025-01-16 05:13:30.912 | DEBUG    | __main__:<module>:313 - Training step 25540: loss = 3.0978 | 3014.95ms | Tokens/s = 173,896.1
2025-01-16 05:14:01.055 | DEBUG    | __main__:<module>:313 - Training step 25550: loss = 3.1605 | 3016.11ms | Tokens/s = 173,829.0
2025-01-16 05:14:31.185 | DEBUG    | __main__:<module>:313 - Training step 25560: loss = 3.0835 | 3012.40ms | Tokens/s = 174,043.2
2025-01-16 05:15:01.301 | DEBUG    | __main__:<module>:313 - Training step 25570: loss = 3.3438 | 3012.28ms | Tokens/s = 174,050.0
2025-01-16 05:15:31.434 | DEBUG    | __main__:<module>:313 - Training step 25580: loss = 3.1707 | 3014.46ms | Tokens/s = 173,924.1
2025-01-16 05:16:01.581 | DEBUG    | __main__:<module>:313 - Training step 25590: loss = 3.1869 | 3015.16ms | Tokens/s = 173,883.7
2025-01-16 05:16:31.709 | DEBUG    | __main__:<module>:313 - Training step 25600: loss = 3.1873 | 3012.27ms | Tokens/s = 174,050.6
2025-01-16 05:17:01.829 | DEBUG    | __main__:<module>:313 - Training step 25610: loss = 3.0868 | 3012.37ms | Tokens/s = 174,044.9
2025-01-16 05:17:31.950 | DEBUG    | __main__:<module>:313 - Training step 25620: loss = 3.2838 | 3013.04ms | Tokens/s = 174,006.1
2025-01-16 05:18:02.080 | DEBUG    | __main__:<module>:313 - Training step 25630: loss = 3.1078 | 3015.70ms | Tokens/s = 173,852.9
2025-01-16 05:18:32.235 | DEBUG    | __main__:<module>:313 - Training step 25640: loss = 3.1792 | 3012.59ms | Tokens/s = 174,032.0
2025-01-16 05:19:02.378 | DEBUG    | __main__:<module>:313 - Training step 25650: loss = 3.3115 | 3011.50ms | Tokens/s = 174,095.3
2025-01-16 05:19:32.513 | DEBUG    | __main__:<module>:313 - Training step 25660: loss = 3.2081 | 3012.20ms | Tokens/s = 174,054.9
2025-01-16 05:20:02.658 | DEBUG    | __main__:<module>:313 - Training step 25670: loss = 2.9141 | 3014.03ms | Tokens/s = 173,948.9
2025-01-16 05:20:32.828 | DEBUG    | __main__:<module>:313 - Training step 25680: loss = 3.1361 | 3017.42ms | Tokens/s = 173,753.6
2025-01-16 05:21:02.977 | DEBUG    | __main__:<module>:313 - Training step 25690: loss = 3.1733 | 3012.24ms | Tokens/s = 174,052.8
2025-01-16 05:21:33.100 | DEBUG    | __main__:<module>:313 - Training step 25700: loss = 3.0521 | 3011.54ms | Tokens/s = 174,092.8
2025-01-16 05:22:03.242 | DEBUG    | __main__:<module>:313 - Training step 25710: loss = 3.1798 | 3015.31ms | Tokens/s = 173,875.5
2025-01-16 05:22:33.389 | DEBUG    | __main__:<module>:313 - Training step 25720: loss = 3.1205 | 3013.74ms | Tokens/s = 173,965.7
2025-01-16 05:23:03.515 | DEBUG    | __main__:<module>:313 - Training step 25730: loss = 3.3874 | 3011.88ms | Tokens/s = 174,073.2
2025-01-16 05:23:33.637 | DEBUG    | __main__:<module>:313 - Training step 25740: loss = 3.1643 | 3012.46ms | Tokens/s = 174,039.9
2025-01-16 05:24:03.793 | DEBUG    | __main__:<module>:313 - Training step 25750: loss = 3.2961 | 3018.14ms | Tokens/s = 173,712.3
2025-01-16 05:24:33.979 | DEBUG    | __main__:<module>:313 - Training step 25760: loss = 3.1002 | 3018.80ms | Tokens/s = 173,674.1
2025-01-16 05:25:04.126 | DEBUG    | __main__:<module>:313 - Training step 25770: loss = 3.2910 | 3012.52ms | Tokens/s = 174,036.5
2025-01-16 05:25:34.263 | DEBUG    | __main__:<module>:313 - Training step 25780: loss = 3.1885 | 3015.51ms | Tokens/s = 173,863.9
2025-01-16 05:26:04.423 | DEBUG    | __main__:<module>:313 - Training step 25790: loss = 3.2623 | 3016.80ms | Tokens/s = 173,789.6
2025-01-16 05:26:34.613 | DEBUG    | __main__:<module>:313 - Training step 25800: loss = 3.1461 | 3017.84ms | Tokens/s = 173,729.4
2025-01-16 05:27:04.772 | DEBUG    | __main__:<module>:313 - Training step 25810: loss = 3.2068 | 3014.76ms | Tokens/s = 173,907.2
2025-01-16 05:27:34.912 | DEBUG    | __main__:<module>:313 - Training step 25820: loss = 3.2202 | 3013.14ms | Tokens/s = 174,000.7
2025-01-16 05:28:05.055 | DEBUG    | __main__:<module>:313 - Training step 25830: loss = 2.9968 | 3010.81ms | Tokens/s = 174,135.2
2025-01-16 05:28:35.184 | DEBUG    | __main__:<module>:313 - Training step 25840: loss = 3.0890 | 3013.33ms | Tokens/s = 173,989.7
2025-01-16 05:29:05.299 | DEBUG    | __main__:<module>:313 - Training step 25850: loss = 3.1923 | 3012.76ms | Tokens/s = 174,022.2
2025-01-16 05:29:35.437 | DEBUG    | __main__:<module>:313 - Training step 25860: loss = 3.2731 | 3013.53ms | Tokens/s = 173,978.3
2025-01-16 05:30:05.609 | DEBUG    | __main__:<module>:313 - Training step 25870: loss = 2.9403 | 3014.30ms | Tokens/s = 173,933.7
2025-01-16 05:30:35.750 | DEBUG    | __main__:<module>:313 - Training step 25880: loss = 3.1774 | 3013.62ms | Tokens/s = 173,972.8
2025-01-16 05:31:05.871 | DEBUG    | __main__:<module>:313 - Training step 25890: loss = 3.2423 | 3013.70ms | Tokens/s = 173,968.1
2025-01-16 05:31:35.990 | DEBUG    | __main__:<module>:313 - Training step 25900: loss = 2.9166 | 3010.82ms | Tokens/s = 174,134.6
2025-01-16 05:32:06.126 | DEBUG    | __main__:<module>:313 - Training step 25910: loss = 3.2083 | 3015.82ms | Tokens/s = 173,846.1
2025-01-16 05:32:36.281 | DEBUG    | __main__:<module>:313 - Training step 25920: loss = 3.2420 | 3014.52ms | Tokens/s = 173,920.8
2025-01-16 05:33:06.450 | DEBUG    | __main__:<module>:313 - Training step 25930: loss = 3.1239 | 3019.19ms | Tokens/s = 173,651.6
2025-01-16 05:33:36.611 | DEBUG    | __main__:<module>:313 - Training step 25940: loss = 3.2621 | 3016.53ms | Tokens/s = 173,804.9
2025-01-16 05:34:06.758 | DEBUG    | __main__:<module>:313 - Training step 25950: loss = 3.1787 | 3014.55ms | Tokens/s = 173,919.4
2025-01-16 05:34:36.933 | DEBUG    | __main__:<module>:313 - Training step 25960: loss = 3.3711 | 3018.44ms | Tokens/s = 173,695.0
2025-01-16 05:35:07.126 | DEBUG    | __main__:<module>:313 - Training step 25970: loss = 3.2758 | 3021.18ms | Tokens/s = 173,537.4
2025-01-16 05:35:37.311 | DEBUG    | __main__:<module>:313 - Training step 25980: loss = 3.1245 | 3017.43ms | Tokens/s = 173,753.0
2025-01-16 05:36:07.477 | DEBUG    | __main__:<module>:313 - Training step 25990: loss = 3.1375 | 3012.85ms | Tokens/s = 174,017.0
2025-01-16 05:36:41.043 | INFO     | __main__:<module>:265 - Step 26,000/40,000 loss: 3.1699 (T) 3.1572 (V) | lr=3.0e-03
2025-01-16 05:36:41.045 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 05:36:54.625 | DEBUG    | __main__:<module>:313 - Training step 26000: loss = 3.1435 | 20024.98ms | Tokens/s = 26,181.7
2025-01-16 05:37:24.677 | DEBUG    | __main__:<module>:313 - Training step 26010: loss = 3.3809 | 3006.29ms | Tokens/s = 174,397.0
2025-01-16 05:37:54.775 | DEBUG    | __main__:<module>:313 - Training step 26020: loss = 3.2160 | 3011.25ms | Tokens/s = 174,109.8
2025-01-16 05:38:24.914 | DEBUG    | __main__:<module>:313 - Training step 26030: loss = 3.2737 | 3014.27ms | Tokens/s = 173,935.4
2025-01-16 05:38:55.082 | DEBUG    | __main__:<module>:313 - Training step 26040: loss = 3.2487 | 3016.91ms | Tokens/s = 173,783.2
2025-01-16 05:39:25.270 | DEBUG    | __main__:<module>:313 - Training step 26050: loss = 3.0690 | 3019.71ms | Tokens/s = 173,621.7
2025-01-16 05:39:55.432 | DEBUG    | __main__:<module>:313 - Training step 26060: loss = 2.9680 | 3014.41ms | Tokens/s = 173,927.3
2025-01-16 05:40:25.597 | DEBUG    | __main__:<module>:313 - Training step 26070: loss = 3.1893 | 3018.21ms | Tokens/s = 173,708.1
2025-01-16 05:40:55.775 | DEBUG    | __main__:<module>:313 - Training step 26080: loss = 3.0880 | 3016.29ms | Tokens/s = 173,819.0
2025-01-16 05:41:25.947 | DEBUG    | __main__:<module>:313 - Training step 26090: loss = 3.1161 | 3019.75ms | Tokens/s = 173,619.9
2025-01-16 05:41:56.098 | DEBUG    | __main__:<module>:313 - Training step 26100: loss = 3.2625 | 3011.78ms | Tokens/s = 174,078.9
2025-01-16 05:42:26.240 | DEBUG    | __main__:<module>:313 - Training step 26110: loss = 2.9090 | 3017.80ms | Tokens/s = 173,731.9
2025-01-16 05:42:56.411 | DEBUG    | __main__:<module>:313 - Training step 26120: loss = 3.0949 | 3017.88ms | Tokens/s = 173,727.3
2025-01-16 05:43:26.589 | DEBUG    | __main__:<module>:313 - Training step 26130: loss = 3.1077 | 3014.11ms | Tokens/s = 173,944.3
2025-01-16 05:43:56.748 | DEBUG    | __main__:<module>:313 - Training step 26140: loss = 3.0662 | 3013.58ms | Tokens/s = 173,974.9
2025-01-16 05:44:26.920 | DEBUG    | __main__:<module>:313 - Training step 26150: loss = 3.0971 | 3019.63ms | Tokens/s = 173,626.7
2025-01-16 05:44:57.100 | DEBUG    | __main__:<module>:313 - Training step 26160: loss = 3.2369 | 3015.83ms | Tokens/s = 173,845.6
2025-01-16 05:45:27.256 | DEBUG    | __main__:<module>:313 - Training step 26170: loss = 3.1031 | 3013.99ms | Tokens/s = 173,951.5
2025-01-16 05:45:57.396 | DEBUG    | __main__:<module>:313 - Training step 26180: loss = 2.8262 | 3011.33ms | Tokens/s = 174,104.9
2025-01-16 05:46:27.545 | DEBUG    | __main__:<module>:313 - Training step 26190: loss = 3.0339 | 3017.99ms | Tokens/s = 173,720.7
2025-01-16 05:46:57.722 | DEBUG    | __main__:<module>:313 - Training step 26200: loss = 3.2533 | 3016.28ms | Tokens/s = 173,819.2
2025-01-16 05:47:27.873 | DEBUG    | __main__:<module>:313 - Training step 26210: loss = 3.2007 | 3012.12ms | Tokens/s = 174,059.2
2025-01-16 05:47:58.040 | DEBUG    | __main__:<module>:313 - Training step 26220: loss = 3.2172 | 3016.62ms | Tokens/s = 173,799.6
2025-01-16 05:48:28.199 | DEBUG    | __main__:<module>:313 - Training step 26230: loss = 3.2181 | 3014.81ms | Tokens/s = 173,904.2
2025-01-16 05:48:58.362 | DEBUG    | __main__:<module>:313 - Training step 26240: loss = 3.2854 | 3019.24ms | Tokens/s = 173,648.9
2025-01-16 05:49:28.540 | DEBUG    | __main__:<module>:313 - Training step 26250: loss = 3.1570 | 3015.16ms | Tokens/s = 173,884.2
2025-01-16 05:49:58.708 | DEBUG    | __main__:<module>:313 - Training step 26260: loss = 3.3448 | 3014.77ms | Tokens/s = 173,906.4
2025-01-16 05:50:28.861 | DEBUG    | __main__:<module>:313 - Training step 26270: loss = 3.2221 | 3013.58ms | Tokens/s = 173,975.2
2025-01-16 05:50:59.016 | DEBUG    | __main__:<module>:313 - Training step 26280: loss = 3.2332 | 3014.89ms | Tokens/s = 173,899.4
2025-01-16 05:51:29.165 | DEBUG    | __main__:<module>:313 - Training step 26290: loss = 3.1859 | 3013.12ms | Tokens/s = 174,001.7
2025-01-16 05:51:59.322 | DEBUG    | __main__:<module>:313 - Training step 26300: loss = 3.0981 | 3015.67ms | Tokens/s = 173,854.5
2025-01-16 05:52:29.479 | DEBUG    | __main__:<module>:313 - Training step 26310: loss = 3.2731 | 3015.43ms | Tokens/s = 173,868.1
2025-01-16 05:52:59.644 | DEBUG    | __main__:<module>:313 - Training step 26320: loss = 3.2194 | 3019.13ms | Tokens/s = 173,655.1
2025-01-16 05:53:29.835 | DEBUG    | __main__:<module>:313 - Training step 26330: loss = 3.0979 | 3018.30ms | Tokens/s = 173,703.1
2025-01-16 05:54:00.015 | DEBUG    | __main__:<module>:313 - Training step 26340: loss = 3.0255 | 3016.90ms | Tokens/s = 173,783.6
2025-01-16 05:54:30.175 | DEBUG    | __main__:<module>:313 - Training step 26350: loss = 3.0867 | 3015.80ms | Tokens/s = 173,847.1
2025-01-16 05:55:00.325 | DEBUG    | __main__:<module>:313 - Training step 26360: loss = 3.1342 | 3016.60ms | Tokens/s = 173,800.9
2025-01-16 05:55:30.473 | DEBUG    | __main__:<module>:313 - Training step 26370: loss = 3.0648 | 3015.11ms | Tokens/s = 173,886.8
2025-01-16 05:56:00.613 | DEBUG    | __main__:<module>:313 - Training step 26380: loss = 3.0552 | 3014.02ms | Tokens/s = 173,949.9
2025-01-16 05:56:30.744 | DEBUG    | __main__:<module>:313 - Training step 26390: loss = 3.1151 | 3012.14ms | Tokens/s = 174,058.5
2025-01-16 05:57:00.898 | DEBUG    | __main__:<module>:313 - Training step 26400: loss = 3.2700 | 3016.91ms | Tokens/s = 173,783.3
2025-01-16 05:57:31.080 | DEBUG    | __main__:<module>:313 - Training step 26410: loss = 3.1847 | 3017.33ms | Tokens/s = 173,759.2
2025-01-16 05:58:01.265 | DEBUG    | __main__:<module>:313 - Training step 26420: loss = 3.0924 | 3017.11ms | Tokens/s = 173,771.4
2025-01-16 05:58:31.430 | DEBUG    | __main__:<module>:313 - Training step 26430: loss = 3.1560 | 3015.66ms | Tokens/s = 173,854.9
2025-01-16 05:59:01.578 | DEBUG    | __main__:<module>:313 - Training step 26440: loss = 3.1406 | 3015.31ms | Tokens/s = 173,875.3
2025-01-16 05:59:31.714 | DEBUG    | __main__:<module>:313 - Training step 26450: loss = 3.0727 | 3012.12ms | Tokens/s = 174,059.2
2025-01-16 06:00:01.844 | DEBUG    | __main__:<module>:313 - Training step 26460: loss = 3.1144 | 3013.82ms | Tokens/s = 173,961.3
2025-01-16 06:00:32.006 | DEBUG    | __main__:<module>:313 - Training step 26470: loss = 3.0398 | 3017.09ms | Tokens/s = 173,772.5
2025-01-16 06:01:02.152 | DEBUG    | __main__:<module>:313 - Training step 26480: loss = 3.1233 | 3015.63ms | Tokens/s = 173,856.6
2025-01-16 06:01:32.321 | DEBUG    | __main__:<module>:313 - Training step 26490: loss = 3.2837 | 3016.52ms | Tokens/s = 173,805.3
2025-01-16 06:02:02.482 | DEBUG    | __main__:<module>:313 - Training step 26500: loss = 3.0175 | 3014.64ms | Tokens/s = 173,913.8
2025-01-16 06:02:32.630 | DEBUG    | __main__:<module>:313 - Training step 26510: loss = 2.9864 | 3013.56ms | Tokens/s = 173,976.2
2025-01-16 06:03:02.758 | DEBUG    | __main__:<module>:313 - Training step 26520: loss = 3.3410 | 3012.51ms | Tokens/s = 174,036.7
2025-01-16 06:03:32.894 | DEBUG    | __main__:<module>:313 - Training step 26530: loss = 3.2375 | 3014.01ms | Tokens/s = 173,950.6
2025-01-16 06:04:03.049 | DEBUG    | __main__:<module>:313 - Training step 26540: loss = 3.2393 | 3015.70ms | Tokens/s = 173,852.8
2025-01-16 06:04:33.218 | DEBUG    | __main__:<module>:313 - Training step 26550: loss = 3.1057 | 3015.47ms | Tokens/s = 173,866.3
2025-01-16 06:05:03.372 | DEBUG    | __main__:<module>:313 - Training step 26560: loss = 3.3558 | 3014.59ms | Tokens/s = 173,916.6
2025-01-16 06:05:33.533 | DEBUG    | __main__:<module>:313 - Training step 26570: loss = 3.0858 | 3017.23ms | Tokens/s = 173,764.5
2025-01-16 06:06:03.708 | DEBUG    | __main__:<module>:313 - Training step 26580: loss = 3.3546 | 3016.09ms | Tokens/s = 173,830.3
2025-01-16 06:06:33.856 | DEBUG    | __main__:<module>:313 - Training step 26590: loss = 3.1558 | 3012.76ms | Tokens/s = 174,022.7
2025-01-16 06:07:03.999 | DEBUG    | __main__:<module>:313 - Training step 26600: loss = 3.1334 | 3013.43ms | Tokens/s = 173,983.6
2025-01-16 06:07:34.134 | DEBUG    | __main__:<module>:313 - Training step 26610: loss = 2.9871 | 3011.76ms | Tokens/s = 174,080.1
2025-01-16 06:08:04.266 | DEBUG    | __main__:<module>:313 - Training step 26620: loss = 2.9493 | 3015.57ms | Tokens/s = 173,860.0
2025-01-16 06:08:34.442 | DEBUG    | __main__:<module>:313 - Training step 26630: loss = 3.1125 | 3021.81ms | Tokens/s = 173,501.2
2025-01-16 06:09:04.610 | DEBUG    | __main__:<module>:313 - Training step 26640: loss = 3.1726 | 3016.05ms | Tokens/s = 173,832.9
2025-01-16 06:09:34.754 | DEBUG    | __main__:<module>:313 - Training step 26650: loss = 3.1425 | 3011.99ms | Tokens/s = 174,066.9
2025-01-16 06:10:04.896 | DEBUG    | __main__:<module>:313 - Training step 26660: loss = 3.0444 | 3012.95ms | Tokens/s = 174,011.5
2025-01-16 06:10:35.039 | DEBUG    | __main__:<module>:313 - Training step 26670: loss = 3.1844 | 3013.18ms | Tokens/s = 173,998.1
2025-01-16 06:11:05.216 | DEBUG    | __main__:<module>:313 - Training step 26680: loss = 3.1424 | 3017.28ms | Tokens/s = 173,761.8
2025-01-16 06:11:35.408 | DEBUG    | __main__:<module>:313 - Training step 26690: loss = 3.0120 | 3015.79ms | Tokens/s = 173,847.9
2025-01-16 06:12:05.576 | DEBUG    | __main__:<module>:313 - Training step 26700: loss = 3.0113 | 3016.49ms | Tokens/s = 173,807.1
2025-01-16 06:12:35.741 | DEBUG    | __main__:<module>:313 - Training step 26710: loss = 3.0815 | 3017.68ms | Tokens/s = 173,738.7
2025-01-16 06:13:05.880 | DEBUG    | __main__:<module>:313 - Training step 26720: loss = 3.2045 | 3014.32ms | Tokens/s = 173,932.4
2025-01-16 06:13:36.014 | DEBUG    | __main__:<module>:313 - Training step 26730: loss = 3.2048 | 3014.32ms | Tokens/s = 173,932.4
2025-01-16 06:14:06.154 | DEBUG    | __main__:<module>:313 - Training step 26740: loss = 3.1435 | 3015.43ms | Tokens/s = 173,868.5
2025-01-16 06:14:36.328 | DEBUG    | __main__:<module>:313 - Training step 26750: loss = 3.1195 | 3019.24ms | Tokens/s = 173,648.9
2025-01-16 06:15:06.519 | DEBUG    | __main__:<module>:313 - Training step 26760: loss = 3.1228 | 3018.68ms | Tokens/s = 173,681.3
2025-01-16 06:15:36.683 | DEBUG    | __main__:<module>:313 - Training step 26770: loss = 3.1859 | 3013.71ms | Tokens/s = 173,967.8
2025-01-16 06:16:06.830 | DEBUG    | __main__:<module>:313 - Training step 26780: loss = 3.2323 | 3015.95ms | Tokens/s = 173,838.7
2025-01-16 06:16:36.980 | DEBUG    | __main__:<module>:313 - Training step 26790: loss = 3.0206 | 3018.42ms | Tokens/s = 173,696.0
2025-01-16 06:17:07.137 | DEBUG    | __main__:<module>:313 - Training step 26800: loss = 3.1431 | 3014.45ms | Tokens/s = 173,924.9
2025-01-16 06:17:37.279 | DEBUG    | __main__:<module>:313 - Training step 26810: loss = 3.1662 | 3013.85ms | Tokens/s = 173,959.6
2025-01-16 06:18:07.409 | DEBUG    | __main__:<module>:313 - Training step 26820: loss = 3.2003 | 3011.57ms | Tokens/s = 174,091.1
2025-01-16 06:18:37.540 | DEBUG    | __main__:<module>:313 - Training step 26830: loss = 3.0792 | 3012.25ms | Tokens/s = 174,051.8
2025-01-16 06:19:07.693 | DEBUG    | __main__:<module>:313 - Training step 26840: loss = 2.8050 | 3016.16ms | Tokens/s = 173,826.2
2025-01-16 06:19:37.864 | DEBUG    | __main__:<module>:313 - Training step 26850: loss = 3.0624 | 3013.82ms | Tokens/s = 173,961.5
2025-01-16 06:20:08.023 | DEBUG    | __main__:<module>:313 - Training step 26860: loss = 3.0993 | 3014.68ms | Tokens/s = 173,911.9
2025-01-16 06:20:38.176 | DEBUG    | __main__:<module>:313 - Training step 26870: loss = 3.0868 | 3019.07ms | Tokens/s = 173,658.7
2025-01-16 06:21:08.314 | DEBUG    | __main__:<module>:313 - Training step 26880: loss = 3.1790 | 3010.85ms | Tokens/s = 174,133.1
2025-01-16 06:21:38.447 | DEBUG    | __main__:<module>:313 - Training step 26890: loss = 3.2464 | 3013.48ms | Tokens/s = 173,981.1
2025-01-16 06:22:08.584 | DEBUG    | __main__:<module>:313 - Training step 26900: loss = 3.0199 | 3014.54ms | Tokens/s = 173,919.9
2025-01-16 06:22:38.717 | DEBUG    | __main__:<module>:313 - Training step 26910: loss = 3.1177 | 3011.51ms | Tokens/s = 174,094.9
2025-01-16 06:23:08.869 | DEBUG    | __main__:<module>:313 - Training step 26920: loss = 3.1778 | 3016.33ms | Tokens/s = 173,816.7
2025-01-16 06:23:39.047 | DEBUG    | __main__:<module>:313 - Training step 26930: loss = 3.2906 | 3017.99ms | Tokens/s = 173,720.6
2025-01-16 06:24:09.231 | DEBUG    | __main__:<module>:313 - Training step 26940: loss = 3.1798 | 3019.30ms | Tokens/s = 173,645.7
2025-01-16 06:24:39.408 | DEBUG    | __main__:<module>:313 - Training step 26950: loss = 3.2321 | 3017.15ms | Tokens/s = 173,769.0
2025-01-16 06:25:09.563 | DEBUG    | __main__:<module>:313 - Training step 26960: loss = 3.2362 | 3016.03ms | Tokens/s = 173,833.9
2025-01-16 06:25:39.722 | DEBUG    | __main__:<module>:313 - Training step 26970: loss = 3.2469 | 3018.53ms | Tokens/s = 173,690.1
2025-01-16 06:26:09.907 | DEBUG    | __main__:<module>:313 - Training step 26980: loss = 3.1304 | 3017.10ms | Tokens/s = 173,771.9
2025-01-16 06:26:40.073 | DEBUG    | __main__:<module>:313 - Training step 26990: loss = 3.2706 | 3014.67ms | Tokens/s = 173,912.3
2025-01-16 06:27:13.659 | INFO     | __main__:<module>:265 - Step 27,000/40,000 loss: 3.1497 (T) 3.1387 (V) | lr=2.6e-03
2025-01-16 06:27:13.660 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 06:27:26.854 | DEBUG    | __main__:<module>:313 - Training step 27000: loss = 3.1698 | 19632.91ms | Tokens/s = 26,704.5
2025-01-16 06:27:56.911 | DEBUG    | __main__:<module>:313 - Training step 27010: loss = 3.2625 | 3008.36ms | Tokens/s = 174,276.8
2025-01-16 06:28:27.039 | DEBUG    | __main__:<module>:313 - Training step 27020: loss = 2.8432 | 3016.46ms | Tokens/s = 173,809.0
2025-01-16 06:28:57.198 | DEBUG    | __main__:<module>:313 - Training step 27030: loss = 3.2081 | 3018.14ms | Tokens/s = 173,712.4
2025-01-16 06:29:27.357 | DEBUG    | __main__:<module>:313 - Training step 27040: loss = 3.2276 | 3013.90ms | Tokens/s = 173,956.9
2025-01-16 06:29:57.495 | DEBUG    | __main__:<module>:313 - Training step 27050: loss = 3.1394 | 3014.63ms | Tokens/s = 173,914.3
2025-01-16 06:30:27.623 | DEBUG    | __main__:<module>:313 - Training step 27060: loss = 3.1227 | 3011.65ms | Tokens/s = 174,086.9
2025-01-16 06:30:57.755 | DEBUG    | __main__:<module>:313 - Training step 27070: loss = 3.2000 | 3013.16ms | Tokens/s = 173,999.3
2025-01-16 06:31:27.911 | DEBUG    | __main__:<module>:313 - Training step 27080: loss = 3.2344 | 3017.77ms | Tokens/s = 173,733.3
2025-01-16 06:31:58.099 | DEBUG    | __main__:<module>:313 - Training step 27090: loss = 3.2379 | 3018.55ms | Tokens/s = 173,688.9
2025-01-16 06:32:28.275 | DEBUG    | __main__:<module>:313 - Training step 27100: loss = 3.2052 | 3016.69ms | Tokens/s = 173,795.6
2025-01-16 06:32:58.429 | DEBUG    | __main__:<module>:313 - Training step 27110: loss = 3.0787 | 3015.37ms | Tokens/s = 173,871.9
2025-01-16 06:33:28.570 | DEBUG    | __main__:<module>:313 - Training step 27120: loss = 3.1886 | 3014.70ms | Tokens/s = 173,910.5
2025-01-16 06:33:58.714 | DEBUG    | __main__:<module>:313 - Training step 27130: loss = 3.1610 | 3013.05ms | Tokens/s = 174,005.6
2025-01-16 06:34:28.895 | DEBUG    | __main__:<module>:313 - Training step 27140: loss = 3.0849 | 3020.91ms | Tokens/s = 173,552.8
2025-01-16 06:34:59.077 | DEBUG    | __main__:<module>:313 - Training step 27150: loss = 3.1012 | 3016.54ms | Tokens/s = 173,804.5
2025-01-16 06:35:29.258 | DEBUG    | __main__:<module>:313 - Training step 27160: loss = 3.0958 | 3019.74ms | Tokens/s = 173,620.2
2025-01-16 06:35:59.443 | DEBUG    | __main__:<module>:313 - Training step 27170: loss = 3.0353 | 3016.99ms | Tokens/s = 173,778.2
2025-01-16 06:36:29.606 | DEBUG    | __main__:<module>:313 - Training step 27180: loss = 3.2138 | 3018.16ms | Tokens/s = 173,711.2
2025-01-16 06:36:59.758 | DEBUG    | __main__:<module>:313 - Training step 27190: loss = 3.1393 | 3013.11ms | Tokens/s = 174,002.4
2025-01-16 06:37:29.906 | DEBUG    | __main__:<module>:313 - Training step 27200: loss = 3.1075 | 3012.44ms | Tokens/s = 174,040.7
2025-01-16 06:38:00.045 | DEBUG    | __main__:<module>:313 - Training step 27210: loss = 3.0327 | 3012.44ms | Tokens/s = 174,040.8
2025-01-16 06:38:30.202 | DEBUG    | __main__:<module>:313 - Training step 27220: loss = 3.0325 | 3017.66ms | Tokens/s = 173,740.0
2025-01-16 06:39:00.384 | DEBUG    | __main__:<module>:313 - Training step 27230: loss = 3.0364 | 3016.48ms | Tokens/s = 173,807.6
2025-01-16 06:39:30.543 | DEBUG    | __main__:<module>:313 - Training step 27240: loss = 3.0788 | 3016.92ms | Tokens/s = 173,782.5
2025-01-16 06:40:00.699 | DEBUG    | __main__:<module>:313 - Training step 27250: loss = 3.1009 | 3013.40ms | Tokens/s = 173,985.7
2025-01-16 06:40:30.831 | DEBUG    | __main__:<module>:313 - Training step 27260: loss = 3.1786 | 3012.26ms | Tokens/s = 174,051.3
2025-01-16 06:41:00.965 | DEBUG    | __main__:<module>:313 - Training step 27270: loss = 3.1832 | 3014.22ms | Tokens/s = 173,938.5
2025-01-16 06:41:31.128 | DEBUG    | __main__:<module>:313 - Training step 27280: loss = 3.1006 | 3019.13ms | Tokens/s = 173,655.4
2025-01-16 06:42:01.310 | DEBUG    | __main__:<module>:313 - Training step 27290: loss = 3.0792 | 3015.93ms | Tokens/s = 173,839.6
2025-01-16 06:42:31.462 | DEBUG    | __main__:<module>:313 - Training step 27300: loss = 2.9871 | 3012.60ms | Tokens/s = 174,031.9
2025-01-16 06:43:01.606 | DEBUG    | __main__:<module>:313 - Training step 27310: loss = 3.0593 | 3014.11ms | Tokens/s = 173,944.6
2025-01-16 06:43:31.740 | DEBUG    | __main__:<module>:313 - Training step 27320: loss = 3.0666 | 3012.46ms | Tokens/s = 174,039.8
2025-01-16 06:44:01.871 | DEBUG    | __main__:<module>:313 - Training step 27330: loss = 3.0424 | 3013.23ms | Tokens/s = 173,995.6
2025-01-16 06:44:32.017 | DEBUG    | __main__:<module>:313 - Training step 27340: loss = 3.0097 | 3017.78ms | Tokens/s = 173,732.8
2025-01-16 06:45:02.206 | DEBUG    | __main__:<module>:313 - Training step 27350: loss = 3.0621 | 3018.52ms | Tokens/s = 173,690.2
2025-01-16 06:45:32.384 | DEBUG    | __main__:<module>:313 - Training step 27360: loss = 3.0558 | 3018.24ms | Tokens/s = 173,706.7
2025-01-16 06:46:02.548 | DEBUG    | __main__:<module>:313 - Training step 27370: loss = 3.0805 | 3015.01ms | Tokens/s = 173,892.9
2025-01-16 06:46:32.695 | DEBUG    | __main__:<module>:313 - Training step 27380: loss = 3.2070 | 3014.65ms | Tokens/s = 173,913.1
2025-01-16 06:47:02.830 | DEBUG    | __main__:<module>:313 - Training step 27390: loss = 3.1190 | 3012.70ms | Tokens/s = 174,026.1
2025-01-16 06:47:32.974 | DEBUG    | __main__:<module>:313 - Training step 27400: loss = 3.3075 | 3016.48ms | Tokens/s = 173,808.0
2025-01-16 06:48:03.144 | DEBUG    | __main__:<module>:313 - Training step 27410: loss = 2.9969 | 3018.73ms | Tokens/s = 173,678.1
2025-01-16 06:48:33.301 | DEBUG    | __main__:<module>:313 - Training step 27420: loss = 3.1202 | 3016.38ms | Tokens/s = 173,813.4
2025-01-16 06:49:03.452 | DEBUG    | __main__:<module>:313 - Training step 27430: loss = 3.1736 | 3015.11ms | Tokens/s = 173,886.8
2025-01-16 06:49:33.613 | DEBUG    | __main__:<module>:313 - Training step 27440: loss = 3.1867 | 3017.51ms | Tokens/s = 173,748.4
2025-01-16 06:50:03.789 | DEBUG    | __main__:<module>:313 - Training step 27450: loss = 3.1636 | 3015.10ms | Tokens/s = 173,887.4
2025-01-16 06:50:33.932 | DEBUG    | __main__:<module>:313 - Training step 27460: loss = 3.0365 | 3014.41ms | Tokens/s = 173,927.3
2025-01-16 06:51:04.075 | DEBUG    | __main__:<module>:313 - Training step 27470: loss = 3.2794 | 3015.20ms | Tokens/s = 173,881.7
2025-01-16 06:51:34.217 | DEBUG    | __main__:<module>:313 - Training step 27480: loss = 3.0848 | 3016.62ms | Tokens/s = 173,799.7
2025-01-16 06:52:04.361 | DEBUG    | __main__:<module>:313 - Training step 27490: loss = 3.1982 | 3012.94ms | Tokens/s = 174,011.9
2025-01-16 06:52:34.499 | DEBUG    | __main__:<module>:313 - Training step 27500: loss = 3.0278 | 3013.01ms | Tokens/s = 174,008.2
2025-01-16 06:53:04.644 | DEBUG    | __main__:<module>:313 - Training step 27510: loss = 3.0433 | 3014.79ms | Tokens/s = 173,905.5
2025-01-16 06:53:34.823 | DEBUG    | __main__:<module>:313 - Training step 27520: loss = 3.2046 | 3016.86ms | Tokens/s = 173,786.2
2025-01-16 06:54:04.989 | DEBUG    | __main__:<module>:313 - Training step 27530: loss = 3.2252 | 3015.50ms | Tokens/s = 173,864.5
2025-01-16 06:54:35.137 | DEBUG    | __main__:<module>:313 - Training step 27540: loss = 3.0885 | 3015.30ms | Tokens/s = 173,875.8
2025-01-16 06:55:05.304 | DEBUG    | __main__:<module>:313 - Training step 27550: loss = 3.1276 | 3018.40ms | Tokens/s = 173,697.3
2025-01-16 06:55:35.500 | DEBUG    | __main__:<module>:313 - Training step 27560: loss = 3.0260 | 3019.08ms | Tokens/s = 173,658.1
2025-01-16 06:56:05.687 | DEBUG    | __main__:<module>:313 - Training step 27570: loss = 3.1615 | 3019.39ms | Tokens/s = 173,640.1
2025-01-16 06:56:35.861 | DEBUG    | __main__:<module>:313 - Training step 27580: loss = 3.1751 | 3015.86ms | Tokens/s = 173,843.4
2025-01-16 06:57:06.035 | DEBUG    | __main__:<module>:313 - Training step 27590: loss = 3.0862 | 3017.53ms | Tokens/s = 173,747.4
2025-01-16 06:57:36.224 | DEBUG    | __main__:<module>:313 - Training step 27600: loss = 3.0446 | 3014.88ms | Tokens/s = 173,900.1
2025-01-16 06:58:06.396 | DEBUG    | __main__:<module>:313 - Training step 27610: loss = 3.3269 | 3019.16ms | Tokens/s = 173,653.6
2025-01-16 06:58:36.582 | DEBUG    | __main__:<module>:313 - Training step 27620: loss = 3.0322 | 3016.10ms | Tokens/s = 173,830.0
2025-01-16 06:59:06.749 | DEBUG    | __main__:<module>:313 - Training step 27630: loss = 3.0004 | 3015.10ms | Tokens/s = 173,887.4
2025-01-16 06:59:36.902 | DEBUG    | __main__:<module>:313 - Training step 27640: loss = 2.9924 | 3014.54ms | Tokens/s = 173,919.6
2025-01-16 07:00:07.047 | DEBUG    | __main__:<module>:313 - Training step 27650: loss = 3.1130 | 3014.28ms | Tokens/s = 173,934.9
2025-01-16 07:00:37.198 | DEBUG    | __main__:<module>:313 - Training step 27660: loss = 3.1764 | 3016.70ms | Tokens/s = 173,795.1
2025-01-16 07:01:07.383 | DEBUG    | __main__:<module>:313 - Training step 27670: loss = 3.2787 | 3015.41ms | Tokens/s = 173,869.4
2025-01-16 07:01:37.548 | DEBUG    | __main__:<module>:313 - Training step 27680: loss = 3.0458 | 3019.34ms | Tokens/s = 173,643.4
2025-01-16 07:02:07.704 | DEBUG    | __main__:<module>:313 - Training step 27690: loss = 2.9588 | 3013.08ms | Tokens/s = 174,003.8
2025-01-16 07:02:37.852 | DEBUG    | __main__:<module>:313 - Training step 27700: loss = 3.0063 | 3011.64ms | Tokens/s = 174,087.0
2025-01-16 07:03:08.021 | DEBUG    | __main__:<module>:313 - Training step 27710: loss = 3.2267 | 3019.27ms | Tokens/s = 173,647.2
2025-01-16 07:03:38.205 | DEBUG    | __main__:<module>:313 - Training step 27720: loss = 3.0985 | 3020.04ms | Tokens/s = 173,603.1
2025-01-16 07:04:08.361 | DEBUG    | __main__:<module>:313 - Training step 27730: loss = 3.2143 | 3015.11ms | Tokens/s = 173,886.8
2025-01-16 07:04:38.547 | DEBUG    | __main__:<module>:313 - Training step 27740: loss = 3.2266 | 3022.30ms | Tokens/s = 173,473.1
2025-01-16 07:05:08.736 | DEBUG    | __main__:<module>:313 - Training step 27750: loss = 3.2999 | 3017.79ms | Tokens/s = 173,732.5
2025-01-16 07:05:38.904 | DEBUG    | __main__:<module>:313 - Training step 27760: loss = 3.2040 | 3014.21ms | Tokens/s = 173,938.9
2025-01-16 07:06:09.050 | DEBUG    | __main__:<module>:313 - Training step 27770: loss = 3.2217 | 3013.87ms | Tokens/s = 173,958.6
2025-01-16 07:06:39.191 | DEBUG    | __main__:<module>:313 - Training step 27780: loss = 2.9777 | 3016.12ms | Tokens/s = 173,828.4
2025-01-16 07:07:09.337 | DEBUG    | __main__:<module>:313 - Training step 27790: loss = 3.2049 | 3015.54ms | Tokens/s = 173,861.8
2025-01-16 07:07:39.498 | DEBUG    | __main__:<module>:313 - Training step 27800: loss = 3.0954 | 3016.78ms | Tokens/s = 173,790.8
2025-01-16 07:08:09.687 | DEBUG    | __main__:<module>:313 - Training step 27810: loss = 3.2166 | 3019.86ms | Tokens/s = 173,613.2
2025-01-16 07:08:39.882 | DEBUG    | __main__:<module>:313 - Training step 27820: loss = 3.0393 | 3019.86ms | Tokens/s = 173,613.1
2025-01-16 07:09:10.050 | DEBUG    | __main__:<module>:313 - Training step 27830: loss = 3.1635 | 3014.94ms | Tokens/s = 173,896.8
2025-01-16 07:09:40.215 | DEBUG    | __main__:<module>:313 - Training step 27840: loss = 3.1423 | 3016.42ms | Tokens/s = 173,811.5
2025-01-16 07:10:10.371 | DEBUG    | __main__:<module>:313 - Training step 27850: loss = 3.3335 | 3015.97ms | Tokens/s = 173,837.2
2025-01-16 07:10:40.515 | DEBUG    | __main__:<module>:313 - Training step 27860: loss = 3.3335 | 3013.18ms | Tokens/s = 173,998.1
2025-01-16 07:11:10.687 | DEBUG    | __main__:<module>:313 - Training step 27870: loss = 3.1812 | 3019.03ms | Tokens/s = 173,661.0
2025-01-16 07:11:40.874 | DEBUG    | __main__:<module>:313 - Training step 27880: loss = 3.1069 | 3019.11ms | Tokens/s = 173,656.3
2025-01-16 07:12:11.047 | DEBUG    | __main__:<module>:313 - Training step 27890: loss = 3.0598 | 3018.24ms | Tokens/s = 173,706.6
2025-01-16 07:12:41.190 | DEBUG    | __main__:<module>:313 - Training step 27900: loss = 3.1707 | 3013.73ms | Tokens/s = 173,966.6
2025-01-16 07:13:11.325 | DEBUG    | __main__:<module>:313 - Training step 27910: loss = 3.1031 | 3012.74ms | Tokens/s = 174,023.6
2025-01-16 07:13:41.459 | DEBUG    | __main__:<module>:313 - Training step 27920: loss = 2.8460 | 3013.11ms | Tokens/s = 174,002.3
2025-01-16 07:14:11.592 | DEBUG    | __main__:<module>:313 - Training step 27930: loss = 3.1283 | 3011.67ms | Tokens/s = 174,085.5
2025-01-16 07:14:41.761 | DEBUG    | __main__:<module>:313 - Training step 27940: loss = 3.1147 | 3015.97ms | Tokens/s = 173,837.5
2025-01-16 07:15:11.952 | DEBUG    | __main__:<module>:313 - Training step 27950: loss = 2.9977 | 3018.91ms | Tokens/s = 173,667.8
2025-01-16 07:15:42.134 | DEBUG    | __main__:<module>:313 - Training step 27960: loss = 2.9972 | 3015.03ms | Tokens/s = 173,891.6
2025-01-16 07:16:12.290 | DEBUG    | __main__:<module>:313 - Training step 27970: loss = 3.1067 | 3015.85ms | Tokens/s = 173,844.0
2025-01-16 07:16:42.439 | DEBUG    | __main__:<module>:313 - Training step 27980: loss = 3.0835 | 3013.34ms | Tokens/s = 173,988.8
2025-01-16 07:17:12.580 | DEBUG    | __main__:<module>:313 - Training step 27990: loss = 3.0376 | 3012.70ms | Tokens/s = 174,025.9
2025-01-16 07:17:46.147 | INFO     | __main__:<module>:265 - Step 28,000/40,000 loss: 3.1295 (T) 3.1462 (V) | lr=2.3e-03
2025-01-16 07:17:49.162 | DEBUG    | __main__:<module>:313 - Training step 28000: loss = 3.0754 | 9449.99ms | Tokens/s = 55,480.3
2025-01-16 07:18:19.307 | DEBUG    | __main__:<module>:313 - Training step 28010: loss = 3.1426 | 3019.07ms | Tokens/s = 173,658.8
2025-01-16 07:18:49.476 | DEBUG    | __main__:<module>:313 - Training step 28020: loss = 3.1401 | 3019.27ms | Tokens/s = 173,647.4
2025-01-16 07:19:19.643 | DEBUG    | __main__:<module>:313 - Training step 28030: loss = 3.0136 | 3016.71ms | Tokens/s = 173,794.8
2025-01-16 07:19:49.800 | DEBUG    | __main__:<module>:313 - Training step 28040: loss = 3.1102 | 3014.35ms | Tokens/s = 173,930.8
2025-01-16 07:20:19.950 | DEBUG    | __main__:<module>:313 - Training step 28050: loss = 3.1139 | 3014.55ms | Tokens/s = 173,919.4
2025-01-16 07:20:50.104 | DEBUG    | __main__:<module>:313 - Training step 28060: loss = 3.2374 | 3015.01ms | Tokens/s = 173,892.8
2025-01-16 07:21:20.283 | DEBUG    | __main__:<module>:313 - Training step 28070: loss = 3.2040 | 3018.23ms | Tokens/s = 173,706.8
2025-01-16 07:21:50.467 | DEBUG    | __main__:<module>:313 - Training step 28080: loss = 2.8455 | 3014.72ms | Tokens/s = 173,909.4
2025-01-16 07:22:20.636 | DEBUG    | __main__:<module>:313 - Training step 28090: loss = 3.3905 | 3015.04ms | Tokens/s = 173,891.0
2025-01-16 07:22:50.787 | DEBUG    | __main__:<module>:313 - Training step 28100: loss = 3.0825 | 3014.05ms | Tokens/s = 173,948.0
2025-01-16 07:23:20.959 | DEBUG    | __main__:<module>:313 - Training step 28110: loss = 2.9911 | 3016.50ms | Tokens/s = 173,806.6
2025-01-16 07:23:51.114 | DEBUG    | __main__:<module>:313 - Training step 28120: loss = 3.2754 | 3015.47ms | Tokens/s = 173,865.8
2025-01-16 07:24:21.265 | DEBUG    | __main__:<module>:313 - Training step 28130: loss = 3.0968 | 3015.46ms | Tokens/s = 173,866.6
2025-01-16 07:24:51.406 | DEBUG    | __main__:<module>:313 - Training step 28140: loss = 2.9594 | 3014.01ms | Tokens/s = 173,950.1
2025-01-16 07:25:21.555 | DEBUG    | __main__:<module>:313 - Training step 28150: loss = 3.1456 | 3014.81ms | Tokens/s = 173,904.3
2025-01-16 07:25:51.723 | DEBUG    | __main__:<module>:313 - Training step 28160: loss = 3.2724 | 3015.73ms | Tokens/s = 173,851.2
2025-01-16 07:26:21.873 | DEBUG    | __main__:<module>:313 - Training step 28170: loss = 3.0443 | 3014.95ms | Tokens/s = 173,896.1
2025-01-16 07:26:52.016 | DEBUG    | __main__:<module>:313 - Training step 28180: loss = 3.1948 | 3013.94ms | Tokens/s = 173,954.5
2025-01-16 07:27:22.154 | DEBUG    | __main__:<module>:313 - Training step 28190: loss = 3.1968 | 3013.69ms | Tokens/s = 173,968.6
2025-01-16 07:27:52.324 | DEBUG    | __main__:<module>:313 - Training step 28200: loss = 3.1845 | 3015.63ms | Tokens/s = 173,856.7
2025-01-16 07:28:22.502 | DEBUG    | __main__:<module>:313 - Training step 28210: loss = 3.1767 | 3017.32ms | Tokens/s = 173,759.4
2025-01-16 07:28:52.671 | DEBUG    | __main__:<module>:313 - Training step 28220: loss = 3.2276 | 3015.51ms | Tokens/s = 173,863.6
2025-01-16 07:29:22.815 | DEBUG    | __main__:<module>:313 - Training step 28230: loss = 3.0504 | 3013.74ms | Tokens/s = 173,966.0
2025-01-16 07:29:52.953 | DEBUG    | __main__:<module>:313 - Training step 28240: loss = 3.1329 | 3013.07ms | Tokens/s = 174,004.8
2025-01-16 07:30:23.096 | DEBUG    | __main__:<module>:313 - Training step 28250: loss = 3.1171 | 3016.87ms | Tokens/s = 173,785.2
2025-01-16 07:30:53.267 | DEBUG    | __main__:<module>:313 - Training step 28260: loss = 3.2179 | 3016.87ms | Tokens/s = 173,785.7
2025-01-16 07:31:23.446 | DEBUG    | __main__:<module>:313 - Training step 28270: loss = 3.1236 | 3016.39ms | Tokens/s = 173,812.9
2025-01-16 07:31:53.613 | DEBUG    | __main__:<module>:313 - Training step 28280: loss = 3.1442 | 3014.67ms | Tokens/s = 173,912.1
2025-01-16 07:32:23.778 | DEBUG    | __main__:<module>:313 - Training step 28290: loss = 2.9805 | 3018.84ms | Tokens/s = 173,672.1
2025-01-16 07:32:53.955 | DEBUG    | __main__:<module>:313 - Training step 28300: loss = 3.0070 | 3014.24ms | Tokens/s = 173,937.0
2025-01-16 07:33:24.117 | DEBUG    | __main__:<module>:313 - Training step 28310: loss = 3.3117 | 3016.58ms | Tokens/s = 173,802.4
2025-01-16 07:33:54.263 | DEBUG    | __main__:<module>:313 - Training step 28320: loss = 3.0631 | 3013.34ms | Tokens/s = 173,989.2
2025-01-16 07:34:24.406 | DEBUG    | __main__:<module>:313 - Training step 28330: loss = 3.1410 | 3016.73ms | Tokens/s = 173,793.4
2025-01-16 07:34:54.559 | DEBUG    | __main__:<module>:313 - Training step 28340: loss = 3.1492 | 3016.67ms | Tokens/s = 173,796.8
2025-01-16 07:35:24.730 | DEBUG    | __main__:<module>:313 - Training step 28350: loss = 3.2789 | 3018.12ms | Tokens/s = 173,713.2
2025-01-16 07:35:54.882 | DEBUG    | __main__:<module>:313 - Training step 28360: loss = 3.1799 | 3014.44ms | Tokens/s = 173,925.7
2025-01-16 07:36:25.023 | DEBUG    | __main__:<module>:313 - Training step 28370: loss = 3.3120 | 3013.00ms | Tokens/s = 174,008.8
2025-01-16 07:36:55.178 | DEBUG    | __main__:<module>:313 - Training step 28380: loss = 3.1600 | 3013.69ms | Tokens/s = 173,969.0
2025-01-16 07:37:25.331 | DEBUG    | __main__:<module>:313 - Training step 28390: loss = 3.1726 | 3013.44ms | Tokens/s = 173,983.3
2025-01-16 07:37:55.472 | DEBUG    | __main__:<module>:313 - Training step 28400: loss = 3.2835 | 3013.78ms | Tokens/s = 173,963.3
2025-01-16 07:38:25.619 | DEBUG    | __main__:<module>:313 - Training step 28410: loss = 3.2081 | 3015.16ms | Tokens/s = 173,883.7
2025-01-16 07:38:55.764 | DEBUG    | __main__:<module>:313 - Training step 28420: loss = 3.1075 | 3014.07ms | Tokens/s = 173,947.1
2025-01-16 07:39:25.909 | DEBUG    | __main__:<module>:313 - Training step 28430: loss = 3.1613 | 3014.20ms | Tokens/s = 173,939.2
2025-01-16 07:39:56.073 | DEBUG    | __main__:<module>:313 - Training step 28440: loss = 2.8867 | 3017.54ms | Tokens/s = 173,746.8
2025-01-16 07:40:26.230 | DEBUG    | __main__:<module>:313 - Training step 28450: loss = 3.1654 | 3014.76ms | Tokens/s = 173,907.2
2025-01-16 07:40:56.381 | DEBUG    | __main__:<module>:313 - Training step 28460: loss = 3.2841 | 3014.80ms | Tokens/s = 173,905.0
2025-01-16 07:41:26.553 | DEBUG    | __main__:<module>:313 - Training step 28470: loss = 3.0690 | 3020.01ms | Tokens/s = 173,604.9
2025-01-16 07:41:56.749 | DEBUG    | __main__:<module>:313 - Training step 28480: loss = 3.0570 | 3016.68ms | Tokens/s = 173,796.6
2025-01-16 07:42:26.929 | DEBUG    | __main__:<module>:313 - Training step 28490: loss = 3.2573 | 3017.70ms | Tokens/s = 173,737.5
2025-01-16 07:42:57.109 | DEBUG    | __main__:<module>:313 - Training step 28500: loss = 3.2126 | 3014.43ms | Tokens/s = 173,926.3
2025-01-16 07:43:27.277 | DEBUG    | __main__:<module>:313 - Training step 28510: loss = 3.0325 | 3016.60ms | Tokens/s = 173,801.1
2025-01-16 07:43:57.426 | DEBUG    | __main__:<module>:313 - Training step 28520: loss = 3.1983 | 3012.57ms | Tokens/s = 174,033.7
2025-01-16 07:44:27.573 | DEBUG    | __main__:<module>:313 - Training step 28530: loss = 3.0436 | 3014.34ms | Tokens/s = 173,931.3
2025-01-16 07:44:57.720 | DEBUG    | __main__:<module>:313 - Training step 28540: loss = 2.9644 | 3013.08ms | Tokens/s = 174,004.2
2025-01-16 07:45:27.856 | DEBUG    | __main__:<module>:313 - Training step 28550: loss = 3.1907 | 3013.15ms | Tokens/s = 174,000.0
2025-01-16 07:45:57.997 | DEBUG    | __main__:<module>:313 - Training step 28560: loss = 3.1278 | 3014.77ms | Tokens/s = 173,906.3
2025-01-16 07:46:28.166 | DEBUG    | __main__:<module>:313 - Training step 28570: loss = 3.1133 | 3018.28ms | Tokens/s = 173,704.1
2025-01-16 07:46:58.340 | DEBUG    | __main__:<module>:313 - Training step 28580: loss = 3.1633 | 3018.25ms | Tokens/s = 173,706.0
2025-01-16 07:47:28.506 | DEBUG    | __main__:<module>:313 - Training step 28590: loss = 3.1815 | 3017.13ms | Tokens/s = 173,770.2
2025-01-16 07:47:58.658 | DEBUG    | __main__:<module>:313 - Training step 28600: loss = 2.9028 | 3012.64ms | Tokens/s = 174,029.6
2025-01-16 07:48:28.817 | DEBUG    | __main__:<module>:313 - Training step 28610: loss = 3.1362 | 3019.33ms | Tokens/s = 173,643.5
2025-01-16 07:48:59.002 | DEBUG    | __main__:<module>:313 - Training step 28620: loss = 3.0096 | 3020.75ms | Tokens/s = 173,561.9
2025-01-16 07:49:29.185 | DEBUG    | __main__:<module>:313 - Training step 28630: loss = 3.1066 | 3017.19ms | Tokens/s = 173,766.8
2025-01-16 07:49:59.351 | DEBUG    | __main__:<module>:313 - Training step 28640: loss = 3.1174 | 3015.14ms | Tokens/s = 173,885.3
2025-01-16 07:50:29.503 | DEBUG    | __main__:<module>:313 - Training step 28650: loss = 3.2032 | 3014.46ms | Tokens/s = 173,924.5
2025-01-16 07:50:59.646 | DEBUG    | __main__:<module>:313 - Training step 28660: loss = 3.1024 | 3013.06ms | Tokens/s = 174,004.9
2025-01-16 07:51:29.819 | DEBUG    | __main__:<module>:313 - Training step 28670: loss = 2.9473 | 3016.81ms | Tokens/s = 173,788.7
2025-01-16 07:52:00.015 | DEBUG    | __main__:<module>:313 - Training step 28680: loss = 3.0990 | 3019.98ms | Tokens/s = 173,606.2
2025-01-16 07:52:30.192 | DEBUG    | __main__:<module>:313 - Training step 28690: loss = 3.1180 | 3016.91ms | Tokens/s = 173,783.1
2025-01-16 07:53:00.357 | DEBUG    | __main__:<module>:313 - Training step 28700: loss = 2.8670 | 3015.02ms | Tokens/s = 173,892.3
2025-01-16 07:53:30.506 | DEBUG    | __main__:<module>:313 - Training step 28710: loss = 3.0453 | 3014.01ms | Tokens/s = 173,950.6
2025-01-16 07:54:00.655 | DEBUG    | __main__:<module>:313 - Training step 28720: loss = 3.1780 | 3013.83ms | Tokens/s = 173,960.6
2025-01-16 07:54:30.797 | DEBUG    | __main__:<module>:313 - Training step 28730: loss = 3.0836 | 3014.00ms | Tokens/s = 173,951.0
2025-01-16 07:55:00.945 | DEBUG    | __main__:<module>:313 - Training step 28740: loss = 3.1822 | 3017.58ms | Tokens/s = 173,744.5
2025-01-16 07:55:31.119 | DEBUG    | __main__:<module>:313 - Training step 28750: loss = 2.7761 | 3018.28ms | Tokens/s = 173,704.5
2025-01-16 07:56:01.302 | DEBUG    | __main__:<module>:313 - Training step 28760: loss = 3.1141 | 3018.85ms | Tokens/s = 173,671.7
2025-01-16 07:56:31.467 | DEBUG    | __main__:<module>:313 - Training step 28770: loss = 3.1820 | 3017.95ms | Tokens/s = 173,723.4
2025-01-16 07:57:01.620 | DEBUG    | __main__:<module>:313 - Training step 28780: loss = 3.1213 | 3013.98ms | Tokens/s = 173,951.9
2025-01-16 07:57:31.766 | DEBUG    | __main__:<module>:313 - Training step 28790: loss = 3.2326 | 3015.26ms | Tokens/s = 173,877.9
2025-01-16 07:58:01.915 | DEBUG    | __main__:<module>:313 - Training step 28800: loss = 2.8296 | 3016.76ms | Tokens/s = 173,792.0
2025-01-16 07:58:32.057 | DEBUG    | __main__:<module>:313 - Training step 28810: loss = 3.2471 | 3013.63ms | Tokens/s = 173,972.5
2025-01-16 07:59:02.226 | DEBUG    | __main__:<module>:313 - Training step 28820: loss = 3.0592 | 3018.04ms | Tokens/s = 173,717.9
2025-01-16 07:59:32.398 | DEBUG    | __main__:<module>:313 - Training step 28830: loss = 3.2059 | 3015.71ms | Tokens/s = 173,852.3
2025-01-16 08:00:02.566 | DEBUG    | __main__:<module>:313 - Training step 28840: loss = 2.8095 | 3016.10ms | Tokens/s = 173,829.8
2025-01-16 08:00:32.740 | DEBUG    | __main__:<module>:313 - Training step 28850: loss = 3.0725 | 3019.29ms | Tokens/s = 173,645.9
2025-01-16 08:01:02.903 | DEBUG    | __main__:<module>:313 - Training step 28860: loss = 3.2088 | 3012.68ms | Tokens/s = 174,027.2
2025-01-16 08:01:33.083 | DEBUG    | __main__:<module>:313 - Training step 28870: loss = 3.0765 | 3018.26ms | Tokens/s = 173,705.2
2025-01-16 08:02:03.260 | DEBUG    | __main__:<module>:313 - Training step 28880: loss = 3.1135 | 3018.15ms | Tokens/s = 173,711.8
2025-01-16 08:02:33.420 | DEBUG    | __main__:<module>:313 - Training step 28890: loss = 3.0822 | 3014.53ms | Tokens/s = 173,920.0
2025-01-16 08:03:03.587 | DEBUG    | __main__:<module>:313 - Training step 28900: loss = 3.1865 | 3016.57ms | Tokens/s = 173,802.6
2025-01-16 08:03:33.777 | DEBUG    | __main__:<module>:313 - Training step 28910: loss = 3.1516 | 3017.32ms | Tokens/s = 173,759.8
2025-01-16 08:04:03.945 | DEBUG    | __main__:<module>:313 - Training step 28920: loss = 3.1957 | 3015.13ms | Tokens/s = 173,885.7
2025-01-16 08:04:34.086 | DEBUG    | __main__:<module>:313 - Training step 28930: loss = 3.0995 | 3013.16ms | Tokens/s = 173,999.3
2025-01-16 08:05:04.234 | DEBUG    | __main__:<module>:313 - Training step 28940: loss = 3.2799 | 3013.37ms | Tokens/s = 173,987.0
2025-01-16 08:05:34.371 | DEBUG    | __main__:<module>:313 - Training step 28950: loss = 3.1714 | 3013.67ms | Tokens/s = 173,970.1
2025-01-16 08:06:04.508 | DEBUG    | __main__:<module>:313 - Training step 28960: loss = 2.9337 | 3011.86ms | Tokens/s = 174,074.3
2025-01-16 08:06:34.643 | DEBUG    | __main__:<module>:313 - Training step 28970: loss = 3.2029 | 3015.21ms | Tokens/s = 173,881.1
2025-01-16 08:07:04.803 | DEBUG    | __main__:<module>:313 - Training step 28980: loss = 3.1156 | 3018.15ms | Tokens/s = 173,711.7
2025-01-16 08:07:34.974 | DEBUG    | __main__:<module>:313 - Training step 28990: loss = 3.0448 | 3016.51ms | Tokens/s = 173,806.1
2025-01-16 08:08:08.558 | INFO     | __main__:<module>:265 - Step 29,000/40,000 loss: 3.1068 (T) 3.1133 (V) | lr=1.9e-03
2025-01-16 08:08:08.559 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 08:08:22.968 | DEBUG    | __main__:<module>:313 - Training step 29000: loss = 2.9935 | 20850.77ms | Tokens/s = 25,144.8
2025-01-16 08:08:53.031 | DEBUG    | __main__:<module>:313 - Training step 29010: loss = 3.1320 | 3008.45ms | Tokens/s = 174,272.0
2025-01-16 08:09:23.147 | DEBUG    | __main__:<module>:313 - Training step 29020: loss = 3.2349 | 3013.39ms | Tokens/s = 173,986.1
2025-01-16 08:09:53.306 | DEBUG    | __main__:<module>:313 - Training step 29030: loss = 3.1647 | 3015.80ms | Tokens/s = 173,847.1
2025-01-16 08:10:23.477 | DEBUG    | __main__:<module>:313 - Training step 29040: loss = 3.0590 | 3015.30ms | Tokens/s = 173,876.0
2025-01-16 08:10:53.624 | DEBUG    | __main__:<module>:313 - Training step 29050: loss = 2.9646 | 3014.06ms | Tokens/s = 173,947.6
2025-01-16 08:11:23.762 | DEBUG    | __main__:<module>:313 - Training step 29060: loss = 3.1773 | 3015.94ms | Tokens/s = 173,838.8
2025-01-16 08:11:53.902 | DEBUG    | __main__:<module>:313 - Training step 29070: loss = 3.1714 | 3013.60ms | Tokens/s = 173,974.2
2025-01-16 08:12:24.058 | DEBUG    | __main__:<module>:313 - Training step 29080: loss = 2.9167 | 3016.01ms | Tokens/s = 173,835.0
2025-01-16 08:12:54.237 | DEBUG    | __main__:<module>:313 - Training step 29090: loss = 2.9314 | 3017.23ms | Tokens/s = 173,764.8
2025-01-16 08:13:24.399 | DEBUG    | __main__:<module>:313 - Training step 29100: loss = 3.0936 | 3012.92ms | Tokens/s = 174,013.2
2025-01-16 08:13:54.558 | DEBUG    | __main__:<module>:313 - Training step 29110: loss = 3.1344 | 3015.80ms | Tokens/s = 173,847.0
2025-01-16 08:14:24.691 | DEBUG    | __main__:<module>:313 - Training step 29120: loss = 3.2557 | 3011.95ms | Tokens/s = 174,069.3
2025-01-16 08:14:54.830 | DEBUG    | __main__:<module>:313 - Training step 29130: loss = 3.1349 | 3013.79ms | Tokens/s = 173,963.1
2025-01-16 08:15:24.964 | DEBUG    | __main__:<module>:313 - Training step 29140: loss = 3.2513 | 3014.34ms | Tokens/s = 173,931.5
2025-01-16 08:15:55.091 | DEBUG    | __main__:<module>:313 - Training step 29150: loss = 3.1298 | 3011.97ms | Tokens/s = 174,068.3
2025-01-16 08:16:25.231 | DEBUG    | __main__:<module>:313 - Training step 29160: loss = 3.0951 | 3014.03ms | Tokens/s = 173,949.2
2025-01-16 08:16:55.370 | DEBUG    | __main__:<module>:313 - Training step 29170: loss = 3.0889 | 3013.78ms | Tokens/s = 173,963.8
2025-01-16 08:17:25.513 | DEBUG    | __main__:<module>:313 - Training step 29180: loss = 3.0550 | 3012.92ms | Tokens/s = 174,013.5
2025-01-16 08:17:55.655 | DEBUG    | __main__:<module>:313 - Training step 29190: loss = 3.1968 | 3017.69ms | Tokens/s = 173,738.0
2025-01-16 08:18:25.843 | DEBUG    | __main__:<module>:313 - Training step 29200: loss = 3.1249 | 3019.64ms | Tokens/s = 173,626.0
2025-01-16 08:18:56.011 | DEBUG    | __main__:<module>:313 - Training step 29210: loss = 3.0094 | 3017.85ms | Tokens/s = 173,728.8
2025-01-16 08:19:26.160 | DEBUG    | __main__:<module>:313 - Training step 29220: loss = 3.1386 | 3014.41ms | Tokens/s = 173,927.5
2025-01-16 08:19:56.297 | DEBUG    | __main__:<module>:313 - Training step 29230: loss = 3.1424 | 3013.60ms | Tokens/s = 173,974.0
2025-01-16 08:20:26.429 | DEBUG    | __main__:<module>:313 - Training step 29240: loss = 3.3371 | 3015.47ms | Tokens/s = 173,865.8
2025-01-16 08:20:56.578 | DEBUG    | __main__:<module>:313 - Training step 29250: loss = 3.0189 | 3014.92ms | Tokens/s = 173,898.0
2025-01-16 08:21:26.756 | DEBUG    | __main__:<module>:313 - Training step 29260: loss = 3.1663 | 3017.88ms | Tokens/s = 173,727.4
2025-01-16 08:21:56.948 | DEBUG    | __main__:<module>:313 - Training step 29270: loss = 3.1387 | 3018.41ms | Tokens/s = 173,696.9
2025-01-16 08:22:27.114 | DEBUG    | __main__:<module>:313 - Training step 29280: loss = 3.0645 | 3016.49ms | Tokens/s = 173,807.5
2025-01-16 08:22:57.265 | DEBUG    | __main__:<module>:313 - Training step 29290: loss = 3.0624 | 3017.82ms | Tokens/s = 173,730.8
2025-01-16 08:23:27.401 | DEBUG    | __main__:<module>:313 - Training step 29300: loss = 2.9355 | 3011.81ms | Tokens/s = 174,077.5
2025-01-16 08:23:57.549 | DEBUG    | __main__:<module>:313 - Training step 29310: loss = 3.0425 | 3014.72ms | Tokens/s = 173,909.5
2025-01-16 08:24:27.689 | DEBUG    | __main__:<module>:313 - Training step 29320: loss = 2.9931 | 3014.15ms | Tokens/s = 173,942.2
2025-01-16 08:24:57.825 | DEBUG    | __main__:<module>:313 - Training step 29330: loss = 3.0777 | 3014.65ms | Tokens/s = 173,913.5
2025-01-16 08:25:27.965 | DEBUG    | __main__:<module>:313 - Training step 29340: loss = 3.0359 | 3013.41ms | Tokens/s = 173,985.1
2025-01-16 08:25:58.125 | DEBUG    | __main__:<module>:313 - Training step 29350: loss = 3.0342 | 3017.29ms | Tokens/s = 173,761.3
2025-01-16 08:26:28.284 | DEBUG    | __main__:<module>:313 - Training step 29360: loss = 3.1223 | 3013.83ms | Tokens/s = 173,960.9
2025-01-16 08:26:58.420 | DEBUG    | __main__:<module>:313 - Training step 29370: loss = 3.0555 | 3012.70ms | Tokens/s = 174,026.1
2025-01-16 08:27:28.556 | DEBUG    | __main__:<module>:313 - Training step 29380: loss = 3.2773 | 3015.42ms | Tokens/s = 173,869.2
2025-01-16 08:27:58.722 | DEBUG    | __main__:<module>:313 - Training step 29390: loss = 3.2425 | 3015.97ms | Tokens/s = 173,837.4
2025-01-16 08:28:28.909 | DEBUG    | __main__:<module>:313 - Training step 29400: loss = 2.9842 | 3016.66ms | Tokens/s = 173,797.6
2025-01-16 08:28:59.097 | DEBUG    | __main__:<module>:313 - Training step 29410: loss = 3.0818 | 3017.99ms | Tokens/s = 173,720.7
2025-01-16 08:29:29.281 | DEBUG    | __main__:<module>:313 - Training step 29420: loss = 3.2199 | 3017.34ms | Tokens/s = 173,758.5
2025-01-16 08:29:59.438 | DEBUG    | __main__:<module>:313 - Training step 29430: loss = 3.0474 | 3013.76ms | Tokens/s = 173,964.9
2025-01-16 08:30:29.576 | DEBUG    | __main__:<module>:313 - Training step 29440: loss = 3.0797 | 3014.97ms | Tokens/s = 173,894.8
2025-01-16 08:30:59.742 | DEBUG    | __main__:<module>:313 - Training step 29450: loss = 3.0738 | 3017.43ms | Tokens/s = 173,753.2
2025-01-16 08:31:29.924 | DEBUG    | __main__:<module>:313 - Training step 29460: loss = 3.3250 | 3017.23ms | Tokens/s = 173,764.8
2025-01-16 08:32:00.095 | DEBUG    | __main__:<module>:313 - Training step 29470: loss = 3.1603 | 3015.23ms | Tokens/s = 173,880.0
2025-01-16 08:32:30.239 | DEBUG    | __main__:<module>:313 - Training step 29480: loss = 3.0374 | 3013.42ms | Tokens/s = 173,984.4
2025-01-16 08:33:00.365 | DEBUG    | __main__:<module>:313 - Training step 29490: loss = 3.0835 | 3013.81ms | Tokens/s = 173,962.0
2025-01-16 08:33:30.512 | DEBUG    | __main__:<module>:313 - Training step 29500: loss = 3.2881 | 3013.66ms | Tokens/s = 173,970.5
2025-01-16 08:34:00.677 | DEBUG    | __main__:<module>:313 - Training step 29510: loss = 3.1335 | 3018.36ms | Tokens/s = 173,699.8
2025-01-16 08:34:30.817 | DEBUG    | __main__:<module>:313 - Training step 29520: loss = 3.1970 | 3015.16ms | Tokens/s = 173,884.1
2025-01-16 08:35:00.955 | DEBUG    | __main__:<module>:313 - Training step 29530: loss = 2.8500 | 3016.17ms | Tokens/s = 173,825.8
2025-01-16 08:35:31.108 | DEBUG    | __main__:<module>:313 - Training step 29540: loss = 3.1071 | 3015.57ms | Tokens/s = 173,860.5
2025-01-16 08:36:01.275 | DEBUG    | __main__:<module>:313 - Training step 29550: loss = 3.1066 | 3019.60ms | Tokens/s = 173,628.4
2025-01-16 08:36:31.444 | DEBUG    | __main__:<module>:313 - Training step 29560: loss = 3.0474 | 3016.79ms | Tokens/s = 173,790.3
2025-01-16 08:37:01.598 | DEBUG    | __main__:<module>:313 - Training step 29570: loss = 3.1068 | 3013.17ms | Tokens/s = 173,998.6
2025-01-16 08:37:31.756 | DEBUG    | __main__:<module>:313 - Training step 29580: loss = 3.0223 | 3017.87ms | Tokens/s = 173,727.6
2025-01-16 08:38:01.932 | DEBUG    | __main__:<module>:313 - Training step 29590: loss = 2.9803 | 3017.97ms | Tokens/s = 173,722.0
2025-01-16 08:38:32.107 | DEBUG    | __main__:<module>:313 - Training step 29600: loss = 3.3332 | 3017.23ms | Tokens/s = 173,764.8
2025-01-16 08:39:02.252 | DEBUG    | __main__:<module>:313 - Training step 29610: loss = 3.1552 | 3012.78ms | Tokens/s = 174,021.2
2025-01-16 08:39:32.387 | DEBUG    | __main__:<module>:313 - Training step 29620: loss = 3.0416 | 3015.83ms | Tokens/s = 173,845.5
2025-01-16 08:40:02.542 | DEBUG    | __main__:<module>:313 - Training step 29630: loss = 3.1804 | 3014.16ms | Tokens/s = 173,941.9
2025-01-16 08:40:32.728 | DEBUG    | __main__:<module>:313 - Training step 29640: loss = 3.1011 | 3019.72ms | Tokens/s = 173,621.6
2025-01-16 08:41:02.900 | DEBUG    | __main__:<module>:313 - Training step 29650: loss = 3.1448 | 3012.57ms | Tokens/s = 174,033.3
2025-01-16 08:41:33.046 | DEBUG    | __main__:<module>:313 - Training step 29660: loss = 3.2037 | 3011.70ms | Tokens/s = 174,083.9
2025-01-16 08:42:03.177 | DEBUG    | __main__:<module>:313 - Training step 29670: loss = 2.9888 | 3010.45ms | Tokens/s = 174,155.9
2025-01-16 08:42:33.330 | DEBUG    | __main__:<module>:313 - Training step 29680: loss = 3.0090 | 3016.17ms | Tokens/s = 173,825.7
2025-01-16 08:43:03.503 | DEBUG    | __main__:<module>:313 - Training step 29690: loss = 3.0406 | 3016.44ms | Tokens/s = 173,810.2
2025-01-16 08:43:33.649 | DEBUG    | __main__:<module>:313 - Training step 29700: loss = 3.2034 | 3012.03ms | Tokens/s = 174,064.5
2025-01-16 08:44:03.809 | DEBUG    | __main__:<module>:313 - Training step 29710: loss = 3.0722 | 3015.99ms | Tokens/s = 173,836.3
2025-01-16 08:44:33.992 | DEBUG    | __main__:<module>:313 - Training step 29720: loss = 3.2970 | 3015.57ms | Tokens/s = 173,860.4
2025-01-16 08:45:04.169 | DEBUG    | __main__:<module>:313 - Training step 29730: loss = 3.0144 | 3017.37ms | Tokens/s = 173,756.5
2025-01-16 08:45:34.319 | DEBUG    | __main__:<module>:313 - Training step 29740: loss = 3.0887 | 3014.22ms | Tokens/s = 173,938.3
2025-01-16 08:46:04.456 | DEBUG    | __main__:<module>:313 - Training step 29750: loss = 2.9871 | 3014.55ms | Tokens/s = 173,919.4
2025-01-16 08:46:34.596 | DEBUG    | __main__:<module>:313 - Training step 29760: loss = 3.1945 | 3011.69ms | Tokens/s = 174,084.5
2025-01-16 08:47:04.720 | DEBUG    | __main__:<module>:313 - Training step 29770: loss = 3.0007 | 3011.59ms | Tokens/s = 174,090.3
2025-01-16 08:47:34.858 | DEBUG    | __main__:<module>:313 - Training step 29780: loss = 3.2179 | 3016.97ms | Tokens/s = 173,779.6
2025-01-16 08:48:05.022 | DEBUG    | __main__:<module>:313 - Training step 29790: loss = 3.0494 | 3017.00ms | Tokens/s = 173,777.9
2025-01-16 08:48:35.209 | DEBUG    | __main__:<module>:313 - Training step 29800: loss = 3.0794 | 3017.85ms | Tokens/s = 173,728.9
2025-01-16 08:49:05.361 | DEBUG    | __main__:<module>:313 - Training step 29810: loss = 3.1463 | 3014.54ms | Tokens/s = 173,919.6
2025-01-16 08:49:35.506 | DEBUG    | __main__:<module>:313 - Training step 29820: loss = 3.1174 | 3014.89ms | Tokens/s = 173,899.4
2025-01-16 08:50:05.643 | DEBUG    | __main__:<module>:313 - Training step 29830: loss = 3.0336 | 3015.55ms | Tokens/s = 173,861.3
2025-01-16 08:50:35.805 | DEBUG    | __main__:<module>:313 - Training step 29840: loss = 3.0853 | 3017.54ms | Tokens/s = 173,747.1
2025-01-16 08:51:05.969 | DEBUG    | __main__:<module>:313 - Training step 29850: loss = 3.1839 | 3014.60ms | Tokens/s = 173,916.5
2025-01-16 08:51:36.120 | DEBUG    | __main__:<module>:313 - Training step 29860: loss = 3.0755 | 3015.49ms | Tokens/s = 173,864.9
2025-01-16 08:52:06.290 | DEBUG    | __main__:<module>:313 - Training step 29870: loss = 3.1166 | 3016.77ms | Tokens/s = 173,791.2
2025-01-16 08:52:36.439 | DEBUG    | __main__:<module>:313 - Training step 29880: loss = 3.1138 | 3014.35ms | Tokens/s = 173,930.5
2025-01-16 08:53:06.598 | DEBUG    | __main__:<module>:313 - Training step 29890: loss = 3.2846 | 3016.53ms | Tokens/s = 173,805.1
2025-01-16 08:53:36.739 | DEBUG    | __main__:<module>:313 - Training step 29900: loss = 3.1457 | 3015.30ms | Tokens/s = 173,876.2
2025-01-16 08:54:06.881 | DEBUG    | __main__:<module>:313 - Training step 29910: loss = 2.9719 | 3016.21ms | Tokens/s = 173,823.7
2025-01-16 08:54:37.052 | DEBUG    | __main__:<module>:313 - Training step 29920: loss = 3.0443 | 3015.89ms | Tokens/s = 173,842.2
2025-01-16 08:55:07.214 | DEBUG    | __main__:<module>:313 - Training step 29930: loss = 3.1303 | 3016.31ms | Tokens/s = 173,817.4
2025-01-16 08:55:37.361 | DEBUG    | __main__:<module>:313 - Training step 29940: loss = 2.8850 | 3017.11ms | Tokens/s = 173,771.9
2025-01-16 08:56:07.499 | DEBUG    | __main__:<module>:313 - Training step 29950: loss = 3.0901 | 3013.31ms | Tokens/s = 173,990.6
2025-01-16 08:56:37.668 | DEBUG    | __main__:<module>:313 - Training step 29960: loss = 3.1749 | 3019.39ms | Tokens/s = 173,640.6
2025-01-16 08:57:07.865 | DEBUG    | __main__:<module>:313 - Training step 29970: loss = 2.8034 | 3019.95ms | Tokens/s = 173,608.2
2025-01-16 08:57:38.060 | DEBUG    | __main__:<module>:313 - Training step 29980: loss = 3.1810 | 3017.30ms | Tokens/s = 173,760.9
2025-01-16 08:58:08.233 | DEBUG    | __main__:<module>:313 - Training step 29990: loss = 3.0352 | 3016.73ms | Tokens/s = 173,793.6
2025-01-16 08:58:41.829 | INFO     | __main__:<module>:265 - Step 30,000/40,000 loss: 3.0796 (T) 3.0963 (V) | lr=1.6e-03
2025-01-16 08:58:41.831 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 08:58:55.006 | DEBUG    | __main__:<module>:313 - Training step 30000: loss = 3.1550 | 19622.66ms | Tokens/s = 26,718.5
2025-01-16 08:59:25.067 | DEBUG    | __main__:<module>:313 - Training step 30010: loss = 3.2155 | 3009.85ms | Tokens/s = 174,190.9
2025-01-16 08:59:55.199 | DEBUG    | __main__:<module>:313 - Training step 30020: loss = 3.2022 | 3014.75ms | Tokens/s = 173,907.3
2025-01-16 09:00:25.363 | DEBUG    | __main__:<module>:313 - Training step 30030: loss = 3.1509 | 3015.94ms | Tokens/s = 173,839.2
2025-01-16 09:00:55.526 | DEBUG    | __main__:<module>:313 - Training step 30040: loss = 3.0296 | 3016.95ms | Tokens/s = 173,780.7
2025-01-16 09:01:25.683 | DEBUG    | __main__:<module>:313 - Training step 30050: loss = 3.0676 | 3012.35ms | Tokens/s = 174,046.2
2025-01-16 09:01:55.824 | DEBUG    | __main__:<module>:313 - Training step 30060: loss = 3.1469 | 3014.04ms | Tokens/s = 173,948.5
2025-01-16 09:02:25.972 | DEBUG    | __main__:<module>:313 - Training step 30070: loss = 3.0882 | 3016.08ms | Tokens/s = 173,830.7
2025-01-16 09:02:56.152 | DEBUG    | __main__:<module>:313 - Training step 30080: loss = 3.1542 | 3019.19ms | Tokens/s = 173,651.6
2025-01-16 09:03:26.347 | DEBUG    | __main__:<module>:313 - Training step 30090: loss = 3.2198 | 3020.31ms | Tokens/s = 173,587.3
2025-01-16 09:03:56.521 | DEBUG    | __main__:<module>:313 - Training step 30100: loss = 3.0279 | 3014.44ms | Tokens/s = 173,925.7
2025-01-16 09:04:26.689 | DEBUG    | __main__:<module>:313 - Training step 30110: loss = 3.1378 | 3014.89ms | Tokens/s = 173,899.3
2025-01-16 09:04:56.838 | DEBUG    | __main__:<module>:313 - Training step 30120: loss = 3.1166 | 3017.67ms | Tokens/s = 173,739.3
2025-01-16 09:05:26.984 | DEBUG    | __main__:<module>:313 - Training step 30130: loss = 2.9970 | 3016.66ms | Tokens/s = 173,797.4
2025-01-16 09:05:57.129 | DEBUG    | __main__:<module>:313 - Training step 30140: loss = 3.0532 | 3014.34ms | Tokens/s = 173,931.3
2025-01-16 09:06:27.274 | DEBUG    | __main__:<module>:313 - Training step 30150: loss = 3.0523 | 3014.23ms | Tokens/s = 173,937.7
2025-01-16 09:06:57.414 | DEBUG    | __main__:<module>:313 - Training step 30160: loss = 3.0374 | 3013.80ms | Tokens/s = 173,962.3
2025-01-16 09:07:27.574 | DEBUG    | __main__:<module>:313 - Training step 30170: loss = 2.8976 | 3019.11ms | Tokens/s = 173,656.7
2025-01-16 09:07:57.770 | DEBUG    | __main__:<module>:313 - Training step 30180: loss = 3.0276 | 3020.63ms | Tokens/s = 173,568.9
2025-01-16 09:08:27.952 | DEBUG    | __main__:<module>:313 - Training step 30190: loss = 3.3595 | 3017.16ms | Tokens/s = 173,768.8
2025-01-16 09:08:58.135 | DEBUG    | __main__:<module>:313 - Training step 30200: loss = 3.1285 | 3016.89ms | Tokens/s = 173,784.1
2025-01-16 09:09:28.309 | DEBUG    | __main__:<module>:313 - Training step 30210: loss = 3.0460 | 3014.92ms | Tokens/s = 173,898.0
2025-01-16 09:09:58.489 | DEBUG    | __main__:<module>:313 - Training step 30220: loss = 3.1024 | 3018.56ms | Tokens/s = 173,688.2
2025-01-16 09:10:28.681 | DEBUG    | __main__:<module>:313 - Training step 30230: loss = 3.2701 | 3019.91ms | Tokens/s = 173,610.5
2025-01-16 09:10:58.865 | DEBUG    | __main__:<module>:313 - Training step 30240: loss = 2.9493 | 3016.95ms | Tokens/s = 173,780.9
2025-01-16 09:11:29.030 | DEBUG    | __main__:<module>:313 - Training step 30250: loss = 2.9025 | 3015.39ms | Tokens/s = 173,870.5
2025-01-16 09:11:59.205 | DEBUG    | __main__:<module>:313 - Training step 30260: loss = 3.0106 | 3017.64ms | Tokens/s = 173,740.9
2025-01-16 09:12:29.370 | DEBUG    | __main__:<module>:313 - Training step 30270: loss = 3.0482 | 3016.65ms | Tokens/s = 173,798.0
2025-01-16 09:12:59.526 | DEBUG    | __main__:<module>:313 - Training step 30280: loss = 3.0615 | 3014.73ms | Tokens/s = 173,908.7
2025-01-16 09:13:29.677 | DEBUG    | __main__:<module>:313 - Training step 30290: loss = 3.2056 | 3016.78ms | Tokens/s = 173,790.4
2025-01-16 09:13:59.821 | DEBUG    | __main__:<module>:313 - Training step 30300: loss = 3.1379 | 3015.06ms | Tokens/s = 173,889.9
2025-01-16 09:14:29.970 | DEBUG    | __main__:<module>:313 - Training step 30310: loss = 3.0072 | 3014.27ms | Tokens/s = 173,935.1
2025-01-16 09:15:00.125 | DEBUG    | __main__:<module>:313 - Training step 30320: loss = 3.0408 | 3017.26ms | Tokens/s = 173,762.8
2025-01-16 09:15:30.279 | DEBUG    | __main__:<module>:313 - Training step 30330: loss = 2.9582 | 3019.31ms | Tokens/s = 173,644.9
2025-01-16 09:16:00.461 | DEBUG    | __main__:<module>:313 - Training step 30340: loss = 3.1131 | 3017.79ms | Tokens/s = 173,732.5
2025-01-16 09:16:30.660 | DEBUG    | __main__:<module>:313 - Training step 30350: loss = 3.1101 | 3018.84ms | Tokens/s = 173,671.8
2025-01-16 09:17:00.838 | DEBUG    | __main__:<module>:313 - Training step 30360: loss = 3.1628 | 3014.56ms | Tokens/s = 173,918.7
2025-01-16 09:17:31.000 | DEBUG    | __main__:<module>:313 - Training step 30370: loss = 3.0101 | 3016.28ms | Tokens/s = 173,819.7
2025-01-16 09:18:01.158 | DEBUG    | __main__:<module>:313 - Training step 30380: loss = 3.0984 | 3014.33ms | Tokens/s = 173,932.0
2025-01-16 09:18:31.303 | DEBUG    | __main__:<module>:313 - Training step 30390: loss = 3.0038 | 3014.47ms | Tokens/s = 173,923.7
2025-01-16 09:19:01.445 | DEBUG    | __main__:<module>:313 - Training step 30400: loss = 2.8448 | 3012.38ms | Tokens/s = 174,044.7
2025-01-16 09:19:31.586 | DEBUG    | __main__:<module>:313 - Training step 30410: loss = 2.9682 | 3013.11ms | Tokens/s = 174,002.3
2025-01-16 09:20:01.724 | DEBUG    | __main__:<module>:313 - Training step 30420: loss = 3.1398 | 3012.02ms | Tokens/s = 174,065.5
2025-01-16 09:20:31.868 | DEBUG    | __main__:<module>:313 - Training step 30430: loss = 2.9940 | 3015.46ms | Tokens/s = 173,866.4
2025-01-16 09:21:02.008 | DEBUG    | __main__:<module>:313 - Training step 30440: loss = 2.9641 | 3013.69ms | Tokens/s = 173,968.8
2025-01-16 09:21:32.151 | DEBUG    | __main__:<module>:313 - Training step 30450: loss = 3.0631 | 3014.38ms | Tokens/s = 173,928.9
2025-01-16 09:22:02.310 | DEBUG    | __main__:<module>:313 - Training step 30460: loss = 2.8888 | 3016.39ms | Tokens/s = 173,812.8
2025-01-16 09:22:32.489 | DEBUG    | __main__:<module>:313 - Training step 30470: loss = 3.0055 | 3015.76ms | Tokens/s = 173,849.6
2025-01-16 09:23:02.659 | DEBUG    | __main__:<module>:313 - Training step 30480: loss = 3.1253 | 3015.31ms | Tokens/s = 173,875.5
2025-01-16 09:23:32.823 | DEBUG    | __main__:<module>:313 - Training step 30490: loss = 2.9362 | 3018.04ms | Tokens/s = 173,718.2
2025-01-16 09:24:03.004 | DEBUG    | __main__:<module>:313 - Training step 30500: loss = 2.9490 | 3016.64ms | Tokens/s = 173,798.8
2025-01-16 09:24:33.202 | DEBUG    | __main__:<module>:313 - Training step 30510: loss = 2.9973 | 3019.40ms | Tokens/s = 173,639.8
2025-01-16 09:25:03.400 | DEBUG    | __main__:<module>:313 - Training step 30520: loss = 3.1974 | 3018.07ms | Tokens/s = 173,716.0
2025-01-16 09:25:33.585 | DEBUG    | __main__:<module>:313 - Training step 30530: loss = 3.0629 | 3018.27ms | Tokens/s = 173,704.9
2025-01-16 09:26:03.756 | DEBUG    | __main__:<module>:313 - Training step 30540: loss = 2.9585 | 3015.75ms | Tokens/s = 173,849.8
2025-01-16 09:26:33.921 | DEBUG    | __main__:<module>:313 - Training step 30550: loss = 2.9004 | 3016.00ms | Tokens/s = 173,835.4
2025-01-16 09:27:04.088 | DEBUG    | __main__:<module>:313 - Training step 30560: loss = 3.1196 | 3015.70ms | Tokens/s = 173,853.0
2025-01-16 09:27:34.250 | DEBUG    | __main__:<module>:313 - Training step 30570: loss = 3.0460 | 3015.22ms | Tokens/s = 173,880.3
2025-01-16 09:28:04.413 | DEBUG    | __main__:<module>:313 - Training step 30580: loss = 3.0740 | 3015.07ms | Tokens/s = 173,889.2
2025-01-16 09:28:34.601 | DEBUG    | __main__:<module>:313 - Training step 30590: loss = 3.0276 | 3018.53ms | Tokens/s = 173,690.1
2025-01-16 09:29:04.794 | DEBUG    | __main__:<module>:313 - Training step 30600: loss = 3.0735 | 3019.40ms | Tokens/s = 173,639.8
2025-01-16 09:29:34.980 | DEBUG    | __main__:<module>:313 - Training step 30610: loss = 3.1717 | 3018.73ms | Tokens/s = 173,678.1
2025-01-16 09:30:05.164 | DEBUG    | __main__:<module>:313 - Training step 30620: loss = 3.1300 | 3018.99ms | Tokens/s = 173,663.1
2025-01-16 09:30:35.326 | DEBUG    | __main__:<module>:313 - Training step 30630: loss = 3.1003 | 3014.34ms | Tokens/s = 173,931.3
2025-01-16 09:31:05.488 | DEBUG    | __main__:<module>:313 - Training step 30640: loss = 3.0061 | 3018.82ms | Tokens/s = 173,673.1
2025-01-16 09:31:35.676 | DEBUG    | __main__:<module>:313 - Training step 30650: loss = 3.0446 | 3021.88ms | Tokens/s = 173,497.4
2025-01-16 09:32:05.866 | DEBUG    | __main__:<module>:313 - Training step 30660: loss = 2.8012 | 3017.89ms | Tokens/s = 173,726.7
2025-01-16 09:32:36.042 | DEBUG    | __main__:<module>:313 - Training step 30670: loss = 3.1075 | 3017.97ms | Tokens/s = 173,721.9
2025-01-16 09:33:06.219 | DEBUG    | __main__:<module>:313 - Training step 30680: loss = 3.1444 | 3016.90ms | Tokens/s = 173,783.7
2025-01-16 09:33:36.413 | DEBUG    | __main__:<module>:313 - Training step 30690: loss = 3.0252 | 3018.30ms | Tokens/s = 173,702.8
2025-01-16 09:34:06.582 | DEBUG    | __main__:<module>:313 - Training step 30700: loss = 2.9751 | 3015.49ms | Tokens/s = 173,864.9
2025-01-16 09:34:36.748 | DEBUG    | __main__:<module>:313 - Training step 30710: loss = 3.2056 | 3014.41ms | Tokens/s = 173,927.5
2025-01-16 09:35:06.898 | DEBUG    | __main__:<module>:313 - Training step 30720: loss = 3.0217 | 3013.78ms | Tokens/s = 173,963.5
2025-01-16 09:35:37.047 | DEBUG    | __main__:<module>:313 - Training step 30730: loss = 2.9612 | 3014.62ms | Tokens/s = 173,915.3
2025-01-16 09:36:07.195 | DEBUG    | __main__:<module>:313 - Training step 30740: loss = 3.1942 | 3012.32ms | Tokens/s = 174,048.2
2025-01-16 09:36:37.345 | DEBUG    | __main__:<module>:313 - Training step 30750: loss = 2.8958 | 3017.09ms | Tokens/s = 173,772.5
2025-01-16 09:37:07.533 | DEBUG    | __main__:<module>:313 - Training step 30760: loss = 3.1225 | 3021.55ms | Tokens/s = 173,516.0
2025-01-16 09:37:37.716 | DEBUG    | __main__:<module>:313 - Training step 30770: loss = 3.1597 | 3016.79ms | Tokens/s = 173,789.8
2025-01-16 09:38:07.885 | DEBUG    | __main__:<module>:313 - Training step 30780: loss = 3.0383 | 3017.79ms | Tokens/s = 173,732.5
2025-01-16 09:38:38.031 | DEBUG    | __main__:<module>:313 - Training step 30790: loss = 2.9861 | 3013.75ms | Tokens/s = 173,965.5
2025-01-16 09:39:08.182 | DEBUG    | __main__:<module>:313 - Training step 30800: loss = 3.0639 | 3013.47ms | Tokens/s = 173,981.3
2025-01-16 09:39:38.326 | DEBUG    | __main__:<module>:313 - Training step 30810: loss = 3.0124 | 3012.18ms | Tokens/s = 174,056.0
2025-01-16 09:40:08.473 | DEBUG    | __main__:<module>:313 - Training step 30820: loss = 3.0623 | 3015.09ms | Tokens/s = 173,888.0
2025-01-16 09:40:38.620 | DEBUG    | __main__:<module>:313 - Training step 30830: loss = 3.0503 | 3015.65ms | Tokens/s = 173,855.5
2025-01-16 09:41:08.773 | DEBUG    | __main__:<module>:313 - Training step 30840: loss = 2.9058 | 3015.02ms | Tokens/s = 173,892.0
2025-01-16 09:41:38.916 | DEBUG    | __main__:<module>:313 - Training step 30850: loss = 3.0178 | 3013.09ms | Tokens/s = 174,003.6
2025-01-16 09:42:09.058 | DEBUG    | __main__:<module>:313 - Training step 30860: loss = 3.1091 | 3013.50ms | Tokens/s = 173,980.0
2025-01-16 09:42:39.201 | DEBUG    | __main__:<module>:313 - Training step 30870: loss = 3.1723 | 3016.90ms | Tokens/s = 173,783.5
2025-01-16 09:43:09.348 | DEBUG    | __main__:<module>:313 - Training step 30880: loss = 3.0019 | 3017.27ms | Tokens/s = 173,762.6
2025-01-16 09:43:39.489 | DEBUG    | __main__:<module>:313 - Training step 30890: loss = 2.9535 | 3011.95ms | Tokens/s = 174,069.3
2025-01-16 09:44:09.631 | DEBUG    | __main__:<module>:313 - Training step 30900: loss = 3.2276 | 3014.22ms | Tokens/s = 173,938.0
2025-01-16 09:44:39.781 | DEBUG    | __main__:<module>:313 - Training step 30910: loss = 3.1223 | 3013.41ms | Tokens/s = 173,985.1
2025-01-16 09:45:09.931 | DEBUG    | __main__:<module>:313 - Training step 30920: loss = 3.0740 | 3015.69ms | Tokens/s = 173,853.5
2025-01-16 09:45:40.097 | DEBUG    | __main__:<module>:313 - Training step 30930: loss = 3.0027 | 3018.17ms | Tokens/s = 173,710.4
2025-01-16 09:46:10.292 | DEBUG    | __main__:<module>:313 - Training step 30940: loss = 3.0765 | 3017.74ms | Tokens/s = 173,735.2
2025-01-16 09:46:40.477 | DEBUG    | __main__:<module>:313 - Training step 30950: loss = 3.1548 | 3018.79ms | Tokens/s = 173,674.7
2025-01-16 09:47:10.663 | DEBUG    | __main__:<module>:313 - Training step 30960: loss = 3.0614 | 3018.43ms | Tokens/s = 173,695.7
2025-01-16 09:47:40.850 | DEBUG    | __main__:<module>:313 - Training step 30970: loss = 2.9644 | 3017.12ms | Tokens/s = 173,770.9
2025-01-16 09:48:11.020 | DEBUG    | __main__:<module>:313 - Training step 30980: loss = 3.0299 | 3013.10ms | Tokens/s = 174,003.1
2025-01-16 09:48:41.182 | DEBUG    | __main__:<module>:313 - Training step 30990: loss = 2.9733 | 3014.62ms | Tokens/s = 173,915.1
2025-01-16 09:49:14.766 | INFO     | __main__:<module>:265 - Step 31,000/40,000 loss: 3.0627 (T) 3.0660 (V) | lr=1.3e-03
2025-01-16 09:49:14.768 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 09:49:27.832 | DEBUG    | __main__:<module>:313 - Training step 31000: loss = 3.1049 | 19509.30ms | Tokens/s = 26,873.7
2025-01-16 09:49:57.889 | DEBUG    | __main__:<module>:313 - Training step 31010: loss = 3.1868 | 3008.21ms | Tokens/s = 174,286.0
2025-01-16 09:50:28.021 | DEBUG    | __main__:<module>:313 - Training step 31020: loss = 3.1646 | 3012.50ms | Tokens/s = 174,037.2
2025-01-16 09:50:58.194 | DEBUG    | __main__:<module>:313 - Training step 31030: loss = 3.0803 | 3018.02ms | Tokens/s = 173,719.2
2025-01-16 09:51:28.371 | DEBUG    | __main__:<module>:313 - Training step 31040: loss = 3.0587 | 3015.41ms | Tokens/s = 173,869.7
2025-01-16 09:51:58.535 | DEBUG    | __main__:<module>:313 - Training step 31050: loss = 2.9032 | 3015.97ms | Tokens/s = 173,837.5
2025-01-16 09:52:28.687 | DEBUG    | __main__:<module>:313 - Training step 31060: loss = 3.0763 | 3013.22ms | Tokens/s = 173,995.9
2025-01-16 09:52:58.837 | DEBUG    | __main__:<module>:313 - Training step 31070: loss = 3.0873 | 3013.96ms | Tokens/s = 173,953.4
2025-01-16 09:53:28.980 | DEBUG    | __main__:<module>:313 - Training step 31080: loss = 2.8720 | 3013.71ms | Tokens/s = 173,967.7
2025-01-16 09:53:59.155 | DEBUG    | __main__:<module>:313 - Training step 31090: loss = 3.0288 | 3017.83ms | Tokens/s = 173,730.0
2025-01-16 09:54:29.343 | DEBUG    | __main__:<module>:313 - Training step 31100: loss = 3.2720 | 3018.09ms | Tokens/s = 173,715.4
2025-01-16 09:54:59.517 | DEBUG    | __main__:<module>:313 - Training step 31110: loss = 3.0616 | 3017.50ms | Tokens/s = 173,749.1
2025-01-16 09:55:29.680 | DEBUG    | __main__:<module>:313 - Training step 31120: loss = 3.1112 | 3019.92ms | Tokens/s = 173,609.6
2025-01-16 09:55:59.833 | DEBUG    | __main__:<module>:313 - Training step 31130: loss = 3.0388 | 3014.32ms | Tokens/s = 173,932.7
2025-01-16 09:56:29.982 | DEBUG    | __main__:<module>:313 - Training step 31140: loss = 3.0622 | 3017.23ms | Tokens/s = 173,764.8
2025-01-16 09:57:00.180 | DEBUG    | __main__:<module>:313 - Training step 31150: loss = 3.0660 | 3020.58ms | Tokens/s = 173,572.1
2025-01-16 09:57:30.370 | DEBUG    | __main__:<module>:313 - Training step 31160: loss = 3.0934 | 3016.49ms | Tokens/s = 173,807.4
2025-01-16 09:58:00.545 | DEBUG    | __main__:<module>:313 - Training step 31170: loss = 3.0321 | 3016.53ms | Tokens/s = 173,804.8
2025-01-16 09:58:30.725 | DEBUG    | __main__:<module>:313 - Training step 31180: loss = 2.8418 | 3017.51ms | Tokens/s = 173,748.8
2025-01-16 09:59:00.923 | DEBUG    | __main__:<module>:313 - Training step 31190: loss = 3.1531 | 3017.72ms | Tokens/s = 173,736.3
2025-01-16 09:59:31.117 | DEBUG    | __main__:<module>:313 - Training step 31200: loss = 3.0448 | 3016.88ms | Tokens/s = 173,785.0
2025-01-16 10:00:01.295 | DEBUG    | __main__:<module>:313 - Training step 31210: loss = 3.1629 | 3018.34ms | Tokens/s = 173,700.7
2025-01-16 10:00:31.464 | DEBUG    | __main__:<module>:313 - Training step 31220: loss = 2.9881 | 3017.90ms | Tokens/s = 173,726.0
2025-01-16 10:01:01.651 | DEBUG    | __main__:<module>:313 - Training step 31230: loss = 2.9195 | 3019.68ms | Tokens/s = 173,623.6
2025-01-16 10:01:31.860 | DEBUG    | __main__:<module>:313 - Training step 31240: loss = 3.0459 | 3019.25ms | Tokens/s = 173,648.4
2025-01-16 10:02:02.046 | DEBUG    | __main__:<module>:313 - Training step 31250: loss = 2.8674 | 3018.01ms | Tokens/s = 173,719.7
2025-01-16 10:02:32.217 | DEBUG    | __main__:<module>:313 - Training step 31260: loss = 3.0596 | 3016.19ms | Tokens/s = 173,824.4
2025-01-16 10:03:02.377 | DEBUG    | __main__:<module>:313 - Training step 31270: loss = 2.9899 | 3015.74ms | Tokens/s = 173,850.5
2025-01-16 10:03:32.536 | DEBUG    | __main__:<module>:313 - Training step 31280: loss = 3.1406 | 3016.07ms | Tokens/s = 173,831.3
2025-01-16 10:04:02.686 | DEBUG    | __main__:<module>:313 - Training step 31290: loss = 3.0870 | 3015.50ms | Tokens/s = 173,864.1
2025-01-16 10:04:32.856 | DEBUG    | __main__:<module>:313 - Training step 31300: loss = 3.0403 | 3018.53ms | Tokens/s = 173,689.6
2025-01-16 10:05:03.040 | DEBUG    | __main__:<module>:313 - Training step 31310: loss = 2.9544 | 3017.39ms | Tokens/s = 173,755.5
2025-01-16 10:05:33.205 | DEBUG    | __main__:<module>:313 - Training step 31320: loss = 3.0955 | 3016.15ms | Tokens/s = 173,827.0
2025-01-16 10:06:03.362 | DEBUG    | __main__:<module>:313 - Training step 31330: loss = 3.0744 | 3013.39ms | Tokens/s = 173,986.1
2025-01-16 10:06:33.511 | DEBUG    | __main__:<module>:313 - Training step 31340: loss = 3.0871 | 3013.02ms | Tokens/s = 174,007.5
2025-01-16 10:07:03.669 | DEBUG    | __main__:<module>:313 - Training step 31350: loss = 3.1461 | 3017.22ms | Tokens/s = 173,765.0
2025-01-16 10:07:33.830 | DEBUG    | __main__:<module>:313 - Training step 31360: loss = 3.0602 | 3017.02ms | Tokens/s = 173,776.8
2025-01-16 10:08:03.981 | DEBUG    | __main__:<module>:313 - Training step 31370: loss = 3.0747 | 3014.16ms | Tokens/s = 173,941.4
2025-01-16 10:08:34.124 | DEBUG    | __main__:<module>:313 - Training step 31380: loss = 3.0628 | 3014.98ms | Tokens/s = 173,894.6
2025-01-16 10:09:04.267 | DEBUG    | __main__:<module>:313 - Training step 31390: loss = 3.0781 | 3013.09ms | Tokens/s = 174,003.4
2025-01-16 10:09:34.426 | DEBUG    | __main__:<module>:313 - Training step 31400: loss = 2.9583 | 3015.64ms | Tokens/s = 173,856.3
2025-01-16 10:10:04.624 | DEBUG    | __main__:<module>:313 - Training step 31410: loss = 3.2765 | 3018.35ms | Tokens/s = 173,700.3
2025-01-16 10:10:34.827 | DEBUG    | __main__:<module>:313 - Training step 31420: loss = 3.1659 | 3019.89ms | Tokens/s = 173,611.7
2025-01-16 10:11:05.009 | DEBUG    | __main__:<module>:313 - Training step 31430: loss = 3.0678 | 3018.59ms | Tokens/s = 173,686.1
2025-01-16 10:11:35.179 | DEBUG    | __main__:<module>:313 - Training step 31440: loss = 3.0707 | 3015.91ms | Tokens/s = 173,840.7
2025-01-16 10:12:05.387 | DEBUG    | __main__:<module>:313 - Training step 31450: loss = 2.9513 | 3023.12ms | Tokens/s = 173,426.0
2025-01-16 10:12:35.576 | DEBUG    | __main__:<module>:313 - Training step 31460: loss = 3.1270 | 3021.09ms | Tokens/s = 173,542.6
2025-01-16 10:13:05.750 | DEBUG    | __main__:<module>:313 - Training step 31470: loss = 3.1051 | 3016.07ms | Tokens/s = 173,831.4
2025-01-16 10:13:35.919 | DEBUG    | __main__:<module>:313 - Training step 31480: loss = 3.1000 | 3016.71ms | Tokens/s = 173,794.9
2025-01-16 10:14:06.089 | DEBUG    | __main__:<module>:313 - Training step 31490: loss = 3.1144 | 3018.26ms | Tokens/s = 173,705.4
2025-01-16 10:14:36.245 | DEBUG    | __main__:<module>:313 - Training step 31500: loss = 3.0740 | 3015.14ms | Tokens/s = 173,885.2
2025-01-16 10:15:06.404 | DEBUG    | __main__:<module>:313 - Training step 31510: loss = 3.2066 | 3014.46ms | Tokens/s = 173,924.2
2025-01-16 10:15:36.556 | DEBUG    | __main__:<module>:313 - Training step 31520: loss = 3.0704 | 3012.76ms | Tokens/s = 174,022.5
2025-01-16 10:16:06.724 | DEBUG    | __main__:<module>:313 - Training step 31530: loss = 2.9613 | 3018.18ms | Tokens/s = 173,709.8
2025-01-16 10:16:36.919 | DEBUG    | __main__:<module>:313 - Training step 31540: loss = 3.0562 | 3020.01ms | Tokens/s = 173,604.7
2025-01-16 10:17:07.101 | DEBUG    | __main__:<module>:313 - Training step 31550: loss = 2.9163 | 3017.14ms | Tokens/s = 173,769.8
2025-01-16 10:17:37.273 | DEBUG    | __main__:<module>:313 - Training step 31560: loss = 2.9782 | 3017.81ms | Tokens/s = 173,731.5
2025-01-16 10:18:07.442 | DEBUG    | __main__:<module>:313 - Training step 31570: loss = 2.8916 | 3015.32ms | Tokens/s = 173,874.6
2025-01-16 10:18:37.598 | DEBUG    | __main__:<module>:313 - Training step 31580: loss = 2.8572 | 3015.34ms | Tokens/s = 173,873.6
2025-01-16 10:19:07.752 | DEBUG    | __main__:<module>:313 - Training step 31590: loss = 3.0068 | 3015.49ms | Tokens/s = 173,864.9
2025-01-16 10:19:37.914 | DEBUG    | __main__:<module>:313 - Training step 31600: loss = 2.9168 | 3016.80ms | Tokens/s = 173,789.2
2025-01-16 10:20:08.101 | DEBUG    | __main__:<module>:313 - Training step 31610: loss = 3.0812 | 3019.58ms | Tokens/s = 173,629.5
2025-01-16 10:20:38.307 | DEBUG    | __main__:<module>:313 - Training step 31620: loss = 3.0917 | 3017.99ms | Tokens/s = 173,720.8
2025-01-16 10:21:08.499 | DEBUG    | __main__:<module>:313 - Training step 31630: loss = 3.1605 | 3017.00ms | Tokens/s = 173,777.8
2025-01-16 10:21:38.687 | DEBUG    | __main__:<module>:313 - Training step 31640: loss = 3.1601 | 3020.71ms | Tokens/s = 173,564.3
2025-01-16 10:22:08.874 | DEBUG    | __main__:<module>:313 - Training step 31650: loss = 3.0018 | 3017.42ms | Tokens/s = 173,754.0
2025-01-16 10:22:39.049 | DEBUG    | __main__:<module>:313 - Training step 31660: loss = 3.0576 | 3017.66ms | Tokens/s = 173,739.7
2025-01-16 10:23:09.221 | DEBUG    | __main__:<module>:313 - Training step 31670: loss = 3.1252 | 3016.02ms | Tokens/s = 173,834.5
2025-01-16 10:23:39.384 | DEBUG    | __main__:<module>:313 - Training step 31680: loss = 3.0491 | 3015.81ms | Tokens/s = 173,846.4
2025-01-16 10:24:09.544 | DEBUG    | __main__:<module>:313 - Training step 31690: loss = 3.1120 | 3017.65ms | Tokens/s = 173,740.6
2025-01-16 10:24:39.712 | DEBUG    | __main__:<module>:313 - Training step 31700: loss = 3.1242 | 3017.38ms | Tokens/s = 173,756.0
2025-01-16 10:25:09.868 | DEBUG    | __main__:<module>:313 - Training step 31710: loss = 3.0051 | 3013.64ms | Tokens/s = 173,971.5
2025-01-16 10:25:40.026 | DEBUG    | __main__:<module>:313 - Training step 31720: loss = 3.0149 | 3018.41ms | Tokens/s = 173,696.9
2025-01-16 10:26:10.175 | DEBUG    | __main__:<module>:313 - Training step 31730: loss = 3.0605 | 3016.91ms | Tokens/s = 173,783.4
2025-01-16 10:26:40.325 | DEBUG    | __main__:<module>:313 - Training step 31740: loss = 3.0866 | 3013.99ms | Tokens/s = 173,951.5
2025-01-16 10:27:10.467 | DEBUG    | __main__:<module>:313 - Training step 31750: loss = 3.0678 | 3014.61ms | Tokens/s = 173,915.7
2025-01-16 10:27:40.613 | DEBUG    | __main__:<module>:313 - Training step 31760: loss = 3.1167 | 3015.46ms | Tokens/s = 173,866.9
2025-01-16 10:28:10.784 | DEBUG    | __main__:<module>:313 - Training step 31770: loss = 3.1146 | 3017.37ms | Tokens/s = 173,756.5
2025-01-16 10:28:40.950 | DEBUG    | __main__:<module>:313 - Training step 31780: loss = 2.8887 | 3018.82ms | Tokens/s = 173,673.3
2025-01-16 10:29:11.105 | DEBUG    | __main__:<module>:313 - Training step 31790: loss = 3.0533 | 3016.22ms | Tokens/s = 173,823.0
2025-01-16 10:29:41.282 | DEBUG    | __main__:<module>:313 - Training step 31800: loss = 3.0700 | 3018.42ms | Tokens/s = 173,696.2
2025-01-16 10:30:11.480 | DEBUG    | __main__:<module>:313 - Training step 31810: loss = 2.8871 | 3018.27ms | Tokens/s = 173,705.0
2025-01-16 10:30:41.663 | DEBUG    | __main__:<module>:313 - Training step 31820: loss = 2.9534 | 3018.03ms | Tokens/s = 173,718.6
2025-01-16 10:31:11.835 | DEBUG    | __main__:<module>:313 - Training step 31830: loss = 3.1976 | 3015.49ms | Tokens/s = 173,865.1
2025-01-16 10:31:41.989 | DEBUG    | __main__:<module>:313 - Training step 31840: loss = 3.1555 | 3014.41ms | Tokens/s = 173,927.4
2025-01-16 10:32:12.151 | DEBUG    | __main__:<module>:313 - Training step 31850: loss = 2.9390 | 3019.29ms | Tokens/s = 173,645.9
2025-01-16 10:32:42.338 | DEBUG    | __main__:<module>:313 - Training step 31860: loss = 2.8407 | 3017.71ms | Tokens/s = 173,737.2
2025-01-16 10:33:12.511 | DEBUG    | __main__:<module>:313 - Training step 31870: loss = 3.0808 | 3017.19ms | Tokens/s = 173,767.1
2025-01-16 10:33:42.669 | DEBUG    | __main__:<module>:313 - Training step 31880: loss = 2.8903 | 3015.40ms | Tokens/s = 173,870.1
2025-01-16 10:34:12.810 | DEBUG    | __main__:<module>:313 - Training step 31890: loss = 2.9280 | 3014.10ms | Tokens/s = 173,944.9
2025-01-16 10:34:42.958 | DEBUG    | __main__:<module>:313 - Training step 31900: loss = 3.1795 | 3012.34ms | Tokens/s = 174,046.8
2025-01-16 10:35:13.112 | DEBUG    | __main__:<module>:313 - Training step 31910: loss = 3.0730 | 3017.53ms | Tokens/s = 173,747.3
2025-01-16 10:35:43.300 | DEBUG    | __main__:<module>:313 - Training step 31920: loss = 2.9870 | 3019.97ms | Tokens/s = 173,606.8
2025-01-16 10:36:13.504 | DEBUG    | __main__:<module>:313 - Training step 31930: loss = 2.9540 | 3021.21ms | Tokens/s = 173,535.7
2025-01-16 10:36:43.689 | DEBUG    | __main__:<module>:313 - Training step 31940: loss = 2.9470 | 3018.69ms | Tokens/s = 173,680.8
2025-01-16 10:37:13.862 | DEBUG    | __main__:<module>:313 - Training step 31950: loss = 3.1123 | 3016.47ms | Tokens/s = 173,808.4
2025-01-16 10:37:44.026 | DEBUG    | __main__:<module>:313 - Training step 31960: loss = 3.0803 | 3014.29ms | Tokens/s = 173,934.2
2025-01-16 10:38:14.177 | DEBUG    | __main__:<module>:313 - Training step 31970: loss = 2.9748 | 3015.39ms | Tokens/s = 173,870.8
2025-01-16 10:38:44.326 | DEBUG    | __main__:<module>:313 - Training step 31980: loss = 3.0861 | 3015.32ms | Tokens/s = 173,875.0
2025-01-16 10:39:14.473 | DEBUG    | __main__:<module>:313 - Training step 31990: loss = 3.1257 | 3013.46ms | Tokens/s = 173,981.9
2025-01-16 10:39:48.046 | INFO     | __main__:<module>:265 - Step 32,000/40,000 loss: 3.0306 (T) 3.0489 (V) | lr=1.1e-03
2025-01-16 10:39:48.048 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 10:40:01.197 | DEBUG    | __main__:<module>:313 - Training step 32000: loss = 2.9200 | 19592.33ms | Tokens/s = 26,759.9
2025-01-16 10:40:31.277 | DEBUG    | __main__:<module>:313 - Training step 32010: loss = 2.8694 | 3009.47ms | Tokens/s = 174,212.5
2025-01-16 10:41:01.408 | DEBUG    | __main__:<module>:313 - Training step 32020: loss = 3.0239 | 3017.40ms | Tokens/s = 173,754.7
2025-01-16 10:41:31.579 | DEBUG    | __main__:<module>:313 - Training step 32030: loss = 2.9287 | 3017.06ms | Tokens/s = 173,774.5
2025-01-16 10:42:01.788 | DEBUG    | __main__:<module>:313 - Training step 32040: loss = 3.0238 | 3022.36ms | Tokens/s = 173,469.5
2025-01-16 10:42:31.997 | DEBUG    | __main__:<module>:313 - Training step 32050: loss = 3.1022 | 3019.69ms | Tokens/s = 173,622.9
2025-01-16 10:43:02.182 | DEBUG    | __main__:<module>:313 - Training step 32060: loss = 3.0405 | 3017.65ms | Tokens/s = 173,740.7
2025-01-16 10:43:32.359 | DEBUG    | __main__:<module>:313 - Training step 32070: loss = 3.0336 | 3019.46ms | Tokens/s = 173,636.5
2025-01-16 10:44:02.558 | DEBUG    | __main__:<module>:313 - Training step 32080: loss = 3.0593 | 3019.84ms | Tokens/s = 173,614.5
2025-01-16 10:44:32.743 | DEBUG    | __main__:<module>:313 - Training step 32090: loss = 3.1448 | 3016.94ms | Tokens/s = 173,781.4
2025-01-16 10:45:02.920 | DEBUG    | __main__:<module>:313 - Training step 32100: loss = 3.3675 | 3018.89ms | Tokens/s = 173,669.3
2025-01-16 10:45:33.101 | DEBUG    | __main__:<module>:313 - Training step 32110: loss = 3.1688 | 3018.51ms | Tokens/s = 173,690.8
2025-01-16 10:46:03.287 | DEBUG    | __main__:<module>:313 - Training step 32120: loss = 2.9832 | 3020.06ms | Tokens/s = 173,601.9
2025-01-16 10:46:33.484 | DEBUG    | __main__:<module>:313 - Training step 32130: loss = 3.0525 | 3020.57ms | Tokens/s = 173,572.7
2025-01-16 10:47:03.660 | DEBUG    | __main__:<module>:313 - Training step 32140: loss = 2.9481 | 3017.49ms | Tokens/s = 173,749.5
2025-01-16 10:47:33.827 | DEBUG    | __main__:<module>:313 - Training step 32150: loss = 2.9367 | 3015.61ms | Tokens/s = 173,857.8
2025-01-16 10:48:03.994 | DEBUG    | __main__:<module>:313 - Training step 32160: loss = 2.8645 | 3015.89ms | Tokens/s = 173,841.9
2025-01-16 10:48:34.150 | DEBUG    | __main__:<module>:313 - Training step 32170: loss = 2.9862 | 3017.98ms | Tokens/s = 173,721.4
2025-01-16 10:49:04.326 | DEBUG    | __main__:<module>:313 - Training step 32180: loss = 2.9976 | 3014.68ms | Tokens/s = 173,911.6
2025-01-16 10:49:34.493 | DEBUG    | __main__:<module>:313 - Training step 32190: loss = 2.7285 | 3017.48ms | Tokens/s = 173,750.0
2025-01-16 10:50:04.655 | DEBUG    | __main__:<module>:313 - Training step 32200: loss = 2.9865 | 3014.89ms | Tokens/s = 173,899.8
2025-01-16 10:50:34.813 | DEBUG    | __main__:<module>:313 - Training step 32210: loss = 2.9179 | 3017.05ms | Tokens/s = 173,775.3
2025-01-16 10:51:04.975 | DEBUG    | __main__:<module>:313 - Training step 32220: loss = 3.0090 | 3012.78ms | Tokens/s = 174,021.4
2025-01-16 10:51:35.131 | DEBUG    | __main__:<module>:313 - Training step 32230: loss = 2.8743 | 3013.86ms | Tokens/s = 173,958.9
2025-01-16 10:52:05.280 | DEBUG    | __main__:<module>:313 - Training step 32240: loss = 3.0420 | 3018.30ms | Tokens/s = 173,703.1
2025-01-16 10:52:35.422 | DEBUG    | __main__:<module>:313 - Training step 32250: loss = 3.1438 | 3012.23ms | Tokens/s = 174,053.1
2025-01-16 10:53:05.566 | DEBUG    | __main__:<module>:313 - Training step 32260: loss = 3.1113 | 3018.37ms | Tokens/s = 173,699.3
2025-01-16 10:53:35.740 | DEBUG    | __main__:<module>:313 - Training step 32270: loss = 3.0137 | 3018.93ms | Tokens/s = 173,667.1
2025-01-16 10:54:05.938 | DEBUG    | __main__:<module>:313 - Training step 32280: loss = 3.0351 | 3020.26ms | Tokens/s = 173,590.3
2025-01-16 10:54:36.119 | DEBUG    | __main__:<module>:313 - Training step 32290: loss = 3.0878 | 3015.40ms | Tokens/s = 173,869.9
2025-01-16 10:55:06.294 | DEBUG    | __main__:<module>:313 - Training step 32300: loss = 3.0696 | 3017.55ms | Tokens/s = 173,746.3
2025-01-16 10:55:36.495 | DEBUG    | __main__:<module>:313 - Training step 32310: loss = 3.2422 | 3022.53ms | Tokens/s = 173,459.9
2025-01-16 10:56:06.686 | DEBUG    | __main__:<module>:313 - Training step 32320: loss = 3.1752 | 3019.29ms | Tokens/s = 173,645.9
2025-01-16 10:56:36.865 | DEBUG    | __main__:<module>:313 - Training step 32330: loss = 3.1729 | 3017.74ms | Tokens/s = 173,735.4
2025-01-16 10:57:07.036 | DEBUG    | __main__:<module>:313 - Training step 32340: loss = 3.0325 | 3017.49ms | Tokens/s = 173,749.7
2025-01-16 10:57:37.207 | DEBUG    | __main__:<module>:313 - Training step 32350: loss = 3.0782 | 3018.13ms | Tokens/s = 173,713.0
2025-01-16 10:58:07.370 | DEBUG    | __main__:<module>:313 - Training step 32360: loss = 3.2105 | 3016.25ms | Tokens/s = 173,821.3
2025-01-16 10:58:37.524 | DEBUG    | __main__:<module>:313 - Training step 32370: loss = 3.0947 | 3015.83ms | Tokens/s = 173,845.5
2025-01-16 10:59:07.700 | DEBUG    | __main__:<module>:313 - Training step 32380: loss = 2.9956 | 3018.90ms | Tokens/s = 173,668.4
2025-01-16 10:59:37.894 | DEBUG    | __main__:<module>:313 - Training step 32390: loss = 2.9749 | 3017.66ms | Tokens/s = 173,740.1
2025-01-16 11:00:08.064 | DEBUG    | __main__:<module>:313 - Training step 32400: loss = 2.8563 | 3015.20ms | Tokens/s = 173,881.9
2025-01-16 11:00:38.222 | DEBUG    | __main__:<module>:313 - Training step 32410: loss = 3.0173 | 3015.48ms | Tokens/s = 173,865.5
2025-01-16 11:01:08.381 | DEBUG    | __main__:<module>:313 - Training step 32420: loss = 3.0040 | 3015.79ms | Tokens/s = 173,847.4
2025-01-16 11:01:38.531 | DEBUG    | __main__:<module>:313 - Training step 32430: loss = 2.9405 | 3015.66ms | Tokens/s = 173,855.4
2025-01-16 11:02:08.691 | DEBUG    | __main__:<module>:313 - Training step 32440: loss = 3.0826 | 3018.82ms | Tokens/s = 173,673.1
2025-01-16 11:02:38.882 | DEBUG    | __main__:<module>:313 - Training step 32450: loss = 3.0432 | 3021.75ms | Tokens/s = 173,505.0
2025-01-16 11:03:09.081 | DEBUG    | __main__:<module>:313 - Training step 32460: loss = 2.9701 | 3017.19ms | Tokens/s = 173,767.1
2025-01-16 11:03:39.252 | DEBUG    | __main__:<module>:313 - Training step 32470: loss = 3.0293 | 3015.74ms | Tokens/s = 173,850.6
2025-01-16 11:04:09.417 | DEBUG    | __main__:<module>:313 - Training step 32480: loss = 3.1241 | 3015.95ms | Tokens/s = 173,838.3
2025-01-16 11:04:39.575 | DEBUG    | __main__:<module>:313 - Training step 32490: loss = 3.1166 | 3014.23ms | Tokens/s = 173,937.6
2025-01-16 11:05:09.726 | DEBUG    | __main__:<module>:313 - Training step 32500: loss = 2.9524 | 3013.94ms | Tokens/s = 173,954.2
2025-01-16 11:05:39.878 | DEBUG    | __main__:<module>:313 - Training step 32510: loss = 3.0107 | 3016.90ms | Tokens/s = 173,783.8
2025-01-16 11:06:10.027 | DEBUG    | __main__:<module>:313 - Training step 32520: loss = 3.0540 | 3014.92ms | Tokens/s = 173,897.9
2025-01-16 11:06:40.178 | DEBUG    | __main__:<module>:313 - Training step 32530: loss = 3.1998 | 3012.24ms | Tokens/s = 174,052.4
2025-01-16 11:07:10.326 | DEBUG    | __main__:<module>:313 - Training step 32540: loss = 2.9937 | 3016.41ms | Tokens/s = 173,812.1
2025-01-16 11:07:40.472 | DEBUG    | __main__:<module>:313 - Training step 32550: loss = 2.9814 | 3014.55ms | Tokens/s = 173,919.1
2025-01-16 11:08:10.617 | DEBUG    | __main__:<module>:313 - Training step 32560: loss = 3.0207 | 3013.32ms | Tokens/s = 173,990.3
2025-01-16 11:08:40.765 | DEBUG    | __main__:<module>:313 - Training step 32570: loss = 3.0469 | 3016.63ms | Tokens/s = 173,799.4
2025-01-16 11:09:10.953 | DEBUG    | __main__:<module>:313 - Training step 32580: loss = 3.2060 | 3021.77ms | Tokens/s = 173,503.8
2025-01-16 11:09:41.140 | DEBUG    | __main__:<module>:313 - Training step 32590: loss = 3.1256 | 3019.17ms | Tokens/s = 173,653.3
2025-01-16 11:10:11.315 | DEBUG    | __main__:<module>:313 - Training step 32600: loss = 2.9853 | 3018.73ms | Tokens/s = 173,678.4
2025-01-16 11:10:41.477 | DEBUG    | __main__:<module>:313 - Training step 32610: loss = 2.9131 | 3017.00ms | Tokens/s = 173,778.2
2025-01-16 11:11:11.641 | DEBUG    | __main__:<module>:313 - Training step 32620: loss = 3.0772 | 3016.60ms | Tokens/s = 173,800.8
2025-01-16 11:11:41.837 | DEBUG    | __main__:<module>:313 - Training step 32630: loss = 2.9766 | 3018.22ms | Tokens/s = 173,707.6
2025-01-16 11:12:12.030 | DEBUG    | __main__:<module>:313 - Training step 32640: loss = 3.0895 | 3017.92ms | Tokens/s = 173,725.2
2025-01-16 11:12:42.206 | DEBUG    | __main__:<module>:313 - Training step 32650: loss = 2.9588 | 3018.31ms | Tokens/s = 173,702.7
2025-01-16 11:13:12.372 | DEBUG    | __main__:<module>:313 - Training step 32660: loss = 3.0885 | 3015.41ms | Tokens/s = 173,869.3
2025-01-16 11:13:42.537 | DEBUG    | __main__:<module>:313 - Training step 32670: loss = 3.0553 | 3019.53ms | Tokens/s = 173,632.6
2025-01-16 11:14:12.729 | DEBUG    | __main__:<module>:313 - Training step 32680: loss = 3.0238 | 3021.67ms | Tokens/s = 173,509.6
2025-01-16 11:14:42.938 | DEBUG    | __main__:<module>:313 - Training step 32690: loss = 3.0226 | 3021.17ms | Tokens/s = 173,538.3
2025-01-16 11:15:13.134 | DEBUG    | __main__:<module>:313 - Training step 32700: loss = 3.0114 | 3018.42ms | Tokens/s = 173,696.2
2025-01-16 11:15:43.320 | DEBUG    | __main__:<module>:313 - Training step 32710: loss = 2.9285 | 3017.81ms | Tokens/s = 173,731.1
2025-01-16 11:16:13.490 | DEBUG    | __main__:<module>:313 - Training step 32720: loss = 3.0948 | 3016.87ms | Tokens/s = 173,785.5
2025-01-16 11:16:43.661 | DEBUG    | __main__:<module>:313 - Training step 32730: loss = 3.2189 | 3015.95ms | Tokens/s = 173,838.6
2025-01-16 11:17:13.811 | DEBUG    | __main__:<module>:313 - Training step 32740: loss = 3.0210 | 3014.39ms | Tokens/s = 173,928.1
2025-01-16 11:17:43.962 | DEBUG    | __main__:<module>:313 - Training step 32750: loss = 3.0343 | 3016.57ms | Tokens/s = 173,802.5
2025-01-16 11:18:14.124 | DEBUG    | __main__:<module>:313 - Training step 32760: loss = 2.8660 | 3017.39ms | Tokens/s = 173,755.7
2025-01-16 11:18:44.315 | DEBUG    | __main__:<module>:313 - Training step 32770: loss = 3.1465 | 3019.16ms | Tokens/s = 173,653.8
2025-01-16 11:19:14.495 | DEBUG    | __main__:<module>:313 - Training step 32780: loss = 3.0783 | 3017.31ms | Tokens/s = 173,760.1
2025-01-16 11:19:44.658 | DEBUG    | __main__:<module>:313 - Training step 32790: loss = 3.0455 | 3016.72ms | Tokens/s = 173,794.2
2025-01-16 11:20:14.820 | DEBUG    | __main__:<module>:313 - Training step 32800: loss = 2.9340 | 3016.68ms | Tokens/s = 173,796.1
2025-01-16 11:20:44.976 | DEBUG    | __main__:<module>:313 - Training step 32810: loss = 3.1486 | 3013.88ms | Tokens/s = 173,958.0
2025-01-16 11:21:15.159 | DEBUG    | __main__:<module>:313 - Training step 32820: loss = 3.0475 | 3017.56ms | Tokens/s = 173,745.7
2025-01-16 11:21:45.327 | DEBUG    | __main__:<module>:313 - Training step 32830: loss = 2.9702 | 3014.62ms | Tokens/s = 173,915.2
2025-01-16 11:22:15.493 | DEBUG    | __main__:<module>:313 - Training step 32840: loss = 2.9748 | 3017.34ms | Tokens/s = 173,758.3
2025-01-16 11:22:45.677 | DEBUG    | __main__:<module>:313 - Training step 32850: loss = 3.0531 | 3018.60ms | Tokens/s = 173,685.8
2025-01-16 11:23:15.880 | DEBUG    | __main__:<module>:313 - Training step 32860: loss = 2.9251 | 3019.08ms | Tokens/s = 173,658.4
2025-01-16 11:23:46.057 | DEBUG    | __main__:<module>:313 - Training step 32870: loss = 3.0949 | 3015.96ms | Tokens/s = 173,837.7
2025-01-16 11:24:16.223 | DEBUG    | __main__:<module>:313 - Training step 32880: loss = 3.0451 | 3015.02ms | Tokens/s = 173,891.9
2025-01-16 11:24:46.388 | DEBUG    | __main__:<module>:313 - Training step 32890: loss = 3.1124 | 3018.56ms | Tokens/s = 173,688.0
2025-01-16 11:25:16.544 | DEBUG    | __main__:<module>:313 - Training step 32900: loss = 2.8648 | 3013.88ms | Tokens/s = 173,958.0
2025-01-16 11:25:46.736 | DEBUG    | __main__:<module>:313 - Training step 32910: loss = 3.1081 | 3019.68ms | Tokens/s = 173,623.7
2025-01-16 11:26:16.928 | DEBUG    | __main__:<module>:313 - Training step 32920: loss = 3.1129 | 3016.32ms | Tokens/s = 173,817.0
2025-01-16 11:26:47.108 | DEBUG    | __main__:<module>:313 - Training step 32930: loss = 3.0061 | 3017.92ms | Tokens/s = 173,725.2
2025-01-16 11:27:17.270 | DEBUG    | __main__:<module>:313 - Training step 32940: loss = 3.0775 | 3017.56ms | Tokens/s = 173,745.8
2025-01-16 11:27:47.428 | DEBUG    | __main__:<module>:313 - Training step 32950: loss = 3.0210 | 3017.29ms | Tokens/s = 173,761.0
2025-01-16 11:28:17.584 | DEBUG    | __main__:<module>:313 - Training step 32960: loss = 3.1220 | 3013.37ms | Tokens/s = 173,987.1
2025-01-16 11:28:47.727 | DEBUG    | __main__:<module>:313 - Training step 32970: loss = 3.0940 | 3012.98ms | Tokens/s = 174,009.6
2025-01-16 11:29:17.873 | DEBUG    | __main__:<module>:313 - Training step 32980: loss = 2.9600 | 3013.81ms | Tokens/s = 173,961.7
2025-01-16 11:29:48.020 | DEBUG    | __main__:<module>:313 - Training step 32990: loss = 2.9073 | 3015.64ms | Tokens/s = 173,856.5
2025-01-16 11:30:21.639 | INFO     | __main__:<module>:265 - Step 33,000/40,000 loss: 3.0121 (T) 3.0253 (V) | lr=8.1e-04
2025-01-16 11:30:21.640 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 11:30:34.815 | DEBUG    | __main__:<module>:313 - Training step 33000: loss = 2.9450 | 19629.64ms | Tokens/s = 26,709.0
2025-01-16 11:31:04.858 | DEBUG    | __main__:<module>:313 - Training step 33010: loss = 2.8189 | 3012.32ms | Tokens/s = 174,048.0
2025-01-16 11:31:35.011 | DEBUG    | __main__:<module>:313 - Training step 33020: loss = 3.1268 | 3019.26ms | Tokens/s = 173,647.8
2025-01-16 11:32:05.198 | DEBUG    | __main__:<module>:313 - Training step 33030: loss = 3.0028 | 3020.73ms | Tokens/s = 173,563.3
2025-01-16 11:32:35.390 | DEBUG    | __main__:<module>:313 - Training step 33040: loss = 2.7438 | 3017.46ms | Tokens/s = 173,751.7
2025-01-16 11:33:05.569 | DEBUG    | __main__:<module>:313 - Training step 33050: loss = 3.0087 | 3019.05ms | Tokens/s = 173,659.9
2025-01-16 11:33:35.762 | DEBUG    | __main__:<module>:313 - Training step 33060: loss = 3.2646 | 3016.33ms | Tokens/s = 173,816.8
2025-01-16 11:34:05.929 | DEBUG    | __main__:<module>:313 - Training step 33070: loss = 3.1073 | 3015.04ms | Tokens/s = 173,890.9
2025-01-16 11:34:36.130 | DEBUG    | __main__:<module>:313 - Training step 33080: loss = 3.0823 | 3020.59ms | Tokens/s = 173,571.6
2025-01-16 11:35:06.321 | DEBUG    | __main__:<module>:313 - Training step 33090: loss = 3.0937 | 3021.40ms | Tokens/s = 173,524.7
2025-01-16 11:35:36.497 | DEBUG    | __main__:<module>:313 - Training step 33100: loss = 3.1191 | 3016.56ms | Tokens/s = 173,803.0
2025-01-16 11:36:06.661 | DEBUG    | __main__:<module>:313 - Training step 33110: loss = 3.0010 | 3018.45ms | Tokens/s = 173,694.2
2025-01-16 11:36:36.815 | DEBUG    | __main__:<module>:313 - Training step 33120: loss = 3.1124 | 3016.46ms | Tokens/s = 173,809.1
2025-01-16 11:37:06.969 | DEBUG    | __main__:<module>:313 - Training step 33130: loss = 3.0294 | 3014.85ms | Tokens/s = 173,902.1
2025-01-16 11:37:37.117 | DEBUG    | __main__:<module>:313 - Training step 33140: loss = 2.8644 | 3013.61ms | Tokens/s = 173,973.6
2025-01-16 11:38:07.281 | DEBUG    | __main__:<module>:313 - Training step 33150: loss = 3.0653 | 3018.03ms | Tokens/s = 173,718.7
2025-01-16 11:38:37.471 | DEBUG    | __main__:<module>:313 - Training step 33160: loss = 2.8269 | 3017.77ms | Tokens/s = 173,733.5
2025-01-16 11:39:07.652 | DEBUG    | __main__:<module>:313 - Training step 33170: loss = 3.0090 | 3017.98ms | Tokens/s = 173,721.3
2025-01-16 11:39:37.811 | DEBUG    | __main__:<module>:313 - Training step 33180: loss = 2.9864 | 3014.65ms | Tokens/s = 173,913.3
2025-01-16 11:40:07.973 | DEBUG    | __main__:<module>:313 - Training step 33190: loss = 2.8414 | 3015.71ms | Tokens/s = 173,852.2
2025-01-16 11:40:38.119 | DEBUG    | __main__:<module>:313 - Training step 33200: loss = 3.1363 | 3012.85ms | Tokens/s = 174,017.3
2025-01-16 11:41:08.276 | DEBUG    | __main__:<module>:313 - Training step 33210: loss = 3.0449 | 3016.63ms | Tokens/s = 173,799.1
2025-01-16 11:41:38.464 | DEBUG    | __main__:<module>:313 - Training step 33220: loss = 2.9240 | 3018.57ms | Tokens/s = 173,687.6
2025-01-16 11:42:08.647 | DEBUG    | __main__:<module>:313 - Training step 33230: loss = 3.0353 | 3018.03ms | Tokens/s = 173,718.7
2025-01-16 11:42:38.819 | DEBUG    | __main__:<module>:313 - Training step 33240: loss = 3.1220 | 3014.39ms | Tokens/s = 173,928.2
2025-01-16 11:43:08.978 | DEBUG    | __main__:<module>:313 - Training step 33250: loss = 3.0471 | 3015.79ms | Tokens/s = 173,847.8
2025-01-16 11:43:39.129 | DEBUG    | __main__:<module>:313 - Training step 33260: loss = 2.9627 | 3014.25ms | Tokens/s = 173,936.2
2025-01-16 11:44:09.288 | DEBUG    | __main__:<module>:313 - Training step 33270: loss = 3.0678 | 3019.07ms | Tokens/s = 173,658.6
2025-01-16 11:44:39.452 | DEBUG    | __main__:<module>:313 - Training step 33280: loss = 2.9389 | 3018.19ms | Tokens/s = 173,709.2
2025-01-16 11:45:09.652 | DEBUG    | __main__:<module>:313 - Training step 33290: loss = 3.0069 | 3022.95ms | Tokens/s = 173,435.7
2025-01-16 11:45:39.862 | DEBUG    | __main__:<module>:313 - Training step 33300: loss = 3.0017 | 3019.37ms | Tokens/s = 173,641.2
2025-01-16 11:46:10.044 | DEBUG    | __main__:<module>:313 - Training step 33310: loss = 3.0656 | 3019.73ms | Tokens/s = 173,620.7
2025-01-16 11:46:40.238 | DEBUG    | __main__:<module>:313 - Training step 33320: loss = 2.8643 | 3018.01ms | Tokens/s = 173,720.0
2025-01-16 11:47:10.424 | DEBUG    | __main__:<module>:313 - Training step 33330: loss = 2.9293 | 3019.68ms | Tokens/s = 173,623.9
2025-01-16 11:47:40.607 | DEBUG    | __main__:<module>:313 - Training step 33340: loss = 2.8463 | 3020.10ms | Tokens/s = 173,599.8
2025-01-16 11:48:10.780 | DEBUG    | __main__:<module>:313 - Training step 33350: loss = 2.9425 | 3016.77ms | Tokens/s = 173,790.9
2025-01-16 11:48:40.963 | DEBUG    | __main__:<module>:313 - Training step 33360: loss = 3.1653 | 3017.99ms | Tokens/s = 173,720.8
2025-01-16 11:49:11.152 | DEBUG    | __main__:<module>:313 - Training step 33370: loss = 2.7579 | 3019.14ms | Tokens/s = 173,654.7
2025-01-16 11:49:41.346 | DEBUG    | __main__:<module>:313 - Training step 33380: loss = 3.0133 | 3018.31ms | Tokens/s = 173,702.5
2025-01-16 11:50:11.525 | DEBUG    | __main__:<module>:313 - Training step 33390: loss = 3.0386 | 3018.69ms | Tokens/s = 173,680.6
2025-01-16 11:50:41.693 | DEBUG    | __main__:<module>:313 - Training step 33400: loss = 3.0950 | 3016.91ms | Tokens/s = 173,783.1
2025-01-16 11:51:11.869 | DEBUG    | __main__:<module>:313 - Training step 33410: loss = 3.1826 | 3019.06ms | Tokens/s = 173,659.6
2025-01-16 11:51:42.047 | DEBUG    | __main__:<module>:313 - Training step 33420: loss = 3.0255 | 3018.52ms | Tokens/s = 173,690.1
2025-01-16 11:52:12.232 | DEBUG    | __main__:<module>:313 - Training step 33430: loss = 3.1872 | 3019.15ms | Tokens/s = 173,654.1
2025-01-16 11:52:42.424 | DEBUG    | __main__:<module>:313 - Training step 33440: loss = 2.9936 | 3018.77ms | Tokens/s = 173,676.1
2025-01-16 11:53:12.605 | DEBUG    | __main__:<module>:313 - Training step 33450: loss = 2.9824 | 3016.96ms | Tokens/s = 173,780.0
2025-01-16 11:53:42.796 | DEBUG    | __main__:<module>:313 - Training step 33460: loss = 2.9746 | 3018.60ms | Tokens/s = 173,686.1
2025-01-16 11:54:12.989 | DEBUG    | __main__:<module>:313 - Training step 33470: loss = 2.8684 | 3021.09ms | Tokens/s = 173,542.4
2025-01-16 11:54:43.164 | DEBUG    | __main__:<module>:313 - Training step 33480: loss = 3.0921 | 3016.25ms | Tokens/s = 173,820.9
2025-01-16 11:55:13.332 | DEBUG    | __main__:<module>:313 - Training step 33490: loss = 3.0189 | 3015.26ms | Tokens/s = 173,878.1
2025-01-16 11:55:43.496 | DEBUG    | __main__:<module>:313 - Training step 33500: loss = 2.9927 | 3014.94ms | Tokens/s = 173,896.4
2025-01-16 11:56:13.666 | DEBUG    | __main__:<module>:313 - Training step 33510: loss = 2.9219 | 3020.05ms | Tokens/s = 173,602.7
2025-01-16 11:56:43.838 | DEBUG    | __main__:<module>:313 - Training step 33520: loss = 2.8251 | 3021.28ms | Tokens/s = 173,532.0
2025-01-16 11:57:14.002 | DEBUG    | __main__:<module>:313 - Training step 33530: loss = 2.9538 | 3017.33ms | Tokens/s = 173,759.0
2025-01-16 11:57:44.195 | DEBUG    | __main__:<module>:313 - Training step 33540: loss = 3.0890 | 3016.87ms | Tokens/s = 173,785.2
2025-01-16 11:58:14.379 | DEBUG    | __main__:<module>:313 - Training step 33550: loss = 2.9438 | 3018.02ms | Tokens/s = 173,719.1
2025-01-16 11:58:44.551 | DEBUG    | __main__:<module>:313 - Training step 33560: loss = 3.0136 | 3017.64ms | Tokens/s = 173,741.2
2025-01-16 11:59:14.740 | DEBUG    | __main__:<module>:313 - Training step 33570: loss = 3.0187 | 3021.06ms | Tokens/s = 173,544.4
2025-01-16 11:59:44.931 | DEBUG    | __main__:<module>:313 - Training step 33580: loss = 3.1874 | 3020.44ms | Tokens/s = 173,580.1
2025-01-16 12:00:15.108 | DEBUG    | __main__:<module>:313 - Training step 33590: loss = 2.9888 | 3017.56ms | Tokens/s = 173,745.4
2025-01-16 12:00:45.285 | DEBUG    | __main__:<module>:313 - Training step 33600: loss = 3.0364 | 3015.34ms | Tokens/s = 173,873.4
2025-01-16 12:01:15.464 | DEBUG    | __main__:<module>:313 - Training step 33610: loss = 2.9257 | 3019.29ms | Tokens/s = 173,646.4
2025-01-16 12:01:45.663 | DEBUG    | __main__:<module>:313 - Training step 33620: loss = 2.9505 | 3020.18ms | Tokens/s = 173,594.8
2025-01-16 12:02:15.848 | DEBUG    | __main__:<module>:313 - Training step 33630: loss = 3.0415 | 3014.96ms | Tokens/s = 173,895.8
2025-01-16 12:02:46.023 | DEBUG    | __main__:<module>:313 - Training step 33640: loss = 3.0205 | 3014.59ms | Tokens/s = 173,916.6
2025-01-16 12:03:16.188 | DEBUG    | __main__:<module>:313 - Training step 33650: loss = 2.8723 | 3015.52ms | Tokens/s = 173,863.4
2025-01-16 12:03:46.350 | DEBUG    | __main__:<module>:313 - Training step 33660: loss = 2.9993 | 3017.53ms | Tokens/s = 173,747.2
2025-01-16 12:04:16.504 | DEBUG    | __main__:<module>:313 - Training step 33670: loss = 3.1312 | 3013.22ms | Tokens/s = 173,996.1
2025-01-16 12:04:46.668 | DEBUG    | __main__:<module>:313 - Training step 33680: loss = 2.9436 | 3016.15ms | Tokens/s = 173,826.9
2025-01-16 12:05:16.830 | DEBUG    | __main__:<module>:313 - Training step 33690: loss = 3.0310 | 3016.69ms | Tokens/s = 173,796.1
2025-01-16 12:05:47.021 | DEBUG    | __main__:<module>:313 - Training step 33700: loss = 2.9025 | 3018.29ms | Tokens/s = 173,703.6
2025-01-16 12:06:17.221 | DEBUG    | __main__:<module>:313 - Training step 33710: loss = 3.0078 | 3020.98ms | Tokens/s = 173,548.9
2025-01-16 12:06:47.400 | DEBUG    | __main__:<module>:313 - Training step 33720: loss = 3.0751 | 3019.33ms | Tokens/s = 173,643.6
2025-01-16 12:07:17.579 | DEBUG    | __main__:<module>:313 - Training step 33730: loss = 3.0847 | 3019.12ms | Tokens/s = 173,655.9
2025-01-16 12:07:47.756 | DEBUG    | __main__:<module>:313 - Training step 33740: loss = 3.1445 | 3015.97ms | Tokens/s = 173,837.0
2025-01-16 12:08:17.971 | DEBUG    | __main__:<module>:313 - Training step 33750: loss = 3.0279 | 3020.59ms | Tokens/s = 173,571.6
2025-01-16 12:08:48.169 | DEBUG    | __main__:<module>:313 - Training step 33760: loss = 3.0886 | 3019.45ms | Tokens/s = 173,636.9
2025-01-16 12:09:18.356 | DEBUG    | __main__:<module>:313 - Training step 33770: loss = 2.8659 | 3016.48ms | Tokens/s = 173,807.7
2025-01-16 12:09:48.532 | DEBUG    | __main__:<module>:313 - Training step 33780: loss = 3.0000 | 3016.77ms | Tokens/s = 173,791.0
2025-01-16 12:10:18.705 | DEBUG    | __main__:<module>:313 - Training step 33790: loss = 3.0188 | 3015.49ms | Tokens/s = 173,864.9
2025-01-16 12:10:48.871 | DEBUG    | __main__:<module>:313 - Training step 33800: loss = 3.0487 | 3014.71ms | Tokens/s = 173,910.2
2025-01-16 12:11:19.031 | DEBUG    | __main__:<module>:313 - Training step 33810: loss = 2.9966 | 3015.65ms | Tokens/s = 173,855.7
2025-01-16 12:11:49.200 | DEBUG    | __main__:<module>:313 - Training step 33820: loss = 3.0921 | 3014.13ms | Tokens/s = 173,943.3
2025-01-16 12:12:19.362 | DEBUG    | __main__:<module>:313 - Training step 33830: loss = 3.0226 | 3019.55ms | Tokens/s = 173,631.3
2025-01-16 12:12:49.525 | DEBUG    | __main__:<module>:313 - Training step 33840: loss = 2.9435 | 3013.53ms | Tokens/s = 173,977.9
2025-01-16 12:13:19.682 | DEBUG    | __main__:<module>:313 - Training step 33850: loss = 3.1869 | 3017.93ms | Tokens/s = 173,724.3
2025-01-16 12:13:49.877 | DEBUG    | __main__:<module>:313 - Training step 33860: loss = 3.0633 | 3021.66ms | Tokens/s = 173,510.0
2025-01-16 12:14:20.095 | DEBUG    | __main__:<module>:313 - Training step 33870: loss = 2.8002 | 3019.85ms | Tokens/s = 173,614.1
2025-01-16 12:14:50.292 | DEBUG    | __main__:<module>:313 - Training step 33880: loss = 2.9498 | 3018.99ms | Tokens/s = 173,663.6
2025-01-16 12:15:20.476 | DEBUG    | __main__:<module>:313 - Training step 33890: loss = 2.9801 | 3015.47ms | Tokens/s = 173,866.3
2025-01-16 12:15:50.644 | DEBUG    | __main__:<module>:313 - Training step 33900: loss = 2.9654 | 3013.92ms | Tokens/s = 173,955.4
2025-01-16 12:16:20.806 | DEBUG    | __main__:<module>:313 - Training step 33910: loss = 2.8799 | 3015.70ms | Tokens/s = 173,852.9
2025-01-16 12:16:50.991 | DEBUG    | __main__:<module>:313 - Training step 33920: loss = 3.0203 | 3020.06ms | Tokens/s = 173,602.1
2025-01-16 12:17:21.194 | DEBUG    | __main__:<module>:313 - Training step 33930: loss = 2.9406 | 3018.94ms | Tokens/s = 173,666.2
2025-01-16 12:17:51.361 | DEBUG    | __main__:<module>:313 - Training step 33940: loss = 2.8537 | 3015.93ms | Tokens/s = 173,839.5
2025-01-16 12:18:21.500 | DEBUG    | __main__:<module>:313 - Training step 33950: loss = 3.0694 | 3016.41ms | Tokens/s = 173,812.1
2025-01-16 12:18:51.646 | DEBUG    | __main__:<module>:313 - Training step 33960: loss = 2.8748 | 3015.32ms | Tokens/s = 173,874.8
2025-01-16 12:19:21.791 | DEBUG    | __main__:<module>:313 - Training step 33970: loss = 2.9296 | 3015.15ms | Tokens/s = 173,884.7
2025-01-16 12:19:51.960 | DEBUG    | __main__:<module>:313 - Training step 33980: loss = 2.9316 | 3014.87ms | Tokens/s = 173,900.7
2025-01-16 12:20:22.124 | DEBUG    | __main__:<module>:313 - Training step 33990: loss = 2.9328 | 3015.07ms | Tokens/s = 173,889.4
2025-01-16 12:20:55.722 | INFO     | __main__:<module>:265 - Step 34,000/40,000 loss: 2.9917 (T) 3.0150 (V) | lr=6.0e-04
2025-01-16 12:20:55.723 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 12:21:09.275 | DEBUG    | __main__:<module>:313 - Training step 34000: loss = 2.9312 | 19998.46ms | Tokens/s = 26,216.4
2025-01-16 12:21:39.298 | DEBUG    | __main__:<module>:313 - Training step 34010: loss = 3.0014 | 3008.43ms | Tokens/s = 174,272.9
2025-01-16 12:22:09.433 | DEBUG    | __main__:<module>:313 - Training step 34020: loss = 3.1067 | 3015.10ms | Tokens/s = 173,887.2
2025-01-16 12:22:39.618 | DEBUG    | __main__:<module>:313 - Training step 34030: loss = 2.9424 | 3020.23ms | Tokens/s = 173,591.9
2025-01-16 12:23:09.834 | DEBUG    | __main__:<module>:313 - Training step 34040: loss = 3.0086 | 3021.78ms | Tokens/s = 173,503.1
2025-01-16 12:23:40.030 | DEBUG    | __main__:<module>:313 - Training step 34050: loss = 3.0262 | 3018.55ms | Tokens/s = 173,688.9
2025-01-16 12:24:10.214 | DEBUG    | __main__:<module>:313 - Training step 34060: loss = 3.0363 | 3016.24ms | Tokens/s = 173,822.0
2025-01-16 12:24:40.383 | DEBUG    | __main__:<module>:313 - Training step 34070: loss = 2.9976 | 3017.05ms | Tokens/s = 173,775.2
2025-01-16 12:25:10.564 | DEBUG    | __main__:<module>:313 - Training step 34080: loss = 3.0314 | 3021.84ms | Tokens/s = 173,499.3
2025-01-16 12:25:40.767 | DEBUG    | __main__:<module>:313 - Training step 34090: loss = 3.0232 | 3019.50ms | Tokens/s = 173,634.1
2025-01-16 12:26:10.973 | DEBUG    | __main__:<module>:313 - Training step 34100: loss = 2.9721 | 3016.36ms | Tokens/s = 173,814.9
2025-01-16 12:26:41.154 | DEBUG    | __main__:<module>:313 - Training step 34110: loss = 3.0618 | 3020.12ms | Tokens/s = 173,598.5
2025-01-16 12:27:11.334 | DEBUG    | __main__:<module>:313 - Training step 34120: loss = 2.9789 | 3018.14ms | Tokens/s = 173,712.2
2025-01-16 12:27:41.503 | DEBUG    | __main__:<module>:313 - Training step 34130: loss = 2.9064 | 3016.90ms | Tokens/s = 173,783.6
2025-01-16 12:28:11.666 | DEBUG    | __main__:<module>:313 - Training step 34140: loss = 3.0011 | 3015.52ms | Tokens/s = 173,863.1
2025-01-16 12:28:41.830 | DEBUG    | __main__:<module>:313 - Training step 34150: loss = 2.8399 | 3017.97ms | Tokens/s = 173,722.1
2025-01-16 12:29:11.987 | DEBUG    | __main__:<module>:313 - Training step 34160: loss = 2.9881 | 3018.73ms | Tokens/s = 173,678.4
2025-01-16 12:29:42.156 | DEBUG    | __main__:<module>:313 - Training step 34170: loss = 3.0908 | 3019.79ms | Tokens/s = 173,617.1
2025-01-16 12:30:12.357 | DEBUG    | __main__:<module>:313 - Training step 34180: loss = 3.0284 | 3019.53ms | Tokens/s = 173,632.5
2025-01-16 12:30:42.546 | DEBUG    | __main__:<module>:313 - Training step 34190: loss = 3.0533 | 3017.85ms | Tokens/s = 173,728.9
2025-01-16 12:31:12.730 | DEBUG    | __main__:<module>:313 - Training step 34200: loss = 2.8577 | 3015.33ms | Tokens/s = 173,873.9
2025-01-16 12:31:42.908 | DEBUG    | __main__:<module>:313 - Training step 34210: loss = 3.0670 | 3016.37ms | Tokens/s = 173,814.1
2025-01-16 12:32:13.084 | DEBUG    | __main__:<module>:313 - Training step 34220: loss = 2.8922 | 3018.08ms | Tokens/s = 173,715.8
2025-01-16 12:32:43.287 | DEBUG    | __main__:<module>:313 - Training step 34230: loss = 2.9237 | 3020.46ms | Tokens/s = 173,578.9
2025-01-16 12:33:13.491 | DEBUG    | __main__:<module>:313 - Training step 34240: loss = 3.0338 | 3019.07ms | Tokens/s = 173,658.9
2025-01-16 12:33:43.686 | DEBUG    | __main__:<module>:313 - Training step 34250: loss = 2.9183 | 3016.38ms | Tokens/s = 173,813.5
2025-01-16 12:34:13.871 | DEBUG    | __main__:<module>:313 - Training step 34260: loss = 3.0228 | 3017.84ms | Tokens/s = 173,729.7
2025-01-16 12:34:44.073 | DEBUG    | __main__:<module>:313 - Training step 34270: loss = 3.0555 | 3021.88ms | Tokens/s = 173,497.2
2025-01-16 12:35:14.266 | DEBUG    | __main__:<module>:313 - Training step 34280: loss = 3.0819 | 3020.01ms | Tokens/s = 173,604.5
2025-01-16 12:35:44.437 | DEBUG    | __main__:<module>:313 - Training step 34290: loss = 3.0031 | 3017.57ms | Tokens/s = 173,744.9
2025-01-16 12:36:14.628 | DEBUG    | __main__:<module>:313 - Training step 34300: loss = 3.0048 | 3020.47ms | Tokens/s = 173,578.2
2025-01-16 12:36:44.796 | DEBUG    | __main__:<module>:313 - Training step 34310: loss = 3.0673 | 3014.09ms | Tokens/s = 173,945.5
2025-01-16 12:37:14.910 | DEBUG    | __main__:<module>:313 - Training step 34320: loss = 2.8595 | 3012.32ms | Tokens/s = 174,047.7
2025-01-16 12:37:45.049 | DEBUG    | __main__:<module>:313 - Training step 34330: loss = 3.1199 | 3014.92ms | Tokens/s = 173,897.7
2025-01-16 12:38:15.202 | DEBUG    | __main__:<module>:313 - Training step 34340: loss = 2.8971 | 3017.39ms | Tokens/s = 173,755.5
2025-01-16 12:38:45.363 | DEBUG    | __main__:<module>:313 - Training step 34350: loss = 2.9702 | 3013.95ms | Tokens/s = 173,954.0
2025-01-16 12:39:15.506 | DEBUG    | __main__:<module>:313 - Training step 34360: loss = 3.1194 | 3015.95ms | Tokens/s = 173,838.4
2025-01-16 12:39:45.676 | DEBUG    | __main__:<module>:313 - Training step 34370: loss = 3.2141 | 3018.03ms | Tokens/s = 173,718.4
2025-01-16 12:40:15.850 | DEBUG    | __main__:<module>:313 - Training step 34380: loss = 2.9779 | 3016.46ms | Tokens/s = 173,809.3
2025-01-16 12:40:46.028 | DEBUG    | __main__:<module>:313 - Training step 34390: loss = 3.0035 | 3018.94ms | Tokens/s = 173,666.3
2025-01-16 12:41:16.206 | DEBUG    | __main__:<module>:313 - Training step 34400: loss = 2.8659 | 3017.16ms | Tokens/s = 173,768.6
2025-01-16 12:41:46.388 | DEBUG    | __main__:<module>:313 - Training step 34410: loss = 3.0427 | 3019.74ms | Tokens/s = 173,620.0
2025-01-16 12:42:16.570 | DEBUG    | __main__:<module>:313 - Training step 34420: loss = 3.0716 | 3017.99ms | Tokens/s = 173,721.0
2025-01-16 12:42:46.712 | DEBUG    | __main__:<module>:313 - Training step 34430: loss = 3.0075 | 3013.95ms | Tokens/s = 173,953.9
2025-01-16 12:43:16.861 | DEBUG    | __main__:<module>:313 - Training step 34440: loss = 2.9855 | 3014.17ms | Tokens/s = 173,941.0
2025-01-16 12:43:47.011 | DEBUG    | __main__:<module>:313 - Training step 34450: loss = 3.0760 | 3015.55ms | Tokens/s = 173,861.7
2025-01-16 12:44:17.175 | DEBUG    | __main__:<module>:313 - Training step 34460: loss = 2.9364 | 3016.36ms | Tokens/s = 173,815.0
2025-01-16 12:44:47.349 | DEBUG    | __main__:<module>:313 - Training step 34470: loss = 2.9011 | 3019.17ms | Tokens/s = 173,653.0
2025-01-16 12:45:17.541 | DEBUG    | __main__:<module>:313 - Training step 34480: loss = 2.7888 | 3016.98ms | Tokens/s = 173,779.3
2025-01-16 12:45:47.726 | DEBUG    | __main__:<module>:313 - Training step 34490: loss = 3.0481 | 3018.36ms | Tokens/s = 173,699.5
2025-01-16 12:46:17.913 | DEBUG    | __main__:<module>:313 - Training step 34500: loss = 3.0077 | 3019.76ms | Tokens/s = 173,619.2
2025-01-16 12:46:48.123 | DEBUG    | __main__:<module>:313 - Training step 34510: loss = 2.8675 | 3020.08ms | Tokens/s = 173,600.7
2025-01-16 12:47:18.319 | DEBUG    | __main__:<module>:313 - Training step 34520: loss = 3.1114 | 3019.11ms | Tokens/s = 173,656.5
2025-01-16 12:47:48.496 | DEBUG    | __main__:<module>:313 - Training step 34530: loss = 2.8917 | 3016.64ms | Tokens/s = 173,798.6
2025-01-16 12:48:18.657 | DEBUG    | __main__:<module>:313 - Training step 34540: loss = 2.9601 | 3016.98ms | Tokens/s = 173,779.0
2025-01-16 12:48:48.817 | DEBUG    | __main__:<module>:313 - Training step 34550: loss = 2.8353 | 3015.36ms | Tokens/s = 173,872.6
2025-01-16 12:49:18.966 | DEBUG    | __main__:<module>:313 - Training step 34560: loss = 3.0791 | 3012.54ms | Tokens/s = 174,034.9
2025-01-16 12:49:49.118 | DEBUG    | __main__:<module>:313 - Training step 34570: loss = 2.8370 | 3013.49ms | Tokens/s = 173,980.2
2025-01-16 12:50:19.292 | DEBUG    | __main__:<module>:313 - Training step 34580: loss = 2.9808 | 3019.56ms | Tokens/s = 173,630.9
2025-01-16 12:50:49.498 | DEBUG    | __main__:<module>:313 - Training step 34590: loss = 2.9836 | 3020.57ms | Tokens/s = 173,572.3
2025-01-16 12:51:19.690 | DEBUG    | __main__:<module>:313 - Training step 34600: loss = 3.0600 | 3019.34ms | Tokens/s = 173,643.3
2025-01-16 12:51:49.858 | DEBUG    | __main__:<module>:313 - Training step 34610: loss = 3.0285 | 3013.56ms | Tokens/s = 173,976.2
2025-01-16 12:52:20.026 | DEBUG    | __main__:<module>:313 - Training step 34620: loss = 2.8347 | 3016.79ms | Tokens/s = 173,789.8
2025-01-16 12:52:50.230 | DEBUG    | __main__:<module>:313 - Training step 34630: loss = 3.0829 | 3022.79ms | Tokens/s = 173,444.8
2025-01-16 12:53:20.431 | DEBUG    | __main__:<module>:313 - Training step 34640: loss = 2.9804 | 3018.98ms | Tokens/s = 173,663.9
2025-01-16 12:53:50.613 | DEBUG    | __main__:<module>:313 - Training step 34650: loss = 3.0489 | 3019.43ms | Tokens/s = 173,638.1
2025-01-16 12:54:20.825 | DEBUG    | __main__:<module>:313 - Training step 34660: loss = 2.9881 | 3020.31ms | Tokens/s = 173,587.3
2025-01-16 12:54:51.026 | DEBUG    | __main__:<module>:313 - Training step 34670: loss = 2.9310 | 3017.81ms | Tokens/s = 173,731.3
2025-01-16 12:55:21.220 | DEBUG    | __main__:<module>:313 - Training step 34680: loss = 2.8550 | 3018.42ms | Tokens/s = 173,696.0
2025-01-16 12:55:51.401 | DEBUG    | __main__:<module>:313 - Training step 34690: loss = 3.0625 | 3015.69ms | Tokens/s = 173,853.6
2025-01-16 12:56:21.581 | DEBUG    | __main__:<module>:313 - Training step 34700: loss = 3.0607 | 3018.95ms | Tokens/s = 173,665.8
2025-01-16 12:56:51.748 | DEBUG    | __main__:<module>:313 - Training step 34710: loss = 2.9705 | 3015.71ms | Tokens/s = 173,852.2
2025-01-16 12:57:21.906 | DEBUG    | __main__:<module>:313 - Training step 34720: loss = 3.0672 | 3014.61ms | Tokens/s = 173,915.8
2025-01-16 12:57:52.048 | DEBUG    | __main__:<module>:313 - Training step 34730: loss = 3.1422 | 3011.24ms | Tokens/s = 174,110.6
2025-01-16 12:58:22.184 | DEBUG    | __main__:<module>:313 - Training step 34740: loss = 2.9849 | 3014.07ms | Tokens/s = 173,947.1
2025-01-16 12:58:52.344 | DEBUG    | __main__:<module>:313 - Training step 34750: loss = 3.1456 | 3016.97ms | Tokens/s = 173,779.8
2025-01-16 12:59:22.497 | DEBUG    | __main__:<module>:313 - Training step 34760: loss = 3.0262 | 3017.99ms | Tokens/s = 173,720.8
2025-01-16 12:59:52.659 | DEBUG    | __main__:<module>:313 - Training step 34770: loss = 3.0121 | 3017.15ms | Tokens/s = 173,769.2
2025-01-16 13:00:22.860 | DEBUG    | __main__:<module>:313 - Training step 34780: loss = 3.0066 | 3022.56ms | Tokens/s = 173,458.3
2025-01-16 13:00:53.069 | DEBUG    | __main__:<module>:313 - Training step 34790: loss = 2.9008 | 3018.61ms | Tokens/s = 173,685.2
2025-01-16 13:01:23.255 | DEBUG    | __main__:<module>:313 - Training step 34800: loss = 3.0035 | 3017.92ms | Tokens/s = 173,725.2
2025-01-16 13:01:53.428 | DEBUG    | __main__:<module>:313 - Training step 34810: loss = 2.9955 | 3015.96ms | Tokens/s = 173,838.0
2025-01-16 13:02:23.593 | DEBUG    | __main__:<module>:313 - Training step 34820: loss = 2.9578 | 3015.12ms | Tokens/s = 173,886.0
2025-01-16 13:02:53.765 | DEBUG    | __main__:<module>:313 - Training step 34830: loss = 2.8788 | 3020.04ms | Tokens/s = 173,603.1
2025-01-16 13:03:23.930 | DEBUG    | __main__:<module>:313 - Training step 34840: loss = 3.0791 | 3016.95ms | Tokens/s = 173,780.6
2025-01-16 13:03:54.094 | DEBUG    | __main__:<module>:313 - Training step 34850: loss = 3.2164 | 3014.65ms | Tokens/s = 173,913.6
2025-01-16 13:04:24.262 | DEBUG    | __main__:<module>:313 - Training step 34860: loss = 2.9742 | 3015.57ms | Tokens/s = 173,860.5
2025-01-16 13:04:54.432 | DEBUG    | __main__:<module>:313 - Training step 34870: loss = 3.0462 | 3015.28ms | Tokens/s = 173,877.0
2025-01-16 13:05:24.606 | DEBUG    | __main__:<module>:313 - Training step 34880: loss = 2.9025 | 3016.14ms | Tokens/s = 173,827.4
2025-01-16 13:05:54.783 | DEBUG    | __main__:<module>:313 - Training step 34890: loss = 3.0491 | 3017.30ms | Tokens/s = 173,760.4
2025-01-16 13:06:24.963 | DEBUG    | __main__:<module>:313 - Training step 34900: loss = 3.0333 | 3016.41ms | Tokens/s = 173,812.2
2025-01-16 13:06:55.138 | DEBUG    | __main__:<module>:313 - Training step 34910: loss = 3.0414 | 3016.34ms | Tokens/s = 173,816.0
2025-01-16 13:07:25.314 | DEBUG    | __main__:<module>:313 - Training step 34920: loss = 2.9019 | 3016.66ms | Tokens/s = 173,797.3
2025-01-16 13:07:55.497 | DEBUG    | __main__:<module>:313 - Training step 34930: loss = 3.1718 | 3019.02ms | Tokens/s = 173,661.5
2025-01-16 13:08:25.669 | DEBUG    | __main__:<module>:313 - Training step 34940: loss = 3.0678 | 3016.37ms | Tokens/s = 173,814.4
2025-01-16 13:08:55.836 | DEBUG    | __main__:<module>:313 - Training step 34950: loss = 2.8788 | 3016.07ms | Tokens/s = 173,831.7
2025-01-16 13:09:25.984 | DEBUG    | __main__:<module>:313 - Training step 34960: loss = 3.0229 | 3016.40ms | Tokens/s = 173,812.4
2025-01-16 13:09:56.149 | DEBUG    | __main__:<module>:313 - Training step 34970: loss = 2.7933 | 3015.31ms | Tokens/s = 173,875.4
2025-01-16 13:10:26.302 | DEBUG    | __main__:<module>:313 - Training step 34980: loss = 2.8478 | 3016.71ms | Tokens/s = 173,794.9
2025-01-16 13:10:56.487 | DEBUG    | __main__:<module>:313 - Training step 34990: loss = 2.9600 | 3019.30ms | Tokens/s = 173,645.4
2025-01-16 13:11:30.134 | INFO     | __main__:<module>:265 - Step 35,000/40,000 loss: 2.9787 (T) 2.9716 (V) | lr=4.2e-04
2025-01-16 13:11:30.136 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 13:11:43.400 | DEBUG    | __main__:<module>:313 - Training step 35000: loss = 3.0305 | 19721.32ms | Tokens/s = 26,584.8
2025-01-16 13:12:13.452 | DEBUG    | __main__:<module>:313 - Training step 35010: loss = 3.0010 | 3012.37ms | Tokens/s = 174,045.2
2025-01-16 13:12:43.607 | DEBUG    | __main__:<module>:313 - Training step 35020: loss = 2.8990 | 3016.36ms | Tokens/s = 173,814.6
2025-01-16 13:13:13.810 | DEBUG    | __main__:<module>:313 - Training step 35030: loss = 2.9043 | 3020.56ms | Tokens/s = 173,573.3
2025-01-16 13:13:44.006 | DEBUG    | __main__:<module>:313 - Training step 35040: loss = 2.9759 | 3020.99ms | Tokens/s = 173,548.5
2025-01-16 13:14:14.182 | DEBUG    | __main__:<module>:313 - Training step 35050: loss = 3.0707 | 3018.25ms | Tokens/s = 173,705.8
2025-01-16 13:14:44.349 | DEBUG    | __main__:<module>:313 - Training step 35060: loss = 2.9761 | 3014.01ms | Tokens/s = 173,950.3
2025-01-16 13:15:14.521 | DEBUG    | __main__:<module>:313 - Training step 35070: loss = 3.0729 | 3016.69ms | Tokens/s = 173,795.8
2025-01-16 13:15:44.724 | DEBUG    | __main__:<module>:313 - Training step 35080: loss = 2.9072 | 3019.37ms | Tokens/s = 173,641.3
2025-01-16 13:16:14.911 | DEBUG    | __main__:<module>:313 - Training step 35090: loss = 2.8475 | 3017.58ms | Tokens/s = 173,744.3
2025-01-16 13:16:45.099 | DEBUG    | __main__:<module>:313 - Training step 35100: loss = 2.9915 | 3017.01ms | Tokens/s = 173,777.4
2025-01-16 13:17:15.274 | DEBUG    | __main__:<module>:313 - Training step 35110: loss = 2.8591 | 3016.66ms | Tokens/s = 173,797.7
2025-01-16 13:17:45.448 | DEBUG    | __main__:<module>:313 - Training step 35120: loss = 2.9010 | 3019.88ms | Tokens/s = 173,612.5
2025-01-16 13:18:15.625 | DEBUG    | __main__:<module>:313 - Training step 35130: loss = 2.8283 | 3020.57ms | Tokens/s = 173,572.4
2025-01-16 13:18:45.829 | DEBUG    | __main__:<module>:313 - Training step 35140: loss = 2.8299 | 3021.23ms | Tokens/s = 173,534.7
2025-01-16 13:19:16.042 | DEBUG    | __main__:<module>:313 - Training step 35150: loss = 3.0415 | 3020.90ms | Tokens/s = 173,553.5
2025-01-16 13:19:46.238 | DEBUG    | __main__:<module>:313 - Training step 35160: loss = 3.0936 | 3018.30ms | Tokens/s = 173,703.0
2025-01-16 13:20:16.422 | DEBUG    | __main__:<module>:313 - Training step 35170: loss = 2.9356 | 3018.61ms | Tokens/s = 173,685.0
2025-01-16 13:20:46.593 | DEBUG    | __main__:<module>:313 - Training step 35180: loss = 3.1256 | 3018.68ms | Tokens/s = 173,681.2
2025-01-16 13:21:16.759 | DEBUG    | __main__:<module>:313 - Training step 35190: loss = 3.0499 | 3015.85ms | Tokens/s = 173,844.4
2025-01-16 13:21:46.920 | DEBUG    | __main__:<module>:313 - Training step 35200: loss = 3.0321 | 3014.35ms | Tokens/s = 173,930.6
2025-01-16 13:22:17.076 | DEBUG    | __main__:<module>:313 - Training step 35210: loss = 2.9089 | 3018.30ms | Tokens/s = 173,703.2
2025-01-16 13:22:47.222 | DEBUG    | __main__:<module>:313 - Training step 35220: loss = 2.9104 | 3013.69ms | Tokens/s = 173,968.9
2025-01-16 13:23:17.385 | DEBUG    | __main__:<module>:313 - Training step 35230: loss = 2.9331 | 3018.54ms | Tokens/s = 173,689.5
2025-01-16 13:23:47.553 | DEBUG    | __main__:<module>:313 - Training step 35240: loss = 2.8440 | 3017.71ms | Tokens/s = 173,736.9
2025-01-16 13:24:17.714 | DEBUG    | __main__:<module>:313 - Training step 35250: loss = 2.9850 | 3013.89ms | Tokens/s = 173,957.3
2025-01-16 13:24:47.843 | DEBUG    | __main__:<module>:313 - Training step 35260: loss = 2.7935 | 3011.04ms | Tokens/s = 174,121.7
2025-01-16 13:25:17.977 | DEBUG    | __main__:<module>:313 - Training step 35270: loss = 2.9461 | 3013.76ms | Tokens/s = 173,964.9
2025-01-16 13:25:48.150 | DEBUG    | __main__:<module>:313 - Training step 35280: loss = 2.7911 | 3018.82ms | Tokens/s = 173,673.3
2025-01-16 13:26:18.350 | DEBUG    | __main__:<module>:313 - Training step 35290: loss = 2.9595 | 3017.81ms | Tokens/s = 173,731.3
2025-01-16 13:26:48.546 | DEBUG    | __main__:<module>:313 - Training step 35300: loss = 2.8961 | 3018.00ms | Tokens/s = 173,720.2
2025-01-16 13:27:18.725 | DEBUG    | __main__:<module>:313 - Training step 35310: loss = 3.0398 | 3017.29ms | Tokens/s = 173,761.4
2025-01-16 13:27:48.895 | DEBUG    | __main__:<module>:313 - Training step 35320: loss = 3.0908 | 3018.43ms | Tokens/s = 173,695.6
2025-01-16 13:28:19.054 | DEBUG    | __main__:<module>:313 - Training step 35330: loss = 2.9028 | 3015.91ms | Tokens/s = 173,841.0
2025-01-16 13:28:49.217 | DEBUG    | __main__:<module>:313 - Training step 35340: loss = 3.0496 | 3017.17ms | Tokens/s = 173,768.1
2025-01-16 13:29:19.419 | DEBUG    | __main__:<module>:313 - Training step 35350: loss = 2.8689 | 3019.77ms | Tokens/s = 173,618.3
2025-01-16 13:29:49.602 | DEBUG    | __main__:<module>:313 - Training step 35360: loss = 2.9171 | 3016.89ms | Tokens/s = 173,784.2
2025-01-16 13:30:19.777 | DEBUG    | __main__:<module>:313 - Training step 35370: loss = 3.0618 | 3018.56ms | Tokens/s = 173,688.1
2025-01-16 13:30:49.940 | DEBUG    | __main__:<module>:313 - Training step 35380: loss = 2.8846 | 3014.89ms | Tokens/s = 173,899.8
2025-01-16 13:31:20.103 | DEBUG    | __main__:<module>:313 - Training step 35390: loss = 2.8731 | 3012.21ms | Tokens/s = 174,054.5
2025-01-16 13:31:50.254 | DEBUG    | __main__:<module>:313 - Training step 35400: loss = 3.0124 | 3012.71ms | Tokens/s = 174,025.3
2025-01-16 13:32:20.415 | DEBUG    | __main__:<module>:313 - Training step 35410: loss = 2.8823 | 3013.77ms | Tokens/s = 173,964.0
2025-01-16 13:32:50.575 | DEBUG    | __main__:<module>:313 - Training step 35420: loss = 3.0758 | 3013.32ms | Tokens/s = 173,990.0
2025-01-16 13:33:20.734 | DEBUG    | __main__:<module>:313 - Training step 35430: loss = 3.1080 | 3015.77ms | Tokens/s = 173,848.9
2025-01-16 13:33:50.892 | DEBUG    | __main__:<module>:313 - Training step 35440: loss = 2.9314 | 3015.49ms | Tokens/s = 173,865.0
2025-01-16 13:34:21.049 | DEBUG    | __main__:<module>:313 - Training step 35450: loss = 3.0168 | 3016.19ms | Tokens/s = 173,824.7
2025-01-16 13:34:51.213 | DEBUG    | __main__:<module>:313 - Training step 35460: loss = 3.1393 | 3017.14ms | Tokens/s = 173,770.0
2025-01-16 13:35:21.372 | DEBUG    | __main__:<module>:313 - Training step 35470: loss = 2.9133 | 3013.57ms | Tokens/s = 173,975.7
2025-01-16 13:35:51.505 | DEBUG    | __main__:<module>:313 - Training step 35480: loss = 3.0351 | 3011.06ms | Tokens/s = 174,120.6
2025-01-16 13:36:21.643 | DEBUG    | __main__:<module>:313 - Training step 35490: loss = 2.8719 | 3013.92ms | Tokens/s = 173,955.5
2025-01-16 13:36:51.800 | DEBUG    | __main__:<module>:313 - Training step 35500: loss = 3.0669 | 3016.92ms | Tokens/s = 173,782.8
2025-01-16 13:37:21.962 | DEBUG    | __main__:<module>:313 - Training step 35510: loss = 3.1140 | 3014.21ms | Tokens/s = 173,938.7
2025-01-16 13:37:52.126 | DEBUG    | __main__:<module>:313 - Training step 35520: loss = 3.0242 | 3014.83ms | Tokens/s = 173,903.1
2025-01-16 13:38:22.301 | DEBUG    | __main__:<module>:313 - Training step 35530: loss = 2.8468 | 3015.74ms | Tokens/s = 173,850.6
2025-01-16 13:38:52.463 | DEBUG    | __main__:<module>:313 - Training step 35540: loss = 3.0352 | 3018.12ms | Tokens/s = 173,713.6
2025-01-16 13:39:22.622 | DEBUG    | __main__:<module>:313 - Training step 35550: loss = 3.0471 | 3016.19ms | Tokens/s = 173,824.6
2025-01-16 13:39:52.794 | DEBUG    | __main__:<module>:313 - Training step 35560: loss = 3.0735 | 3019.75ms | Tokens/s = 173,619.8
2025-01-16 13:40:22.964 | DEBUG    | __main__:<module>:313 - Training step 35570: loss = 2.9667 | 3017.00ms | Tokens/s = 173,777.7
2025-01-16 13:40:53.115 | DEBUG    | __main__:<module>:313 - Training step 35580: loss = 2.8436 | 3016.39ms | Tokens/s = 173,813.4
2025-01-16 13:41:23.273 | DEBUG    | __main__:<module>:313 - Training step 35590: loss = 3.0182 | 3015.24ms | Tokens/s = 173,879.4
2025-01-16 13:41:53.440 | DEBUG    | __main__:<module>:313 - Training step 35600: loss = 2.9773 | 3014.45ms | Tokens/s = 173,925.0
2025-01-16 13:42:23.603 | DEBUG    | __main__:<module>:313 - Training step 35610: loss = 2.9387 | 3017.47ms | Tokens/s = 173,750.9
2025-01-16 13:42:53.766 | DEBUG    | __main__:<module>:313 - Training step 35620: loss = 3.0109 | 3016.39ms | Tokens/s = 173,813.0
2025-01-16 13:43:23.941 | DEBUG    | __main__:<module>:313 - Training step 35630: loss = 3.0497 | 3017.06ms | Tokens/s = 173,774.6
2025-01-16 13:43:54.115 | DEBUG    | __main__:<module>:313 - Training step 35640: loss = 2.9889 | 3017.82ms | Tokens/s = 173,730.8
2025-01-16 13:44:24.279 | DEBUG    | __main__:<module>:313 - Training step 35650: loss = 2.7531 | 3014.63ms | Tokens/s = 173,914.4
2025-01-16 13:44:54.451 | DEBUG    | __main__:<module>:313 - Training step 35660: loss = 2.8603 | 3017.81ms | Tokens/s = 173,731.4
2025-01-16 13:45:24.591 | DEBUG    | __main__:<module>:313 - Training step 35670: loss = 3.0196 | 3013.99ms | Tokens/s = 173,951.4
2025-01-16 13:45:54.712 | DEBUG    | __main__:<module>:313 - Training step 35680: loss = 2.8710 | 3013.77ms | Tokens/s = 173,964.3
2025-01-16 13:46:24.848 | DEBUG    | __main__:<module>:313 - Training step 35690: loss = 2.9076 | 3013.20ms | Tokens/s = 173,997.0
2025-01-16 13:46:55.001 | DEBUG    | __main__:<module>:313 - Training step 35700: loss = 2.9380 | 3015.00ms | Tokens/s = 173,893.2
2025-01-16 13:47:25.155 | DEBUG    | __main__:<module>:313 - Training step 35710: loss = 3.0292 | 3015.09ms | Tokens/s = 173,888.2
2025-01-16 13:47:55.312 | DEBUG    | __main__:<module>:313 - Training step 35720: loss = 3.0222 | 3016.97ms | Tokens/s = 173,779.6
2025-01-16 13:48:25.472 | DEBUG    | __main__:<module>:313 - Training step 35730: loss = 2.8872 | 3015.60ms | Tokens/s = 173,858.4
2025-01-16 13:48:55.627 | DEBUG    | __main__:<module>:313 - Training step 35740: loss = 3.2036 | 3014.23ms | Tokens/s = 173,937.6
2025-01-16 13:49:25.786 | DEBUG    | __main__:<module>:313 - Training step 35750: loss = 2.9897 | 3014.60ms | Tokens/s = 173,916.5
2025-01-16 13:49:55.939 | DEBUG    | __main__:<module>:313 - Training step 35760: loss = 2.9680 | 3017.07ms | Tokens/s = 173,774.0
2025-01-16 13:50:26.095 | DEBUG    | __main__:<module>:313 - Training step 35770: loss = 3.1014 | 3016.94ms | Tokens/s = 173,781.1
2025-01-16 13:50:56.250 | DEBUG    | __main__:<module>:313 - Training step 35780: loss = 3.0847 | 3018.79ms | Tokens/s = 173,675.1
2025-01-16 13:51:26.410 | DEBUG    | __main__:<module>:313 - Training step 35790: loss = 2.8909 | 3016.01ms | Tokens/s = 173,835.0
2025-01-16 13:51:56.578 | DEBUG    | __main__:<module>:313 - Training step 35800: loss = 2.9063 | 3014.87ms | Tokens/s = 173,900.7
2025-01-16 13:52:26.747 | DEBUG    | __main__:<module>:313 - Training step 35810: loss = 2.9095 | 3016.40ms | Tokens/s = 173,812.2
2025-01-16 13:52:56.913 | DEBUG    | __main__:<module>:313 - Training step 35820: loss = 2.9950 | 3015.70ms | Tokens/s = 173,852.9
2025-01-16 13:53:27.075 | DEBUG    | __main__:<module>:313 - Training step 35830: loss = 3.0968 | 3015.14ms | Tokens/s = 173,885.0
2025-01-16 13:53:57.236 | DEBUG    | __main__:<module>:313 - Training step 35840: loss = 2.9196 | 3015.09ms | Tokens/s = 173,888.3
2025-01-16 13:54:27.398 | DEBUG    | __main__:<module>:313 - Training step 35850: loss = 3.0282 | 3017.72ms | Tokens/s = 173,736.6
2025-01-16 13:54:57.557 | DEBUG    | __main__:<module>:313 - Training step 35860: loss = 3.0539 | 3014.34ms | Tokens/s = 173,931.1
2025-01-16 13:55:27.721 | DEBUG    | __main__:<module>:313 - Training step 35870: loss = 3.1240 | 3017.36ms | Tokens/s = 173,757.5
2025-01-16 13:55:57.892 | DEBUG    | __main__:<module>:313 - Training step 35880: loss = 2.7921 | 3016.92ms | Tokens/s = 173,782.6
2025-01-16 13:56:28.053 | DEBUG    | __main__:<module>:313 - Training step 35890: loss = 3.0295 | 3015.40ms | Tokens/s = 173,870.4
2025-01-16 13:56:58.217 | DEBUG    | __main__:<module>:313 - Training step 35900: loss = 2.9313 | 3015.19ms | Tokens/s = 173,882.4
2025-01-16 13:57:28.378 | DEBUG    | __main__:<module>:313 - Training step 35910: loss = 2.7795 | 3013.75ms | Tokens/s = 173,965.1
2025-01-16 13:57:58.543 | DEBUG    | __main__:<module>:313 - Training step 35920: loss = 2.9425 | 3016.49ms | Tokens/s = 173,807.6
2025-01-16 13:58:28.706 | DEBUG    | __main__:<module>:313 - Training step 35930: loss = 2.9468 | 3016.03ms | Tokens/s = 173,833.6
2025-01-16 13:58:58.870 | DEBUG    | __main__:<module>:313 - Training step 35940: loss = 2.8483 | 3017.32ms | Tokens/s = 173,759.2
2025-01-16 13:59:29.037 | DEBUG    | __main__:<module>:313 - Training step 35950: loss = 3.0146 | 3017.67ms | Tokens/s = 173,739.4
2025-01-16 13:59:59.199 | DEBUG    | __main__:<module>:313 - Training step 35960: loss = 2.9451 | 3014.63ms | Tokens/s = 173,914.7
2025-01-16 14:00:29.374 | DEBUG    | __main__:<module>:313 - Training step 35970: loss = 3.0011 | 3018.41ms | Tokens/s = 173,696.7
2025-01-16 14:00:59.543 | DEBUG    | __main__:<module>:313 - Training step 35980: loss = 2.8418 | 3018.51ms | Tokens/s = 173,691.2
2025-01-16 14:01:29.702 | DEBUG    | __main__:<module>:313 - Training step 35990: loss = 2.9495 | 3014.17ms | Tokens/s = 173,941.4
2025-01-16 14:02:03.291 | INFO     | __main__:<module>:265 - Step 36,000/40,000 loss: 2.9653 (T) 2.9776 (V) | lr=2.7e-04
2025-01-16 14:02:06.309 | DEBUG    | __main__:<module>:313 - Training step 36000: loss = 2.8935 | 9468.31ms | Tokens/s = 55,372.9
2025-01-16 14:02:36.459 | DEBUG    | __main__:<module>:313 - Training step 36010: loss = 3.0875 | 3016.91ms | Tokens/s = 173,783.0
2025-01-16 14:03:06.619 | DEBUG    | __main__:<module>:313 - Training step 36020: loss = 3.0445 | 3012.89ms | Tokens/s = 174,014.9
2025-01-16 14:03:36.773 | DEBUG    | __main__:<module>:313 - Training step 36030: loss = 3.0403 | 3014.93ms | Tokens/s = 173,897.3
2025-01-16 14:04:06.933 | DEBUG    | __main__:<module>:313 - Training step 36040: loss = 2.8984 | 3018.55ms | Tokens/s = 173,689.0
2025-01-16 14:04:37.104 | DEBUG    | __main__:<module>:313 - Training step 36050: loss = 2.7743 | 3020.01ms | Tokens/s = 173,604.7
2025-01-16 14:05:07.295 | DEBUG    | __main__:<module>:313 - Training step 36060: loss = 2.8119 | 3018.90ms | Tokens/s = 173,668.7
2025-01-16 14:05:37.504 | DEBUG    | __main__:<module>:313 - Training step 36070: loss = 2.9586 | 3020.53ms | Tokens/s = 173,574.9
2025-01-16 14:06:07.712 | DEBUG    | __main__:<module>:313 - Training step 36080: loss = 2.7957 | 3022.33ms | Tokens/s = 173,471.6
2025-01-16 14:06:37.902 | DEBUG    | __main__:<module>:313 - Training step 36090: loss = 3.1482 | 3019.24ms | Tokens/s = 173,648.9
2025-01-16 14:07:08.075 | DEBUG    | __main__:<module>:313 - Training step 36100: loss = 2.9888 | 3015.22ms | Tokens/s = 173,880.5
2025-01-16 14:07:38.245 | DEBUG    | __main__:<module>:313 - Training step 36110: loss = 3.1386 | 3018.29ms | Tokens/s = 173,703.4
2025-01-16 14:08:08.453 | DEBUG    | __main__:<module>:313 - Training step 36120: loss = 2.8037 | 3020.65ms | Tokens/s = 173,567.9
2025-01-16 14:08:38.657 | DEBUG    | __main__:<module>:313 - Training step 36130: loss = 2.9479 | 3020.74ms | Tokens/s = 173,562.5
2025-01-16 14:09:08.840 | DEBUG    | __main__:<module>:313 - Training step 36140: loss = 3.0020 | 3018.09ms | Tokens/s = 173,715.2
2025-01-16 14:09:39.015 | DEBUG    | __main__:<module>:313 - Training step 36150: loss = 2.9411 | 3014.57ms | Tokens/s = 173,917.9
2025-01-16 14:10:09.173 | DEBUG    | __main__:<module>:313 - Training step 36160: loss = 3.0130 | 3016.62ms | Tokens/s = 173,800.0
2025-01-16 14:10:39.310 | DEBUG    | __main__:<module>:313 - Training step 36170: loss = 2.9656 | 3014.96ms | Tokens/s = 173,895.4
2025-01-16 14:11:09.460 | DEBUG    | __main__:<module>:313 - Training step 36180: loss = 2.8825 | 3016.67ms | Tokens/s = 173,797.0
2025-01-16 14:11:39.624 | DEBUG    | __main__:<module>:313 - Training step 36190: loss = 2.9178 | 3014.78ms | Tokens/s = 173,905.8
2025-01-16 14:12:09.777 | DEBUG    | __main__:<module>:313 - Training step 36200: loss = 3.0747 | 3012.85ms | Tokens/s = 174,017.1
2025-01-16 14:12:39.934 | DEBUG    | __main__:<module>:313 - Training step 36210: loss = 2.9790 | 3015.54ms | Tokens/s = 173,862.1
2025-01-16 14:13:10.093 | DEBUG    | __main__:<module>:313 - Training step 36220: loss = 3.0351 | 3017.45ms | Tokens/s = 173,752.3
2025-01-16 14:13:40.263 | DEBUG    | __main__:<module>:313 - Training step 36230: loss = 3.0078 | 3017.83ms | Tokens/s = 173,730.3
2025-01-16 14:14:10.432 | DEBUG    | __main__:<module>:313 - Training step 36240: loss = 2.9503 | 3019.07ms | Tokens/s = 173,659.0
2025-01-16 14:14:40.597 | DEBUG    | __main__:<module>:313 - Training step 36250: loss = 2.9931 | 3015.37ms | Tokens/s = 173,872.1
2025-01-16 14:15:10.768 | DEBUG    | __main__:<module>:313 - Training step 36260: loss = 2.9636 | 3016.10ms | Tokens/s = 173,829.9
2025-01-16 14:15:40.938 | DEBUG    | __main__:<module>:313 - Training step 36270: loss = 2.9731 | 3020.26ms | Tokens/s = 173,590.5
2025-01-16 14:16:11.111 | DEBUG    | __main__:<module>:313 - Training step 36280: loss = 2.9587 | 3019.05ms | Tokens/s = 173,659.9
2025-01-16 14:16:41.281 | DEBUG    | __main__:<module>:313 - Training step 36290: loss = 2.9904 | 3018.67ms | Tokens/s = 173,681.5
2025-01-16 14:17:11.455 | DEBUG    | __main__:<module>:313 - Training step 36300: loss = 2.9667 | 3016.54ms | Tokens/s = 173,804.6
2025-01-16 14:17:41.614 | DEBUG    | __main__:<module>:313 - Training step 36310: loss = 3.0884 | 3014.30ms | Tokens/s = 173,933.6
2025-01-16 14:18:11.784 | DEBUG    | __main__:<module>:313 - Training step 36320: loss = 2.7325 | 3016.94ms | Tokens/s = 173,781.1
2025-01-16 14:18:41.949 | DEBUG    | __main__:<module>:313 - Training step 36330: loss = 3.0127 | 3016.48ms | Tokens/s = 173,808.0
2025-01-16 14:19:12.112 | DEBUG    | __main__:<module>:313 - Training step 36340: loss = 2.9605 | 3015.53ms | Tokens/s = 173,862.9
2025-01-16 14:19:42.274 | DEBUG    | __main__:<module>:313 - Training step 36350: loss = 2.9218 | 3014.15ms | Tokens/s = 173,942.2
2025-01-16 14:20:12.439 | DEBUG    | __main__:<module>:313 - Training step 36360: loss = 2.7763 | 3019.60ms | Tokens/s = 173,628.3
2025-01-16 14:20:42.602 | DEBUG    | __main__:<module>:313 - Training step 36370: loss = 2.8224 | 3015.32ms | Tokens/s = 173,874.6
2025-01-16 14:21:12.765 | DEBUG    | __main__:<module>:313 - Training step 36380: loss = 2.9194 | 3016.33ms | Tokens/s = 173,816.8
2025-01-16 14:21:42.935 | DEBUG    | __main__:<module>:313 - Training step 36390: loss = 2.8017 | 3016.03ms | Tokens/s = 173,834.1
2025-01-16 14:22:13.108 | DEBUG    | __main__:<module>:313 - Training step 36400: loss = 2.8177 | 3015.74ms | Tokens/s = 173,850.5
2025-01-16 14:22:43.274 | DEBUG    | __main__:<module>:313 - Training step 36410: loss = 2.8235 | 3015.36ms | Tokens/s = 173,872.3
2025-01-16 14:23:13.411 | DEBUG    | __main__:<module>:313 - Training step 36420: loss = 2.8834 | 3013.32ms | Tokens/s = 173,990.4
2025-01-16 14:23:43.533 | DEBUG    | __main__:<module>:313 - Training step 36430: loss = 2.9004 | 3011.36ms | Tokens/s = 174,103.1
2025-01-16 14:24:13.671 | DEBUG    | __main__:<module>:313 - Training step 36440: loss = 3.0285 | 3014.35ms | Tokens/s = 173,930.7
2025-01-16 14:24:43.807 | DEBUG    | __main__:<module>:313 - Training step 36450: loss = 2.9970 | 3013.64ms | Tokens/s = 173,971.7
2025-01-16 14:25:13.956 | DEBUG    | __main__:<module>:313 - Training step 36460: loss = 2.8890 | 3015.94ms | Tokens/s = 173,838.8
2025-01-16 14:25:44.108 | DEBUG    | __main__:<module>:313 - Training step 36470: loss = 3.0087 | 3017.88ms | Tokens/s = 173,727.1
2025-01-16 14:26:14.261 | DEBUG    | __main__:<module>:313 - Training step 36480: loss = 3.0073 | 3014.23ms | Tokens/s = 173,937.9
2025-01-16 14:26:44.404 | DEBUG    | __main__:<module>:313 - Training step 36490: loss = 3.0199 | 3013.77ms | Tokens/s = 173,964.0
2025-01-16 14:27:14.562 | DEBUG    | __main__:<module>:313 - Training step 36500: loss = 3.0331 | 3016.16ms | Tokens/s = 173,826.3
2025-01-16 14:27:44.722 | DEBUG    | __main__:<module>:313 - Training step 36510: loss = 2.8665 | 3013.97ms | Tokens/s = 173,952.6
2025-01-16 14:28:14.876 | DEBUG    | __main__:<module>:313 - Training step 36520: loss = 2.9178 | 3015.52ms | Tokens/s = 173,863.0
2025-01-16 14:28:45.039 | DEBUG    | __main__:<module>:313 - Training step 36530: loss = 2.7799 | 3015.65ms | Tokens/s = 173,855.7
2025-01-16 14:29:15.206 | DEBUG    | __main__:<module>:313 - Training step 36540: loss = 2.8103 | 3016.86ms | Tokens/s = 173,786.1
2025-01-16 14:29:45.383 | DEBUG    | __main__:<module>:313 - Training step 36550: loss = 2.9776 | 3016.82ms | Tokens/s = 173,788.2
2025-01-16 14:30:15.545 | DEBUG    | __main__:<module>:313 - Training step 36560: loss = 2.9456 | 3016.14ms | Tokens/s = 173,827.3
2025-01-16 14:30:45.703 | DEBUG    | __main__:<module>:313 - Training step 36570: loss = 2.9936 | 3015.45ms | Tokens/s = 173,867.4
2025-01-16 14:31:15.860 | DEBUG    | __main__:<module>:313 - Training step 36580: loss = 2.6564 | 3015.06ms | Tokens/s = 173,890.0
2025-01-16 14:31:46.025 | DEBUG    | __main__:<module>:313 - Training step 36590: loss = 2.9535 | 3016.73ms | Tokens/s = 173,793.6
2025-01-16 14:32:16.191 | DEBUG    | __main__:<module>:313 - Training step 36600: loss = 2.8324 | 3018.61ms | Tokens/s = 173,685.3
2025-01-16 14:32:46.367 | DEBUG    | __main__:<module>:313 - Training step 36610: loss = 2.8980 | 3016.83ms | Tokens/s = 173,787.9
2025-01-16 14:33:16.542 | DEBUG    | __main__:<module>:313 - Training step 36620: loss = 2.8825 | 3016.86ms | Tokens/s = 173,786.1
2025-01-16 14:33:46.712 | DEBUG    | __main__:<module>:313 - Training step 36630: loss = 2.9165 | 3017.36ms | Tokens/s = 173,757.5
2025-01-16 14:34:16.868 | DEBUG    | __main__:<module>:313 - Training step 36640: loss = 2.9301 | 3016.19ms | Tokens/s = 173,824.6
2025-01-16 14:34:47.018 | DEBUG    | __main__:<module>:313 - Training step 36650: loss = 3.1217 | 3017.24ms | Tokens/s = 173,763.8
2025-01-16 14:35:17.180 | DEBUG    | __main__:<module>:313 - Training step 36660: loss = 2.9420 | 3018.82ms | Tokens/s = 173,673.3
2025-01-16 14:35:47.340 | DEBUG    | __main__:<module>:313 - Training step 36670: loss = 2.9128 | 3014.25ms | Tokens/s = 173,936.6
2025-01-16 14:36:17.507 | DEBUG    | __main__:<module>:313 - Training step 36680: loss = 2.9261 | 3019.24ms | Tokens/s = 173,648.9
2025-01-16 14:36:47.677 | DEBUG    | __main__:<module>:313 - Training step 36690: loss = 2.9174 | 3016.90ms | Tokens/s = 173,784.0
2025-01-16 14:37:17.839 | DEBUG    | __main__:<module>:313 - Training step 36700: loss = 2.7856 | 3016.75ms | Tokens/s = 173,792.4
2025-01-16 14:37:48.006 | DEBUG    | __main__:<module>:313 - Training step 36710: loss = 2.9341 | 3014.90ms | Tokens/s = 173,899.1
2025-01-16 14:38:18.159 | DEBUG    | __main__:<module>:313 - Training step 36720: loss = 2.9763 | 3015.88ms | Tokens/s = 173,842.6
2025-01-16 14:38:48.316 | DEBUG    | __main__:<module>:313 - Training step 36730: loss = 2.8051 | 3017.17ms | Tokens/s = 173,768.2
2025-01-16 14:39:18.475 | DEBUG    | __main__:<module>:313 - Training step 36740: loss = 3.0567 | 3017.73ms | Tokens/s = 173,736.0
2025-01-16 14:39:48.639 | DEBUG    | __main__:<module>:313 - Training step 36750: loss = 2.9983 | 3016.04ms | Tokens/s = 173,833.4
2025-01-16 14:40:18.798 | DEBUG    | __main__:<module>:313 - Training step 36760: loss = 2.9573 | 3015.17ms | Tokens/s = 173,883.3
2025-01-16 14:40:48.960 | DEBUG    | __main__:<module>:313 - Training step 36770: loss = 2.8116 | 3018.22ms | Tokens/s = 173,707.5
2025-01-16 14:41:19.117 | DEBUG    | __main__:<module>:313 - Training step 36780: loss = 2.9084 | 3014.64ms | Tokens/s = 173,914.2
2025-01-16 14:41:49.268 | DEBUG    | __main__:<module>:313 - Training step 36790: loss = 2.7951 | 3017.28ms | Tokens/s = 173,761.7
2025-01-16 14:42:19.430 | DEBUG    | __main__:<module>:313 - Training step 36800: loss = 2.8738 | 3016.14ms | Tokens/s = 173,827.4
2025-01-16 14:42:49.584 | DEBUG    | __main__:<module>:313 - Training step 36810: loss = 2.8197 | 3015.76ms | Tokens/s = 173,849.6
2025-01-16 14:43:19.747 | DEBUG    | __main__:<module>:313 - Training step 36820: loss = 2.9501 | 3014.97ms | Tokens/s = 173,894.7
2025-01-16 14:43:49.900 | DEBUG    | __main__:<module>:313 - Training step 36830: loss = 2.9849 | 3016.18ms | Tokens/s = 173,825.0
2025-01-16 14:44:20.061 | DEBUG    | __main__:<module>:313 - Training step 36840: loss = 2.8833 | 3015.71ms | Tokens/s = 173,852.3
2025-01-16 14:44:50.216 | DEBUG    | __main__:<module>:313 - Training step 36850: loss = 2.9777 | 3013.96ms | Tokens/s = 173,953.2
2025-01-16 14:45:20.373 | DEBUG    | __main__:<module>:313 - Training step 36860: loss = 2.9987 | 3014.80ms | Tokens/s = 173,904.9
2025-01-16 14:45:50.533 | DEBUG    | __main__:<module>:313 - Training step 36870: loss = 2.9238 | 3015.93ms | Tokens/s = 173,839.3
2025-01-16 14:46:20.706 | DEBUG    | __main__:<module>:313 - Training step 36880: loss = 2.8153 | 3017.61ms | Tokens/s = 173,742.7
2025-01-16 14:46:50.882 | DEBUG    | __main__:<module>:313 - Training step 36890: loss = 3.0193 | 3017.61ms | Tokens/s = 173,742.9
2025-01-16 14:47:21.039 | DEBUG    | __main__:<module>:313 - Training step 36900: loss = 3.1235 | 3017.01ms | Tokens/s = 173,777.6
2025-01-16 14:47:51.215 | DEBUG    | __main__:<module>:313 - Training step 36910: loss = 2.8802 | 3018.10ms | Tokens/s = 173,714.4
2025-01-16 14:48:21.403 | DEBUG    | __main__:<module>:313 - Training step 36920: loss = 2.8988 | 3020.44ms | Tokens/s = 173,580.2
2025-01-16 14:48:51.593 | DEBUG    | __main__:<module>:313 - Training step 36930: loss = 3.0006 | 3020.28ms | Tokens/s = 173,589.3
2025-01-16 14:49:21.773 | DEBUG    | __main__:<module>:313 - Training step 36940: loss = 2.9600 | 3018.08ms | Tokens/s = 173,715.8
2025-01-16 14:49:51.952 | DEBUG    | __main__:<module>:313 - Training step 36950: loss = 3.0165 | 3016.30ms | Tokens/s = 173,818.5
2025-01-16 14:50:22.120 | DEBUG    | __main__:<module>:313 - Training step 36960: loss = 3.0696 | 3018.30ms | Tokens/s = 173,703.1
2025-01-16 14:50:52.292 | DEBUG    | __main__:<module>:313 - Training step 36970: loss = 2.9148 | 3015.84ms | Tokens/s = 173,844.8
2025-01-16 14:51:22.463 | DEBUG    | __main__:<module>:313 - Training step 36980: loss = 2.9386 | 3015.51ms | Tokens/s = 173,863.6
2025-01-16 14:51:52.642 | DEBUG    | __main__:<module>:313 - Training step 36990: loss = 2.7737 | 3019.25ms | Tokens/s = 173,648.3
2025-01-16 14:52:26.252 | INFO     | __main__:<module>:265 - Step 37,000/40,000 loss: 2.9296 (T) 2.9323 (V) | lr=1.5e-04
2025-01-16 14:52:26.253 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 14:52:39.671 | DEBUG    | __main__:<module>:313 - Training step 37000: loss = 2.9449 | 19871.68ms | Tokens/s = 26,383.7
2025-01-16 14:53:09.691 | DEBUG    | __main__:<module>:313 - Training step 37010: loss = 3.1135 | 3007.96ms | Tokens/s = 174,300.0
2025-01-16 14:53:39.769 | DEBUG    | __main__:<module>:313 - Training step 37020: loss = 2.7334 | 3007.02ms | Tokens/s = 174,354.6
2025-01-16 14:54:09.863 | DEBUG    | __main__:<module>:313 - Training step 37030: loss = 3.0196 | 3011.73ms | Tokens/s = 174,081.8
2025-01-16 14:54:39.987 | DEBUG    | __main__:<module>:313 - Training step 37040: loss = 2.9145 | 3013.27ms | Tokens/s = 173,993.2
2025-01-16 14:55:10.119 | DEBUG    | __main__:<module>:313 - Training step 37050: loss = 2.9637 | 3010.94ms | Tokens/s = 174,127.6
2025-01-16 14:55:40.251 | DEBUG    | __main__:<module>:313 - Training step 37060: loss = 2.9142 | 3011.53ms | Tokens/s = 174,093.7
2025-01-16 14:56:10.370 | DEBUG    | __main__:<module>:313 - Training step 37070: loss = 2.9186 | 3011.49ms | Tokens/s = 174,096.1
2025-01-16 14:56:40.483 | DEBUG    | __main__:<module>:313 - Training step 37080: loss = 2.9563 | 3010.21ms | Tokens/s = 174,170.1
2025-01-16 14:57:10.593 | DEBUG    | __main__:<module>:313 - Training step 37090: loss = 2.7944 | 3010.70ms | Tokens/s = 174,141.3
2025-01-16 14:57:40.704 | DEBUG    | __main__:<module>:313 - Training step 37100: loss = 2.8516 | 3009.79ms | Tokens/s = 174,194.3
2025-01-16 14:58:10.823 | DEBUG    | __main__:<module>:313 - Training step 37110: loss = 2.8635 | 3009.80ms | Tokens/s = 174,193.4
2025-01-16 14:58:40.952 | DEBUG    | __main__:<module>:313 - Training step 37120: loss = 2.8997 | 3011.62ms | Tokens/s = 174,088.3
2025-01-16 14:59:11.080 | DEBUG    | __main__:<module>:313 - Training step 37130: loss = 2.9604 | 3012.35ms | Tokens/s = 174,046.4
2025-01-16 14:59:41.208 | DEBUG    | __main__:<module>:313 - Training step 37140: loss = 2.9249 | 3013.28ms | Tokens/s = 173,992.6
2025-01-16 15:00:11.333 | DEBUG    | __main__:<module>:313 - Training step 37150: loss = 2.9483 | 3012.71ms | Tokens/s = 174,025.5
2025-01-16 15:00:41.466 | DEBUG    | __main__:<module>:313 - Training step 37160: loss = 2.9474 | 3013.03ms | Tokens/s = 174,007.0
2025-01-16 15:01:11.597 | DEBUG    | __main__:<module>:313 - Training step 37170: loss = 2.8812 | 3014.58ms | Tokens/s = 173,917.3
2025-01-16 15:01:41.730 | DEBUG    | __main__:<module>:313 - Training step 37180: loss = 2.9082 | 3013.34ms | Tokens/s = 173,988.9
2025-01-16 15:02:11.871 | DEBUG    | __main__:<module>:313 - Training step 37190: loss = 2.9098 | 3012.54ms | Tokens/s = 174,035.1
2025-01-16 15:02:42.026 | DEBUG    | __main__:<module>:313 - Training step 37200: loss = 2.7854 | 3015.15ms | Tokens/s = 173,884.8
2025-01-16 15:03:12.200 | DEBUG    | __main__:<module>:313 - Training step 37210: loss = 2.9969 | 3016.51ms | Tokens/s = 173,806.3
2025-01-16 15:03:42.374 | DEBUG    | __main__:<module>:313 - Training step 37220: loss = 2.9081 | 3015.25ms | Tokens/s = 173,878.9
2025-01-16 15:04:12.544 | DEBUG    | __main__:<module>:313 - Training step 37230: loss = 2.7184 | 3017.22ms | Tokens/s = 173,765.1
2025-01-16 15:04:42.702 | DEBUG    | __main__:<module>:313 - Training step 37240: loss = 2.9187 | 3013.38ms | Tokens/s = 173,986.7
2025-01-16 15:05:12.842 | DEBUG    | __main__:<module>:313 - Training step 37250: loss = 2.8047 | 3014.80ms | Tokens/s = 173,904.8
2025-01-16 15:05:42.984 | DEBUG    | __main__:<module>:313 - Training step 37260: loss = 2.8561 | 3014.29ms | Tokens/s = 173,934.4
2025-01-16 15:06:13.126 | DEBUG    | __main__:<module>:313 - Training step 37270: loss = 2.8130 | 3013.28ms | Tokens/s = 173,992.5
2025-01-16 15:06:43.262 | DEBUG    | __main__:<module>:313 - Training step 37280: loss = 2.9143 | 3012.49ms | Tokens/s = 174,037.9
2025-01-16 15:07:13.401 | DEBUG    | __main__:<module>:313 - Training step 37290: loss = 3.0682 | 3012.00ms | Tokens/s = 174,066.6
2025-01-16 15:07:43.539 | DEBUG    | __main__:<module>:313 - Training step 37300: loss = 2.9398 | 3011.92ms | Tokens/s = 174,071.1
2025-01-16 15:08:13.655 | DEBUG    | __main__:<module>:313 - Training step 37310: loss = 2.9539 | 3012.08ms | Tokens/s = 174,061.5
2025-01-16 15:08:43.776 | DEBUG    | __main__:<module>:313 - Training step 37320: loss = 3.0080 | 3011.23ms | Tokens/s = 174,111.1
2025-01-16 15:09:13.900 | DEBUG    | __main__:<module>:313 - Training step 37330: loss = 2.9909 | 3015.29ms | Tokens/s = 173,876.2
2025-01-16 15:09:44.023 | DEBUG    | __main__:<module>:313 - Training step 37340: loss = 2.9185 | 3011.46ms | Tokens/s = 174,097.8
2025-01-16 15:10:14.137 | DEBUG    | __main__:<module>:313 - Training step 37350: loss = 2.7133 | 3010.91ms | Tokens/s = 174,129.4
2025-01-16 15:10:44.251 | DEBUG    | __main__:<module>:313 - Training step 37360: loss = 2.7507 | 3010.96ms | Tokens/s = 174,126.3
2025-01-16 15:11:14.366 | DEBUG    | __main__:<module>:313 - Training step 37370: loss = 2.9061 | 3011.37ms | Tokens/s = 174,102.9
2025-01-16 15:11:44.480 | DEBUG    | __main__:<module>:313 - Training step 37380: loss = 2.8928 | 3012.15ms | Tokens/s = 174,058.0
2025-01-16 15:12:14.585 | DEBUG    | __main__:<module>:313 - Training step 37390: loss = 3.0549 | 3009.84ms | Tokens/s = 174,191.2
2025-01-16 15:12:44.692 | DEBUG    | __main__:<module>:313 - Training step 37400: loss = 2.8563 | 3009.74ms | Tokens/s = 174,197.3
2025-01-16 15:13:14.803 | DEBUG    | __main__:<module>:313 - Training step 37410: loss = 3.0492 | 3012.25ms | Tokens/s = 174,052.0
2025-01-16 15:13:44.902 | DEBUG    | __main__:<module>:313 - Training step 37420: loss = 2.9613 | 3009.42ms | Tokens/s = 174,215.4
2025-01-16 15:14:15.006 | DEBUG    | __main__:<module>:313 - Training step 37430: loss = 2.9975 | 3012.23ms | Tokens/s = 174,052.9
2025-01-16 15:14:45.123 | DEBUG    | __main__:<module>:313 - Training step 37440: loss = 3.0270 | 3010.87ms | Tokens/s = 174,132.0
2025-01-16 15:15:15.226 | DEBUG    | __main__:<module>:313 - Training step 37450: loss = 3.0939 | 3009.06ms | Tokens/s = 174,236.6
2025-01-16 15:15:45.342 | DEBUG    | __main__:<module>:313 - Training step 37460: loss = 2.9810 | 3011.76ms | Tokens/s = 174,080.2
2025-01-16 15:16:15.447 | DEBUG    | __main__:<module>:313 - Training step 37470: loss = 2.9202 | 3012.63ms | Tokens/s = 174,029.8
2025-01-16 15:16:45.556 | DEBUG    | __main__:<module>:313 - Training step 37480: loss = 2.8201 | 3013.07ms | Tokens/s = 174,004.4
2025-01-16 15:17:15.655 | DEBUG    | __main__:<module>:313 - Training step 37490: loss = 3.0797 | 3008.43ms | Tokens/s = 174,272.9
2025-01-16 15:17:45.757 | DEBUG    | __main__:<module>:313 - Training step 37500: loss = 3.0300 | 3007.36ms | Tokens/s = 174,334.9
2025-01-16 15:18:15.859 | DEBUG    | __main__:<module>:313 - Training step 37510: loss = 2.8228 | 3010.10ms | Tokens/s = 174,176.3
2025-01-16 15:18:45.955 | DEBUG    | __main__:<module>:313 - Training step 37520: loss = 2.8519 | 3010.82ms | Tokens/s = 174,134.4
2025-01-16 15:19:16.075 | DEBUG    | __main__:<module>:313 - Training step 37530: loss = 2.8874 | 3013.90ms | Tokens/s = 173,956.5
2025-01-16 15:19:46.208 | DEBUG    | __main__:<module>:313 - Training step 37540: loss = 3.0285 | 3014.01ms | Tokens/s = 173,950.0
2025-01-16 15:20:16.339 | DEBUG    | __main__:<module>:313 - Training step 37550: loss = 2.7676 | 3010.16ms | Tokens/s = 174,172.5
2025-01-16 15:20:46.450 | DEBUG    | __main__:<module>:313 - Training step 37560: loss = 3.0513 | 3010.63ms | Tokens/s = 174,145.6
2025-01-16 15:21:16.562 | DEBUG    | __main__:<module>:313 - Training step 37570: loss = 2.9951 | 3010.64ms | Tokens/s = 174,145.2
2025-01-16 15:21:46.668 | DEBUG    | __main__:<module>:313 - Training step 37580: loss = 2.9262 | 3010.17ms | Tokens/s = 174,172.0
2025-01-16 15:22:16.778 | DEBUG    | __main__:<module>:313 - Training step 37590: loss = 2.8937 | 3014.65ms | Tokens/s = 173,913.4
2025-01-16 15:22:46.903 | DEBUG    | __main__:<module>:313 - Training step 37600: loss = 3.0035 | 3012.57ms | Tokens/s = 174,033.6
2025-01-16 15:23:17.022 | DEBUG    | __main__:<module>:313 - Training step 37610: loss = 2.7097 | 3011.41ms | Tokens/s = 174,100.6
2025-01-16 15:23:47.145 | DEBUG    | __main__:<module>:313 - Training step 37620: loss = 3.0976 | 3012.91ms | Tokens/s = 174,014.1
2025-01-16 15:24:17.262 | DEBUG    | __main__:<module>:313 - Training step 37630: loss = 2.8259 | 3011.93ms | Tokens/s = 174,070.4
2025-01-16 15:24:47.388 | DEBUG    | __main__:<module>:313 - Training step 37640: loss = 2.9145 | 3012.12ms | Tokens/s = 174,059.8
2025-01-16 15:25:17.506 | DEBUG    | __main__:<module>:313 - Training step 37650: loss = 3.0170 | 3012.47ms | Tokens/s = 174,039.4
2025-01-16 15:25:47.628 | DEBUG    | __main__:<module>:313 - Training step 37660: loss = 3.0392 | 3012.09ms | Tokens/s = 174,061.4
2025-01-16 15:26:17.751 | DEBUG    | __main__:<module>:313 - Training step 37670: loss = 2.9389 | 3012.85ms | Tokens/s = 174,017.1
2025-01-16 15:26:47.893 | DEBUG    | __main__:<module>:313 - Training step 37680: loss = 2.9578 | 3013.31ms | Tokens/s = 173,990.7
2025-01-16 15:27:18.054 | DEBUG    | __main__:<module>:313 - Training step 37690: loss = 2.8200 | 3017.40ms | Tokens/s = 173,755.1
2025-01-16 15:27:48.217 | DEBUG    | __main__:<module>:313 - Training step 37700: loss = 2.8892 | 3014.83ms | Tokens/s = 173,903.0
2025-01-16 15:28:18.370 | DEBUG    | __main__:<module>:313 - Training step 37710: loss = 2.9647 | 3012.22ms | Tokens/s = 174,053.6
2025-01-16 15:28:48.499 | DEBUG    | __main__:<module>:313 - Training step 37720: loss = 2.8526 | 3011.49ms | Tokens/s = 174,095.9
2025-01-16 15:29:18.602 | DEBUG    | __main__:<module>:313 - Training step 37730: loss = 2.8703 | 3008.89ms | Tokens/s = 174,246.1
2025-01-16 15:29:48.667 | DEBUG    | __main__:<module>:313 - Training step 37740: loss = 3.0533 | 3005.84ms | Tokens/s = 174,423.1
2025-01-16 15:30:18.753 | DEBUG    | __main__:<module>:313 - Training step 37750: loss = 2.7415 | 3009.80ms | Tokens/s = 174,193.4
2025-01-16 15:30:48.866 | DEBUG    | __main__:<module>:313 - Training step 37760: loss = 3.0687 | 3011.66ms | Tokens/s = 174,086.1
2025-01-16 15:31:18.994 | DEBUG    | __main__:<module>:313 - Training step 37770: loss = 2.9704 | 3013.99ms | Tokens/s = 173,951.3
2025-01-16 15:31:49.127 | DEBUG    | __main__:<module>:313 - Training step 37780: loss = 2.6886 | 3015.09ms | Tokens/s = 173,887.8
2025-01-16 15:32:19.259 | DEBUG    | __main__:<module>:313 - Training step 37790: loss = 2.8815 | 3013.15ms | Tokens/s = 174,000.1
2025-01-16 15:32:49.393 | DEBUG    | __main__:<module>:313 - Training step 37800: loss = 2.9712 | 3014.29ms | Tokens/s = 173,934.4
2025-01-16 15:33:19.530 | DEBUG    | __main__:<module>:313 - Training step 37810: loss = 2.9403 | 3015.49ms | Tokens/s = 173,864.8
2025-01-16 15:33:49.671 | DEBUG    | __main__:<module>:313 - Training step 37820: loss = 2.8193 | 3013.17ms | Tokens/s = 173,998.9
2025-01-16 15:34:19.809 | DEBUG    | __main__:<module>:313 - Training step 37830: loss = 2.8756 | 3015.64ms | Tokens/s = 173,856.1
2025-01-16 15:34:49.950 | DEBUG    | __main__:<module>:313 - Training step 37840: loss = 2.9648 | 3013.89ms | Tokens/s = 173,957.2
2025-01-16 15:35:20.091 | DEBUG    | __main__:<module>:313 - Training step 37850: loss = 3.0166 | 3012.40ms | Tokens/s = 174,043.2
2025-01-16 15:35:50.230 | DEBUG    | __main__:<module>:313 - Training step 37860: loss = 3.0231 | 3014.22ms | Tokens/s = 173,938.4
2025-01-16 15:36:20.363 | DEBUG    | __main__:<module>:313 - Training step 37870: loss = 2.9840 | 3011.28ms | Tokens/s = 174,108.3
2025-01-16 15:36:50.496 | DEBUG    | __main__:<module>:313 - Training step 37880: loss = 2.9145 | 3013.53ms | Tokens/s = 173,978.2
2025-01-16 15:37:20.631 | DEBUG    | __main__:<module>:313 - Training step 37890: loss = 2.7991 | 3013.37ms | Tokens/s = 173,987.1
2025-01-16 15:37:50.775 | DEBUG    | __main__:<module>:313 - Training step 37900: loss = 2.9571 | 3013.43ms | Tokens/s = 173,983.8
2025-01-16 15:38:20.917 | DEBUG    | __main__:<module>:313 - Training step 37910: loss = 2.9475 | 3012.82ms | Tokens/s = 174,019.0
2025-01-16 15:38:51.067 | DEBUG    | __main__:<module>:313 - Training step 37920: loss = 3.0844 | 3015.76ms | Tokens/s = 173,849.3
2025-01-16 15:39:21.216 | DEBUG    | __main__:<module>:313 - Training step 37930: loss = 2.9909 | 3016.01ms | Tokens/s = 173,835.1
2025-01-16 15:39:51.358 | DEBUG    | __main__:<module>:313 - Training step 37940: loss = 2.8388 | 3015.53ms | Tokens/s = 173,862.6
2025-01-16 15:40:21.509 | DEBUG    | __main__:<module>:313 - Training step 37950: loss = 2.9833 | 3014.47ms | Tokens/s = 173,924.0
2025-01-16 15:40:51.658 | DEBUG    | __main__:<module>:313 - Training step 37960: loss = 3.0308 | 3014.99ms | Tokens/s = 173,893.9
2025-01-16 15:41:21.806 | DEBUG    | __main__:<module>:313 - Training step 37970: loss = 2.9696 | 3014.70ms | Tokens/s = 173,910.3
2025-01-16 15:41:51.960 | DEBUG    | __main__:<module>:313 - Training step 37980: loss = 2.9005 | 3014.92ms | Tokens/s = 173,898.1
2025-01-16 15:42:22.113 | DEBUG    | __main__:<module>:313 - Training step 37990: loss = 3.0387 | 3013.98ms | Tokens/s = 173,952.2
2025-01-16 15:42:55.701 | INFO     | __main__:<module>:265 - Step 38,000/40,000 loss: 2.9315 (T) 2.9346 (V) | lr=6.8e-05
2025-01-16 15:42:58.717 | DEBUG    | __main__:<module>:313 - Training step 38000: loss = 2.8874 | 9467.74ms | Tokens/s = 55,376.3
2025-01-16 15:43:28.859 | DEBUG    | __main__:<module>:313 - Training step 38010: loss = 3.0473 | 3014.52ms | Tokens/s = 173,920.9
2025-01-16 15:43:59.000 | DEBUG    | __main__:<module>:313 - Training step 38020: loss = 2.9572 | 3013.71ms | Tokens/s = 173,967.4
2025-01-16 15:44:29.145 | DEBUG    | __main__:<module>:313 - Training step 38030: loss = 2.6167 | 3016.52ms | Tokens/s = 173,805.7
2025-01-16 15:44:59.295 | DEBUG    | __main__:<module>:313 - Training step 38040: loss = 2.9801 | 3015.54ms | Tokens/s = 173,861.9
2025-01-16 15:45:29.428 | DEBUG    | __main__:<module>:313 - Training step 38050: loss = 2.7772 | 3014.00ms | Tokens/s = 173,950.9
2025-01-16 15:45:59.580 | DEBUG    | __main__:<module>:313 - Training step 38060: loss = 2.9461 | 3013.40ms | Tokens/s = 173,985.3
2025-01-16 15:46:29.724 | DEBUG    | __main__:<module>:313 - Training step 38070: loss = 2.9726 | 3013.86ms | Tokens/s = 173,958.8
2025-01-16 15:46:59.882 | DEBUG    | __main__:<module>:313 - Training step 38080: loss = 2.8730 | 3017.07ms | Tokens/s = 173,773.6
2025-01-16 15:47:30.047 | DEBUG    | __main__:<module>:313 - Training step 38090: loss = 2.8933 | 3018.25ms | Tokens/s = 173,706.2
2025-01-16 15:48:00.218 | DEBUG    | __main__:<module>:313 - Training step 38100: loss = 2.8021 | 3015.46ms | Tokens/s = 173,866.6
2025-01-16 15:48:30.379 | DEBUG    | __main__:<module>:313 - Training step 38110: loss = 2.9527 | 3015.47ms | Tokens/s = 173,866.3
2025-01-16 15:49:00.531 | DEBUG    | __main__:<module>:313 - Training step 38120: loss = 3.0072 | 3014.22ms | Tokens/s = 173,938.0
2025-01-16 15:49:30.660 | DEBUG    | __main__:<module>:313 - Training step 38130: loss = 2.8313 | 3012.70ms | Tokens/s = 174,025.8
2025-01-16 15:50:00.777 | DEBUG    | __main__:<module>:313 - Training step 38140: loss = 2.8755 | 3011.06ms | Tokens/s = 174,120.5
2025-01-16 15:50:30.890 | DEBUG    | __main__:<module>:313 - Training step 38150: loss = 2.9098 | 3011.34ms | Tokens/s = 174,104.4
2025-01-16 15:51:00.994 | DEBUG    | __main__:<module>:313 - Training step 38160: loss = 2.9828 | 3010.40ms | Tokens/s = 174,158.7
2025-01-16 15:51:31.108 | DEBUG    | __main__:<module>:313 - Training step 38170: loss = 2.9420 | 3012.54ms | Tokens/s = 174,034.9
2025-01-16 15:52:01.229 | DEBUG    | __main__:<module>:313 - Training step 38180: loss = 2.8631 | 3012.22ms | Tokens/s = 174,053.4
2025-01-16 15:52:31.356 | DEBUG    | __main__:<module>:313 - Training step 38190: loss = 2.8754 | 3012.82ms | Tokens/s = 174,018.9
2025-01-16 15:53:01.481 | DEBUG    | __main__:<module>:313 - Training step 38200: loss = 2.8113 | 3013.08ms | Tokens/s = 174,003.8
2025-01-16 15:53:31.626 | DEBUG    | __main__:<module>:313 - Training step 38210: loss = 3.1331 | 3014.41ms | Tokens/s = 173,927.0
2025-01-16 15:54:01.781 | DEBUG    | __main__:<module>:313 - Training step 38220: loss = 2.9063 | 3015.61ms | Tokens/s = 173,858.3
2025-01-16 15:54:31.937 | DEBUG    | __main__:<module>:313 - Training step 38230: loss = 3.0229 | 3014.60ms | Tokens/s = 173,916.4
2025-01-16 15:55:02.096 | DEBUG    | __main__:<module>:313 - Training step 38240: loss = 2.9106 | 3014.86ms | Tokens/s = 173,901.1
2025-01-16 15:55:32.257 | DEBUG    | __main__:<module>:313 - Training step 38250: loss = 2.8970 | 3017.18ms | Tokens/s = 173,767.4
2025-01-16 15:56:02.439 | DEBUG    | __main__:<module>:313 - Training step 38260: loss = 3.1118 | 3019.89ms | Tokens/s = 173,611.8
2025-01-16 15:56:32.617 | DEBUG    | __main__:<module>:313 - Training step 38270: loss = 2.9349 | 3017.12ms | Tokens/s = 173,771.0
2025-01-16 15:57:02.789 | DEBUG    | __main__:<module>:313 - Training step 38280: loss = 3.0250 | 3016.97ms | Tokens/s = 173,779.5
2025-01-16 15:57:32.965 | DEBUG    | __main__:<module>:313 - Training step 38290: loss = 2.9138 | 3015.93ms | Tokens/s = 173,839.3
2025-01-16 15:58:03.123 | DEBUG    | __main__:<module>:313 - Training step 38300: loss = 2.9207 | 3016.36ms | Tokens/s = 173,815.0
2025-01-16 15:58:33.302 | DEBUG    | __main__:<module>:313 - Training step 38310: loss = 2.8039 | 3017.50ms | Tokens/s = 173,749.0
2025-01-16 15:59:03.492 | DEBUG    | __main__:<module>:313 - Training step 38320: loss = 2.8967 | 3020.21ms | Tokens/s = 173,593.0
2025-01-16 15:59:33.686 | DEBUG    | __main__:<module>:313 - Training step 38330: loss = 2.9156 | 3019.65ms | Tokens/s = 173,625.3
2025-01-16 16:00:03.884 | DEBUG    | __main__:<module>:313 - Training step 38340: loss = 3.0454 | 3021.19ms | Tokens/s = 173,537.0
2025-01-16 16:00:34.080 | DEBUG    | __main__:<module>:313 - Training step 38350: loss = 2.8749 | 3021.04ms | Tokens/s = 173,545.4
2025-01-16 16:01:04.283 | DEBUG    | __main__:<module>:313 - Training step 38360: loss = 3.0727 | 3022.08ms | Tokens/s = 173,485.8
2025-01-16 16:01:34.486 | DEBUG    | __main__:<module>:313 - Training step 38370: loss = 2.9260 | 3019.72ms | Tokens/s = 173,621.1
2025-01-16 16:02:04.666 | DEBUG    | __main__:<module>:313 - Training step 38380: loss = 2.9179 | 3017.75ms | Tokens/s = 173,734.7
2025-01-16 16:02:34.818 | DEBUG    | __main__:<module>:313 - Training step 38390: loss = 3.0339 | 3016.85ms | Tokens/s = 173,786.3
2025-01-16 16:03:05.002 | DEBUG    | __main__:<module>:313 - Training step 38400: loss = 2.9962 | 3017.28ms | Tokens/s = 173,762.0
2025-01-16 16:03:35.206 | DEBUG    | __main__:<module>:313 - Training step 38410: loss = 2.8958 | 3020.09ms | Tokens/s = 173,600.0
2025-01-16 16:04:05.375 | DEBUG    | __main__:<module>:313 - Training step 38420: loss = 2.8217 | 3017.57ms | Tokens/s = 173,745.2
2025-01-16 16:04:35.542 | DEBUG    | __main__:<module>:313 - Training step 38430: loss = 2.7453 | 3019.89ms | Tokens/s = 173,611.6
2025-01-16 16:05:05.728 | DEBUG    | __main__:<module>:313 - Training step 38440: loss = 2.8953 | 3020.92ms | Tokens/s = 173,552.6
2025-01-16 16:05:35.924 | DEBUG    | __main__:<module>:313 - Training step 38450: loss = 2.7649 | 3018.36ms | Tokens/s = 173,699.8
2025-01-16 16:06:06.126 | DEBUG    | __main__:<module>:313 - Training step 38460: loss = 2.8819 | 3018.33ms | Tokens/s = 173,701.2
2025-01-16 16:06:36.332 | DEBUG    | __main__:<module>:313 - Training step 38470: loss = 2.9414 | 3020.43ms | Tokens/s = 173,580.9
2025-01-16 16:07:06.528 | DEBUG    | __main__:<module>:313 - Training step 38480: loss = 3.0406 | 3017.81ms | Tokens/s = 173,731.4
2025-01-16 16:07:36.696 | DEBUG    | __main__:<module>:313 - Training step 38490: loss = 3.0496 | 3015.75ms | Tokens/s = 173,849.9
2025-01-16 16:08:06.834 | DEBUG    | __main__:<module>:313 - Training step 38500: loss = 2.8307 | 3011.78ms | Tokens/s = 174,079.0
2025-01-16 16:08:36.973 | DEBUG    | __main__:<module>:313 - Training step 38510: loss = 2.6290 | 3012.72ms | Tokens/s = 174,025.0
2025-01-16 16:09:07.129 | DEBUG    | __main__:<module>:313 - Training step 38520: loss = 2.8957 | 3019.28ms | Tokens/s = 173,646.8
2025-01-16 16:09:37.312 | DEBUG    | __main__:<module>:313 - Training step 38530: loss = 2.6562 | 3016.94ms | Tokens/s = 173,781.1
2025-01-16 16:10:07.506 | DEBUG    | __main__:<module>:313 - Training step 38540: loss = 2.9346 | 3018.88ms | Tokens/s = 173,669.8
2025-01-16 16:10:37.697 | DEBUG    | __main__:<module>:313 - Training step 38550: loss = 2.9858 | 3018.73ms | Tokens/s = 173,678.1
2025-01-16 16:11:07.855 | DEBUG    | __main__:<module>:313 - Training step 38560: loss = 2.9509 | 3011.98ms | Tokens/s = 174,067.5
2025-01-16 16:11:38.008 | DEBUG    | __main__:<module>:313 - Training step 38570: loss = 2.7259 | 3015.14ms | Tokens/s = 173,885.0
2025-01-16 16:12:08.196 | DEBUG    | __main__:<module>:313 - Training step 38580: loss = 2.9852 | 3020.30ms | Tokens/s = 173,587.9
2025-01-16 16:12:38.405 | DEBUG    | __main__:<module>:313 - Training step 38590: loss = 2.9244 | 3021.73ms | Tokens/s = 173,506.1
2025-01-16 16:13:08.621 | DEBUG    | __main__:<module>:313 - Training step 38600: loss = 2.9040 | 3023.68ms | Tokens/s = 173,394.1
2025-01-16 16:13:38.817 | DEBUG    | __main__:<module>:313 - Training step 38610: loss = 2.8515 | 3019.73ms | Tokens/s = 173,620.7
2025-01-16 16:14:08.989 | DEBUG    | __main__:<module>:313 - Training step 38620: loss = 2.8925 | 3019.68ms | Tokens/s = 173,623.7
2025-01-16 16:14:39.178 | DEBUG    | __main__:<module>:313 - Training step 38630: loss = 2.8996 | 3019.02ms | Tokens/s = 173,661.4
2025-01-16 16:15:09.383 | DEBUG    | __main__:<module>:313 - Training step 38640: loss = 2.8796 | 3021.64ms | Tokens/s = 173,511.1
2025-01-16 16:15:39.605 | DEBUG    | __main__:<module>:313 - Training step 38650: loss = 2.8098 | 3022.62ms | Tokens/s = 173,454.9
2025-01-16 16:16:09.828 | DEBUG    | __main__:<module>:313 - Training step 38660: loss = 2.9855 | 3019.49ms | Tokens/s = 173,634.6
2025-01-16 16:16:40.034 | DEBUG    | __main__:<module>:313 - Training step 38670: loss = 2.7189 | 3021.55ms | Tokens/s = 173,516.5
2025-01-16 16:17:10.253 | DEBUG    | __main__:<module>:313 - Training step 38680: loss = 2.9312 | 3020.76ms | Tokens/s = 173,561.9
2025-01-16 16:17:40.457 | DEBUG    | __main__:<module>:313 - Training step 38690: loss = 2.9716 | 3020.49ms | Tokens/s = 173,576.9
2025-01-16 16:18:10.638 | DEBUG    | __main__:<module>:313 - Training step 38700: loss = 2.9821 | 3016.53ms | Tokens/s = 173,805.2
2025-01-16 16:18:40.804 | DEBUG    | __main__:<module>:313 - Training step 38710: loss = 2.9075 | 3018.12ms | Tokens/s = 173,713.5
2025-01-16 16:19:10.959 | DEBUG    | __main__:<module>:313 - Training step 38720: loss = 3.0050 | 3016.11ms | Tokens/s = 173,829.1
2025-01-16 16:19:41.123 | DEBUG    | __main__:<module>:313 - Training step 38730: loss = 2.7969 | 3018.49ms | Tokens/s = 173,692.1
2025-01-16 16:20:11.304 | DEBUG    | __main__:<module>:313 - Training step 38740: loss = 2.9240 | 3018.23ms | Tokens/s = 173,707.1
2025-01-16 16:20:41.512 | DEBUG    | __main__:<module>:313 - Training step 38750: loss = 2.7946 | 3020.97ms | Tokens/s = 173,549.6
2025-01-16 16:21:11.726 | DEBUG    | __main__:<module>:313 - Training step 38760: loss = 2.7990 | 3020.82ms | Tokens/s = 173,558.5
2025-01-16 16:21:41.904 | DEBUG    | __main__:<module>:313 - Training step 38770: loss = 2.9521 | 3016.02ms | Tokens/s = 173,834.3
2025-01-16 16:22:12.090 | DEBUG    | __main__:<module>:313 - Training step 38780: loss = 3.0880 | 3024.01ms | Tokens/s = 173,375.2
2025-01-16 16:22:42.303 | DEBUG    | __main__:<module>:313 - Training step 38790: loss = 2.8527 | 3022.01ms | Tokens/s = 173,489.6
2025-01-16 16:23:12.498 | DEBUG    | __main__:<module>:313 - Training step 38800: loss = 2.9523 | 3018.96ms | Tokens/s = 173,665.1
2025-01-16 16:23:42.705 | DEBUG    | __main__:<module>:313 - Training step 38810: loss = 2.8945 | 3020.42ms | Tokens/s = 173,580.9
2025-01-16 16:24:12.913 | DEBUG    | __main__:<module>:313 - Training step 38820: loss = 2.8671 | 3019.84ms | Tokens/s = 173,614.4
2025-01-16 16:24:43.088 | DEBUG    | __main__:<module>:313 - Training step 38830: loss = 3.0301 | 3016.19ms | Tokens/s = 173,824.7
2025-01-16 16:25:13.245 | DEBUG    | __main__:<module>:313 - Training step 38840: loss = 2.7878 | 3014.30ms | Tokens/s = 173,933.5
2025-01-16 16:25:43.402 | DEBUG    | __main__:<module>:313 - Training step 38850: loss = 2.9726 | 3015.33ms | Tokens/s = 173,874.4
2025-01-16 16:26:13.597 | DEBUG    | __main__:<module>:313 - Training step 38860: loss = 2.9713 | 3020.27ms | Tokens/s = 173,589.7
2025-01-16 16:26:43.804 | DEBUG    | __main__:<module>:313 - Training step 38870: loss = 3.0412 | 3021.02ms | Tokens/s = 173,546.6
2025-01-16 16:27:13.985 | DEBUG    | __main__:<module>:313 - Training step 38880: loss = 2.9554 | 3016.58ms | Tokens/s = 173,802.3
2025-01-16 16:27:44.151 | DEBUG    | __main__:<module>:313 - Training step 38890: loss = 2.9071 | 3016.29ms | Tokens/s = 173,818.6
2025-01-16 16:28:14.335 | DEBUG    | __main__:<module>:313 - Training step 38900: loss = 2.8783 | 3020.64ms | Tokens/s = 173,568.5
2025-01-16 16:28:44.538 | DEBUG    | __main__:<module>:313 - Training step 38910: loss = 2.9075 | 3018.59ms | Tokens/s = 173,686.4
2025-01-16 16:29:14.724 | DEBUG    | __main__:<module>:313 - Training step 38920: loss = 2.7994 | 3017.08ms | Tokens/s = 173,773.2
2025-01-16 16:29:44.881 | DEBUG    | __main__:<module>:313 - Training step 38930: loss = 2.8490 | 3014.72ms | Tokens/s = 173,909.2
2025-01-16 16:30:15.036 | DEBUG    | __main__:<module>:313 - Training step 38940: loss = 2.9213 | 3019.67ms | Tokens/s = 173,624.1
2025-01-16 16:30:45.221 | DEBUG    | __main__:<module>:313 - Training step 38950: loss = 3.1232 | 3019.91ms | Tokens/s = 173,610.3
2025-01-16 16:31:15.436 | DEBUG    | __main__:<module>:313 - Training step 38960: loss = 2.8596 | 3022.37ms | Tokens/s = 173,469.3
2025-01-16 16:31:45.622 | DEBUG    | __main__:<module>:313 - Training step 38970: loss = 2.8029 | 3018.47ms | Tokens/s = 173,693.4
2025-01-16 16:32:15.815 | DEBUG    | __main__:<module>:313 - Training step 38980: loss = 3.0007 | 3022.33ms | Tokens/s = 173,471.5
2025-01-16 16:32:46.016 | DEBUG    | __main__:<module>:313 - Training step 38990: loss = 3.0343 | 3020.82ms | Tokens/s = 173,558.4
2025-01-16 16:33:19.623 | INFO     | __main__:<module>:265 - Step 39,000/40,000 loss: 2.9249 (T) 2.9198 (V) | lr=1.7e-05
2025-01-16 16:33:19.624 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-16 16:33:33.363 | DEBUG    | __main__:<module>:313 - Training step 39000: loss = 3.0102 | 20189.59ms | Tokens/s = 25,968.2
2025-01-16 16:34:03.387 | DEBUG    | __main__:<module>:313 - Training step 39010: loss = 2.8012 | 3008.01ms | Tokens/s = 174,297.4
2025-01-16 16:34:33.532 | DEBUG    | __main__:<module>:313 - Training step 39020: loss = 2.9630 | 3017.79ms | Tokens/s = 173,732.6
2025-01-16 16:35:03.722 | DEBUG    | __main__:<module>:313 - Training step 39030: loss = 2.8255 | 3020.07ms | Tokens/s = 173,601.2
2025-01-16 16:35:33.936 | DEBUG    | __main__:<module>:313 - Training step 39040: loss = 2.8400 | 3021.87ms | Tokens/s = 173,497.6
2025-01-16 16:36:04.141 | DEBUG    | __main__:<module>:313 - Training step 39050: loss = 2.7700 | 3019.27ms | Tokens/s = 173,647.3
2025-01-16 16:36:34.323 | DEBUG    | __main__:<module>:313 - Training step 39060: loss = 2.9044 | 3017.65ms | Tokens/s = 173,740.3
2025-01-16 16:37:04.523 | DEBUG    | __main__:<module>:313 - Training step 39070: loss = 3.0842 | 3021.73ms | Tokens/s = 173,505.8
2025-01-16 16:37:34.741 | DEBUG    | __main__:<module>:313 - Training step 39080: loss = 2.8336 | 3020.76ms | Tokens/s = 173,561.7
2025-01-16 16:38:04.953 | DEBUG    | __main__:<module>:313 - Training step 39090: loss = 3.0568 | 3021.53ms | Tokens/s = 173,517.2
2025-01-16 16:38:35.148 | DEBUG    | __main__:<module>:313 - Training step 39100: loss = 3.0041 | 3017.20ms | Tokens/s = 173,766.3
2025-01-16 16:39:05.338 | DEBUG    | __main__:<module>:313 - Training step 39110: loss = 3.0955 | 3021.62ms | Tokens/s = 173,512.3
2025-01-16 16:39:35.531 | DEBUG    | __main__:<module>:313 - Training step 39120: loss = 2.7342 | 3016.98ms | Tokens/s = 173,779.0
2025-01-16 16:40:05.710 | DEBUG    | __main__:<module>:313 - Training step 39130: loss = 2.9276 | 3017.91ms | Tokens/s = 173,725.6
2025-01-16 16:40:35.908 | DEBUG    | __main__:<module>:313 - Training step 39140: loss = 2.9589 | 3022.45ms | Tokens/s = 173,464.5
2025-01-16 16:41:06.111 | DEBUG    | __main__:<module>:313 - Training step 39150: loss = 2.8354 | 3018.44ms | Tokens/s = 173,695.1
2025-01-16 16:41:36.287 | DEBUG    | __main__:<module>:313 - Training step 39160: loss = 2.8868 | 3018.21ms | Tokens/s = 173,708.1
2025-01-16 16:42:06.450 | DEBUG    | __main__:<module>:313 - Training step 39170: loss = 2.8794 | 3015.61ms | Tokens/s = 173,857.9
2025-01-16 16:42:36.615 | DEBUG    | __main__:<module>:313 - Training step 39180: loss = 2.9355 | 3017.71ms | Tokens/s = 173,737.3
2025-01-16 16:43:06.815 | DEBUG    | __main__:<module>:313 - Training step 39190: loss = 2.7722 | 3021.36ms | Tokens/s = 173,527.2
2025-01-16 16:43:37.036 | DEBUG    | __main__:<module>:313 - Training step 39200: loss = 2.8191 | 3019.73ms | Tokens/s = 173,620.6
2025-01-16 16:44:07.234 | DEBUG    | __main__:<module>:313 - Training step 39210: loss = 2.8737 | 3019.04ms | Tokens/s = 173,660.6
2025-01-16 16:44:37.411 | DEBUG    | __main__:<module>:313 - Training step 39220: loss = 2.6852 | 3015.23ms | Tokens/s = 173,879.7
2025-01-16 16:45:07.599 | DEBUG    | __main__:<module>:313 - Training step 39230: loss = 2.9716 | 3018.70ms | Tokens/s = 173,680.1
2025-01-16 16:45:37.799 | DEBUG    | __main__:<module>:313 - Training step 39240: loss = 2.8812 | 3022.54ms | Tokens/s = 173,459.3
2025-01-16 16:46:08.017 | DEBUG    | __main__:<module>:313 - Training step 39250: loss = 3.0118 | 3023.30ms | Tokens/s = 173,415.8
2025-01-16 16:46:38.240 | DEBUG    | __main__:<module>:313 - Training step 39260: loss = 2.8452 | 3019.51ms | Tokens/s = 173,633.5
2025-01-16 16:47:08.440 | DEBUG    | __main__:<module>:313 - Training step 39270: loss = 2.7957 | 3018.02ms | Tokens/s = 173,719.3
2025-01-16 16:47:38.623 | DEBUG    | __main__:<module>:313 - Training step 39280: loss = 2.9323 | 3019.85ms | Tokens/s = 173,613.9
2025-01-16 16:48:08.782 | DEBUG    | __main__:<module>:313 - Training step 39290: loss = 2.8906 | 3015.91ms | Tokens/s = 173,840.7
2025-01-16 16:48:38.965 | DEBUG    | __main__:<module>:313 - Training step 39300: loss = 2.8021 | 3020.95ms | Tokens/s = 173,550.9
2025-01-16 16:49:09.172 | DEBUG    | __main__:<module>:313 - Training step 39310: loss = 2.9259 | 3021.64ms | Tokens/s = 173,511.1
2025-01-16 16:49:39.395 | DEBUG    | __main__:<module>:313 - Training step 39320: loss = 3.0412 | 3019.96ms | Tokens/s = 173,607.7
2025-01-16 16:50:09.593 | DEBUG    | __main__:<module>:313 - Training step 39330: loss = 2.7347 | 3016.25ms | Tokens/s = 173,821.3
2025-01-16 16:50:39.766 | DEBUG    | __main__:<module>:313 - Training step 39340: loss = 2.8426 | 3018.34ms | Tokens/s = 173,701.0
2025-01-16 16:51:09.943 | DEBUG    | __main__:<module>:313 - Training step 39350: loss = 2.9362 | 3020.95ms | Tokens/s = 173,550.6
2025-01-16 16:51:40.145 | DEBUG    | __main__:<module>:313 - Training step 39360: loss = 2.8385 | 3020.96ms | Tokens/s = 173,549.9
2025-01-16 16:52:10.337 | DEBUG    | __main__:<module>:313 - Training step 39370: loss = 2.9154 | 3020.86ms | Tokens/s = 173,555.6
2025-01-16 16:52:40.503 | DEBUG    | __main__:<module>:313 - Training step 39380: loss = 2.9125 | 3016.41ms | Tokens/s = 173,811.8
2025-01-16 16:53:10.672 | DEBUG    | __main__:<module>:313 - Training step 39390: loss = 2.9950 | 3018.99ms | Tokens/s = 173,663.4
2025-01-16 16:53:40.867 | DEBUG    | __main__:<module>:313 - Training step 39400: loss = 2.9708 | 3021.11ms | Tokens/s = 173,541.3
2025-01-16 16:54:11.065 | DEBUG    | __main__:<module>:313 - Training step 39410: loss = 2.9386 | 3020.08ms | Tokens/s = 173,600.6
2025-01-16 16:54:41.242 | DEBUG    | __main__:<module>:313 - Training step 39420: loss = 2.9868 | 3014.73ms | Tokens/s = 173,908.7
2025-01-16 16:55:11.406 | DEBUG    | __main__:<module>:313 - Training step 39430: loss = 2.9184 | 3019.85ms | Tokens/s = 173,613.9
2025-01-16 16:55:41.590 | DEBUG    | __main__:<module>:313 - Training step 39440: loss = 2.8026 | 3020.83ms | Tokens/s = 173,557.5
2025-01-16 16:56:11.794 | DEBUG    | __main__:<module>:313 - Training step 39450: loss = 2.9343 | 3021.62ms | Tokens/s = 173,512.4
2025-01-16 16:56:41.987 | DEBUG    | __main__:<module>:313 - Training step 39460: loss = 2.9002 | 3018.69ms | Tokens/s = 173,680.5
2025-01-16 16:57:12.166 | DEBUG    | __main__:<module>:313 - Training step 39470: loss = 2.9600 | 3019.69ms | Tokens/s = 173,623.3
2025-01-16 16:57:42.335 | DEBUG    | __main__:<module>:313 - Training step 39480: loss = 2.8695 | 3015.60ms | Tokens/s = 173,858.9
2025-01-16 16:58:12.517 | DEBUG    | __main__:<module>:313 - Training step 39490: loss = 2.9427 | 3019.41ms | Tokens/s = 173,639.2
2025-01-16 16:58:42.734 | DEBUG    | __main__:<module>:313 - Training step 39500: loss = 2.8996 | 3020.01ms | Tokens/s = 173,604.5
2025-01-16 16:59:12.928 | DEBUG    | __main__:<module>:313 - Training step 39510: loss = 2.9373 | 3019.70ms | Tokens/s = 173,622.4
2025-01-16 16:59:43.096 | DEBUG    | __main__:<module>:313 - Training step 39520: loss = 2.8879 | 3015.22ms | Tokens/s = 173,880.4
2025-01-16 17:00:13.246 | DEBUG    | __main__:<module>:313 - Training step 39530: loss = 2.8928 | 3014.16ms | Tokens/s = 173,941.9
2025-01-16 17:00:43.404 | DEBUG    | __main__:<module>:313 - Training step 39540: loss = 3.0368 | 3018.77ms | Tokens/s = 173,675.7
2025-01-16 17:01:13.596 | DEBUG    | __main__:<module>:313 - Training step 39550: loss = 3.0348 | 3018.64ms | Tokens/s = 173,683.4
2025-01-16 17:01:43.805 | DEBUG    | __main__:<module>:313 - Training step 39560: loss = 2.9039 | 3023.32ms | Tokens/s = 173,414.6
2025-01-16 17:02:13.995 | DEBUG    | __main__:<module>:313 - Training step 39570: loss = 2.7490 | 3017.77ms | Tokens/s = 173,733.7
2025-01-16 17:02:44.162 | DEBUG    | __main__:<module>:313 - Training step 39580: loss = 2.8767 | 3016.68ms | Tokens/s = 173,796.6
2025-01-16 17:03:14.336 | DEBUG    | __main__:<module>:313 - Training step 39590: loss = 2.9638 | 3019.60ms | Tokens/s = 173,628.1
2025-01-16 17:03:44.528 | DEBUG    | __main__:<module>:313 - Training step 39600: loss = 2.8359 | 3019.04ms | Tokens/s = 173,660.6
2025-01-16 17:04:14.701 | DEBUG    | __main__:<module>:313 - Training step 39610: loss = 2.8589 | 3016.79ms | Tokens/s = 173,790.0
2025-01-16 17:04:44.879 | DEBUG    | __main__:<module>:313 - Training step 39620: loss = 2.8142 | 3018.41ms | Tokens/s = 173,696.5
2025-01-16 17:05:15.082 | DEBUG    | __main__:<module>:313 - Training step 39630: loss = 2.9155 | 3022.55ms | Tokens/s = 173,458.6
2025-01-16 17:05:45.280 | DEBUG    | __main__:<module>:313 - Training step 39640: loss = 2.7707 | 3016.61ms | Tokens/s = 173,800.2
2025-01-16 17:06:15.451 | DEBUG    | __main__:<module>:313 - Training step 39650: loss = 2.6756 | 3016.55ms | Tokens/s = 173,803.6
2025-01-16 17:06:45.605 | DEBUG    | __main__:<module>:313 - Training step 39660: loss = 2.8862 | 3011.81ms | Tokens/s = 174,077.3
2025-01-16 17:07:15.753 | DEBUG    | __main__:<module>:313 - Training step 39670: loss = 2.8921 | 3018.12ms | Tokens/s = 173,713.5
2025-01-16 17:07:45.932 | DEBUG    | __main__:<module>:313 - Training step 39680: loss = 2.9294 | 3018.69ms | Tokens/s = 173,680.7
2025-01-16 17:08:16.108 | DEBUG    | __main__:<module>:313 - Training step 39690: loss = 2.8733 | 3015.43ms | Tokens/s = 173,868.5
2025-01-16 17:08:46.271 | DEBUG    | __main__:<module>:313 - Training step 39700: loss = 2.9024 | 3015.12ms | Tokens/s = 173,886.2
2025-01-16 17:09:16.418 | DEBUG    | __main__:<module>:313 - Training step 39710: loss = 2.8419 | 3014.44ms | Tokens/s = 173,925.7
2025-01-16 17:09:46.591 | DEBUG    | __main__:<module>:313 - Training step 39720: loss = 2.8459 | 3018.53ms | Tokens/s = 173,690.0
2025-01-16 17:10:16.791 | DEBUG    | __main__:<module>:313 - Training step 39730: loss = 2.7156 | 3019.70ms | Tokens/s = 173,622.3
2025-01-16 17:10:47.000 | DEBUG    | __main__:<module>:313 - Training step 39740: loss = 2.9134 | 3020.70ms | Tokens/s = 173,565.0
2025-01-16 17:11:17.182 | DEBUG    | __main__:<module>:313 - Training step 39750: loss = 2.9167 | 3017.37ms | Tokens/s = 173,756.5
2025-01-16 17:11:47.344 | DEBUG    | __main__:<module>:313 - Training step 39760: loss = 2.8963 | 3017.65ms | Tokens/s = 173,740.7
2025-01-16 17:12:17.534 | DEBUG    | __main__:<module>:313 - Training step 39770: loss = 3.0689 | 3019.63ms | Tokens/s = 173,626.7
2025-01-16 17:12:47.747 | DEBUG    | __main__:<module>:313 - Training step 39780: loss = 2.9879 | 3021.81ms | Tokens/s = 173,501.1
2025-01-16 17:13:17.943 | DEBUG    | __main__:<module>:313 - Training step 39790: loss = 2.8801 | 3017.36ms | Tokens/s = 173,757.4
2025-01-16 17:13:48.117 | DEBUG    | __main__:<module>:313 - Training step 39800: loss = 3.0632 | 3014.72ms | Tokens/s = 173,909.5
2025-01-16 17:14:18.273 | DEBUG    | __main__:<module>:313 - Training step 39810: loss = 2.8335 | 3016.36ms | Tokens/s = 173,814.7
2025-01-16 17:14:48.466 | DEBUG    | __main__:<module>:313 - Training step 39820: loss = 2.8376 | 3019.84ms | Tokens/s = 173,614.6
2025-01-16 17:15:18.676 | DEBUG    | __main__:<module>:313 - Training step 39830: loss = 3.0215 | 3021.58ms | Tokens/s = 173,514.6
2025-01-16 17:15:48.904 | DEBUG    | __main__:<module>:313 - Training step 39840: loss = 2.9504 | 3022.11ms | Tokens/s = 173,483.9
2025-01-16 17:16:19.109 | DEBUG    | __main__:<module>:313 - Training step 39850: loss = 2.9949 | 3017.13ms | Tokens/s = 173,770.7
2025-01-16 17:16:49.295 | DEBUG    | __main__:<module>:313 - Training step 39860: loss = 2.8722 | 3017.98ms | Tokens/s = 173,721.4
2025-01-16 17:17:19.471 | DEBUG    | __main__:<module>:313 - Training step 39870: loss = 2.9597 | 3020.17ms | Tokens/s = 173,595.3
2025-01-16 17:17:49.674 | DEBUG    | __main__:<module>:313 - Training step 39880: loss = 2.9451 | 3019.79ms | Tokens/s = 173,617.5
2025-01-16 17:18:19.897 | DEBUG    | __main__:<module>:313 - Training step 39890: loss = 2.8806 | 3022.75ms | Tokens/s = 173,447.4
2025-01-16 17:18:50.117 | DEBUG    | __main__:<module>:313 - Training step 39900: loss = 3.0304 | 3019.79ms | Tokens/s = 173,617.6
2025-01-16 17:19:20.309 | DEBUG    | __main__:<module>:313 - Training step 39910: loss = 2.9065 | 3017.85ms | Tokens/s = 173,729.1
2025-01-16 17:19:50.483 | DEBUG    | __main__:<module>:313 - Training step 39920: loss = 3.0145 | 3015.71ms | Tokens/s = 173,852.5
2025-01-16 17:20:20.649 | DEBUG    | __main__:<module>:313 - Training step 39930: loss = 2.9471 | 3018.33ms | Tokens/s = 173,701.6
2025-01-16 17:20:50.834 | DEBUG    | __main__:<module>:313 - Training step 39940: loss = 2.9794 | 3017.20ms | Tokens/s = 173,766.3
2025-01-16 17:21:21.024 | DEBUG    | __main__:<module>:313 - Training step 39950: loss = 2.8979 | 3019.22ms | Tokens/s = 173,650.0
2025-01-16 17:21:51.218 | DEBUG    | __main__:<module>:313 - Training step 39960: loss = 2.9224 | 3019.95ms | Tokens/s = 173,608.4
2025-01-16 17:22:21.402 | DEBUG    | __main__:<module>:313 - Training step 39970: loss = 3.0140 | 3016.31ms | Tokens/s = 173,817.6
2025-01-16 17:22:51.589 | DEBUG    | __main__:<module>:313 - Training step 39980: loss = 3.0208 | 3018.80ms | Tokens/s = 173,674.3
2025-01-16 17:23:21.777 | DEBUG    | __main__:<module>:313 - Training step 39990: loss = 2.8995 | 3018.31ms | Tokens/s = 173,702.5
2025-01-16 17:23:48.825 | INFO     | __main__:<module>:319 - job_name='gpt2-training-124M-2025-01-15-07-42-42' finished in 121260.85s
2025-01-16 17:23:48.825 | INFO     | __main__:<module>:320 - Trained for 40,000 steps total_training_tokens=20,971,520,000 and achieved  best eval loss=2.9197826385498047
2025-01-16 17:23:55.374 | INFO     | __main__:<module>:345 - Loss: 2.9130 (T) 2.9177 (V) | 121270.69469547272s
