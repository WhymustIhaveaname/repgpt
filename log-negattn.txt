2025-01-18 00:28:44.606 | INFO     | __main__:<module>:63 - Got args Namespace(max_steps=20000, eval_interval=1000, eval_steps=100, batch_size=16, gradient_accum=32, model_size='124M', tensorboard=1)
2025-01-18 00:28:44.606 | INFO     | __main__:<module>:88 - Training 124M model with config.n_layer=12, config.n_embd=768config.n_head=12, config.context_size=1024, config.dropout=0.0, config.vocab_size=50304
2025-01-18 00:28:46.843 | DEBUG    | __main__:<module>:138 - ddp_rank=0 ddp_local_rank=0
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:152 - Training data is 9,035,582,489 tokens
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:153 - Evaluation data is 4,434,606 tokens
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:160 - job_name='gpt2-training-124M-2025-01-18-00-28-44'
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:161 - Tokens / step: 524,288
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:162 - Total training tokens: 10,485,760,000
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:163 - Effective batch size with grad accumulation: batch_size * gradient_accumulation_steps=512
2025-01-18 00:28:46.843 | DEBUG    | __main__:<module>:164 - gradient_accumulation_steps_per_gpu=32
2025-01-18 00:28:46.843 | DEBUG    | __main__:<module>:165 - Directories: train_dir='/home/v-youransun/repgpt/input/data/train', eval_dir='/home/v-youransun/repgpt/input/data/eval' model_dir='/home/v-youransun/repgpt/model' log_dir='/home/v-youransun/repgpt/output/tensorboard/nov/gpt2-training-124M-2025-01-18-00-28-44'
2025-01-18 00:28:46.843 | INFO     | __main__:<module>:166 - Loaded dataset 2.2381248474121094
2025-01-18 00:28:50.135 | INFO     | __main__:<module>:200 - OptimizedModule(
  (_orig_mod): DistributedDataParallel(
    (module): GPT2(
      (token_embedding_table): Embedding(50304, 768)
      (position_embedding_table): Embedding(1024, 768)
      (blocks): Sequential(
        (0): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Transformer(
          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (multi_headed_attention): MultiheadedAttention(
            (key): Linear(in_features=768, out_features=768, bias=False)
            (query): Linear(in_features=768, out_features=768, bias=False)
            (value): Linear(in_features=768, out_features=768, bias=False)
            (proj): Linear(in_features=768, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (feed_forward): FeedForward(
            (linear1): Linear(in_features=768, out_features=3072, bias=False)
            (activation): GELU(approximate='none')
            (linear2): Linear(in_features=3072, out_features=768, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (layer_norm_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lm_head): Linear(in_features=768, out_features=50304, bias=False)
    )
  )
)
2025-01-18 00:28:50.135 | INFO     | __main__:<module>:203 - Training model with 123,587,328 parameters for max_steps=20,000 on total_training_tokens=10,485,760,000
2025-01-18 00:28:50.135 | INFO     | __main__:<module>:206 - Decayed parameter tensors: 74, with 124,354,560 parameters
2025-01-18 00:28:50.135 | INFO     | __main__:<module>:207 - Non-decayed parameter tensors: 25, with 19,200 parameters
[rank0]:W0118 00:28:50.154000 526967 torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2025-01-18 00:29:02.431 | INFO     | __main__:<module>:265 - Step 0/20,000 loss: 10.9300 (T) 10.9307 (V) | lr=0.0e+00
2025-01-18 00:29:02.433 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 00:29:17.800 | DEBUG    | __main__:<module>:313 - Training step 0: loss = 10.9348 | 27662.61ms | Tokens/s = 18,952.9
2025-01-18 00:29:46.879 | DEBUG    | __main__:<module>:313 - Training step 10: loss = 9.3947 | 2920.73ms | Tokens/s = 179,505.9
2025-01-18 00:30:16.272 | DEBUG    | __main__:<module>:313 - Training step 20: loss = 8.6846 | 2954.09ms | Tokens/s = 177,478.7
2025-01-18 00:30:45.947 | DEBUG    | __main__:<module>:313 - Training step 30: loss = 7.9378 | 2976.59ms | Tokens/s = 176,137.4
2025-01-18 00:31:15.756 | DEBUG    | __main__:<module>:313 - Training step 40: loss = 7.2859 | 2985.72ms | Tokens/s = 175,598.4
2025-01-18 00:31:45.722 | DEBUG    | __main__:<module>:313 - Training step 50: loss = 6.8285 | 3003.58ms | Tokens/s = 174,554.4
2025-01-18 00:32:15.813 | DEBUG    | __main__:<module>:313 - Training step 60: loss = 6.5684 | 3011.99ms | Tokens/s = 174,067.1
2025-01-18 00:32:45.955 | DEBUG    | __main__:<module>:313 - Training step 70: loss = 6.7155 | 3013.80ms | Tokens/s = 173,962.2
2025-01-18 00:33:16.125 | DEBUG    | __main__:<module>:313 - Training step 80: loss = 6.4189 | 3018.04ms | Tokens/s = 173,718.2
2025-01-18 00:33:46.328 | DEBUG    | __main__:<module>:313 - Training step 90: loss = 6.2406 | 3020.14ms | Tokens/s = 173,597.3
2025-01-18 00:34:16.559 | DEBUG    | __main__:<module>:313 - Training step 100: loss = 6.0449 | 3019.60ms | Tokens/s = 173,628.1
2025-01-18 00:34:46.778 | DEBUG    | __main__:<module>:313 - Training step 110: loss = 6.3097 | 3025.05ms | Tokens/s = 173,315.4
2025-01-18 00:35:17.029 | DEBUG    | __main__:<module>:313 - Training step 120: loss = 6.1720 | 3026.35ms | Tokens/s = 173,240.9
2025-01-18 00:35:47.304 | DEBUG    | __main__:<module>:313 - Training step 130: loss = 5.9718 | 3027.11ms | Tokens/s = 173,197.8
2025-01-18 00:36:17.563 | DEBUG    | __main__:<module>:313 - Training step 140: loss = 5.8899 | 3024.36ms | Tokens/s = 173,354.9
2025-01-18 00:36:47.797 | DEBUG    | __main__:<module>:313 - Training step 150: loss = 5.8043 | 3020.79ms | Tokens/s = 173,559.8
2025-01-18 00:37:18.022 | DEBUG    | __main__:<module>:313 - Training step 160: loss = 5.9043 | 3023.34ms | Tokens/s = 173,413.4
2025-01-18 00:37:48.278 | DEBUG    | __main__:<module>:313 - Training step 170: loss = 5.8775 | 3026.78ms | Tokens/s = 173,216.2
2025-01-18 00:38:18.558 | DEBUG    | __main__:<module>:313 - Training step 180: loss = 5.7683 | 3029.15ms | Tokens/s = 173,080.8
2025-01-18 00:38:48.833 | DEBUG    | __main__:<module>:313 - Training step 190: loss = 5.9386 | 3027.46ms | Tokens/s = 173,177.8
2025-01-18 00:39:19.084 | DEBUG    | __main__:<module>:313 - Training step 200: loss = 5.7240 | 3026.16ms | Tokens/s = 173,252.0
2025-01-18 00:39:49.325 | DEBUG    | __main__:<module>:313 - Training step 210: loss = 5.5983 | 3024.85ms | Tokens/s = 173,327.2
2025-01-18 00:40:19.577 | DEBUG    | __main__:<module>:313 - Training step 220: loss = 5.7317 | 3025.24ms | Tokens/s = 173,304.3
2025-01-18 00:40:49.849 | DEBUG    | __main__:<module>:313 - Training step 230: loss = 5.6520 | 3028.11ms | Tokens/s = 173,140.4
2025-01-18 00:41:20.127 | DEBUG    | __main__:<module>:313 - Training step 240: loss = 5.5113 | 3028.07ms | Tokens/s = 173,142.5
2025-01-18 00:41:50.395 | DEBUG    | __main__:<module>:313 - Training step 250: loss = 5.5146 | 3026.79ms | Tokens/s = 173,215.8
2025-01-18 00:42:20.662 | DEBUG    | __main__:<module>:313 - Training step 260: loss = 5.6489 | 3027.45ms | Tokens/s = 173,177.9
2025-01-18 00:42:50.924 | DEBUG    | __main__:<module>:313 - Training step 270: loss = 5.4731 | 3024.27ms | Tokens/s = 173,360.0
2025-01-18 00:43:21.192 | DEBUG    | __main__:<module>:313 - Training step 280: loss = 5.3521 | 3027.02ms | Tokens/s = 173,202.6
2025-01-18 00:43:51.468 | DEBUG    | __main__:<module>:313 - Training step 290: loss = 5.3693 | 3027.82ms | Tokens/s = 173,157.2
2025-01-18 00:44:21.760 | DEBUG    | __main__:<module>:313 - Training step 300: loss = 5.3586 | 3028.91ms | Tokens/s = 173,094.7
2025-01-18 00:44:52.049 | DEBUG    | __main__:<module>:313 - Training step 310: loss = 5.2349 | 3028.91ms | Tokens/s = 173,094.6
2025-01-18 00:45:22.333 | DEBUG    | __main__:<module>:313 - Training step 320: loss = 5.2413 | 3026.18ms | Tokens/s = 173,250.5
2025-01-18 00:45:52.616 | DEBUG    | __main__:<module>:313 - Training step 330: loss = 5.1795 | 3028.82ms | Tokens/s = 173,099.7
2025-01-18 00:46:22.904 | DEBUG    | __main__:<module>:313 - Training step 340: loss = 5.2185 | 3027.16ms | Tokens/s = 173,194.4
2025-01-18 00:46:53.185 | DEBUG    | __main__:<module>:313 - Training step 350: loss = 5.0355 | 3028.97ms | Tokens/s = 173,091.0
2025-01-18 00:47:23.477 | DEBUG    | __main__:<module>:313 - Training step 360: loss = 5.0311 | 3028.87ms | Tokens/s = 173,096.9
2025-01-18 00:47:53.761 | DEBUG    | __main__:<module>:313 - Training step 370: loss = 5.1385 | 3029.08ms | Tokens/s = 173,085.0
2025-01-18 00:48:24.054 | DEBUG    | __main__:<module>:313 - Training step 380: loss = 4.9719 | 3027.94ms | Tokens/s = 173,150.3
2025-01-18 00:48:54.347 | DEBUG    | __main__:<module>:313 - Training step 390: loss = 5.1636 | 3029.00ms | Tokens/s = 173,089.7
2025-01-18 00:49:24.659 | DEBUG    | __main__:<module>:313 - Training step 400: loss = 5.0273 | 3032.42ms | Tokens/s = 172,894.5
2025-01-18 00:49:54.966 | DEBUG    | __main__:<module>:313 - Training step 410: loss = 4.7260 | 3029.10ms | Tokens/s = 173,084.0
2025-01-18 00:50:25.268 | DEBUG    | __main__:<module>:313 - Training step 420: loss = 4.7261 | 3030.31ms | Tokens/s = 173,014.5
2025-01-18 00:50:55.569 | DEBUG    | __main__:<module>:313 - Training step 430: loss = 4.7839 | 3030.91ms | Tokens/s = 172,980.3
2025-01-18 00:51:25.871 | DEBUG    | __main__:<module>:313 - Training step 440: loss = 4.8518 | 3029.04ms | Tokens/s = 173,087.2
2025-01-18 00:51:56.172 | DEBUG    | __main__:<module>:313 - Training step 450: loss = 4.8183 | 3029.48ms | Tokens/s = 173,062.0
2025-01-18 00:52:26.461 | DEBUG    | __main__:<module>:313 - Training step 460: loss = 4.6987 | 3027.45ms | Tokens/s = 173,178.0
2025-01-18 00:52:56.763 | DEBUG    | __main__:<module>:313 - Training step 470: loss = 4.6252 | 3029.94ms | Tokens/s = 173,035.8
2025-01-18 00:53:27.062 | DEBUG    | __main__:<module>:313 - Training step 480: loss = 4.6217 | 3030.69ms | Tokens/s = 172,993.2
2025-01-18 00:53:57.372 | DEBUG    | __main__:<module>:313 - Training step 490: loss = 4.6323 | 3031.58ms | Tokens/s = 172,942.3
2025-01-18 00:54:27.675 | DEBUG    | __main__:<module>:313 - Training step 500: loss = 4.4105 | 3029.50ms | Tokens/s = 173,061.1
2025-01-18 00:54:57.983 | DEBUG    | __main__:<module>:313 - Training step 510: loss = 4.5422 | 3030.97ms | Tokens/s = 172,976.8
2025-01-18 00:55:28.295 | DEBUG    | __main__:<module>:313 - Training step 520: loss = 4.5255 | 3031.96ms | Tokens/s = 172,920.6
2025-01-18 00:55:58.606 | DEBUG    | __main__:<module>:313 - Training step 530: loss = 4.4575 | 3029.90ms | Tokens/s = 173,038.1
2025-01-18 00:56:28.920 | DEBUG    | __main__:<module>:313 - Training step 540: loss = 4.5650 | 3033.73ms | Tokens/s = 172,819.6
2025-01-18 00:56:59.226 | DEBUG    | __main__:<module>:313 - Training step 550: loss = 4.4070 | 3029.76ms | Tokens/s = 173,046.2
2025-01-18 00:57:29.541 | DEBUG    | __main__:<module>:313 - Training step 560: loss = 4.4046 | 3032.17ms | Tokens/s = 172,908.2
2025-01-18 00:57:59.843 | DEBUG    | __main__:<module>:313 - Training step 570: loss = 4.4725 | 3030.90ms | Tokens/s = 172,980.8
2025-01-18 00:58:30.164 | DEBUG    | __main__:<module>:313 - Training step 580: loss = 4.3530 | 3032.74ms | Tokens/s = 172,876.3
2025-01-18 00:59:00.481 | DEBUG    | __main__:<module>:313 - Training step 590: loss = 4.2821 | 3031.42ms | Tokens/s = 172,951.2
2025-01-18 00:59:30.786 | DEBUG    | __main__:<module>:313 - Training step 600: loss = 4.3701 | 3028.76ms | Tokens/s = 173,103.2
2025-01-18 01:00:01.088 | DEBUG    | __main__:<module>:313 - Training step 610: loss = 4.3842 | 3030.06ms | Tokens/s = 173,029.1
2025-01-18 01:00:31.391 | DEBUG    | __main__:<module>:313 - Training step 620: loss = 4.4193 | 3032.45ms | Tokens/s = 172,892.6
2025-01-18 01:01:01.696 | DEBUG    | __main__:<module>:313 - Training step 630: loss = 4.2621 | 3030.23ms | Tokens/s = 173,019.2
2025-01-18 01:01:32.004 | DEBUG    | __main__:<module>:313 - Training step 640: loss = 4.3847 | 3030.54ms | Tokens/s = 173,001.4
2025-01-18 01:02:02.322 | DEBUG    | __main__:<module>:313 - Training step 650: loss = 4.2090 | 3031.28ms | Tokens/s = 172,959.4
2025-01-18 01:02:32.632 | DEBUG    | __main__:<module>:313 - Training step 660: loss = 4.1363 | 3030.69ms | Tokens/s = 172,993.1
2025-01-18 01:03:02.947 | DEBUG    | __main__:<module>:313 - Training step 670: loss = 4.2326 | 3030.17ms | Tokens/s = 173,022.8
2025-01-18 01:03:33.254 | DEBUG    | __main__:<module>:313 - Training step 680: loss = 4.1811 | 3029.97ms | Tokens/s = 173,034.2
2025-01-18 01:04:03.564 | DEBUG    | __main__:<module>:313 - Training step 690: loss = 4.1206 | 3030.34ms | Tokens/s = 173,012.9
2025-01-18 01:04:33.878 | DEBUG    | __main__:<module>:313 - Training step 700: loss = 4.1884 | 3031.10ms | Tokens/s = 172,969.6
2025-01-18 01:05:04.184 | DEBUG    | __main__:<module>:313 - Training step 710: loss = 4.1556 | 3029.60ms | Tokens/s = 173,055.3
2025-01-18 01:05:34.482 | DEBUG    | __main__:<module>:313 - Training step 720: loss = 4.1328 | 3029.71ms | Tokens/s = 173,049.1
2025-01-18 01:06:04.788 | DEBUG    | __main__:<module>:313 - Training step 730: loss = 4.1333 | 3028.94ms | Tokens/s = 173,092.9
2025-01-18 01:06:35.092 | DEBUG    | __main__:<module>:313 - Training step 740: loss = 4.0971 | 3031.00ms | Tokens/s = 172,975.2
2025-01-18 01:07:05.393 | DEBUG    | __main__:<module>:313 - Training step 750: loss = 4.0677 | 3029.81ms | Tokens/s = 173,043.0
2025-01-18 01:07:35.695 | DEBUG    | __main__:<module>:313 - Training step 760: loss = 4.0599 | 3030.54ms | Tokens/s = 173,001.6
2025-01-18 01:08:06.000 | DEBUG    | __main__:<module>:313 - Training step 770: loss = 4.1030 | 3029.11ms | Tokens/s = 173,083.4
2025-01-18 01:08:36.298 | DEBUG    | __main__:<module>:313 - Training step 780: loss = 4.0358 | 3030.45ms | Tokens/s = 173,006.6
2025-01-18 01:09:06.600 | DEBUG    | __main__:<module>:313 - Training step 790: loss = 4.0906 | 3031.03ms | Tokens/s = 172,973.8
2025-01-18 01:09:36.898 | DEBUG    | __main__:<module>:313 - Training step 800: loss = 4.0213 | 3027.86ms | Tokens/s = 173,154.9
2025-01-18 01:10:07.188 | DEBUG    | __main__:<module>:313 - Training step 810: loss = 4.1291 | 3028.75ms | Tokens/s = 173,103.9
2025-01-18 01:10:37.493 | DEBUG    | __main__:<module>:313 - Training step 820: loss = 4.0655 | 3030.85ms | Tokens/s = 172,983.6
2025-01-18 01:11:07.804 | DEBUG    | __main__:<module>:313 - Training step 830: loss = 4.0917 | 3030.65ms | Tokens/s = 172,995.1
2025-01-18 01:11:38.112 | DEBUG    | __main__:<module>:313 - Training step 840: loss = 4.2620 | 3030.06ms | Tokens/s = 173,028.8
2025-01-18 01:12:08.421 | DEBUG    | __main__:<module>:313 - Training step 850: loss = 4.0648 | 3032.23ms | Tokens/s = 172,904.9
2025-01-18 01:12:38.729 | DEBUG    | __main__:<module>:313 - Training step 860: loss = 3.8010 | 3030.83ms | Tokens/s = 172,984.9
2025-01-18 01:13:09.040 | DEBUG    | __main__:<module>:313 - Training step 870: loss = 4.2595 | 3030.79ms | Tokens/s = 172,987.3
2025-01-18 01:13:39.335 | DEBUG    | __main__:<module>:313 - Training step 880: loss = 3.9522 | 3031.23ms | Tokens/s = 172,961.9
2025-01-18 01:14:09.630 | DEBUG    | __main__:<module>:313 - Training step 890: loss = 3.9552 | 3030.04ms | Tokens/s = 173,029.9
2025-01-18 01:14:39.926 | DEBUG    | __main__:<module>:313 - Training step 900: loss = 3.9570 | 3029.36ms | Tokens/s = 173,069.1
2025-01-18 01:15:10.225 | DEBUG    | __main__:<module>:313 - Training step 910: loss = 3.9950 | 3031.57ms | Tokens/s = 172,943.0
2025-01-18 01:15:40.528 | DEBUG    | __main__:<module>:313 - Training step 920: loss = 3.9199 | 3030.46ms | Tokens/s = 173,005.8
2025-01-18 01:16:10.827 | DEBUG    | __main__:<module>:313 - Training step 930: loss = 4.0244 | 3029.30ms | Tokens/s = 173,072.6
2025-01-18 01:16:41.129 | DEBUG    | __main__:<module>:313 - Training step 940: loss = 3.9700 | 3030.22ms | Tokens/s = 173,019.8
2025-01-18 01:17:11.428 | DEBUG    | __main__:<module>:313 - Training step 950: loss = 3.8174 | 3030.63ms | Tokens/s = 172,996.5
2025-01-18 01:17:41.737 | DEBUG    | __main__:<module>:313 - Training step 960: loss = 3.8825 | 3029.80ms | Tokens/s = 173,043.9
2025-01-18 01:18:12.042 | DEBUG    | __main__:<module>:313 - Training step 970: loss = 3.9894 | 3030.96ms | Tokens/s = 172,977.5
2025-01-18 01:18:42.347 | DEBUG    | __main__:<module>:313 - Training step 980: loss = 3.9842 | 3030.78ms | Tokens/s = 172,987.7
2025-01-18 01:19:12.646 | DEBUG    | __main__:<module>:313 - Training step 990: loss = 3.9502 | 3029.85ms | Tokens/s = 173,040.9
2025-01-18 01:19:46.397 | INFO     | __main__:<module>:265 - Step 1,000/20,000 loss: 3.9253 (T) 3.9154 (V) | lr=5.0e-03
2025-01-18 01:19:46.398 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 01:19:59.551 | DEBUG    | __main__:<module>:313 - Training step 1000: loss = 3.8914 | 19631.06ms | Tokens/s = 26,707.1
2025-01-18 01:20:29.722 | DEBUG    | __main__:<module>:313 - Training step 1010: loss = 3.8395 | 3022.60ms | Tokens/s = 173,455.9
2025-01-18 01:20:59.992 | DEBUG    | __main__:<module>:313 - Training step 1020: loss = 3.9714 | 3030.92ms | Tokens/s = 172,979.6
2025-01-18 01:21:30.268 | DEBUG    | __main__:<module>:313 - Training step 1030: loss = 3.9011 | 3027.68ms | Tokens/s = 173,164.8
2025-01-18 01:22:00.540 | DEBUG    | __main__:<module>:313 - Training step 1040: loss = 3.9358 | 3027.37ms | Tokens/s = 173,182.6
2025-01-18 01:22:30.845 | DEBUG    | __main__:<module>:313 - Training step 1050: loss = 3.8229 | 3030.31ms | Tokens/s = 173,014.5
2025-01-18 01:23:01.150 | DEBUG    | __main__:<module>:313 - Training step 1060: loss = 4.0296 | 3029.00ms | Tokens/s = 173,089.5
2025-01-18 01:23:31.441 | DEBUG    | __main__:<module>:313 - Training step 1070: loss = 3.9054 | 3029.22ms | Tokens/s = 173,076.7
2025-01-18 01:24:01.722 | DEBUG    | __main__:<module>:313 - Training step 1080: loss = 3.9454 | 3027.26ms | Tokens/s = 173,189.2
2025-01-18 01:24:31.999 | DEBUG    | __main__:<module>:313 - Training step 1090: loss = 3.7683 | 3027.02ms | Tokens/s = 173,202.5
2025-01-18 01:25:02.270 | DEBUG    | __main__:<module>:313 - Training step 1100: loss = 3.8050 | 3026.01ms | Tokens/s = 173,260.6
2025-01-18 01:25:32.546 | DEBUG    | __main__:<module>:313 - Training step 1110: loss = 3.9280 | 3027.56ms | Tokens/s = 173,171.5
2025-01-18 01:26:02.847 | DEBUG    | __main__:<module>:313 - Training step 1120: loss = 3.6729 | 3031.16ms | Tokens/s = 172,965.8
2025-01-18 01:26:33.140 | DEBUG    | __main__:<module>:313 - Training step 1130: loss = 3.9893 | 3029.13ms | Tokens/s = 173,082.1
2025-01-18 01:27:03.432 | DEBUG    | __main__:<module>:313 - Training step 1140: loss = 3.9513 | 3027.60ms | Tokens/s = 173,169.7
2025-01-18 01:27:33.713 | DEBUG    | __main__:<module>:313 - Training step 1150: loss = 3.9156 | 3029.29ms | Tokens/s = 173,072.6
2025-01-18 01:28:03.990 | DEBUG    | __main__:<module>:313 - Training step 1160: loss = 3.9900 | 3027.01ms | Tokens/s = 173,203.4
2025-01-18 01:28:34.270 | DEBUG    | __main__:<module>:313 - Training step 1170: loss = 4.0165 | 3029.79ms | Tokens/s = 173,044.2
2025-01-18 01:29:04.550 | DEBUG    | __main__:<module>:313 - Training step 1180: loss = 3.9674 | 3028.73ms | Tokens/s = 173,104.9
2025-01-18 01:29:34.829 | DEBUG    | __main__:<module>:313 - Training step 1190: loss = 3.8403 | 3026.63ms | Tokens/s = 173,225.0
2025-01-18 01:30:05.120 | DEBUG    | __main__:<module>:313 - Training step 1200: loss = 3.7910 | 3028.17ms | Tokens/s = 173,136.9
2025-01-18 01:30:35.409 | DEBUG    | __main__:<module>:313 - Training step 1210: loss = 3.8216 | 3028.44ms | Tokens/s = 173,121.5
2025-01-18 01:31:05.694 | DEBUG    | __main__:<module>:313 - Training step 1220: loss = 3.7043 | 3028.20ms | Tokens/s = 173,135.1
2025-01-18 01:31:36.015 | DEBUG    | __main__:<module>:313 - Training step 1230: loss = 3.7437 | 3033.23ms | Tokens/s = 172,847.9
2025-01-18 01:32:06.337 | DEBUG    | __main__:<module>:313 - Training step 1240: loss = 3.9156 | 3031.77ms | Tokens/s = 172,931.1
2025-01-18 01:32:36.639 | DEBUG    | __main__:<module>:313 - Training step 1250: loss = 3.7865 | 3029.48ms | Tokens/s = 173,061.9
2025-01-18 01:33:06.944 | DEBUG    | __main__:<module>:313 - Training step 1260: loss = 3.8538 | 3029.64ms | Tokens/s = 173,052.7
2025-01-18 01:33:37.249 | DEBUG    | __main__:<module>:313 - Training step 1270: loss = 3.7436 | 3030.54ms | Tokens/s = 173,001.3
2025-01-18 01:34:07.552 | DEBUG    | __main__:<module>:313 - Training step 1280: loss = 3.7337 | 3032.08ms | Tokens/s = 172,913.4
2025-01-18 01:34:37.853 | DEBUG    | __main__:<module>:313 - Training step 1290: loss = 3.8496 | 3030.31ms | Tokens/s = 173,014.6
2025-01-18 01:35:08.146 | DEBUG    | __main__:<module>:313 - Training step 1300: loss = 3.8408 | 3028.78ms | Tokens/s = 173,101.8
2025-01-18 01:35:38.435 | DEBUG    | __main__:<module>:313 - Training step 1310: loss = 3.8804 | 3027.41ms | Tokens/s = 173,180.6
2025-01-18 01:36:08.712 | DEBUG    | __main__:<module>:313 - Training step 1320: loss = 3.7780 | 3026.37ms | Tokens/s = 173,239.8
2025-01-18 01:36:38.990 | DEBUG    | __main__:<module>:313 - Training step 1330: loss = 3.7976 | 3026.63ms | Tokens/s = 173,225.1
2025-01-18 01:37:09.265 | DEBUG    | __main__:<module>:313 - Training step 1340: loss = 3.8191 | 3029.35ms | Tokens/s = 173,069.4
2025-01-18 01:37:39.545 | DEBUG    | __main__:<module>:313 - Training step 1350: loss = 3.6626 | 3028.34ms | Tokens/s = 173,127.0
2025-01-18 01:38:09.828 | DEBUG    | __main__:<module>:313 - Training step 1360: loss = 3.6850 | 3027.34ms | Tokens/s = 173,184.5
2025-01-18 01:38:40.110 | DEBUG    | __main__:<module>:313 - Training step 1370: loss = 3.6737 | 3030.25ms | Tokens/s = 173,017.9
2025-01-18 01:39:10.384 | DEBUG    | __main__:<module>:313 - Training step 1380: loss = 3.8027 | 3026.37ms | Tokens/s = 173,239.6
2025-01-18 01:39:40.672 | DEBUG    | __main__:<module>:313 - Training step 1390: loss = 3.9063 | 3031.91ms | Tokens/s = 172,923.3
2025-01-18 01:40:10.995 | DEBUG    | __main__:<module>:313 - Training step 1400: loss = 3.6921 | 3032.59ms | Tokens/s = 172,884.5
2025-01-18 01:40:41.316 | DEBUG    | __main__:<module>:313 - Training step 1410: loss = 3.7459 | 3031.56ms | Tokens/s = 172,943.4
2025-01-18 01:41:11.617 | DEBUG    | __main__:<module>:313 - Training step 1420: loss = 3.7403 | 3029.27ms | Tokens/s = 173,073.9
2025-01-18 01:41:41.912 | DEBUG    | __main__:<module>:313 - Training step 1430: loss = 3.8358 | 3028.40ms | Tokens/s = 173,123.8
2025-01-18 01:42:12.201 | DEBUG    | __main__:<module>:313 - Training step 1440: loss = 3.6938 | 3028.14ms | Tokens/s = 173,138.4
2025-01-18 01:42:42.481 | DEBUG    | __main__:<module>:313 - Training step 1450: loss = 3.7495 | 3026.71ms | Tokens/s = 173,220.2
2025-01-18 01:43:12.764 | DEBUG    | __main__:<module>:313 - Training step 1460: loss = 3.6061 | 3029.42ms | Tokens/s = 173,065.2
2025-01-18 01:43:43.046 | DEBUG    | __main__:<module>:313 - Training step 1470: loss = 3.7076 | 3027.57ms | Tokens/s = 173,171.3
2025-01-18 01:44:13.320 | DEBUG    | __main__:<module>:313 - Training step 1480: loss = 3.8260 | 3028.84ms | Tokens/s = 173,098.4
2025-01-18 01:44:43.597 | DEBUG    | __main__:<module>:313 - Training step 1490: loss = 3.4605 | 3026.48ms | Tokens/s = 173,233.4
2025-01-18 01:45:13.875 | DEBUG    | __main__:<module>:313 - Training step 1500: loss = 3.6815 | 3025.94ms | Tokens/s = 173,264.5
2025-01-18 01:45:44.150 | DEBUG    | __main__:<module>:313 - Training step 1510: loss = 3.8857 | 3027.45ms | Tokens/s = 173,178.3
2025-01-18 01:46:14.427 | DEBUG    | __main__:<module>:313 - Training step 1520: loss = 3.7501 | 3028.00ms | Tokens/s = 173,146.7
2025-01-18 01:46:44.705 | DEBUG    | __main__:<module>:313 - Training step 1530: loss = 3.7601 | 3028.89ms | Tokens/s = 173,095.6
2025-01-18 01:47:14.983 | DEBUG    | __main__:<module>:313 - Training step 1540: loss = 3.7578 | 3029.34ms | Tokens/s = 173,070.1
2025-01-18 01:47:45.260 | DEBUG    | __main__:<module>:313 - Training step 1550: loss = 3.8919 | 3031.31ms | Tokens/s = 172,957.7
2025-01-18 01:48:15.575 | DEBUG    | __main__:<module>:313 - Training step 1560: loss = 3.5746 | 3030.61ms | Tokens/s = 172,997.7
2025-01-18 01:48:45.882 | DEBUG    | __main__:<module>:313 - Training step 1570: loss = 3.5905 | 3030.76ms | Tokens/s = 172,989.1
2025-01-18 01:49:16.172 | DEBUG    | __main__:<module>:313 - Training step 1580: loss = 3.6331 | 3027.91ms | Tokens/s = 173,152.0
2025-01-18 01:49:46.451 | DEBUG    | __main__:<module>:313 - Training step 1590: loss = 3.6516 | 3027.04ms | Tokens/s = 173,201.6
2025-01-18 01:50:16.727 | DEBUG    | __main__:<module>:313 - Training step 1600: loss = 3.7973 | 3026.43ms | Tokens/s = 173,236.3
2025-01-18 01:50:47.001 | DEBUG    | __main__:<module>:313 - Training step 1610: loss = 3.8382 | 3027.86ms | Tokens/s = 173,154.8
2025-01-18 01:51:17.284 | DEBUG    | __main__:<module>:313 - Training step 1620: loss = 3.7319 | 3028.91ms | Tokens/s = 173,094.7
2025-01-18 01:51:47.575 | DEBUG    | __main__:<module>:313 - Training step 1630: loss = 3.5369 | 3030.21ms | Tokens/s = 173,020.1
2025-01-18 01:52:17.887 | DEBUG    | __main__:<module>:313 - Training step 1640: loss = 3.7696 | 3029.85ms | Tokens/s = 173,040.7
2025-01-18 01:52:48.194 | DEBUG    | __main__:<module>:313 - Training step 1650: loss = 3.6332 | 3031.38ms | Tokens/s = 172,953.7
2025-01-18 01:53:18.488 | DEBUG    | __main__:<module>:313 - Training step 1660: loss = 3.7772 | 3029.16ms | Tokens/s = 173,080.6
2025-01-18 01:53:48.771 | DEBUG    | __main__:<module>:313 - Training step 1670: loss = 3.5880 | 3028.09ms | Tokens/s = 173,141.7
2025-01-18 01:54:19.050 | DEBUG    | __main__:<module>:313 - Training step 1680: loss = 3.5343 | 3027.28ms | Tokens/s = 173,187.8
2025-01-18 01:54:49.333 | DEBUG    | __main__:<module>:313 - Training step 1690: loss = 3.6429 | 3027.42ms | Tokens/s = 173,179.6
2025-01-18 01:55:19.608 | DEBUG    | __main__:<module>:313 - Training step 1700: loss = 3.6624 | 3026.69ms | Tokens/s = 173,221.6
2025-01-18 01:55:49.888 | DEBUG    | __main__:<module>:313 - Training step 1710: loss = 3.3320 | 3026.80ms | Tokens/s = 173,215.6
2025-01-18 01:56:20.162 | DEBUG    | __main__:<module>:313 - Training step 1720: loss = 3.7521 | 3026.85ms | Tokens/s = 173,212.4
2025-01-18 01:56:50.437 | DEBUG    | __main__:<module>:313 - Training step 1730: loss = 3.7696 | 3027.60ms | Tokens/s = 173,169.6
2025-01-18 01:57:20.708 | DEBUG    | __main__:<module>:313 - Training step 1740: loss = 3.7014 | 3026.39ms | Tokens/s = 173,239.0
2025-01-18 01:57:50.981 | DEBUG    | __main__:<module>:313 - Training step 1750: loss = 3.5022 | 3026.10ms | Tokens/s = 173,255.4
2025-01-18 01:58:21.259 | DEBUG    | __main__:<module>:313 - Training step 1760: loss = 3.7178 | 3030.75ms | Tokens/s = 172,989.3
2025-01-18 01:58:51.575 | DEBUG    | __main__:<module>:313 - Training step 1770: loss = 3.6845 | 3031.31ms | Tokens/s = 172,957.3
2025-01-18 01:59:21.882 | DEBUG    | __main__:<module>:313 - Training step 1780: loss = 3.6457 | 3030.52ms | Tokens/s = 173,002.8
2025-01-18 01:59:52.175 | DEBUG    | __main__:<module>:313 - Training step 1790: loss = 3.6492 | 3027.51ms | Tokens/s = 173,174.5
2025-01-18 02:00:22.465 | DEBUG    | __main__:<module>:313 - Training step 1800: loss = 3.7266 | 3028.74ms | Tokens/s = 173,104.1
2025-01-18 02:00:52.749 | DEBUG    | __main__:<module>:313 - Training step 1810: loss = 3.6827 | 3028.04ms | Tokens/s = 173,144.6
2025-01-18 02:01:23.026 | DEBUG    | __main__:<module>:313 - Training step 1820: loss = 3.5903 | 3026.19ms | Tokens/s = 173,249.9
2025-01-18 02:01:53.300 | DEBUG    | __main__:<module>:313 - Training step 1830: loss = 3.5841 | 3028.94ms | Tokens/s = 173,092.7
2025-01-18 02:02:23.574 | DEBUG    | __main__:<module>:313 - Training step 1840: loss = 3.5232 | 3026.53ms | Tokens/s = 173,230.5
2025-01-18 02:02:53.852 | DEBUG    | __main__:<module>:313 - Training step 1850: loss = 3.6229 | 3027.50ms | Tokens/s = 173,175.2
2025-01-18 02:03:24.124 | DEBUG    | __main__:<module>:313 - Training step 1860: loss = 3.4385 | 3028.05ms | Tokens/s = 173,143.7
2025-01-18 02:03:54.388 | DEBUG    | __main__:<module>:313 - Training step 1870: loss = 3.7455 | 3027.11ms | Tokens/s = 173,197.7
2025-01-18 02:04:24.687 | DEBUG    | __main__:<module>:313 - Training step 1880: loss = 3.6106 | 3031.97ms | Tokens/s = 172,920.1
2025-01-18 02:04:55.008 | DEBUG    | __main__:<module>:313 - Training step 1890: loss = 3.6719 | 3031.69ms | Tokens/s = 172,935.9
2025-01-18 02:05:25.311 | DEBUG    | __main__:<module>:313 - Training step 1900: loss = 3.5190 | 3032.01ms | Tokens/s = 172,917.7
2025-01-18 02:05:55.602 | DEBUG    | __main__:<module>:313 - Training step 1910: loss = 3.5916 | 3027.27ms | Tokens/s = 173,188.1
2025-01-18 02:06:25.891 | DEBUG    | __main__:<module>:313 - Training step 1920: loss = 3.8088 | 3028.50ms | Tokens/s = 173,118.3
2025-01-18 02:06:56.172 | DEBUG    | __main__:<module>:313 - Training step 1930: loss = 3.7458 | 3028.29ms | Tokens/s = 173,130.0
2025-01-18 02:07:26.444 | DEBUG    | __main__:<module>:313 - Training step 1940: loss = 3.6418 | 3028.60ms | Tokens/s = 173,112.4
2025-01-18 02:07:56.712 | DEBUG    | __main__:<module>:313 - Training step 1950: loss = 3.6675 | 3027.11ms | Tokens/s = 173,197.5
2025-01-18 02:08:26.973 | DEBUG    | __main__:<module>:313 - Training step 1960: loss = 3.7754 | 3029.47ms | Tokens/s = 173,062.7
2025-01-18 02:08:57.247 | DEBUG    | __main__:<module>:313 - Training step 1970: loss = 3.5886 | 3025.76ms | Tokens/s = 173,274.9
2025-01-18 02:09:27.513 | DEBUG    | __main__:<module>:313 - Training step 1980: loss = 3.6772 | 3027.16ms | Tokens/s = 173,194.7
2025-01-18 02:09:57.784 | DEBUG    | __main__:<module>:313 - Training step 1990: loss = 3.5664 | 3027.91ms | Tokens/s = 173,151.7
2025-01-18 02:10:31.509 | INFO     | __main__:<module>:265 - Step 2,000/20,000 loss: 3.6450 (T) 3.6317 (V) | lr=1.0e-02
2025-01-18 02:10:31.511 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 02:10:45.061 | DEBUG    | __main__:<module>:313 - Training step 2000: loss = 3.6138 | 20024.96ms | Tokens/s = 26,181.7
2025-01-18 02:11:15.223 | DEBUG    | __main__:<module>:313 - Training step 2010: loss = 3.6823 | 3022.40ms | Tokens/s = 173,467.6
2025-01-18 02:11:45.473 | DEBUG    | __main__:<module>:313 - Training step 2020: loss = 3.6481 | 3026.97ms | Tokens/s = 173,205.8
2025-01-18 02:12:15.769 | DEBUG    | __main__:<module>:313 - Training step 2030: loss = 3.4595 | 3030.06ms | Tokens/s = 173,029.0
2025-01-18 02:12:46.078 | DEBUG    | __main__:<module>:313 - Training step 2040: loss = 3.3949 | 3028.90ms | Tokens/s = 173,095.0
2025-01-18 02:13:16.366 | DEBUG    | __main__:<module>:313 - Training step 2050: loss = 3.5746 | 3030.35ms | Tokens/s = 173,012.3
2025-01-18 02:13:46.643 | DEBUG    | __main__:<module>:313 - Training step 2060: loss = 3.4701 | 3029.96ms | Tokens/s = 173,034.4
2025-01-18 02:14:16.950 | DEBUG    | __main__:<module>:313 - Training step 2070: loss = 3.3869 | 3030.09ms | Tokens/s = 173,027.2
2025-01-18 02:14:47.253 | DEBUG    | __main__:<module>:313 - Training step 2080: loss = 3.6063 | 3028.69ms | Tokens/s = 173,107.3
2025-01-18 02:15:17.534 | DEBUG    | __main__:<module>:313 - Training step 2090: loss = 3.7488 | 3027.12ms | Tokens/s = 173,197.1
2025-01-18 02:15:47.811 | DEBUG    | __main__:<module>:313 - Training step 2100: loss = 3.7704 | 3029.21ms | Tokens/s = 173,077.5
2025-01-18 02:16:18.080 | DEBUG    | __main__:<module>:313 - Training step 2110: loss = 3.6502 | 3028.07ms | Tokens/s = 173,142.8
2025-01-18 02:16:48.354 | DEBUG    | __main__:<module>:313 - Training step 2120: loss = 3.5975 | 3027.32ms | Tokens/s = 173,185.8
2025-01-18 02:17:18.625 | DEBUG    | __main__:<module>:313 - Training step 2130: loss = 3.5968 | 3025.95ms | Tokens/s = 173,264.1
2025-01-18 02:17:48.891 | DEBUG    | __main__:<module>:313 - Training step 2140: loss = 3.6734 | 3027.55ms | Tokens/s = 173,172.5
2025-01-18 02:18:19.154 | DEBUG    | __main__:<module>:313 - Training step 2150: loss = 3.6266 | 3024.79ms | Tokens/s = 173,330.3
2025-01-18 02:18:49.424 | DEBUG    | __main__:<module>:313 - Training step 2160: loss = 3.5108 | 3029.56ms | Tokens/s = 173,057.2
2025-01-18 02:19:19.722 | DEBUG    | __main__:<module>:313 - Training step 2170: loss = 3.6727 | 3028.67ms | Tokens/s = 173,108.1
2025-01-18 02:19:50.005 | DEBUG    | __main__:<module>:313 - Training step 2180: loss = 3.5549 | 3026.94ms | Tokens/s = 173,207.1
2025-01-18 02:20:20.275 | DEBUG    | __main__:<module>:313 - Training step 2190: loss = 3.8087 | 3029.94ms | Tokens/s = 173,036.0
2025-01-18 02:20:50.576 | DEBUG    | __main__:<module>:313 - Training step 2200: loss = 3.6613 | 3029.69ms | Tokens/s = 173,049.8
2025-01-18 02:21:20.851 | DEBUG    | __main__:<module>:313 - Training step 2210: loss = 3.7394 | 3026.99ms | Tokens/s = 173,204.2
2025-01-18 02:21:51.125 | DEBUG    | __main__:<module>:313 - Training step 2220: loss = 3.6043 | 3028.41ms | Tokens/s = 173,122.9
2025-01-18 02:22:21.388 | DEBUG    | __main__:<module>:313 - Training step 2230: loss = 3.6259 | 3023.55ms | Tokens/s = 173,401.7
2025-01-18 02:22:51.632 | DEBUG    | __main__:<module>:313 - Training step 2240: loss = 3.6075 | 3024.71ms | Tokens/s = 173,334.7
2025-01-18 02:23:21.863 | DEBUG    | __main__:<module>:313 - Training step 2250: loss = 3.6724 | 3025.47ms | Tokens/s = 173,291.7
2025-01-18 02:23:52.120 | DEBUG    | __main__:<module>:313 - Training step 2260: loss = 3.6485 | 3027.32ms | Tokens/s = 173,185.4
2025-01-18 02:24:22.397 | DEBUG    | __main__:<module>:313 - Training step 2270: loss = 3.6783 | 3028.71ms | Tokens/s = 173,106.0
2025-01-18 02:24:52.698 | DEBUG    | __main__:<module>:313 - Training step 2280: loss = 3.5814 | 3029.51ms | Tokens/s = 173,060.2
2025-01-18 02:25:22.999 | DEBUG    | __main__:<module>:313 - Training step 2290: loss = 3.4986 | 3027.91ms | Tokens/s = 173,152.0
2025-01-18 02:25:53.257 | DEBUG    | __main__:<module>:313 - Training step 2300: loss = 3.5688 | 3023.70ms | Tokens/s = 173,392.7
2025-01-18 02:26:23.506 | DEBUG    | __main__:<module>:313 - Training step 2310: loss = 3.5644 | 3023.74ms | Tokens/s = 173,390.5
2025-01-18 02:26:53.737 | DEBUG    | __main__:<module>:313 - Training step 2320: loss = 3.6459 | 3023.06ms | Tokens/s = 173,429.4
2025-01-18 02:27:23.992 | DEBUG    | __main__:<module>:313 - Training step 2330: loss = 3.6455 | 3026.00ms | Tokens/s = 173,261.0
2025-01-18 02:27:54.240 | DEBUG    | __main__:<module>:313 - Training step 2340: loss = 3.4155 | 3023.17ms | Tokens/s = 173,423.2
2025-01-18 02:28:24.511 | DEBUG    | __main__:<module>:313 - Training step 2350: loss = 3.6465 | 3025.96ms | Tokens/s = 173,263.3
2025-01-18 02:28:54.797 | DEBUG    | __main__:<module>:313 - Training step 2360: loss = 3.4005 | 3027.32ms | Tokens/s = 173,185.5
2025-01-18 02:29:25.064 | DEBUG    | __main__:<module>:313 - Training step 2370: loss = 3.5094 | 3027.01ms | Tokens/s = 173,203.5
2025-01-18 02:29:55.301 | DEBUG    | __main__:<module>:313 - Training step 2380: loss = 3.5057 | 3022.33ms | Tokens/s = 173,471.6
2025-01-18 02:30:25.529 | DEBUG    | __main__:<module>:313 - Training step 2390: loss = 3.6325 | 3025.46ms | Tokens/s = 173,291.9
2025-01-18 02:30:55.782 | DEBUG    | __main__:<module>:313 - Training step 2400: loss = 3.4879 | 3026.24ms | Tokens/s = 173,247.5
2025-01-18 02:31:26.057 | DEBUG    | __main__:<module>:313 - Training step 2410: loss = 3.5745 | 3027.24ms | Tokens/s = 173,190.0
2025-01-18 02:31:56.323 | DEBUG    | __main__:<module>:313 - Training step 2420: loss = 3.4859 | 3023.27ms | Tokens/s = 173,417.7
2025-01-18 02:32:26.561 | DEBUG    | __main__:<module>:313 - Training step 2430: loss = 3.4955 | 3023.59ms | Tokens/s = 173,399.3
2025-01-18 02:32:56.783 | DEBUG    | __main__:<module>:313 - Training step 2440: loss = 3.7325 | 3021.24ms | Tokens/s = 173,534.2
2025-01-18 02:33:27.027 | DEBUG    | __main__:<module>:313 - Training step 2450: loss = 3.5449 | 3026.15ms | Tokens/s = 173,252.5
2025-01-18 02:33:57.300 | DEBUG    | __main__:<module>:313 - Training step 2460: loss = 3.6378 | 3028.56ms | Tokens/s = 173,114.9
2025-01-18 02:34:27.589 | DEBUG    | __main__:<module>:313 - Training step 2470: loss = 3.6710 | 3028.21ms | Tokens/s = 173,134.7
2025-01-18 02:34:57.875 | DEBUG    | __main__:<module>:313 - Training step 2480: loss = 3.5545 | 3027.75ms | Tokens/s = 173,161.0
2025-01-18 02:35:28.139 | DEBUG    | __main__:<module>:313 - Training step 2490: loss = 3.5524 | 3026.83ms | Tokens/s = 173,213.8
2025-01-18 02:35:58.397 | DEBUG    | __main__:<module>:313 - Training step 2500: loss = 3.6644 | 3025.37ms | Tokens/s = 173,297.0
2025-01-18 02:36:28.638 | DEBUG    | __main__:<module>:313 - Training step 2510: loss = 3.6082 | 3021.92ms | Tokens/s = 173,495.0
2025-01-18 02:36:58.903 | DEBUG    | __main__:<module>:313 - Training step 2520: loss = 3.4782 | 3027.07ms | Tokens/s = 173,200.1
2025-01-18 02:37:29.194 | DEBUG    | __main__:<module>:313 - Training step 2530: loss = 3.5684 | 3031.52ms | Tokens/s = 172,945.5
2025-01-18 02:37:59.500 | DEBUG    | __main__:<module>:313 - Training step 2540: loss = 3.5306 | 3027.75ms | Tokens/s = 173,161.2
2025-01-18 02:38:29.773 | DEBUG    | __main__:<module>:313 - Training step 2550: loss = 3.5126 | 3027.94ms | Tokens/s = 173,149.9
2025-01-18 02:39:00.033 | DEBUG    | __main__:<module>:313 - Training step 2560: loss = 3.6551 | 3025.80ms | Tokens/s = 173,272.5
2025-01-18 02:39:30.294 | DEBUG    | __main__:<module>:313 - Training step 2570: loss = 3.5585 | 3026.02ms | Tokens/s = 173,260.1
2025-01-18 02:40:00.542 | DEBUG    | __main__:<module>:313 - Training step 2580: loss = 3.4588 | 3025.00ms | Tokens/s = 173,318.5
2025-01-18 02:40:30.830 | DEBUG    | __main__:<module>:313 - Training step 2590: loss = 3.3135 | 3032.17ms | Tokens/s = 172,908.5
2025-01-18 02:41:01.131 | DEBUG    | __main__:<module>:313 - Training step 2600: loss = 3.5404 | 3027.62ms | Tokens/s = 173,168.3
2025-01-18 02:41:31.416 | DEBUG    | __main__:<module>:313 - Training step 2610: loss = 3.4677 | 3028.92ms | Tokens/s = 173,093.9
2025-01-18 02:42:01.687 | DEBUG    | __main__:<module>:313 - Training step 2620: loss = 3.5765 | 3025.40ms | Tokens/s = 173,295.3
2025-01-18 02:42:31.939 | DEBUG    | __main__:<module>:313 - Training step 2630: loss = 3.6219 | 3024.47ms | Tokens/s = 173,348.5
2025-01-18 02:43:02.214 | DEBUG    | __main__:<module>:313 - Training step 2640: loss = 3.4793 | 3029.78ms | Tokens/s = 173,045.1
2025-01-18 02:43:32.506 | DEBUG    | __main__:<module>:313 - Training step 2650: loss = 3.5272 | 3027.96ms | Tokens/s = 173,148.8
2025-01-18 02:44:02.776 | DEBUG    | __main__:<module>:313 - Training step 2660: loss = 3.5241 | 3028.22ms | Tokens/s = 173,134.3
2025-01-18 02:44:33.068 | DEBUG    | __main__:<module>:313 - Training step 2670: loss = 3.7604 | 3029.16ms | Tokens/s = 173,080.4
2025-01-18 02:45:03.349 | DEBUG    | __main__:<module>:313 - Training step 2680: loss = 3.6632 | 3027.94ms | Tokens/s = 173,150.2
2025-01-18 02:45:33.638 | DEBUG    | __main__:<module>:313 - Training step 2690: loss = 3.6886 | 3029.62ms | Tokens/s = 173,054.0
2025-01-18 02:46:03.906 | DEBUG    | __main__:<module>:313 - Training step 2700: loss = 3.4396 | 3024.70ms | Tokens/s = 173,335.6
2025-01-18 02:46:34.157 | DEBUG    | __main__:<module>:313 - Training step 2710: loss = 3.6523 | 3025.28ms | Tokens/s = 173,302.5
2025-01-18 02:47:04.408 | DEBUG    | __main__:<module>:313 - Training step 2720: loss = 3.4884 | 3026.60ms | Tokens/s = 173,226.6
2025-01-18 02:47:34.656 | DEBUG    | __main__:<module>:313 - Training step 2730: loss = 3.5307 | 3024.88ms | Tokens/s = 173,325.4
2025-01-18 02:48:04.912 | DEBUG    | __main__:<module>:313 - Training step 2740: loss = 3.7312 | 3027.26ms | Tokens/s = 173,189.2
2025-01-18 02:48:35.198 | DEBUG    | __main__:<module>:313 - Training step 2750: loss = 3.6329 | 3028.85ms | Tokens/s = 173,098.2
2025-01-18 02:49:05.482 | DEBUG    | __main__:<module>:313 - Training step 2760: loss = 3.5430 | 3026.69ms | Tokens/s = 173,221.4
2025-01-18 02:49:35.752 | DEBUG    | __main__:<module>:313 - Training step 2770: loss = 3.7157 | 3025.38ms | Tokens/s = 173,296.6
2025-01-18 02:50:06.001 | DEBUG    | __main__:<module>:313 - Training step 2780: loss = 3.9710 | 3024.47ms | Tokens/s = 173,348.7
2025-01-18 02:50:36.263 | DEBUG    | __main__:<module>:313 - Training step 2790: loss = 3.7338 | 3029.04ms | Tokens/s = 173,087.5
2025-01-18 02:51:06.539 | DEBUG    | __main__:<module>:313 - Training step 2800: loss = 3.6798 | 3024.54ms | Tokens/s = 173,344.4
2025-01-18 02:51:36.790 | DEBUG    | __main__:<module>:313 - Training step 2810: loss = 3.7419 | 3022.23ms | Tokens/s = 173,477.0
2025-01-18 02:52:07.049 | DEBUG    | __main__:<module>:313 - Training step 2820: loss = 3.4938 | 3025.79ms | Tokens/s = 173,273.2
2025-01-18 02:52:37.330 | DEBUG    | __main__:<module>:313 - Training step 2830: loss = 3.4964 | 3029.86ms | Tokens/s = 173,040.6
2025-01-18 02:53:07.614 | DEBUG    | __main__:<module>:313 - Training step 2840: loss = 3.4761 | 3028.35ms | Tokens/s = 173,126.7
2025-01-18 02:53:37.877 | DEBUG    | __main__:<module>:313 - Training step 2850: loss = 3.6132 | 3025.85ms | Tokens/s = 173,269.8
2025-01-18 02:54:08.128 | DEBUG    | __main__:<module>:313 - Training step 2860: loss = 3.4747 | 3025.66ms | Tokens/s = 173,280.6
2025-01-18 02:54:38.371 | DEBUG    | __main__:<module>:313 - Training step 2870: loss = 3.5090 | 3023.18ms | Tokens/s = 173,422.6
2025-01-18 02:55:08.610 | DEBUG    | __main__:<module>:313 - Training step 2880: loss = 3.3481 | 3026.29ms | Tokens/s = 173,244.3
2025-01-18 02:55:38.873 | DEBUG    | __main__:<module>:313 - Training step 2890: loss = 3.5154 | 3026.64ms | Tokens/s = 173,224.7
2025-01-18 02:56:09.166 | DEBUG    | __main__:<module>:313 - Training step 2900: loss = 3.4163 | 3030.27ms | Tokens/s = 173,016.8
2025-01-18 02:56:39.475 | DEBUG    | __main__:<module>:313 - Training step 2910: loss = 3.2690 | 3029.30ms | Tokens/s = 173,072.5
2025-01-18 02:57:09.757 | DEBUG    | __main__:<module>:313 - Training step 2920: loss = 3.6150 | 3028.21ms | Tokens/s = 173,134.8
2025-01-18 02:57:40.050 | DEBUG    | __main__:<module>:313 - Training step 2930: loss = 3.6701 | 3026.92ms | Tokens/s = 173,208.5
2025-01-18 02:58:10.319 | DEBUG    | __main__:<module>:313 - Training step 2940: loss = 3.5177 | 3025.01ms | Tokens/s = 173,318.0
2025-01-18 02:58:40.571 | DEBUG    | __main__:<module>:313 - Training step 2950: loss = 3.8722 | 3025.40ms | Tokens/s = 173,295.7
2025-01-18 02:59:10.817 | DEBUG    | __main__:<module>:313 - Training step 2960: loss = 3.5065 | 3024.43ms | Tokens/s = 173,351.2
2025-01-18 02:59:41.091 | DEBUG    | __main__:<module>:313 - Training step 2970: loss = 3.3391 | 3029.01ms | Tokens/s = 173,089.1
2025-01-18 03:00:11.376 | DEBUG    | __main__:<module>:313 - Training step 2980: loss = 3.7881 | 3026.15ms | Tokens/s = 173,252.5
2025-01-18 03:00:41.633 | DEBUG    | __main__:<module>:313 - Training step 2990: loss = 3.7279 | 3026.25ms | Tokens/s = 173,246.7
2025-01-18 03:01:15.354 | INFO     | __main__:<module>:265 - Step 3,000/20,000 loss: 3.6196 (T) 3.6004 (V) | lr=9.9e-03
2025-01-18 03:01:15.356 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 03:01:28.743 | DEBUG    | __main__:<module>:313 - Training step 3000: loss = 3.5389 | 19860.95ms | Tokens/s = 26,397.9
2025-01-18 03:01:58.879 | DEBUG    | __main__:<module>:313 - Training step 3010: loss = 3.5367 | 3017.21ms | Tokens/s = 173,765.9
2025-01-18 03:02:29.112 | DEBUG    | __main__:<module>:313 - Training step 3020: loss = 3.6860 | 3023.98ms | Tokens/s = 173,377.0
2025-01-18 03:02:59.377 | DEBUG    | __main__:<module>:313 - Training step 3030: loss = 3.6169 | 3026.79ms | Tokens/s = 173,215.7
2025-01-18 03:03:29.634 | DEBUG    | __main__:<module>:313 - Training step 3040: loss = 3.4472 | 3024.77ms | Tokens/s = 173,331.4
2025-01-18 03:03:59.873 | DEBUG    | __main__:<module>:313 - Training step 3050: loss = 3.5997 | 3024.57ms | Tokens/s = 173,343.1
2025-01-18 03:04:30.129 | DEBUG    | __main__:<module>:313 - Training step 3060: loss = 3.5959 | 3027.20ms | Tokens/s = 173,192.2
2025-01-18 03:05:00.413 | DEBUG    | __main__:<module>:313 - Training step 3070: loss = 3.4979 | 3026.32ms | Tokens/s = 173,242.8
2025-01-18 03:05:30.679 | DEBUG    | __main__:<module>:313 - Training step 3080: loss = 3.6793 | 3025.03ms | Tokens/s = 173,316.8
2025-01-18 03:06:00.926 | DEBUG    | __main__:<module>:313 - Training step 3090: loss = 3.5807 | 3024.12ms | Tokens/s = 173,368.8
2025-01-18 03:06:31.161 | DEBUG    | __main__:<module>:313 - Training step 3100: loss = 3.5131 | 3021.60ms | Tokens/s = 173,513.5
2025-01-18 03:07:01.392 | DEBUG    | __main__:<module>:313 - Training step 3110: loss = 3.4879 | 3024.40ms | Tokens/s = 173,352.7
2025-01-18 03:07:31.651 | DEBUG    | __main__:<module>:313 - Training step 3120: loss = 3.4948 | 3026.03ms | Tokens/s = 173,259.4
2025-01-18 03:08:01.930 | DEBUG    | __main__:<module>:313 - Training step 3130: loss = 3.4359 | 3027.01ms | Tokens/s = 173,203.3
2025-01-18 03:08:32.194 | DEBUG    | __main__:<module>:313 - Training step 3140: loss = 3.5985 | 3026.36ms | Tokens/s = 173,240.4
2025-01-18 03:09:02.438 | DEBUG    | __main__:<module>:313 - Training step 3150: loss = 3.5065 | 3024.06ms | Tokens/s = 173,372.4
2025-01-18 03:09:32.671 | DEBUG    | __main__:<module>:313 - Training step 3160: loss = 3.6176 | 3022.65ms | Tokens/s = 173,452.9
2025-01-18 03:10:02.910 | DEBUG    | __main__:<module>:313 - Training step 3170: loss = 3.4944 | 3025.60ms | Tokens/s = 173,284.2
2025-01-18 03:10:33.172 | DEBUG    | __main__:<module>:313 - Training step 3180: loss = 3.6421 | 3026.58ms | Tokens/s = 173,227.8
2025-01-18 03:11:03.421 | DEBUG    | __main__:<module>:313 - Training step 3190: loss = 3.4427 | 3024.99ms | Tokens/s = 173,318.7
2025-01-18 03:11:33.654 | DEBUG    | __main__:<module>:313 - Training step 3200: loss = 3.4400 | 3022.96ms | Tokens/s = 173,435.3
2025-01-18 03:12:03.879 | DEBUG    | __main__:<module>:313 - Training step 3210: loss = 3.3387 | 3022.91ms | Tokens/s = 173,438.0
2025-01-18 03:12:34.134 | DEBUG    | __main__:<module>:313 - Training step 3220: loss = 3.5083 | 3027.80ms | Tokens/s = 173,158.2
2025-01-18 03:13:04.380 | DEBUG    | __main__:<module>:313 - Training step 3230: loss = 3.5469 | 3026.41ms | Tokens/s = 173,237.7
2025-01-18 03:13:34.626 | DEBUG    | __main__:<module>:313 - Training step 3240: loss = 3.4481 | 3026.75ms | Tokens/s = 173,218.3
2025-01-18 03:14:04.903 | DEBUG    | __main__:<module>:313 - Training step 3250: loss = 3.3493 | 3028.63ms | Tokens/s = 173,110.5
2025-01-18 03:14:35.161 | DEBUG    | __main__:<module>:313 - Training step 3260: loss = 3.5318 | 3026.33ms | Tokens/s = 173,242.2
2025-01-18 03:15:05.406 | DEBUG    | __main__:<module>:313 - Training step 3270: loss = 3.5403 | 3023.91ms | Tokens/s = 173,380.8
2025-01-18 03:15:35.644 | DEBUG    | __main__:<module>:313 - Training step 3280: loss = 3.5809 | 3024.26ms | Tokens/s = 173,361.0
2025-01-18 03:16:05.906 | DEBUG    | __main__:<module>:313 - Training step 3290: loss = 3.6473 | 3026.03ms | Tokens/s = 173,259.5
2025-01-18 03:16:36.188 | DEBUG    | __main__:<module>:313 - Training step 3300: loss = 3.5746 | 3025.39ms | Tokens/s = 173,295.7
2025-01-18 03:17:06.458 | DEBUG    | __main__:<module>:313 - Training step 3310: loss = 3.5284 | 3025.61ms | Tokens/s = 173,283.6
2025-01-18 03:17:36.718 | DEBUG    | __main__:<module>:313 - Training step 3320: loss = 3.6046 | 3026.80ms | Tokens/s = 173,215.3
2025-01-18 03:18:06.998 | DEBUG    | __main__:<module>:313 - Training step 3330: loss = 3.5341 | 3028.07ms | Tokens/s = 173,142.5
2025-01-18 03:18:37.281 | DEBUG    | __main__:<module>:313 - Training step 3340: loss = 3.5340 | 3028.51ms | Tokens/s = 173,117.7
2025-01-18 03:19:07.554 | DEBUG    | __main__:<module>:313 - Training step 3350: loss = 3.3768 | 3025.01ms | Tokens/s = 173,317.5
2025-01-18 03:19:37.808 | DEBUG    | __main__:<module>:313 - Training step 3360: loss = 3.4740 | 3023.76ms | Tokens/s = 173,389.7
2025-01-18 03:20:08.040 | DEBUG    | __main__:<module>:313 - Training step 3370: loss = 3.3370 | 3021.15ms | Tokens/s = 173,539.2
2025-01-18 03:20:38.262 | DEBUG    | __main__:<module>:313 - Training step 3380: loss = 3.4005 | 3022.04ms | Tokens/s = 173,488.3
2025-01-18 03:21:08.492 | DEBUG    | __main__:<module>:313 - Training step 3390: loss = 3.6917 | 3022.55ms | Tokens/s = 173,458.6
2025-01-18 03:21:38.726 | DEBUG    | __main__:<module>:313 - Training step 3400: loss = 3.4357 | 3025.08ms | Tokens/s = 173,314.0
2025-01-18 03:22:08.994 | DEBUG    | __main__:<module>:313 - Training step 3410: loss = 3.5210 | 3026.18ms | Tokens/s = 173,250.5
2025-01-18 03:22:39.253 | DEBUG    | __main__:<module>:313 - Training step 3420: loss = 3.5991 | 3024.28ms | Tokens/s = 173,359.8
2025-01-18 03:23:09.505 | DEBUG    | __main__:<module>:313 - Training step 3430: loss = 3.6148 | 3025.75ms | Tokens/s = 173,275.6
2025-01-18 03:23:39.761 | DEBUG    | __main__:<module>:313 - Training step 3440: loss = 3.2831 | 3026.61ms | Tokens/s = 173,226.0
2025-01-18 03:24:10.044 | DEBUG    | __main__:<module>:313 - Training step 3450: loss = 3.5867 | 3030.03ms | Tokens/s = 173,030.7
2025-01-18 03:24:40.332 | DEBUG    | __main__:<module>:313 - Training step 3460: loss = 3.3297 | 3026.68ms | Tokens/s = 173,222.4
2025-01-18 03:25:10.604 | DEBUG    | __main__:<module>:313 - Training step 3470: loss = 3.5077 | 3026.55ms | Tokens/s = 173,229.3
2025-01-18 03:25:40.863 | DEBUG    | __main__:<module>:313 - Training step 3480: loss = 3.3363 | 3026.86ms | Tokens/s = 173,211.7
2025-01-18 03:26:11.119 | DEBUG    | __main__:<module>:313 - Training step 3490: loss = 3.3048 | 3025.17ms | Tokens/s = 173,308.9
2025-01-18 03:26:41.387 | DEBUG    | __main__:<module>:313 - Training step 3500: loss = 3.6265 | 3027.53ms | Tokens/s = 173,173.4
2025-01-18 03:27:11.686 | DEBUG    | __main__:<module>:313 - Training step 3510: loss = 3.5307 | 3029.34ms | Tokens/s = 173,070.0
2025-01-18 03:27:41.976 | DEBUG    | __main__:<module>:313 - Training step 3520: loss = 3.3748 | 3028.52ms | Tokens/s = 173,116.7
2025-01-18 03:28:12.243 | DEBUG    | __main__:<module>:313 - Training step 3530: loss = 3.4405 | 3025.66ms | Tokens/s = 173,280.5
2025-01-18 03:28:42.496 | DEBUG    | __main__:<module>:313 - Training step 3540: loss = 3.2977 | 3023.22ms | Tokens/s = 173,420.6
2025-01-18 03:29:12.737 | DEBUG    | __main__:<module>:313 - Training step 3550: loss = 3.4130 | 3023.60ms | Tokens/s = 173,398.8
2025-01-18 03:29:42.972 | DEBUG    | __main__:<module>:313 - Training step 3560: loss = 3.5197 | 3023.29ms | Tokens/s = 173,416.6
2025-01-18 03:30:13.204 | DEBUG    | __main__:<module>:313 - Training step 3570: loss = 3.4190 | 3022.15ms | Tokens/s = 173,481.5
2025-01-18 03:30:43.456 | DEBUG    | __main__:<module>:313 - Training step 3580: loss = 3.3466 | 3027.02ms | Tokens/s = 173,202.6
2025-01-18 03:31:13.736 | DEBUG    | __main__:<module>:313 - Training step 3590: loss = 3.5657 | 3029.73ms | Tokens/s = 173,047.7
2025-01-18 03:31:44.040 | DEBUG    | __main__:<module>:313 - Training step 3600: loss = 3.6159 | 3029.98ms | Tokens/s = 173,033.5
2025-01-18 03:32:14.331 | DEBUG    | __main__:<module>:313 - Training step 3610: loss = 3.3667 | 3028.86ms | Tokens/s = 173,097.4
2025-01-18 03:32:44.602 | DEBUG    | __main__:<module>:313 - Training step 3620: loss = 3.4508 | 3026.40ms | Tokens/s = 173,238.3
2025-01-18 03:33:14.859 | DEBUG    | __main__:<module>:313 - Training step 3630: loss = 3.6276 | 3025.46ms | Tokens/s = 173,291.8
2025-01-18 03:33:45.105 | DEBUG    | __main__:<module>:313 - Training step 3640: loss = 3.4641 | 3022.44ms | Tokens/s = 173,465.0
2025-01-18 03:34:15.368 | DEBUG    | __main__:<module>:313 - Training step 3650: loss = 3.4937 | 3029.90ms | Tokens/s = 173,037.9
2025-01-18 03:34:45.661 | DEBUG    | __main__:<module>:313 - Training step 3660: loss = 3.4091 | 3030.80ms | Tokens/s = 172,986.6
2025-01-18 03:35:15.954 | DEBUG    | __main__:<module>:313 - Training step 3670: loss = 3.4470 | 3027.05ms | Tokens/s = 173,201.2
2025-01-18 03:35:46.214 | DEBUG    | __main__:<module>:313 - Training step 3680: loss = 3.5304 | 3025.93ms | Tokens/s = 173,265.2
2025-01-18 03:36:16.459 | DEBUG    | __main__:<module>:313 - Training step 3690: loss = 3.3787 | 3023.33ms | Tokens/s = 173,414.1
2025-01-18 03:36:46.698 | DEBUG    | __main__:<module>:313 - Training step 3700: loss = 3.3884 | 3021.43ms | Tokens/s = 173,523.2
2025-01-18 03:37:16.931 | DEBUG    | __main__:<module>:313 - Training step 3710: loss = 3.6153 | 3024.50ms | Tokens/s = 173,346.8
2025-01-18 03:37:47.207 | DEBUG    | __main__:<module>:313 - Training step 3720: loss = 3.3300 | 3027.85ms | Tokens/s = 173,155.2
2025-01-18 03:38:17.499 | DEBUG    | __main__:<module>:313 - Training step 3730: loss = 3.3677 | 3030.93ms | Tokens/s = 172,979.2
2025-01-18 03:38:47.778 | DEBUG    | __main__:<module>:313 - Training step 3740: loss = 3.4396 | 3024.56ms | Tokens/s = 173,343.7
2025-01-18 03:39:18.034 | DEBUG    | __main__:<module>:313 - Training step 3750: loss = 3.6277 | 3025.92ms | Tokens/s = 173,265.4
2025-01-18 03:39:48.279 | DEBUG    | __main__:<module>:313 - Training step 3760: loss = 3.5157 | 3024.29ms | Tokens/s = 173,359.0
2025-01-18 03:40:18.510 | DEBUG    | __main__:<module>:313 - Training step 3770: loss = 3.5076 | 3021.01ms | Tokens/s = 173,547.0
2025-01-18 03:40:48.743 | DEBUG    | __main__:<module>:313 - Training step 3780: loss = 3.5623 | 3026.49ms | Tokens/s = 173,232.9
2025-01-18 03:41:19.002 | DEBUG    | __main__:<module>:313 - Training step 3790: loss = 3.4846 | 3026.80ms | Tokens/s = 173,215.4
2025-01-18 03:41:49.287 | DEBUG    | __main__:<module>:313 - Training step 3800: loss = 3.5133 | 3027.69ms | Tokens/s = 173,164.3
2025-01-18 03:42:19.589 | DEBUG    | __main__:<module>:313 - Training step 3810: loss = 3.4950 | 3028.88ms | Tokens/s = 173,096.1
2025-01-18 03:42:49.866 | DEBUG    | __main__:<module>:313 - Training step 3820: loss = 3.3831 | 3026.22ms | Tokens/s = 173,248.7
2025-01-18 03:43:20.130 | DEBUG    | __main__:<module>:313 - Training step 3830: loss = 3.3072 | 3025.72ms | Tokens/s = 173,277.4
2025-01-18 03:43:50.379 | DEBUG    | __main__:<module>:313 - Training step 3840: loss = 3.3672 | 3026.16ms | Tokens/s = 173,251.7
2025-01-18 03:44:20.627 | DEBUG    | __main__:<module>:313 - Training step 3850: loss = 3.5022 | 3025.32ms | Tokens/s = 173,300.1
2025-01-18 03:44:50.901 | DEBUG    | __main__:<module>:313 - Training step 3860: loss = 3.6307 | 3028.60ms | Tokens/s = 173,112.4
2025-01-18 03:45:21.199 | DEBUG    | __main__:<module>:313 - Training step 3870: loss = 3.4817 | 3029.47ms | Tokens/s = 173,062.4
2025-01-18 03:45:51.478 | DEBUG    | __main__:<module>:313 - Training step 3880: loss = 3.5582 | 3027.20ms | Tokens/s = 173,192.3
2025-01-18 03:46:21.742 | DEBUG    | __main__:<module>:313 - Training step 3890: loss = 3.4950 | 3026.36ms | Tokens/s = 173,240.3
2025-01-18 03:46:51.991 | DEBUG    | __main__:<module>:313 - Training step 3900: loss = 3.6350 | 3023.66ms | Tokens/s = 173,395.4
2025-01-18 03:47:22.235 | DEBUG    | __main__:<module>:313 - Training step 3910: loss = 3.3116 | 3025.26ms | Tokens/s = 173,303.3
2025-01-18 03:47:52.505 | DEBUG    | __main__:<module>:313 - Training step 3920: loss = 3.5183 | 3028.65ms | Tokens/s = 173,109.2
2025-01-18 03:48:22.778 | DEBUG    | __main__:<module>:313 - Training step 3930: loss = 3.3143 | 3026.72ms | Tokens/s = 173,219.7
2025-01-18 03:48:53.036 | DEBUG    | __main__:<module>:313 - Training step 3940: loss = 3.4868 | 3023.19ms | Tokens/s = 173,421.9
2025-01-18 03:49:23.294 | DEBUG    | __main__:<module>:313 - Training step 3950: loss = 3.5038 | 3026.76ms | Tokens/s = 173,217.7
2025-01-18 03:49:53.571 | DEBUG    | __main__:<module>:313 - Training step 3960: loss = 3.4657 | 3026.97ms | Tokens/s = 173,205.3
2025-01-18 03:50:23.839 | DEBUG    | __main__:<module>:313 - Training step 3970: loss = 3.4243 | 3025.78ms | Tokens/s = 173,273.7
2025-01-18 03:50:54.102 | DEBUG    | __main__:<module>:313 - Training step 3980: loss = 3.3834 | 3027.47ms | Tokens/s = 173,177.2
2025-01-18 03:51:24.384 | DEBUG    | __main__:<module>:313 - Training step 3990: loss = 3.4148 | 3028.26ms | Tokens/s = 173,131.7
2025-01-18 03:51:58.099 | INFO     | __main__:<module>:265 - Step 4,000/20,000 loss: 3.4489 (T) 3.4844 (V) | lr=9.7e-03
2025-01-18 03:51:58.101 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 03:52:13.307 | DEBUG    | __main__:<module>:313 - Training step 4000: loss = 3.3599 | 21671.94ms | Tokens/s = 24,192.0
2025-01-18 03:52:43.399 | DEBUG    | __main__:<module>:313 - Training step 4010: loss = 3.4650 | 3016.82ms | Tokens/s = 173,788.2
2025-01-18 03:53:13.600 | DEBUG    | __main__:<module>:313 - Training step 4020: loss = 3.5659 | 3023.57ms | Tokens/s = 173,400.2
2025-01-18 03:53:43.852 | DEBUG    | __main__:<module>:313 - Training step 4030: loss = 3.4264 | 3027.13ms | Tokens/s = 173,196.7
2025-01-18 03:54:14.123 | DEBUG    | __main__:<module>:313 - Training step 4040: loss = 3.3390 | 3027.90ms | Tokens/s = 173,152.5
2025-01-18 03:54:44.397 | DEBUG    | __main__:<module>:313 - Training step 4050: loss = 3.3892 | 3025.52ms | Tokens/s = 173,288.3
2025-01-18 03:55:14.647 | DEBUG    | __main__:<module>:313 - Training step 4060: loss = 3.4275 | 3025.14ms | Tokens/s = 173,310.1
2025-01-18 03:55:44.883 | DEBUG    | __main__:<module>:313 - Training step 4070: loss = 3.4520 | 3025.36ms | Tokens/s = 173,297.8
2025-01-18 03:56:15.151 | DEBUG    | __main__:<module>:313 - Training step 4080: loss = 3.3277 | 3027.58ms | Tokens/s = 173,170.9
2025-01-18 03:56:45.440 | DEBUG    | __main__:<module>:313 - Training step 4090: loss = 3.4242 | 3029.64ms | Tokens/s = 173,053.0
2025-01-18 03:57:15.718 | DEBUG    | __main__:<module>:313 - Training step 4100: loss = 3.4513 | 3024.48ms | Tokens/s = 173,348.1
2025-01-18 03:57:45.974 | DEBUG    | __main__:<module>:313 - Training step 4110: loss = 3.5562 | 3023.46ms | Tokens/s = 173,406.9
2025-01-18 03:58:16.240 | DEBUG    | __main__:<module>:313 - Training step 4120: loss = 3.6015 | 3027.62ms | Tokens/s = 173,168.4
2025-01-18 03:58:46.539 | DEBUG    | __main__:<module>:313 - Training step 4130: loss = 3.3679 | 3033.14ms | Tokens/s = 172,853.0
2025-01-18 03:59:16.830 | DEBUG    | __main__:<module>:313 - Training step 4140: loss = 3.5289 | 3026.62ms | Tokens/s = 173,225.4
2025-01-18 03:59:47.095 | DEBUG    | __main__:<module>:313 - Training step 4150: loss = 3.4094 | 3026.14ms | Tokens/s = 173,253.0
2025-01-18 04:00:17.360 | DEBUG    | __main__:<module>:313 - Training step 4160: loss = 3.5518 | 3026.00ms | Tokens/s = 173,261.3
2025-01-18 04:00:47.636 | DEBUG    | __main__:<module>:313 - Training step 4170: loss = 3.5460 | 3027.71ms | Tokens/s = 173,163.3
2025-01-18 04:01:17.901 | DEBUG    | __main__:<module>:313 - Training step 4180: loss = 3.3707 | 3025.95ms | Tokens/s = 173,263.7
2025-01-18 04:01:48.148 | DEBUG    | __main__:<module>:313 - Training step 4190: loss = 3.4537 | 3026.21ms | Tokens/s = 173,248.8
2025-01-18 04:02:18.421 | DEBUG    | __main__:<module>:313 - Training step 4200: loss = 3.4698 | 3029.37ms | Tokens/s = 173,068.3
2025-01-18 04:02:48.693 | DEBUG    | __main__:<module>:313 - Training step 4210: loss = 3.3047 | 3029.18ms | Tokens/s = 173,079.3
2025-01-18 04:03:18.979 | DEBUG    | __main__:<module>:313 - Training step 4220: loss = 3.5127 | 3028.42ms | Tokens/s = 173,122.4
2025-01-18 04:03:49.241 | DEBUG    | __main__:<module>:313 - Training step 4230: loss = 3.3809 | 3023.81ms | Tokens/s = 173,386.3
2025-01-18 04:04:19.475 | DEBUG    | __main__:<module>:313 - Training step 4240: loss = 3.5339 | 3025.68ms | Tokens/s = 173,279.5
2025-01-18 04:04:49.730 | DEBUG    | __main__:<module>:313 - Training step 4250: loss = 3.5040 | 3025.68ms | Tokens/s = 173,279.7
2025-01-18 04:05:20.001 | DEBUG    | __main__:<module>:313 - Training step 4260: loss = 3.3417 | 3029.32ms | Tokens/s = 173,071.4
2025-01-18 04:05:50.291 | DEBUG    | __main__:<module>:313 - Training step 4270: loss = 3.6465 | 3027.40ms | Tokens/s = 173,180.8
2025-01-18 04:06:20.560 | DEBUG    | __main__:<module>:313 - Training step 4280: loss = 3.5983 | 3025.21ms | Tokens/s = 173,306.2
2025-01-18 04:06:50.795 | DEBUG    | __main__:<module>:313 - Training step 4290: loss = 3.4725 | 3024.57ms | Tokens/s = 173,343.0
2025-01-18 04:07:21.035 | DEBUG    | __main__:<module>:313 - Training step 4300: loss = 3.3474 | 3022.55ms | Tokens/s = 173,458.9
2025-01-18 04:07:51.302 | DEBUG    | __main__:<module>:313 - Training step 4310: loss = 3.6181 | 3029.81ms | Tokens/s = 173,043.3
2025-01-18 04:08:21.575 | DEBUG    | __main__:<module>:313 - Training step 4320: loss = 3.6088 | 3025.55ms | Tokens/s = 173,287.0
2025-01-18 04:08:51.813 | DEBUG    | __main__:<module>:313 - Training step 4330: loss = 3.3710 | 3024.60ms | Tokens/s = 173,341.0
2025-01-18 04:09:22.075 | DEBUG    | __main__:<module>:313 - Training step 4340: loss = 3.3626 | 3027.43ms | Tokens/s = 173,179.4
2025-01-18 04:09:52.361 | DEBUG    | __main__:<module>:313 - Training step 4350: loss = 3.4951 | 3029.17ms | Tokens/s = 173,079.5
2025-01-18 04:10:22.660 | DEBUG    | __main__:<module>:313 - Training step 4360: loss = 3.4505 | 3028.38ms | Tokens/s = 173,125.0
2025-01-18 04:10:52.929 | DEBUG    | __main__:<module>:313 - Training step 4370: loss = 3.4101 | 3026.29ms | Tokens/s = 173,244.7
2025-01-18 04:11:23.209 | DEBUG    | __main__:<module>:313 - Training step 4380: loss = 3.5851 | 3029.88ms | Tokens/s = 173,039.0
2025-01-18 04:11:53.489 | DEBUG    | __main__:<module>:313 - Training step 4390: loss = 3.4492 | 3025.73ms | Tokens/s = 173,276.7
2025-01-18 04:12:23.756 | DEBUG    | __main__:<module>:313 - Training step 4400: loss = 3.4412 | 3026.49ms | Tokens/s = 173,232.9
2025-01-18 04:12:54.015 | DEBUG    | __main__:<module>:313 - Training step 4410: loss = 3.4080 | 3025.53ms | Tokens/s = 173,288.2
2025-01-18 04:13:24.254 | DEBUG    | __main__:<module>:313 - Training step 4420: loss = 3.6238 | 3025.63ms | Tokens/s = 173,282.2
2025-01-18 04:13:54.488 | DEBUG    | __main__:<module>:313 - Training step 4430: loss = 3.3904 | 3022.28ms | Tokens/s = 173,474.4
2025-01-18 04:14:24.714 | DEBUG    | __main__:<module>:313 - Training step 4440: loss = 3.4599 | 3024.26ms | Tokens/s = 173,360.6
2025-01-18 04:14:54.945 | DEBUG    | __main__:<module>:313 - Training step 4450: loss = 3.4644 | 3023.84ms | Tokens/s = 173,384.9
2025-01-18 04:15:25.200 | DEBUG    | __main__:<module>:313 - Training step 4460: loss = 3.4094 | 3026.09ms | Tokens/s = 173,256.2
2025-01-18 04:15:55.488 | DEBUG    | __main__:<module>:313 - Training step 4470: loss = 3.4186 | 3027.97ms | Tokens/s = 173,148.3
2025-01-18 04:16:25.759 | DEBUG    | __main__:<module>:313 - Training step 4480: loss = 3.5282 | 3028.10ms | Tokens/s = 173,141.2
2025-01-18 04:16:56.015 | DEBUG    | __main__:<module>:313 - Training step 4490: loss = 3.4734 | 3026.32ms | Tokens/s = 173,242.9
2025-01-18 04:17:26.267 | DEBUG    | __main__:<module>:313 - Training step 4500: loss = 3.6104 | 3022.89ms | Tokens/s = 173,439.5
2025-01-18 04:17:56.547 | DEBUG    | __main__:<module>:313 - Training step 4510: loss = 3.3315 | 3028.92ms | Tokens/s = 173,093.8
2025-01-18 04:18:26.850 | DEBUG    | __main__:<module>:313 - Training step 4520: loss = 3.5180 | 3028.34ms | Tokens/s = 173,127.2
2025-01-18 04:18:57.126 | DEBUG    | __main__:<module>:313 - Training step 4530: loss = 3.5422 | 3027.24ms | Tokens/s = 173,190.3
2025-01-18 04:19:27.411 | DEBUG    | __main__:<module>:313 - Training step 4540: loss = 3.2594 | 3029.74ms | Tokens/s = 173,047.2
2025-01-18 04:19:57.714 | DEBUG    | __main__:<module>:313 - Training step 4550: loss = 3.5013 | 3030.46ms | Tokens/s = 173,006.2
2025-01-18 04:20:27.988 | DEBUG    | __main__:<module>:313 - Training step 4560: loss = 3.3062 | 3029.52ms | Tokens/s = 173,059.7
2025-01-18 04:20:58.241 | DEBUG    | __main__:<module>:313 - Training step 4570: loss = 3.3949 | 3024.77ms | Tokens/s = 173,331.7
2025-01-18 04:21:28.523 | DEBUG    | __main__:<module>:313 - Training step 4580: loss = 3.4019 | 3029.31ms | Tokens/s = 173,071.5
2025-01-18 04:21:58.824 | DEBUG    | __main__:<module>:313 - Training step 4590: loss = 3.3356 | 3029.34ms | Tokens/s = 173,070.2
2025-01-18 04:22:29.123 | DEBUG    | __main__:<module>:313 - Training step 4600: loss = 3.3280 | 3029.36ms | Tokens/s = 173,069.0
2025-01-18 04:22:59.417 | DEBUG    | __main__:<module>:313 - Training step 4610: loss = 3.5374 | 3026.88ms | Tokens/s = 173,210.7
2025-01-18 04:23:29.697 | DEBUG    | __main__:<module>:313 - Training step 4620: loss = 3.4275 | 3027.68ms | Tokens/s = 173,165.0
2025-01-18 04:23:59.968 | DEBUG    | __main__:<module>:313 - Training step 4630: loss = 3.5746 | 3027.53ms | Tokens/s = 173,173.6
2025-01-18 04:24:30.218 | DEBUG    | __main__:<module>:313 - Training step 4640: loss = 3.5602 | 3023.01ms | Tokens/s = 173,432.3
2025-01-18 04:25:00.474 | DEBUG    | __main__:<module>:313 - Training step 4650: loss = 3.3747 | 3025.43ms | Tokens/s = 173,293.7
2025-01-18 04:25:30.764 | DEBUG    | __main__:<module>:313 - Training step 4660: loss = 3.3909 | 3028.99ms | Tokens/s = 173,090.0
2025-01-18 04:26:01.040 | DEBUG    | __main__:<module>:313 - Training step 4670: loss = 3.4116 | 3027.61ms | Tokens/s = 173,168.8
2025-01-18 04:26:31.295 | DEBUG    | __main__:<module>:313 - Training step 4680: loss = 3.4510 | 3023.73ms | Tokens/s = 173,391.2
2025-01-18 04:27:01.561 | DEBUG    | __main__:<module>:313 - Training step 4690: loss = 3.6144 | 3029.07ms | Tokens/s = 173,085.3
2025-01-18 04:27:31.858 | DEBUG    | __main__:<module>:313 - Training step 4700: loss = 3.4955 | 3028.00ms | Tokens/s = 173,146.4
2025-01-18 04:28:02.144 | DEBUG    | __main__:<module>:313 - Training step 4710: loss = 3.1593 | 3027.78ms | Tokens/s = 173,159.5
2025-01-18 04:28:32.416 | DEBUG    | __main__:<module>:313 - Training step 4720: loss = 3.4952 | 3025.87ms | Tokens/s = 173,268.3
2025-01-18 04:29:02.677 | DEBUG    | __main__:<module>:313 - Training step 4730: loss = 3.2665 | 3025.65ms | Tokens/s = 173,281.1
2025-01-18 04:29:32.929 | DEBUG    | __main__:<module>:313 - Training step 4740: loss = 3.3848 | 3023.71ms | Tokens/s = 173,392.5
2025-01-18 04:30:03.174 | DEBUG    | __main__:<module>:313 - Training step 4750: loss = 3.5541 | 3025.23ms | Tokens/s = 173,305.1
2025-01-18 04:30:33.414 | DEBUG    | __main__:<module>:313 - Training step 4760: loss = 3.4014 | 3026.42ms | Tokens/s = 173,236.8
2025-01-18 04:31:03.681 | DEBUG    | __main__:<module>:313 - Training step 4770: loss = 3.5956 | 3029.43ms | Tokens/s = 173,065.1
2025-01-18 04:31:33.961 | DEBUG    | __main__:<module>:313 - Training step 4780: loss = 3.6976 | 3026.50ms | Tokens/s = 173,232.3
2025-01-18 04:32:04.243 | DEBUG    | __main__:<module>:313 - Training step 4790: loss = 3.3752 | 3028.66ms | Tokens/s = 173,108.8
2025-01-18 04:32:34.525 | DEBUG    | __main__:<module>:313 - Training step 4800: loss = 3.5237 | 3028.55ms | Tokens/s = 173,115.3
2025-01-18 04:33:04.785 | DEBUG    | __main__:<module>:313 - Training step 4810: loss = 3.5429 | 3025.88ms | Tokens/s = 173,268.1
2025-01-18 04:33:35.029 | DEBUG    | __main__:<module>:313 - Training step 4820: loss = 3.4784 | 3022.51ms | Tokens/s = 173,460.9
2025-01-18 04:34:05.269 | DEBUG    | __main__:<module>:313 - Training step 4830: loss = 3.4081 | 3024.29ms | Tokens/s = 173,359.1
2025-01-18 04:34:35.501 | DEBUG    | __main__:<module>:313 - Training step 4840: loss = 3.4627 | 3021.98ms | Tokens/s = 173,491.3
2025-01-18 04:35:05.745 | DEBUG    | __main__:<module>:313 - Training step 4850: loss = 3.3938 | 3026.18ms | Tokens/s = 173,250.5
2025-01-18 04:35:36.015 | DEBUG    | __main__:<module>:313 - Training step 4860: loss = 3.2724 | 3026.65ms | Tokens/s = 173,223.8
2025-01-18 04:36:06.277 | DEBUG    | __main__:<module>:313 - Training step 4870: loss = 3.4163 | 3027.59ms | Tokens/s = 173,169.9
2025-01-18 04:36:36.565 | DEBUG    | __main__:<module>:313 - Training step 4880: loss = 3.3519 | 3028.51ms | Tokens/s = 173,117.3
2025-01-18 04:37:06.836 | DEBUG    | __main__:<module>:313 - Training step 4890: loss = 3.3384 | 3028.26ms | Tokens/s = 173,132.0
2025-01-18 04:37:37.132 | DEBUG    | __main__:<module>:313 - Training step 4900: loss = 3.5362 | 3030.28ms | Tokens/s = 173,016.3
2025-01-18 04:38:07.428 | DEBUG    | __main__:<module>:313 - Training step 4910: loss = 3.5891 | 3026.26ms | Tokens/s = 173,246.0
2025-01-18 04:38:37.716 | DEBUG    | __main__:<module>:313 - Training step 4920: loss = 3.4326 | 3027.34ms | Tokens/s = 173,184.6
2025-01-18 04:39:07.989 | DEBUG    | __main__:<module>:313 - Training step 4930: loss = 3.4645 | 3027.85ms | Tokens/s = 173,155.1
2025-01-18 04:39:38.251 | DEBUG    | __main__:<module>:313 - Training step 4940: loss = 3.3556 | 3026.50ms | Tokens/s = 173,232.4
2025-01-18 04:40:08.497 | DEBUG    | __main__:<module>:313 - Training step 4950: loss = 3.3556 | 3026.25ms | Tokens/s = 173,247.0
2025-01-18 04:40:38.776 | DEBUG    | __main__:<module>:313 - Training step 4960: loss = 3.4831 | 3029.41ms | Tokens/s = 173,066.2
2025-01-18 04:41:09.053 | DEBUG    | __main__:<module>:313 - Training step 4970: loss = 3.4154 | 3026.49ms | Tokens/s = 173,233.0
2025-01-18 04:41:39.317 | DEBUG    | __main__:<module>:313 - Training step 4980: loss = 3.5428 | 3025.83ms | Tokens/s = 173,270.8
2025-01-18 04:42:09.570 | DEBUG    | __main__:<module>:313 - Training step 4990: loss = 3.4408 | 3024.28ms | Tokens/s = 173,359.5
2025-01-18 04:42:43.259 | INFO     | __main__:<module>:265 - Step 5,000/20,000 loss: 3.4280 (T) 3.4509 (V) | lr=9.3e-03
2025-01-18 04:42:43.260 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 04:42:56.582 | DEBUG    | __main__:<module>:313 - Training step 5000: loss = 3.2940 | 19786.22ms | Tokens/s = 26,497.6
2025-01-18 04:43:26.709 | DEBUG    | __main__:<module>:313 - Training step 5010: loss = 3.3620 | 3023.15ms | Tokens/s = 173,424.4
2025-01-18 04:43:56.959 | DEBUG    | __main__:<module>:313 - Training step 5020: loss = 3.5323 | 3027.71ms | Tokens/s = 173,163.5
2025-01-18 04:44:27.238 | DEBUG    | __main__:<module>:313 - Training step 5030: loss = 3.3834 | 3027.13ms | Tokens/s = 173,196.1
2025-01-18 04:44:57.537 | DEBUG    | __main__:<module>:313 - Training step 5040: loss = 3.4347 | 3031.31ms | Tokens/s = 172,957.7
2025-01-18 04:45:27.830 | DEBUG    | __main__:<module>:313 - Training step 5050: loss = 3.5337 | 3029.28ms | Tokens/s = 173,073.5
2025-01-18 04:45:58.094 | DEBUG    | __main__:<module>:313 - Training step 5060: loss = 3.3871 | 3025.17ms | Tokens/s = 173,308.6
2025-01-18 04:46:28.344 | DEBUG    | __main__:<module>:313 - Training step 5070: loss = 3.4378 | 3023.81ms | Tokens/s = 173,386.5
2025-01-18 04:46:58.607 | DEBUG    | __main__:<module>:313 - Training step 5080: loss = 3.5616 | 3027.21ms | Tokens/s = 173,191.7
2025-01-18 04:47:28.905 | DEBUG    | __main__:<module>:313 - Training step 5090: loss = 3.4613 | 3031.77ms | Tokens/s = 172,931.3
2025-01-18 04:47:59.198 | DEBUG    | __main__:<module>:313 - Training step 5100: loss = 3.6539 | 3027.10ms | Tokens/s = 173,198.4
2025-01-18 04:48:29.475 | DEBUG    | __main__:<module>:313 - Training step 5110: loss = 3.3829 | 3027.19ms | Tokens/s = 173,193.1
2025-01-18 04:48:59.739 | DEBUG    | __main__:<module>:313 - Training step 5120: loss = 3.5667 | 3026.47ms | Tokens/s = 173,234.2
2025-01-18 04:49:29.996 | DEBUG    | __main__:<module>:313 - Training step 5130: loss = 3.3938 | 3025.01ms | Tokens/s = 173,317.8
2025-01-18 04:50:00.258 | DEBUG    | __main__:<module>:313 - Training step 5140: loss = 3.6223 | 3029.10ms | Tokens/s = 173,083.6
2025-01-18 04:50:30.551 | DEBUG    | __main__:<module>:313 - Training step 5150: loss = 3.3892 | 3029.84ms | Tokens/s = 173,041.5
2025-01-18 04:51:00.856 | DEBUG    | __main__:<module>:313 - Training step 5160: loss = 3.4819 | 3029.25ms | Tokens/s = 173,075.1
2025-01-18 04:51:31.142 | DEBUG    | __main__:<module>:313 - Training step 5170: loss = 3.4264 | 3029.16ms | Tokens/s = 173,080.6
2025-01-18 04:52:01.407 | DEBUG    | __main__:<module>:313 - Training step 5180: loss = 3.5246 | 3024.84ms | Tokens/s = 173,327.6
2025-01-18 04:52:31.663 | DEBUG    | __main__:<module>:313 - Training step 5190: loss = 3.4516 | 3025.10ms | Tokens/s = 173,312.5
2025-01-18 04:53:01.916 | DEBUG    | __main__:<module>:313 - Training step 5200: loss = 3.3738 | 3024.00ms | Tokens/s = 173,375.4
2025-01-18 04:53:32.176 | DEBUG    | __main__:<module>:313 - Training step 5210: loss = 3.4624 | 3028.55ms | Tokens/s = 173,115.2
2025-01-18 04:54:02.462 | DEBUG    | __main__:<module>:313 - Training step 5220: loss = 3.5080 | 3025.81ms | Tokens/s = 173,272.2
2025-01-18 04:54:32.734 | DEBUG    | __main__:<module>:313 - Training step 5230: loss = 3.2784 | 3024.57ms | Tokens/s = 173,343.0
2025-01-18 04:55:03.006 | DEBUG    | __main__:<module>:313 - Training step 5240: loss = 3.5746 | 3027.80ms | Tokens/s = 173,158.3
2025-01-18 04:55:33.267 | DEBUG    | __main__:<module>:313 - Training step 5250: loss = 3.4119 | 3026.15ms | Tokens/s = 173,252.6
2025-01-18 04:56:03.522 | DEBUG    | __main__:<module>:313 - Training step 5260: loss = 3.5850 | 3026.33ms | Tokens/s = 173,242.0
2025-01-18 04:56:33.765 | DEBUG    | __main__:<module>:313 - Training step 5270: loss = 3.3743 | 3022.13ms | Tokens/s = 173,482.7
2025-01-18 04:57:04.006 | DEBUG    | __main__:<module>:313 - Training step 5280: loss = 3.4489 | 3026.69ms | Tokens/s = 173,221.7
2025-01-18 04:57:34.283 | DEBUG    | __main__:<module>:313 - Training step 5290: loss = 3.4102 | 3028.17ms | Tokens/s = 173,136.7
2025-01-18 04:58:04.571 | DEBUG    | __main__:<module>:313 - Training step 5300: loss = 3.3830 | 3024.68ms | Tokens/s = 173,336.9
2025-01-18 04:58:34.834 | DEBUG    | __main__:<module>:313 - Training step 5310: loss = 3.5867 | 3024.51ms | Tokens/s = 173,346.3
2025-01-18 04:59:05.088 | DEBUG    | __main__:<module>:313 - Training step 5320: loss = 3.4329 | 3024.08ms | Tokens/s = 173,371.0
2025-01-18 04:59:35.355 | DEBUG    | __main__:<module>:313 - Training step 5330: loss = 3.4595 | 3026.55ms | Tokens/s = 173,229.8
2025-01-18 05:00:05.637 | DEBUG    | __main__:<module>:313 - Training step 5340: loss = 3.4545 | 3025.84ms | Tokens/s = 173,270.5
2025-01-18 05:00:35.894 | DEBUG    | __main__:<module>:313 - Training step 5350: loss = 3.3544 | 3023.67ms | Tokens/s = 173,394.8
2025-01-18 05:01:06.151 | DEBUG    | __main__:<module>:313 - Training step 5360: loss = 3.4392 | 3026.19ms | Tokens/s = 173,250.2
2025-01-18 05:01:36.428 | DEBUG    | __main__:<module>:313 - Training step 5370: loss = 3.5088 | 3028.70ms | Tokens/s = 173,106.5
2025-01-18 05:02:06.707 | DEBUG    | __main__:<module>:313 - Training step 5380: loss = 3.4210 | 3026.57ms | Tokens/s = 173,228.3
2025-01-18 05:02:36.962 | DEBUG    | __main__:<module>:313 - Training step 5390: loss = 3.4923 | 3024.39ms | Tokens/s = 173,353.5
2025-01-18 05:03:07.219 | DEBUG    | __main__:<module>:313 - Training step 5400: loss = 3.4549 | 3025.12ms | Tokens/s = 173,311.2
2025-01-18 05:03:37.468 | DEBUG    | __main__:<module>:313 - Training step 5410: loss = 3.6286 | 3024.57ms | Tokens/s = 173,342.7
2025-01-18 05:04:07.710 | DEBUG    | __main__:<module>:313 - Training step 5420: loss = 3.5154 | 3025.80ms | Tokens/s = 173,272.2
2025-01-18 05:04:37.962 | DEBUG    | __main__:<module>:313 - Training step 5430: loss = 3.2995 | 3028.57ms | Tokens/s = 173,114.2
2025-01-18 05:05:08.260 | DEBUG    | __main__:<module>:313 - Training step 5440: loss = 3.4818 | 3030.33ms | Tokens/s = 173,013.3
2025-01-18 05:05:38.549 | DEBUG    | __main__:<module>:313 - Training step 5450: loss = 3.4543 | 3028.19ms | Tokens/s = 173,135.6
2025-01-18 05:06:08.812 | DEBUG    | __main__:<module>:313 - Training step 5460: loss = 3.5032 | 3024.75ms | Tokens/s = 173,332.6
2025-01-18 05:06:39.066 | DEBUG    | __main__:<module>:313 - Training step 5470: loss = 3.5643 | 3025.89ms | Tokens/s = 173,267.3
2025-01-18 05:07:09.315 | DEBUG    | __main__:<module>:313 - Training step 5480: loss = 3.3999 | 3025.28ms | Tokens/s = 173,302.5
2025-01-18 05:07:39.559 | DEBUG    | __main__:<module>:313 - Training step 5490: loss = 3.5021 | 3026.59ms | Tokens/s = 173,227.5
2025-01-18 05:08:09.805 | DEBUG    | __main__:<module>:313 - Training step 5500: loss = 3.3876 | 3023.67ms | Tokens/s = 173,394.7
2025-01-18 05:08:40.048 | DEBUG    | __main__:<module>:313 - Training step 5510: loss = 3.4251 | 3024.25ms | Tokens/s = 173,361.2
2025-01-18 05:09:10.299 | DEBUG    | __main__:<module>:313 - Training step 5520: loss = 3.5700 | 3025.92ms | Tokens/s = 173,265.5
2025-01-18 05:09:40.583 | DEBUG    | __main__:<module>:313 - Training step 5530: loss = 3.4852 | 3030.11ms | Tokens/s = 173,026.2
2025-01-18 05:10:10.873 | DEBUG    | __main__:<module>:313 - Training step 5540: loss = 3.4467 | 3027.74ms | Tokens/s = 173,161.4
2025-01-18 05:10:41.140 | DEBUG    | __main__:<module>:313 - Training step 5550: loss = 3.3758 | 3024.15ms | Tokens/s = 173,367.0
2025-01-18 05:11:11.402 | DEBUG    | __main__:<module>:313 - Training step 5560: loss = 3.2353 | 3027.78ms | Tokens/s = 173,159.3
2025-01-18 05:11:41.657 | DEBUG    | __main__:<module>:313 - Training step 5570: loss = 3.1434 | 3024.97ms | Tokens/s = 173,320.2
2025-01-18 05:12:11.911 | DEBUG    | __main__:<module>:313 - Training step 5580: loss = 3.4512 | 3025.27ms | Tokens/s = 173,302.9
2025-01-18 05:12:42.153 | DEBUG    | __main__:<module>:313 - Training step 5590: loss = 3.3884 | 3024.77ms | Tokens/s = 173,331.3
2025-01-18 05:13:12.406 | DEBUG    | __main__:<module>:313 - Training step 5600: loss = 3.3669 | 3026.01ms | Tokens/s = 173,260.2
2025-01-18 05:13:42.711 | DEBUG    | __main__:<module>:313 - Training step 5610: loss = 3.4365 | 3030.70ms | Tokens/s = 172,992.5
2025-01-18 05:14:13.023 | DEBUG    | __main__:<module>:313 - Training step 5620: loss = 3.5112 | 3030.02ms | Tokens/s = 173,031.1
2025-01-18 05:14:43.313 | DEBUG    | __main__:<module>:313 - Training step 5630: loss = 3.3735 | 3026.79ms | Tokens/s = 173,215.9
2025-01-18 05:15:13.581 | DEBUG    | __main__:<module>:313 - Training step 5640: loss = 3.4600 | 3025.51ms | Tokens/s = 173,289.2
2025-01-18 05:15:43.844 | DEBUG    | __main__:<module>:313 - Training step 5650: loss = 3.4080 | 3025.00ms | Tokens/s = 173,318.2
2025-01-18 05:16:14.097 | DEBUG    | __main__:<module>:313 - Training step 5660: loss = 3.4013 | 3026.29ms | Tokens/s = 173,244.3
2025-01-18 05:16:44.384 | DEBUG    | __main__:<module>:313 - Training step 5670: loss = 3.4183 | 3029.99ms | Tokens/s = 173,033.1
2025-01-18 05:17:14.699 | DEBUG    | __main__:<module>:313 - Training step 5680: loss = 3.3457 | 3031.35ms | Tokens/s = 172,955.3
2025-01-18 05:17:45.003 | DEBUG    | __main__:<module>:313 - Training step 5690: loss = 3.3625 | 3028.72ms | Tokens/s = 173,105.4
2025-01-18 05:18:15.288 | DEBUG    | __main__:<module>:313 - Training step 5700: loss = 3.4698 | 3026.83ms | Tokens/s = 173,213.8
2025-01-18 05:18:45.550 | DEBUG    | __main__:<module>:313 - Training step 5710: loss = 3.4740 | 3026.96ms | Tokens/s = 173,206.3
2025-01-18 05:19:15.805 | DEBUG    | __main__:<module>:313 - Training step 5720: loss = 3.4718 | 3026.40ms | Tokens/s = 173,238.5
2025-01-18 05:19:46.093 | DEBUG    | __main__:<module>:313 - Training step 5730: loss = 3.5614 | 3030.79ms | Tokens/s = 172,987.4
2025-01-18 05:20:16.384 | DEBUG    | __main__:<module>:313 - Training step 5740: loss = 3.4247 | 3027.02ms | Tokens/s = 173,202.6
2025-01-18 05:20:46.646 | DEBUG    | __main__:<module>:313 - Training step 5750: loss = 3.5246 | 3024.26ms | Tokens/s = 173,360.7
2025-01-18 05:21:16.893 | DEBUG    | __main__:<module>:313 - Training step 5760: loss = 3.4202 | 3024.29ms | Tokens/s = 173,358.9
2025-01-18 05:21:47.159 | DEBUG    | __main__:<module>:313 - Training step 5770: loss = 3.3384 | 3026.21ms | Tokens/s = 173,249.1
2025-01-18 05:22:17.412 | DEBUG    | __main__:<module>:313 - Training step 5780: loss = 3.5361 | 3024.43ms | Tokens/s = 173,350.8
2025-01-18 05:22:47.657 | DEBUG    | __main__:<module>:313 - Training step 5790: loss = 3.5428 | 3026.33ms | Tokens/s = 173,242.0
2025-01-18 05:23:17.897 | DEBUG    | __main__:<module>:313 - Training step 5800: loss = 3.3337 | 3023.99ms | Tokens/s = 173,376.2
2025-01-18 05:23:48.167 | DEBUG    | __main__:<module>:313 - Training step 5810: loss = 3.4637 | 3027.81ms | Tokens/s = 173,157.4
2025-01-18 05:24:18.469 | DEBUG    | __main__:<module>:313 - Training step 5820: loss = 3.3980 | 3029.57ms | Tokens/s = 173,057.0
2025-01-18 05:24:48.751 | DEBUG    | __main__:<module>:313 - Training step 5830: loss = 3.4750 | 3029.92ms | Tokens/s = 173,036.8
2025-01-18 05:25:19.056 | DEBUG    | __main__:<module>:313 - Training step 5840: loss = 3.5004 | 3032.03ms | Tokens/s = 172,916.3
2025-01-18 05:25:49.342 | DEBUG    | __main__:<module>:313 - Training step 5850: loss = 3.3082 | 3028.65ms | Tokens/s = 173,109.4
2025-01-18 05:26:19.618 | DEBUG    | __main__:<module>:313 - Training step 5860: loss = 3.3289 | 3025.93ms | Tokens/s = 173,265.0
2025-01-18 05:26:49.880 | DEBUG    | __main__:<module>:313 - Training step 5870: loss = 3.4425 | 3026.56ms | Tokens/s = 173,228.8
2025-01-18 05:27:20.173 | DEBUG    | __main__:<module>:313 - Training step 5880: loss = 3.4337 | 3029.36ms | Tokens/s = 173,068.8
2025-01-18 05:27:50.470 | DEBUG    | __main__:<module>:313 - Training step 5890: loss = 3.3566 | 3028.47ms | Tokens/s = 173,119.8
2025-01-18 05:28:20.741 | DEBUG    | __main__:<module>:313 - Training step 5900: loss = 3.2580 | 3025.85ms | Tokens/s = 173,269.6
2025-01-18 05:28:50.999 | DEBUG    | __main__:<module>:313 - Training step 5910: loss = 3.4992 | 3025.81ms | Tokens/s = 173,272.0
2025-01-18 05:29:21.246 | DEBUG    | __main__:<module>:313 - Training step 5920: loss = 3.6626 | 3024.97ms | Tokens/s = 173,320.2
2025-01-18 05:29:51.482 | DEBUG    | __main__:<module>:313 - Training step 5930: loss = 3.5726 | 3023.65ms | Tokens/s = 173,396.0
2025-01-18 05:30:21.741 | DEBUG    | __main__:<module>:313 - Training step 5940: loss = 3.1266 | 3029.09ms | Tokens/s = 173,084.2
2025-01-18 05:30:52.023 | DEBUG    | __main__:<module>:313 - Training step 5950: loss = 3.4494 | 3027.69ms | Tokens/s = 173,164.5
2025-01-18 05:31:22.291 | DEBUG    | __main__:<module>:313 - Training step 5960: loss = 3.4590 | 3027.15ms | Tokens/s = 173,195.0
2025-01-18 05:31:52.570 | DEBUG    | __main__:<module>:313 - Training step 5970: loss = 3.3621 | 3027.54ms | Tokens/s = 173,172.9
2025-01-18 05:32:22.862 | DEBUG    | __main__:<module>:313 - Training step 5980: loss = 3.4857 | 3029.20ms | Tokens/s = 173,077.9
2025-01-18 05:32:53.137 | DEBUG    | __main__:<module>:313 - Training step 5990: loss = 3.4234 | 3027.48ms | Tokens/s = 173,176.3
2025-01-18 05:33:26.856 | INFO     | __main__:<module>:265 - Step 6,000/20,000 loss: 3.4128 (T) 3.4329 (V) | lr=8.8e-03
2025-01-18 05:33:26.858 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 05:33:40.202 | DEBUG    | __main__:<module>:313 - Training step 6000: loss = 3.3552 | 19815.40ms | Tokens/s = 26,458.6
2025-01-18 05:34:10.342 | DEBUG    | __main__:<module>:313 - Training step 6010: loss = 3.2867 | 3022.71ms | Tokens/s = 173,449.7
2025-01-18 05:34:40.594 | DEBUG    | __main__:<module>:313 - Training step 6020: loss = 3.5219 | 3026.19ms | Tokens/s = 173,250.0
2025-01-18 05:35:10.883 | DEBUG    | __main__:<module>:313 - Training step 6030: loss = 3.4425 | 3031.34ms | Tokens/s = 172,955.7
2025-01-18 05:35:41.194 | DEBUG    | __main__:<module>:313 - Training step 6040: loss = 3.4290 | 3029.89ms | Tokens/s = 173,038.9
2025-01-18 05:36:11.476 | DEBUG    | __main__:<module>:313 - Training step 6050: loss = 3.3317 | 3027.97ms | Tokens/s = 173,148.2
2025-01-18 05:36:41.748 | DEBUG    | __main__:<module>:313 - Training step 6060: loss = 3.1856 | 3027.26ms | Tokens/s = 173,188.9
2025-01-18 05:37:12.003 | DEBUG    | __main__:<module>:313 - Training step 6070: loss = 3.3876 | 3024.70ms | Tokens/s = 173,335.3
2025-01-18 05:37:42.248 | DEBUG    | __main__:<module>:313 - Training step 6080: loss = 3.4007 | 3023.03ms | Tokens/s = 173,431.3
2025-01-18 05:38:12.508 | DEBUG    | __main__:<module>:313 - Training step 6090: loss = 3.4275 | 3027.29ms | Tokens/s = 173,187.5
2025-01-18 05:38:42.800 | DEBUG    | __main__:<module>:313 - Training step 6100: loss = 3.3695 | 3031.05ms | Tokens/s = 172,972.2
2025-01-18 05:39:13.087 | DEBUG    | __main__:<module>:313 - Training step 6110: loss = 3.5673 | 3028.17ms | Tokens/s = 173,137.1
2025-01-18 05:39:43.355 | DEBUG    | __main__:<module>:313 - Training step 6120: loss = 3.3595 | 3024.89ms | Tokens/s = 173,324.6
2025-01-18 05:40:13.614 | DEBUG    | __main__:<module>:313 - Training step 6130: loss = 3.5104 | 3026.83ms | Tokens/s = 173,213.7
2025-01-18 05:40:43.907 | DEBUG    | __main__:<module>:313 - Training step 6140: loss = 3.3917 | 3030.17ms | Tokens/s = 173,022.5
2025-01-18 05:41:14.196 | DEBUG    | __main__:<module>:313 - Training step 6150: loss = 3.5348 | 3028.15ms | Tokens/s = 173,138.1
2025-01-18 05:41:44.473 | DEBUG    | __main__:<module>:313 - Training step 6160: loss = 3.5075 | 3027.57ms | Tokens/s = 173,171.5
2025-01-18 05:42:14.738 | DEBUG    | __main__:<module>:313 - Training step 6170: loss = 3.4538 | 3024.92ms | Tokens/s = 173,322.8
2025-01-18 05:42:44.989 | DEBUG    | __main__:<module>:313 - Training step 6180: loss = 3.4116 | 3023.92ms | Tokens/s = 173,380.2
2025-01-18 05:43:15.240 | DEBUG    | __main__:<module>:313 - Training step 6190: loss = 3.4673 | 3024.43ms | Tokens/s = 173,350.8
2025-01-18 05:43:45.480 | DEBUG    | __main__:<module>:313 - Training step 6200: loss = 3.3668 | 3022.18ms | Tokens/s = 173,480.1
2025-01-18 05:44:15.754 | DEBUG    | __main__:<module>:313 - Training step 6210: loss = 3.4943 | 3028.63ms | Tokens/s = 173,110.6
2025-01-18 05:44:46.037 | DEBUG    | __main__:<module>:313 - Training step 6220: loss = 3.4168 | 3028.81ms | Tokens/s = 173,100.1
2025-01-18 05:45:16.344 | DEBUG    | __main__:<module>:313 - Training step 6230: loss = 3.4012 | 3033.79ms | Tokens/s = 172,816.0
2025-01-18 05:45:46.653 | DEBUG    | __main__:<module>:313 - Training step 6240: loss = 3.5375 | 3029.61ms | Tokens/s = 173,054.4
2025-01-18 05:46:16.927 | DEBUG    | __main__:<module>:313 - Training step 6250: loss = 3.4934 | 3026.25ms | Tokens/s = 173,246.5
2025-01-18 05:46:47.184 | DEBUG    | __main__:<module>:313 - Training step 6260: loss = 3.4499 | 3024.22ms | Tokens/s = 173,363.1
2025-01-18 05:47:17.431 | DEBUG    | __main__:<module>:313 - Training step 6270: loss = 3.3415 | 3026.64ms | Tokens/s = 173,224.2
2025-01-18 05:47:47.711 | DEBUG    | __main__:<module>:313 - Training step 6280: loss = 3.5748 | 3030.62ms | Tokens/s = 172,996.7
2025-01-18 05:48:18.008 | DEBUG    | __main__:<module>:313 - Training step 6290: loss = 3.3682 | 3028.10ms | Tokens/s = 173,140.9
2025-01-18 05:48:48.279 | DEBUG    | __main__:<module>:313 - Training step 6300: loss = 3.3281 | 3024.90ms | Tokens/s = 173,324.0
2025-01-18 05:49:18.542 | DEBUG    | __main__:<module>:313 - Training step 6310: loss = 3.4426 | 3024.07ms | Tokens/s = 173,371.7
2025-01-18 05:49:48.807 | DEBUG    | __main__:<module>:313 - Training step 6320: loss = 3.3493 | 3026.68ms | Tokens/s = 173,222.1
2025-01-18 05:50:19.078 | DEBUG    | __main__:<module>:313 - Training step 6330: loss = 3.4158 | 3025.99ms | Tokens/s = 173,261.6
2025-01-18 05:50:49.358 | DEBUG    | __main__:<module>:313 - Training step 6340: loss = 3.4348 | 3028.81ms | Tokens/s = 173,100.3
2025-01-18 05:51:19.647 | DEBUG    | __main__:<module>:313 - Training step 6350: loss = 3.4416 | 3028.34ms | Tokens/s = 173,127.2
2025-01-18 05:51:49.925 | DEBUG    | __main__:<module>:313 - Training step 6360: loss = 3.3433 | 3025.21ms | Tokens/s = 173,306.3
2025-01-18 05:52:20.181 | DEBUG    | __main__:<module>:313 - Training step 6370: loss = 3.5356 | 3024.19ms | Tokens/s = 173,364.8
2025-01-18 05:52:50.447 | DEBUG    | __main__:<module>:313 - Training step 6380: loss = 3.5158 | 3026.95ms | Tokens/s = 173,207.0
2025-01-18 05:53:20.745 | DEBUG    | __main__:<module>:313 - Training step 6390: loss = 3.4220 | 3029.79ms | Tokens/s = 173,044.2
2025-01-18 05:53:51.051 | DEBUG    | __main__:<module>:313 - Training step 6400: loss = 3.3580 | 3028.19ms | Tokens/s = 173,135.7
2025-01-18 05:54:21.327 | DEBUG    | __main__:<module>:313 - Training step 6410: loss = 3.2770 | 3026.65ms | Tokens/s = 173,224.0
2025-01-18 05:54:51.588 | DEBUG    | __main__:<module>:313 - Training step 6420: loss = 3.4424 | 3026.34ms | Tokens/s = 173,241.7
2025-01-18 05:55:21.846 | DEBUG    | __main__:<module>:313 - Training step 6430: loss = 3.2688 | 3027.04ms | Tokens/s = 173,201.8
2025-01-18 05:55:52.130 | DEBUG    | __main__:<module>:313 - Training step 6440: loss = 3.5134 | 3031.51ms | Tokens/s = 172,946.2
2025-01-18 05:56:22.411 | DEBUG    | __main__:<module>:313 - Training step 6450: loss = 3.2821 | 3027.21ms | Tokens/s = 173,191.8
2025-01-18 05:56:52.669 | DEBUG    | __main__:<module>:313 - Training step 6460: loss = 3.3420 | 3024.04ms | Tokens/s = 173,373.4
2025-01-18 05:57:22.921 | DEBUG    | __main__:<module>:313 - Training step 6470: loss = 3.5410 | 3025.39ms | Tokens/s = 173,295.8
2025-01-18 05:57:53.169 | DEBUG    | __main__:<module>:313 - Training step 6480: loss = 3.3789 | 3025.84ms | Tokens/s = 173,270.1
2025-01-18 05:58:23.448 | DEBUG    | __main__:<module>:313 - Training step 6490: loss = 3.2492 | 3027.73ms | Tokens/s = 173,162.0
2025-01-18 05:58:53.729 | DEBUG    | __main__:<module>:313 - Training step 6500: loss = 3.5108 | 3028.10ms | Tokens/s = 173,141.1
2025-01-18 05:59:23.999 | DEBUG    | __main__:<module>:313 - Training step 6510: loss = 3.1733 | 3028.37ms | Tokens/s = 173,125.4
2025-01-18 05:59:54.257 | DEBUG    | __main__:<module>:313 - Training step 6520: loss = 3.2487 | 3027.39ms | Tokens/s = 173,181.7
2025-01-18 06:00:24.510 | DEBUG    | __main__:<module>:313 - Training step 6530: loss = 3.2164 | 3026.39ms | Tokens/s = 173,238.7
2025-01-18 06:00:54.753 | DEBUG    | __main__:<module>:313 - Training step 6540: loss = 3.2968 | 3022.73ms | Tokens/s = 173,448.6
2025-01-18 06:01:25.025 | DEBUG    | __main__:<module>:313 - Training step 6550: loss = 3.2934 | 3027.46ms | Tokens/s = 173,177.4
2025-01-18 06:01:55.329 | DEBUG    | __main__:<module>:313 - Training step 6560: loss = 3.6232 | 3032.01ms | Tokens/s = 172,917.4
2025-01-18 06:02:25.629 | DEBUG    | __main__:<module>:313 - Training step 6570: loss = 3.2833 | 3027.46ms | Tokens/s = 173,177.3
2025-01-18 06:02:55.911 | DEBUG    | __main__:<module>:313 - Training step 6580: loss = 3.2608 | 3027.70ms | Tokens/s = 173,163.9
2025-01-18 06:03:26.178 | DEBUG    | __main__:<module>:313 - Training step 6590: loss = 3.5161 | 3022.57ms | Tokens/s = 173,457.6
2025-01-18 06:03:56.440 | DEBUG    | __main__:<module>:313 - Training step 6600: loss = 3.4390 | 3025.99ms | Tokens/s = 173,261.4
2025-01-18 06:04:26.708 | DEBUG    | __main__:<module>:313 - Training step 6610: loss = 3.4293 | 3027.36ms | Tokens/s = 173,183.2
2025-01-18 06:04:57.009 | DEBUG    | __main__:<module>:313 - Training step 6620: loss = 3.3968 | 3032.18ms | Tokens/s = 172,908.2
2025-01-18 06:05:27.293 | DEBUG    | __main__:<module>:313 - Training step 6630: loss = 3.2723 | 3026.13ms | Tokens/s = 173,253.5
2025-01-18 06:05:57.556 | DEBUG    | __main__:<module>:313 - Training step 6640: loss = 3.3595 | 3024.98ms | Tokens/s = 173,319.6
2025-01-18 06:06:27.799 | DEBUG    | __main__:<module>:313 - Training step 6650: loss = 3.5628 | 3024.20ms | Tokens/s = 173,364.1
2025-01-18 06:06:58.069 | DEBUG    | __main__:<module>:313 - Training step 6660: loss = 3.4476 | 3027.69ms | Tokens/s = 173,164.2
2025-01-18 06:07:28.354 | DEBUG    | __main__:<module>:313 - Training step 6670: loss = 3.4699 | 3027.11ms | Tokens/s = 173,197.5
2025-01-18 06:07:58.621 | DEBUG    | __main__:<module>:313 - Training step 6680: loss = 3.3225 | 3025.05ms | Tokens/s = 173,315.7
2025-01-18 06:08:28.888 | DEBUG    | __main__:<module>:313 - Training step 6690: loss = 3.3230 | 3025.56ms | Tokens/s = 173,286.2
2025-01-18 06:08:59.169 | DEBUG    | __main__:<module>:313 - Training step 6700: loss = 3.5369 | 3028.28ms | Tokens/s = 173,130.5
2025-01-18 06:09:29.465 | DEBUG    | __main__:<module>:313 - Training step 6710: loss = 3.3306 | 3029.46ms | Tokens/s = 173,063.4
2025-01-18 06:09:59.761 | DEBUG    | __main__:<module>:313 - Training step 6720: loss = 3.3121 | 3031.20ms | Tokens/s = 172,963.6
2025-01-18 06:10:30.040 | DEBUG    | __main__:<module>:313 - Training step 6730: loss = 3.5066 | 3026.94ms | Tokens/s = 173,207.0
2025-01-18 06:11:00.287 | DEBUG    | __main__:<module>:313 - Training step 6740: loss = 3.4479 | 3024.53ms | Tokens/s = 173,345.2
2025-01-18 06:11:30.547 | DEBUG    | __main__:<module>:313 - Training step 6750: loss = 3.4676 | 3027.78ms | Tokens/s = 173,158.9
2025-01-18 06:12:00.821 | DEBUG    | __main__:<module>:313 - Training step 6760: loss = 3.4690 | 3025.61ms | Tokens/s = 173,283.5
2025-01-18 06:12:31.070 | DEBUG    | __main__:<module>:313 - Training step 6770: loss = 3.2782 | 3021.80ms | Tokens/s = 173,501.7
2025-01-18 06:13:01.297 | DEBUG    | __main__:<module>:313 - Training step 6780: loss = 3.3585 | 3023.61ms | Tokens/s = 173,397.9
2025-01-18 06:13:31.545 | DEBUG    | __main__:<module>:313 - Training step 6790: loss = 3.4279 | 3023.39ms | Tokens/s = 173,410.6
2025-01-18 06:14:01.806 | DEBUG    | __main__:<module>:313 - Training step 6800: loss = 3.3595 | 3025.38ms | Tokens/s = 173,296.5
2025-01-18 06:14:32.082 | DEBUG    | __main__:<module>:313 - Training step 6810: loss = 3.3595 | 3029.12ms | Tokens/s = 173,082.7
2025-01-18 06:15:02.352 | DEBUG    | __main__:<module>:313 - Training step 6820: loss = 3.3433 | 3025.20ms | Tokens/s = 173,306.6
2025-01-18 06:15:32.590 | DEBUG    | __main__:<module>:313 - Training step 6830: loss = 3.3566 | 3023.72ms | Tokens/s = 173,391.7
2025-01-18 06:16:02.831 | DEBUG    | __main__:<module>:313 - Training step 6840: loss = 3.4615 | 3026.83ms | Tokens/s = 173,213.6
2025-01-18 06:16:33.098 | DEBUG    | __main__:<module>:313 - Training step 6850: loss = 3.3374 | 3027.84ms | Tokens/s = 173,155.6
2025-01-18 06:17:03.381 | DEBUG    | __main__:<module>:313 - Training step 6860: loss = 3.3351 | 3027.87ms | Tokens/s = 173,153.9
2025-01-18 06:17:33.645 | DEBUG    | __main__:<module>:313 - Training step 6870: loss = 3.3220 | 3023.81ms | Tokens/s = 173,386.8
2025-01-18 06:18:03.893 | DEBUG    | __main__:<module>:313 - Training step 6880: loss = 3.3468 | 3027.10ms | Tokens/s = 173,198.0
2025-01-18 06:18:34.166 | DEBUG    | __main__:<module>:313 - Training step 6890: loss = 3.3415 | 3027.97ms | Tokens/s = 173,148.6
2025-01-18 06:19:04.451 | DEBUG    | __main__:<module>:313 - Training step 6900: loss = 3.4397 | 3028.84ms | Tokens/s = 173,098.4
2025-01-18 06:19:34.736 | DEBUG    | __main__:<module>:313 - Training step 6910: loss = 3.3534 | 3027.94ms | Tokens/s = 173,150.1
2025-01-18 06:20:05.025 | DEBUG    | __main__:<module>:313 - Training step 6920: loss = 3.3943 | 3029.29ms | Tokens/s = 173,073.1
2025-01-18 06:20:35.324 | DEBUG    | __main__:<module>:313 - Training step 6930: loss = 3.4015 | 3031.19ms | Tokens/s = 172,964.1
2025-01-18 06:21:05.590 | DEBUG    | __main__:<module>:313 - Training step 6940: loss = 3.4137 | 3025.66ms | Tokens/s = 173,280.4
2025-01-18 06:21:35.827 | DEBUG    | __main__:<module>:313 - Training step 6950: loss = 3.4949 | 3021.04ms | Tokens/s = 173,545.5
2025-01-18 06:22:06.041 | DEBUG    | __main__:<module>:313 - Training step 6960: loss = 3.3591 | 3020.46ms | Tokens/s = 173,579.0
2025-01-18 06:22:36.272 | DEBUG    | __main__:<module>:313 - Training step 6970: loss = 3.3225 | 3025.55ms | Tokens/s = 173,286.8
2025-01-18 06:23:06.529 | DEBUG    | __main__:<module>:313 - Training step 6980: loss = 3.5180 | 3023.65ms | Tokens/s = 173,395.8
2025-01-18 06:23:36.796 | DEBUG    | __main__:<module>:313 - Training step 6990: loss = 3.3404 | 3028.19ms | Tokens/s = 173,135.8
2025-01-18 06:24:10.475 | INFO     | __main__:<module>:265 - Step 7,000/20,000 loss: 3.3840 (T) 3.3756 (V) | lr=8.2e-03
2025-01-18 06:24:10.476 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 06:24:24.909 | DEBUG    | __main__:<module>:313 - Training step 7000: loss = 3.4123 | 20890.83ms | Tokens/s = 25,096.6
2025-01-18 06:24:54.990 | DEBUG    | __main__:<module>:313 - Training step 7010: loss = 3.3016 | 3015.76ms | Tokens/s = 173,849.5
2025-01-18 06:25:25.195 | DEBUG    | __main__:<module>:313 - Training step 7020: loss = 3.5948 | 3022.90ms | Tokens/s = 173,438.8
2025-01-18 06:25:55.432 | DEBUG    | __main__:<module>:313 - Training step 7030: loss = 3.4838 | 3023.23ms | Tokens/s = 173,419.8
2025-01-18 06:26:25.678 | DEBUG    | __main__:<module>:313 - Training step 7040: loss = 3.5515 | 3025.95ms | Tokens/s = 173,264.1
2025-01-18 06:26:55.941 | DEBUG    | __main__:<module>:313 - Training step 7050: loss = 3.4072 | 3026.81ms | Tokens/s = 173,214.4
2025-01-18 06:27:26.206 | DEBUG    | __main__:<module>:313 - Training step 7060: loss = 3.2364 | 3024.72ms | Tokens/s = 173,334.6
2025-01-18 06:27:56.454 | DEBUG    | __main__:<module>:313 - Training step 7070: loss = 3.4185 | 3024.95ms | Tokens/s = 173,321.4
2025-01-18 06:28:26.692 | DEBUG    | __main__:<module>:313 - Training step 7080: loss = 3.4880 | 3025.22ms | Tokens/s = 173,306.0
2025-01-18 06:28:56.951 | DEBUG    | __main__:<module>:313 - Training step 7090: loss = 3.5097 | 3027.31ms | Tokens/s = 173,186.3
2025-01-18 06:29:27.214 | DEBUG    | __main__:<module>:313 - Training step 7100: loss = 3.5193 | 3024.98ms | Tokens/s = 173,319.6
2025-01-18 06:29:57.489 | DEBUG    | __main__:<module>:313 - Training step 7110: loss = 3.4779 | 3028.56ms | Tokens/s = 173,114.6
2025-01-18 06:30:27.768 | DEBUG    | __main__:<module>:313 - Training step 7120: loss = 3.5046 | 3026.40ms | Tokens/s = 173,238.3
2025-01-18 06:30:58.055 | DEBUG    | __main__:<module>:313 - Training step 7130: loss = 3.3911 | 3028.71ms | Tokens/s = 173,105.8
2025-01-18 06:31:28.318 | DEBUG    | __main__:<module>:313 - Training step 7140: loss = 3.4116 | 3025.95ms | Tokens/s = 173,263.9
2025-01-18 06:31:58.568 | DEBUG    | __main__:<module>:313 - Training step 7150: loss = 3.4741 | 3025.11ms | Tokens/s = 173,312.2
2025-01-18 06:32:28.849 | DEBUG    | __main__:<module>:313 - Training step 7160: loss = 3.5555 | 3028.94ms | Tokens/s = 173,093.1
2025-01-18 06:32:59.130 | DEBUG    | __main__:<module>:313 - Training step 7170: loss = 3.3618 | 3026.67ms | Tokens/s = 173,222.5
2025-01-18 06:33:29.365 | DEBUG    | __main__:<module>:313 - Training step 7180: loss = 3.4127 | 3021.05ms | Tokens/s = 173,545.2
2025-01-18 06:33:59.588 | DEBUG    | __main__:<module>:313 - Training step 7190: loss = 3.4798 | 3023.72ms | Tokens/s = 173,391.9
2025-01-18 06:34:29.840 | DEBUG    | __main__:<module>:313 - Training step 7200: loss = 3.4983 | 3025.14ms | Tokens/s = 173,310.1
2025-01-18 06:35:00.100 | DEBUG    | __main__:<module>:313 - Training step 7210: loss = 3.4477 | 3025.42ms | Tokens/s = 173,294.1
2025-01-18 06:35:30.342 | DEBUG    | __main__:<module>:313 - Training step 7220: loss = 3.5644 | 3023.23ms | Tokens/s = 173,419.9
2025-01-18 06:36:00.593 | DEBUG    | __main__:<module>:313 - Training step 7230: loss = 3.1923 | 3024.68ms | Tokens/s = 173,336.7
2025-01-18 06:36:30.866 | DEBUG    | __main__:<module>:313 - Training step 7240: loss = 3.4904 | 3027.58ms | Tokens/s = 173,170.5
2025-01-18 06:37:01.146 | DEBUG    | __main__:<module>:313 - Training step 7250: loss = 3.4378 | 3027.41ms | Tokens/s = 173,180.3
2025-01-18 06:37:31.429 | DEBUG    | __main__:<module>:313 - Training step 7260: loss = 3.3999 | 3028.90ms | Tokens/s = 173,095.1
2025-01-18 06:38:01.720 | DEBUG    | __main__:<module>:313 - Training step 7270: loss = 3.4441 | 3028.57ms | Tokens/s = 173,114.0
2025-01-18 06:38:31.973 | DEBUG    | __main__:<module>:313 - Training step 7280: loss = 3.3091 | 3024.29ms | Tokens/s = 173,359.2
2025-01-18 06:39:02.189 | DEBUG    | __main__:<module>:313 - Training step 7290: loss = 3.2418 | 3021.76ms | Tokens/s = 173,504.4
2025-01-18 06:39:32.432 | DEBUG    | __main__:<module>:313 - Training step 7300: loss = 3.2916 | 3025.59ms | Tokens/s = 173,284.8
2025-01-18 06:40:02.701 | DEBUG    | __main__:<module>:313 - Training step 7310: loss = 3.3077 | 3027.93ms | Tokens/s = 173,150.9
2025-01-18 06:40:32.979 | DEBUG    | __main__:<module>:313 - Training step 7320: loss = 3.3624 | 3027.09ms | Tokens/s = 173,198.5
2025-01-18 06:41:03.260 | DEBUG    | __main__:<module>:313 - Training step 7330: loss = 3.7009 | 3028.35ms | Tokens/s = 173,126.7
2025-01-18 06:41:33.545 | DEBUG    | __main__:<module>:313 - Training step 7340: loss = 3.4072 | 3028.67ms | Tokens/s = 173,108.1
2025-01-18 06:42:03.840 | DEBUG    | __main__:<module>:313 - Training step 7350: loss = 3.4922 | 3031.14ms | Tokens/s = 172,967.5
2025-01-18 06:42:34.135 | DEBUG    | __main__:<module>:313 - Training step 7360: loss = 3.3151 | 3028.98ms | Tokens/s = 173,090.6
2025-01-18 06:43:04.429 | DEBUG    | __main__:<module>:313 - Training step 7370: loss = 3.4412 | 3028.72ms | Tokens/s = 173,105.7
2025-01-18 06:43:34.715 | DEBUG    | __main__:<module>:313 - Training step 7380: loss = 3.4179 | 3027.46ms | Tokens/s = 173,177.2
2025-01-18 06:44:04.967 | DEBUG    | __main__:<module>:313 - Training step 7390: loss = 3.2796 | 3024.06ms | Tokens/s = 173,371.9
2025-01-18 06:44:35.184 | DEBUG    | __main__:<module>:313 - Training step 7400: loss = 3.2502 | 3020.81ms | Tokens/s = 173,558.5
2025-01-18 06:45:05.438 | DEBUG    | __main__:<module>:313 - Training step 7410: loss = 3.3815 | 3026.73ms | Tokens/s = 173,219.4
2025-01-18 06:45:35.701 | DEBUG    | __main__:<module>:313 - Training step 7420: loss = 3.3917 | 3026.05ms | Tokens/s = 173,258.1
2025-01-18 06:46:05.955 | DEBUG    | __main__:<module>:313 - Training step 7430: loss = 3.5019 | 3024.29ms | Tokens/s = 173,359.0
2025-01-18 06:46:36.177 | DEBUG    | __main__:<module>:313 - Training step 7440: loss = 3.5175 | 3020.83ms | Tokens/s = 173,557.5
2025-01-18 06:47:06.406 | DEBUG    | __main__:<module>:313 - Training step 7450: loss = 3.3308 | 3026.43ms | Tokens/s = 173,236.4
2025-01-18 06:47:36.661 | DEBUG    | __main__:<module>:313 - Training step 7460: loss = 3.3908 | 3027.53ms | Tokens/s = 173,173.4
2025-01-18 06:48:06.922 | DEBUG    | __main__:<module>:313 - Training step 7470: loss = 3.3299 | 3024.06ms | Tokens/s = 173,372.1
2025-01-18 06:48:37.193 | DEBUG    | __main__:<module>:313 - Training step 7480: loss = 3.5657 | 3025.06ms | Tokens/s = 173,314.8
2025-01-18 06:49:07.451 | DEBUG    | __main__:<module>:313 - Training step 7490: loss = 3.5981 | 3027.79ms | Tokens/s = 173,158.8
2025-01-18 06:49:37.726 | DEBUG    | __main__:<module>:313 - Training step 7500: loss = 3.5149 | 3025.76ms | Tokens/s = 173,274.7
2025-01-18 06:50:08.013 | DEBUG    | __main__:<module>:313 - Training step 7510: loss = 3.5075 | 3030.05ms | Tokens/s = 173,029.6
2025-01-18 06:50:38.291 | DEBUG    | __main__:<module>:313 - Training step 7520: loss = 3.4758 | 3028.55ms | Tokens/s = 173,115.0
2025-01-18 06:51:08.577 | DEBUG    | __main__:<module>:313 - Training step 7530: loss = 3.4018 | 3028.67ms | Tokens/s = 173,108.3
2025-01-18 06:51:38.858 | DEBUG    | __main__:<module>:313 - Training step 7540: loss = 3.4370 | 3027.06ms | Tokens/s = 173,200.2
2025-01-18 06:52:09.137 | DEBUG    | __main__:<module>:313 - Training step 7550: loss = 3.3219 | 3028.38ms | Tokens/s = 173,124.7
2025-01-18 06:52:39.417 | DEBUG    | __main__:<module>:313 - Training step 7560: loss = 3.3538 | 3027.40ms | Tokens/s = 173,180.7
2025-01-18 06:53:09.672 | DEBUG    | __main__:<module>:313 - Training step 7570: loss = 3.4159 | 3024.29ms | Tokens/s = 173,359.3
2025-01-18 06:53:39.902 | DEBUG    | __main__:<module>:313 - Training step 7580: loss = 3.2731 | 3024.10ms | Tokens/s = 173,369.8
2025-01-18 06:54:10.148 | DEBUG    | __main__:<module>:313 - Training step 7590: loss = 3.4765 | 3026.85ms | Tokens/s = 173,212.6
2025-01-18 06:54:40.410 | DEBUG    | __main__:<module>:313 - Training step 7600: loss = 3.2799 | 3027.64ms | Tokens/s = 173,167.3
2025-01-18 06:55:10.677 | DEBUG    | __main__:<module>:313 - Training step 7610: loss = 3.3911 | 3026.92ms | Tokens/s = 173,208.3
2025-01-18 06:55:40.925 | DEBUG    | __main__:<module>:313 - Training step 7620: loss = 3.3807 | 3022.81ms | Tokens/s = 173,443.9
2025-01-18 06:56:11.175 | DEBUG    | __main__:<module>:313 - Training step 7630: loss = 3.5747 | 3024.59ms | Tokens/s = 173,341.9
2025-01-18 06:56:41.444 | DEBUG    | __main__:<module>:313 - Training step 7640: loss = 3.3767 | 3027.69ms | Tokens/s = 173,164.5
2025-01-18 06:57:11.727 | DEBUG    | __main__:<module>:313 - Training step 7650: loss = 3.4444 | 3030.38ms | Tokens/s = 173,010.7
2025-01-18 06:57:42.010 | DEBUG    | __main__:<module>:313 - Training step 7660: loss = 3.4167 | 3027.57ms | Tokens/s = 173,171.3
2025-01-18 06:58:12.303 | DEBUG    | __main__:<module>:313 - Training step 7670: loss = 3.3218 | 3029.70ms | Tokens/s = 173,049.6
2025-01-18 06:58:42.555 | DEBUG    | __main__:<module>:313 - Training step 7680: loss = 3.1794 | 3025.40ms | Tokens/s = 173,295.6
2025-01-18 06:59:12.805 | DEBUG    | __main__:<module>:313 - Training step 7690: loss = 3.4686 | 3023.48ms | Tokens/s = 173,405.5
2025-01-18 06:59:43.044 | DEBUG    | __main__:<module>:313 - Training step 7700: loss = 3.1902 | 3022.66ms | Tokens/s = 173,452.4
2025-01-18 07:00:13.288 | DEBUG    | __main__:<module>:313 - Training step 7710: loss = 3.4390 | 3024.29ms | Tokens/s = 173,358.8
2025-01-18 07:00:43.544 | DEBUG    | __main__:<module>:313 - Training step 7720: loss = 3.3913 | 3024.53ms | Tokens/s = 173,345.2
2025-01-18 07:01:13.809 | DEBUG    | __main__:<module>:313 - Training step 7730: loss = 3.5015 | 3027.13ms | Tokens/s = 173,196.5
2025-01-18 07:01:44.067 | DEBUG    | __main__:<module>:313 - Training step 7740: loss = 3.4186 | 3026.24ms | Tokens/s = 173,247.4
2025-01-18 07:02:14.301 | DEBUG    | __main__:<module>:313 - Training step 7750: loss = 3.4126 | 3022.73ms | Tokens/s = 173,448.3
2025-01-18 07:02:44.558 | DEBUG    | __main__:<module>:313 - Training step 7760: loss = 3.3529 | 3026.12ms | Tokens/s = 173,254.4
2025-01-18 07:03:14.823 | DEBUG    | __main__:<module>:313 - Training step 7770: loss = 3.3861 | 3027.71ms | Tokens/s = 173,163.1
2025-01-18 07:03:45.104 | DEBUG    | __main__:<module>:313 - Training step 7780: loss = 3.4978 | 3028.86ms | Tokens/s = 173,097.6
2025-01-18 07:04:15.388 | DEBUG    | __main__:<module>:313 - Training step 7790: loss = 3.3218 | 3029.87ms | Tokens/s = 173,039.8
2025-01-18 07:04:45.673 | DEBUG    | __main__:<module>:313 - Training step 7800: loss = 3.3827 | 3030.53ms | Tokens/s = 173,002.0
2025-01-18 07:05:15.966 | DEBUG    | __main__:<module>:313 - Training step 7810: loss = 3.2278 | 3028.40ms | Tokens/s = 173,123.7
2025-01-18 07:05:46.245 | DEBUG    | __main__:<module>:313 - Training step 7820: loss = 3.2401 | 3027.46ms | Tokens/s = 173,177.3
2025-01-18 07:06:16.488 | DEBUG    | __main__:<module>:313 - Training step 7830: loss = 3.5052 | 3024.97ms | Tokens/s = 173,320.2
2025-01-18 07:06:46.722 | DEBUG    | __main__:<module>:313 - Training step 7840: loss = 3.3232 | 3025.22ms | Tokens/s = 173,306.0
2025-01-18 07:07:16.974 | DEBUG    | __main__:<module>:313 - Training step 7850: loss = 3.3027 | 3024.51ms | Tokens/s = 173,346.6
2025-01-18 07:07:47.245 | DEBUG    | __main__:<module>:313 - Training step 7860: loss = 3.3947 | 3028.97ms | Tokens/s = 173,091.0
2025-01-18 07:08:17.521 | DEBUG    | __main__:<module>:313 - Training step 7870: loss = 3.4346 | 3027.67ms | Tokens/s = 173,165.2
2025-01-18 07:08:47.770 | DEBUG    | __main__:<module>:313 - Training step 7880: loss = 3.5076 | 3022.05ms | Tokens/s = 173,487.3
2025-01-18 07:09:18.002 | DEBUG    | __main__:<module>:313 - Training step 7890: loss = 3.2912 | 3023.95ms | Tokens/s = 173,378.7
2025-01-18 07:09:48.262 | DEBUG    | __main__:<module>:313 - Training step 7900: loss = 3.3867 | 3025.29ms | Tokens/s = 173,301.6
2025-01-18 07:10:18.529 | DEBUG    | __main__:<module>:313 - Training step 7910: loss = 3.4336 | 3028.16ms | Tokens/s = 173,137.5
2025-01-18 07:10:48.777 | DEBUG    | __main__:<module>:313 - Training step 7920: loss = 3.3666 | 3022.65ms | Tokens/s = 173,453.3
2025-01-18 07:11:18.994 | DEBUG    | __main__:<module>:313 - Training step 7930: loss = 3.3128 | 3021.54ms | Tokens/s = 173,516.9
2025-01-18 07:11:49.228 | DEBUG    | __main__:<module>:313 - Training step 7940: loss = 3.4711 | 3023.66ms | Tokens/s = 173,395.1
2025-01-18 07:12:19.467 | DEBUG    | __main__:<module>:313 - Training step 7950: loss = 3.3771 | 3026.34ms | Tokens/s = 173,241.4
2025-01-18 07:12:49.725 | DEBUG    | __main__:<module>:313 - Training step 7960: loss = 3.4675 | 3026.11ms | Tokens/s = 173,254.9
2025-01-18 07:13:19.990 | DEBUG    | __main__:<module>:313 - Training step 7970: loss = 3.2113 | 3024.43ms | Tokens/s = 173,350.9
2025-01-18 07:13:50.267 | DEBUG    | __main__:<module>:313 - Training step 7980: loss = 3.3716 | 3029.52ms | Tokens/s = 173,059.9
2025-01-18 07:14:20.547 | DEBUG    | __main__:<module>:313 - Training step 7990: loss = 3.4959 | 3028.76ms | Tokens/s = 173,103.0
2025-01-18 07:14:54.277 | INFO     | __main__:<module>:265 - Step 8,000/20,000 loss: 3.3541 (T) 3.3671 (V) | lr=7.5e-03
2025-01-18 07:14:54.278 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 07:15:07.592 | DEBUG    | __main__:<module>:313 - Training step 8000: loss = 3.3401 | 19791.40ms | Tokens/s = 26,490.7
2025-01-18 07:15:37.705 | DEBUG    | __main__:<module>:313 - Training step 8010: loss = 3.3019 | 3015.23ms | Tokens/s = 173,880.1
2025-01-18 07:16:07.916 | DEBUG    | __main__:<module>:313 - Training step 8020: loss = 3.2694 | 3022.86ms | Tokens/s = 173,441.1
2025-01-18 07:16:38.165 | DEBUG    | __main__:<module>:313 - Training step 8030: loss = 3.3201 | 3027.95ms | Tokens/s = 173,149.2
2025-01-18 07:17:08.429 | DEBUG    | __main__:<module>:313 - Training step 8040: loss = 3.3716 | 3026.21ms | Tokens/s = 173,249.0
2025-01-18 07:17:38.696 | DEBUG    | __main__:<module>:313 - Training step 8050: loss = 3.5468 | 3028.55ms | Tokens/s = 173,115.0
2025-01-18 07:18:08.976 | DEBUG    | __main__:<module>:313 - Training step 8060: loss = 3.3783 | 3027.27ms | Tokens/s = 173,188.5
2025-01-18 07:18:39.260 | DEBUG    | __main__:<module>:313 - Training step 8070: loss = 3.6082 | 3028.16ms | Tokens/s = 173,137.6
2025-01-18 07:19:09.513 | DEBUG    | __main__:<module>:313 - Training step 8080: loss = 3.3876 | 3022.16ms | Tokens/s = 173,481.4
2025-01-18 07:19:39.749 | DEBUG    | __main__:<module>:313 - Training step 8090: loss = 3.4162 | 3022.42ms | Tokens/s = 173,466.1
2025-01-18 07:20:10.012 | DEBUG    | __main__:<module>:313 - Training step 8100: loss = 3.0849 | 3026.80ms | Tokens/s = 173,215.5
2025-01-18 07:20:40.273 | DEBUG    | __main__:<module>:313 - Training step 8110: loss = 3.2524 | 3024.79ms | Tokens/s = 173,330.6
2025-01-18 07:21:10.511 | DEBUG    | __main__:<module>:313 - Training step 8120: loss = 3.2682 | 3023.80ms | Tokens/s = 173,386.9
2025-01-18 07:21:40.763 | DEBUG    | __main__:<module>:313 - Training step 8130: loss = 3.4154 | 3026.78ms | Tokens/s = 173,216.4
2025-01-18 07:22:11.026 | DEBUG    | __main__:<module>:313 - Training step 8140: loss = 3.2673 | 3027.55ms | Tokens/s = 173,172.3
2025-01-18 07:22:41.295 | DEBUG    | __main__:<module>:313 - Training step 8150: loss = 3.3855 | 3027.55ms | Tokens/s = 173,172.2
2025-01-18 07:23:11.573 | DEBUG    | __main__:<module>:313 - Training step 8160: loss = 3.3393 | 3027.80ms | Tokens/s = 173,158.3
2025-01-18 07:23:41.836 | DEBUG    | __main__:<module>:313 - Training step 8170: loss = 3.3196 | 3022.12ms | Tokens/s = 173,483.5
2025-01-18 07:24:12.078 | DEBUG    | __main__:<module>:313 - Training step 8180: loss = 3.4712 | 3023.09ms | Tokens/s = 173,428.1
2025-01-18 07:24:42.336 | DEBUG    | __main__:<module>:313 - Training step 8190: loss = 3.1733 | 3024.73ms | Tokens/s = 173,333.6
2025-01-18 07:25:12.589 | DEBUG    | __main__:<module>:313 - Training step 8200: loss = 3.3287 | 3021.81ms | Tokens/s = 173,501.3
2025-01-18 07:25:42.835 | DEBUG    | __main__:<module>:313 - Training step 8210: loss = 3.2718 | 3025.86ms | Tokens/s = 173,269.1
2025-01-18 07:26:13.093 | DEBUG    | __main__:<module>:313 - Training step 8220: loss = 3.3315 | 3026.86ms | Tokens/s = 173,211.8
2025-01-18 07:26:43.362 | DEBUG    | __main__:<module>:313 - Training step 8230: loss = 3.3429 | 3026.15ms | Tokens/s = 173,252.7
2025-01-18 07:27:13.634 | DEBUG    | __main__:<module>:313 - Training step 8240: loss = 3.3413 | 3025.58ms | Tokens/s = 173,284.9
2025-01-18 07:27:43.910 | DEBUG    | __main__:<module>:313 - Training step 8250: loss = 3.4498 | 3027.69ms | Tokens/s = 173,164.6
2025-01-18 07:28:14.190 | DEBUG    | __main__:<module>:313 - Training step 8260: loss = 3.1714 | 3027.24ms | Tokens/s = 173,190.3
2025-01-18 07:28:44.439 | DEBUG    | __main__:<module>:313 - Training step 8270: loss = 3.3986 | 3024.20ms | Tokens/s = 173,364.2
2025-01-18 07:29:14.674 | DEBUG    | __main__:<module>:313 - Training step 8280: loss = 3.4254 | 3023.15ms | Tokens/s = 173,424.6
2025-01-18 07:29:44.930 | DEBUG    | __main__:<module>:313 - Training step 8290: loss = 3.3416 | 3027.93ms | Tokens/s = 173,150.7
2025-01-18 07:30:15.197 | DEBUG    | __main__:<module>:313 - Training step 8300: loss = 3.4333 | 3026.25ms | Tokens/s = 173,246.8
2025-01-18 07:30:45.467 | DEBUG    | __main__:<module>:313 - Training step 8310: loss = 3.4296 | 3026.63ms | Tokens/s = 173,225.1
2025-01-18 07:31:15.715 | DEBUG    | __main__:<module>:313 - Training step 8320: loss = 3.3895 | 3023.84ms | Tokens/s = 173,385.0
2025-01-18 07:31:45.938 | DEBUG    | __main__:<module>:313 - Training step 8330: loss = 3.3722 | 3023.02ms | Tokens/s = 173,432.2
2025-01-18 07:32:16.182 | DEBUG    | __main__:<module>:313 - Training step 8340: loss = 3.1910 | 3025.19ms | Tokens/s = 173,307.7
2025-01-18 07:32:46.437 | DEBUG    | __main__:<module>:313 - Training step 8350: loss = 3.4189 | 3027.67ms | Tokens/s = 173,165.5
2025-01-18 07:33:16.703 | DEBUG    | __main__:<module>:313 - Training step 8360: loss = 3.3619 | 3026.77ms | Tokens/s = 173,216.8
2025-01-18 07:33:46.979 | DEBUG    | __main__:<module>:313 - Training step 8370: loss = 3.4663 | 3027.55ms | Tokens/s = 173,172.2
2025-01-18 07:34:17.249 | DEBUG    | __main__:<module>:313 - Training step 8380: loss = 3.1949 | 3024.39ms | Tokens/s = 173,353.3
2025-01-18 07:34:47.485 | DEBUG    | __main__:<module>:313 - Training step 8390: loss = 3.4650 | 3019.29ms | Tokens/s = 173,646.3
2025-01-18 07:35:17.709 | DEBUG    | __main__:<module>:313 - Training step 8400: loss = 3.2063 | 3022.60ms | Tokens/s = 173,456.2
2025-01-18 07:35:47.960 | DEBUG    | __main__:<module>:313 - Training step 8410: loss = 3.4203 | 3024.76ms | Tokens/s = 173,331.9
2025-01-18 07:36:18.219 | DEBUG    | __main__:<module>:313 - Training step 8420: loss = 3.0364 | 3026.76ms | Tokens/s = 173,217.4
2025-01-18 07:36:48.494 | DEBUG    | __main__:<module>:313 - Training step 8430: loss = 3.4710 | 3025.70ms | Tokens/s = 173,278.3
2025-01-18 07:37:18.768 | DEBUG    | __main__:<module>:313 - Training step 8440: loss = 3.0571 | 3027.70ms | Tokens/s = 173,163.9
2025-01-18 07:37:49.043 | DEBUG    | __main__:<module>:313 - Training step 8450: loss = 3.4435 | 3027.07ms | Tokens/s = 173,199.6
2025-01-18 07:38:19.315 | DEBUG    | __main__:<module>:313 - Training step 8460: loss = 3.2676 | 3026.40ms | Tokens/s = 173,238.1
2025-01-18 07:38:49.594 | DEBUG    | __main__:<module>:313 - Training step 8470: loss = 3.4235 | 3027.53ms | Tokens/s = 173,173.7
2025-01-18 07:39:19.867 | DEBUG    | __main__:<module>:313 - Training step 8480: loss = 3.2592 | 3025.13ms | Tokens/s = 173,311.1
2025-01-18 07:39:50.103 | DEBUG    | __main__:<module>:313 - Training step 8490: loss = 3.1355 | 3023.95ms | Tokens/s = 173,378.3
2025-01-18 07:40:20.340 | DEBUG    | __main__:<module>:313 - Training step 8500: loss = 3.2297 | 3023.00ms | Tokens/s = 173,433.2
2025-01-18 07:40:50.589 | DEBUG    | __main__:<module>:313 - Training step 8510: loss = 3.2068 | 3024.75ms | Tokens/s = 173,332.4
2025-01-18 07:41:20.828 | DEBUG    | __main__:<module>:313 - Training step 8520: loss = 3.3754 | 3023.75ms | Tokens/s = 173,389.9
2025-01-18 07:41:51.083 | DEBUG    | __main__:<module>:313 - Training step 8530: loss = 3.2998 | 3025.02ms | Tokens/s = 173,317.5
2025-01-18 07:42:21.349 | DEBUG    | __main__:<module>:313 - Training step 8540: loss = 3.2932 | 3025.03ms | Tokens/s = 173,316.6
2025-01-18 07:42:51.586 | DEBUG    | __main__:<module>:313 - Training step 8550: loss = 3.4833 | 3025.17ms | Tokens/s = 173,308.8
2025-01-18 07:43:21.835 | DEBUG    | __main__:<module>:313 - Training step 8560: loss = 3.1332 | 3025.53ms | Tokens/s = 173,287.7
2025-01-18 07:43:52.101 | DEBUG    | __main__:<module>:313 - Training step 8570: loss = 3.2814 | 3025.78ms | Tokens/s = 173,273.7
2025-01-18 07:44:22.368 | DEBUG    | __main__:<module>:313 - Training step 8580: loss = 3.2273 | 3026.88ms | Tokens/s = 173,210.4
2025-01-18 07:44:52.645 | DEBUG    | __main__:<module>:313 - Training step 8590: loss = 3.3125 | 3026.87ms | Tokens/s = 173,211.4
2025-01-18 07:45:22.920 | DEBUG    | __main__:<module>:313 - Training step 8600: loss = 3.3918 | 3027.12ms | Tokens/s = 173,196.9
2025-01-18 07:45:53.183 | DEBUG    | __main__:<module>:313 - Training step 8610: loss = 3.2908 | 3023.97ms | Tokens/s = 173,377.5
2025-01-18 07:46:23.422 | DEBUG    | __main__:<module>:313 - Training step 8620: loss = 3.4060 | 3024.83ms | Tokens/s = 173,328.1
2025-01-18 07:46:53.690 | DEBUG    | __main__:<module>:313 - Training step 8630: loss = 3.3983 | 3027.32ms | Tokens/s = 173,185.5
2025-01-18 07:47:23.967 | DEBUG    | __main__:<module>:313 - Training step 8640: loss = 3.3034 | 3027.37ms | Tokens/s = 173,182.4
2025-01-18 07:47:54.234 | DEBUG    | __main__:<module>:313 - Training step 8650: loss = 3.3218 | 3024.85ms | Tokens/s = 173,326.9
2025-01-18 07:48:24.463 | DEBUG    | __main__:<module>:313 - Training step 8660: loss = 3.2151 | 3022.41ms | Tokens/s = 173,467.1
2025-01-18 07:48:54.688 | DEBUG    | __main__:<module>:313 - Training step 8670: loss = 3.2492 | 3023.03ms | Tokens/s = 173,431.2
2025-01-18 07:49:24.934 | DEBUG    | __main__:<module>:313 - Training step 8680: loss = 3.2774 | 3025.43ms | Tokens/s = 173,293.6
2025-01-18 07:49:55.190 | DEBUG    | __main__:<module>:313 - Training step 8690: loss = 3.1821 | 3025.32ms | Tokens/s = 173,299.9
2025-01-18 07:50:25.444 | DEBUG    | __main__:<module>:313 - Training step 8700: loss = 3.4699 | 3021.04ms | Tokens/s = 173,545.5
2025-01-18 07:50:55.654 | DEBUG    | __main__:<module>:313 - Training step 8710: loss = 3.3335 | 3019.87ms | Tokens/s = 173,612.6
2025-01-18 07:51:25.859 | DEBUG    | __main__:<module>:313 - Training step 8720: loss = 3.4582 | 3022.38ms | Tokens/s = 173,468.8
2025-01-18 07:51:56.085 | DEBUG    | __main__:<module>:313 - Training step 8730: loss = 3.2922 | 3022.52ms | Tokens/s = 173,460.6
2025-01-18 07:52:26.326 | DEBUG    | __main__:<module>:313 - Training step 8740: loss = 3.0887 | 3024.54ms | Tokens/s = 173,344.7
2025-01-18 07:52:56.579 | DEBUG    | __main__:<module>:313 - Training step 8750: loss = 3.3212 | 3027.55ms | Tokens/s = 173,172.6
2025-01-18 07:53:26.840 | DEBUG    | __main__:<module>:313 - Training step 8760: loss = 3.1877 | 3026.84ms | Tokens/s = 173,212.7
2025-01-18 07:53:57.107 | DEBUG    | __main__:<module>:313 - Training step 8770: loss = 3.4189 | 3026.67ms | Tokens/s = 173,222.8
2025-01-18 07:54:27.382 | DEBUG    | __main__:<module>:313 - Training step 8780: loss = 3.3892 | 3029.57ms | Tokens/s = 173,056.8
2025-01-18 07:54:57.647 | DEBUG    | __main__:<module>:313 - Training step 8790: loss = 3.2214 | 3027.80ms | Tokens/s = 173,158.3
2025-01-18 07:55:27.912 | DEBUG    | __main__:<module>:313 - Training step 8800: loss = 3.3581 | 3025.63ms | Tokens/s = 173,282.3
2025-01-18 07:55:58.174 | DEBUG    | __main__:<module>:313 - Training step 8810: loss = 3.3219 | 3027.43ms | Tokens/s = 173,179.2
2025-01-18 07:56:28.446 | DEBUG    | __main__:<module>:313 - Training step 8820: loss = 3.3087 | 3026.43ms | Tokens/s = 173,236.6
2025-01-18 07:56:58.727 | DEBUG    | __main__:<module>:313 - Training step 8830: loss = 3.2962 | 3029.02ms | Tokens/s = 173,088.2
2025-01-18 07:57:28.973 | DEBUG    | __main__:<module>:313 - Training step 8840: loss = 3.5067 | 3024.12ms | Tokens/s = 173,368.7
2025-01-18 07:57:59.191 | DEBUG    | __main__:<module>:313 - Training step 8850: loss = 3.2863 | 3021.60ms | Tokens/s = 173,513.5
2025-01-18 07:58:29.412 | DEBUG    | __main__:<module>:313 - Training step 8860: loss = 3.4119 | 3023.60ms | Tokens/s = 173,398.7
2025-01-18 07:58:59.660 | DEBUG    | __main__:<module>:313 - Training step 8870: loss = 3.2382 | 3026.84ms | Tokens/s = 173,212.9
2025-01-18 07:59:29.919 | DEBUG    | __main__:<module>:313 - Training step 8880: loss = 3.3423 | 3026.57ms | Tokens/s = 173,228.5
2025-01-18 08:00:00.182 | DEBUG    | __main__:<module>:313 - Training step 8890: loss = 3.4639 | 3025.93ms | Tokens/s = 173,264.9
2025-01-18 08:00:30.441 | DEBUG    | __main__:<module>:313 - Training step 8900: loss = 3.2195 | 3025.36ms | Tokens/s = 173,297.8
2025-01-18 08:01:00.693 | DEBUG    | __main__:<module>:313 - Training step 8910: loss = 3.2653 | 3024.76ms | Tokens/s = 173,331.9
2025-01-18 08:01:30.924 | DEBUG    | __main__:<module>:313 - Training step 8920: loss = 3.4309 | 3021.83ms | Tokens/s = 173,500.4
2025-01-18 08:02:01.144 | DEBUG    | __main__:<module>:313 - Training step 8930: loss = 3.3377 | 3023.38ms | Tokens/s = 173,411.4
2025-01-18 08:02:31.385 | DEBUG    | __main__:<module>:313 - Training step 8940: loss = 3.2922 | 3027.12ms | Tokens/s = 173,197.2
2025-01-18 08:03:01.631 | DEBUG    | __main__:<module>:313 - Training step 8950: loss = 3.4158 | 3022.80ms | Tokens/s = 173,444.7
2025-01-18 08:03:31.846 | DEBUG    | __main__:<module>:313 - Training step 8960: loss = 3.3559 | 3022.24ms | Tokens/s = 173,476.7
2025-01-18 08:04:02.063 | DEBUG    | __main__:<module>:313 - Training step 8970: loss = 3.3376 | 3023.11ms | Tokens/s = 173,427.0
2025-01-18 08:04:32.305 | DEBUG    | __main__:<module>:313 - Training step 8980: loss = 3.4716 | 3023.81ms | Tokens/s = 173,386.3
2025-01-18 08:05:02.561 | DEBUG    | __main__:<module>:313 - Training step 8990: loss = 3.2191 | 3024.20ms | Tokens/s = 173,364.0
2025-01-18 08:05:36.260 | INFO     | __main__:<module>:265 - Step 9,000/20,000 loss: 3.3324 (T) 3.3631 (V) | lr=6.7e-03
2025-01-18 08:05:36.261 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 08:05:49.504 | DEBUG    | __main__:<module>:313 - Training step 9000: loss = 3.2056 | 19709.80ms | Tokens/s = 26,600.4
2025-01-18 08:06:19.617 | DEBUG    | __main__:<module>:313 - Training step 9010: loss = 3.5558 | 3018.14ms | Tokens/s = 173,712.5
2025-01-18 08:06:49.821 | DEBUG    | __main__:<module>:313 - Training step 9020: loss = 3.1382 | 3022.24ms | Tokens/s = 173,476.5
2025-01-18 08:07:20.058 | DEBUG    | __main__:<module>:313 - Training step 9030: loss = 3.4014 | 3022.64ms | Tokens/s = 173,453.7
2025-01-18 08:07:50.301 | DEBUG    | __main__:<module>:313 - Training step 9040: loss = 3.0689 | 3025.89ms | Tokens/s = 173,267.4
2025-01-18 08:08:20.561 | DEBUG    | __main__:<module>:313 - Training step 9050: loss = 3.2857 | 3025.10ms | Tokens/s = 173,312.9
2025-01-18 08:08:50.816 | DEBUG    | __main__:<module>:313 - Training step 9060: loss = 3.3830 | 3024.98ms | Tokens/s = 173,319.8
2025-01-18 08:09:21.079 | DEBUG    | __main__:<module>:313 - Training step 9070: loss = 3.2386 | 3027.48ms | Tokens/s = 173,176.3
2025-01-18 08:09:51.347 | DEBUG    | __main__:<module>:313 - Training step 9080: loss = 3.2109 | 3027.21ms | Tokens/s = 173,191.7
2025-01-18 08:10:21.617 | DEBUG    | __main__:<module>:313 - Training step 9090: loss = 3.2331 | 3027.13ms | Tokens/s = 173,196.6
2025-01-18 08:10:51.886 | DEBUG    | __main__:<module>:313 - Training step 9100: loss = 3.5033 | 3025.08ms | Tokens/s = 173,313.9
2025-01-18 08:11:22.159 | DEBUG    | __main__:<module>:313 - Training step 9110: loss = 3.4603 | 3026.09ms | Tokens/s = 173,255.7
2025-01-18 08:11:52.434 | DEBUG    | __main__:<module>:313 - Training step 9120: loss = 3.5625 | 3024.83ms | Tokens/s = 173,328.1
2025-01-18 08:12:22.676 | DEBUG    | __main__:<module>:313 - Training step 9130: loss = 3.2771 | 3022.15ms | Tokens/s = 173,481.6
2025-01-18 08:12:52.930 | DEBUG    | __main__:<module>:313 - Training step 9140: loss = 3.1799 | 3026.73ms | Tokens/s = 173,219.4
2025-01-18 08:13:23.166 | DEBUG    | __main__:<module>:313 - Training step 9150: loss = 3.3413 | 3021.76ms | Tokens/s = 173,504.2
2025-01-18 08:13:53.382 | DEBUG    | __main__:<module>:313 - Training step 9160: loss = 3.3665 | 3022.15ms | Tokens/s = 173,482.0
2025-01-18 08:14:23.613 | DEBUG    | __main__:<module>:313 - Training step 9170: loss = 3.3125 | 3027.16ms | Tokens/s = 173,194.9
2025-01-18 08:14:53.865 | DEBUG    | __main__:<module>:313 - Training step 9180: loss = 3.2775 | 3025.64ms | Tokens/s = 173,281.8
2025-01-18 08:15:24.123 | DEBUG    | __main__:<module>:313 - Training step 9190: loss = 3.4179 | 3025.82ms | Tokens/s = 173,271.2
2025-01-18 08:15:54.366 | DEBUG    | __main__:<module>:313 - Training step 9200: loss = 3.3045 | 3022.01ms | Tokens/s = 173,489.8
2025-01-18 08:16:24.606 | DEBUG    | __main__:<module>:313 - Training step 9210: loss = 3.3750 | 3024.75ms | Tokens/s = 173,332.8
2025-01-18 08:16:54.841 | DEBUG    | __main__:<module>:313 - Training step 9220: loss = 3.4472 | 3022.03ms | Tokens/s = 173,488.6
2025-01-18 08:17:25.063 | DEBUG    | __main__:<module>:313 - Training step 9230: loss = 3.4364 | 3024.14ms | Tokens/s = 173,367.7
2025-01-18 08:17:55.302 | DEBUG    | __main__:<module>:313 - Training step 9240: loss = 3.2451 | 3025.31ms | Tokens/s = 173,300.7
2025-01-18 08:18:25.564 | DEBUG    | __main__:<module>:313 - Training step 9250: loss = 3.1470 | 3027.16ms | Tokens/s = 173,194.8
2025-01-18 08:18:55.830 | DEBUG    | __main__:<module>:313 - Training step 9260: loss = 3.3710 | 3025.08ms | Tokens/s = 173,313.5
2025-01-18 08:19:26.081 | DEBUG    | __main__:<module>:313 - Training step 9270: loss = 3.2319 | 3024.66ms | Tokens/s = 173,338.1
2025-01-18 08:19:56.311 | DEBUG    | __main__:<module>:313 - Training step 9280: loss = 3.1858 | 3023.85ms | Tokens/s = 173,384.0
2025-01-18 08:20:26.562 | DEBUG    | __main__:<module>:313 - Training step 9290: loss = 3.4062 | 3026.74ms | Tokens/s = 173,218.7
2025-01-18 08:20:56.823 | DEBUG    | __main__:<module>:313 - Training step 9300: loss = 3.4298 | 3024.36ms | Tokens/s = 173,355.1
2025-01-18 08:21:27.066 | DEBUG    | __main__:<module>:313 - Training step 9310: loss = 3.2770 | 3026.30ms | Tokens/s = 173,244.0
2025-01-18 08:21:57.310 | DEBUG    | __main__:<module>:313 - Training step 9320: loss = 3.3363 | 3024.31ms | Tokens/s = 173,357.7
2025-01-18 08:22:27.566 | DEBUG    | __main__:<module>:313 - Training step 9330: loss = 3.2044 | 3027.36ms | Tokens/s = 173,183.1
2025-01-18 08:22:57.828 | DEBUG    | __main__:<module>:313 - Training step 9340: loss = 3.0983 | 3024.01ms | Tokens/s = 173,375.0
2025-01-18 08:23:28.063 | DEBUG    | __main__:<module>:313 - Training step 9350: loss = 3.1737 | 3021.23ms | Tokens/s = 173,534.6
2025-01-18 08:23:58.296 | DEBUG    | __main__:<module>:313 - Training step 9360: loss = 3.2881 | 3024.35ms | Tokens/s = 173,355.5
2025-01-18 08:24:28.545 | DEBUG    | __main__:<module>:313 - Training step 9370: loss = 3.1212 | 3023.10ms | Tokens/s = 173,427.1
2025-01-18 08:24:58.810 | DEBUG    | __main__:<module>:313 - Training step 9380: loss = 3.3713 | 3027.18ms | Tokens/s = 173,193.3
2025-01-18 08:25:29.080 | DEBUG    | __main__:<module>:313 - Training step 9390: loss = 3.2723 | 3026.28ms | Tokens/s = 173,245.2
2025-01-18 08:25:59.355 | DEBUG    | __main__:<module>:313 - Training step 9400: loss = 3.3983 | 3027.59ms | Tokens/s = 173,170.0
2025-01-18 08:26:29.624 | DEBUG    | __main__:<module>:313 - Training step 9410: loss = 3.2380 | 3026.50ms | Tokens/s = 173,232.6
2025-01-18 08:26:59.862 | DEBUG    | __main__:<module>:313 - Training step 9420: loss = 3.6899 | 3025.00ms | Tokens/s = 173,318.5
2025-01-18 08:27:30.106 | DEBUG    | __main__:<module>:313 - Training step 9430: loss = 3.3132 | 3026.33ms | Tokens/s = 173,242.4
2025-01-18 08:28:00.368 | DEBUG    | __main__:<module>:313 - Training step 9440: loss = 3.3360 | 3025.89ms | Tokens/s = 173,267.3
2025-01-18 08:28:30.655 | DEBUG    | __main__:<module>:313 - Training step 9450: loss = 3.3888 | 3029.80ms | Tokens/s = 173,043.6
2025-01-18 08:29:00.943 | DEBUG    | __main__:<module>:313 - Training step 9460: loss = 3.3947 | 3027.81ms | Tokens/s = 173,157.6
2025-01-18 08:29:31.222 | DEBUG    | __main__:<module>:313 - Training step 9470: loss = 3.1289 | 3029.41ms | Tokens/s = 173,066.2
2025-01-18 08:30:01.516 | DEBUG    | __main__:<module>:313 - Training step 9480: loss = 3.2165 | 3028.11ms | Tokens/s = 173,140.2
2025-01-18 08:30:31.786 | DEBUG    | __main__:<module>:313 - Training step 9490: loss = 3.3868 | 3024.45ms | Tokens/s = 173,349.7
2025-01-18 08:31:02.035 | DEBUG    | __main__:<module>:313 - Training step 9500: loss = 3.3371 | 3023.12ms | Tokens/s = 173,426.1
2025-01-18 08:31:32.276 | DEBUG    | __main__:<module>:313 - Training step 9510: loss = 3.2567 | 3024.83ms | Tokens/s = 173,328.2
2025-01-18 08:32:02.550 | DEBUG    | __main__:<module>:313 - Training step 9520: loss = 3.3178 | 3029.09ms | Tokens/s = 173,084.4
2025-01-18 08:32:32.833 | DEBUG    | __main__:<module>:313 - Training step 9530: loss = 3.2285 | 3025.10ms | Tokens/s = 173,312.9
2025-01-18 08:33:03.095 | DEBUG    | __main__:<module>:313 - Training step 9540: loss = 3.2761 | 3024.92ms | Tokens/s = 173,323.0
2025-01-18 08:33:33.342 | DEBUG    | __main__:<module>:313 - Training step 9550: loss = 3.4207 | 3025.73ms | Tokens/s = 173,276.8
2025-01-18 08:34:03.593 | DEBUG    | __main__:<module>:313 - Training step 9560: loss = 3.1679 | 3026.34ms | Tokens/s = 173,241.6
2025-01-18 08:34:33.872 | DEBUG    | __main__:<module>:313 - Training step 9570: loss = 3.3242 | 3027.94ms | Tokens/s = 173,150.3
2025-01-18 08:35:04.167 | DEBUG    | __main__:<module>:313 - Training step 9580: loss = 3.2804 | 3028.03ms | Tokens/s = 173,145.1
2025-01-18 08:35:34.440 | DEBUG    | __main__:<module>:313 - Training step 9590: loss = 3.3174 | 3026.17ms | Tokens/s = 173,251.5
2025-01-18 08:36:04.697 | DEBUG    | __main__:<module>:313 - Training step 9600: loss = 3.3660 | 3022.07ms | Tokens/s = 173,486.6
2025-01-18 08:36:34.941 | DEBUG    | __main__:<module>:313 - Training step 9610: loss = 3.3768 | 3023.17ms | Tokens/s = 173,423.0
2025-01-18 08:37:05.174 | DEBUG    | __main__:<module>:313 - Training step 9620: loss = 3.2206 | 3022.13ms | Tokens/s = 173,482.7
2025-01-18 08:37:35.434 | DEBUG    | __main__:<module>:313 - Training step 9630: loss = 3.3929 | 3026.52ms | Tokens/s = 173,231.1
2025-01-18 08:38:05.715 | DEBUG    | __main__:<module>:313 - Training step 9640: loss = 3.2034 | 3026.36ms | Tokens/s = 173,240.5
2025-01-18 08:38:35.972 | DEBUG    | __main__:<module>:313 - Training step 9650: loss = 3.3953 | 3024.64ms | Tokens/s = 173,339.1
2025-01-18 08:39:06.238 | DEBUG    | __main__:<module>:313 - Training step 9660: loss = 3.3660 | 3029.03ms | Tokens/s = 173,087.9
2025-01-18 08:39:36.529 | DEBUG    | __main__:<module>:313 - Training step 9670: loss = 3.2410 | 3029.11ms | Tokens/s = 173,083.4
2025-01-18 08:40:06.826 | DEBUG    | __main__:<module>:313 - Training step 9680: loss = 3.3112 | 3028.01ms | Tokens/s = 173,146.3
2025-01-18 08:40:37.099 | DEBUG    | __main__:<module>:313 - Training step 9690: loss = 3.1217 | 3027.51ms | Tokens/s = 173,174.9
2025-01-18 08:41:07.352 | DEBUG    | __main__:<module>:313 - Training step 9700: loss = 3.4181 | 3022.75ms | Tokens/s = 173,447.4
2025-01-18 08:41:37.597 | DEBUG    | __main__:<module>:313 - Training step 9710: loss = 3.5453 | 3021.95ms | Tokens/s = 173,493.5
2025-01-18 08:42:07.839 | DEBUG    | __main__:<module>:313 - Training step 9720: loss = 3.2243 | 3021.64ms | Tokens/s = 173,511.3
2025-01-18 08:42:38.069 | DEBUG    | __main__:<module>:313 - Training step 9730: loss = 3.3209 | 3022.99ms | Tokens/s = 173,433.8
2025-01-18 08:43:08.326 | DEBUG    | __main__:<module>:313 - Training step 9740: loss = 3.3042 | 3027.32ms | Tokens/s = 173,185.5
2025-01-18 08:43:38.604 | DEBUG    | __main__:<module>:313 - Training step 9750: loss = 3.4405 | 3027.04ms | Tokens/s = 173,201.3
2025-01-18 08:44:08.875 | DEBUG    | __main__:<module>:313 - Training step 9760: loss = 3.2280 | 3026.42ms | Tokens/s = 173,237.0
2025-01-18 08:44:39.126 | DEBUG    | __main__:<module>:313 - Training step 9770: loss = 3.4517 | 3024.42ms | Tokens/s = 173,351.9
2025-01-18 08:45:09.370 | DEBUG    | __main__:<module>:313 - Training step 9780: loss = 3.3314 | 3025.51ms | Tokens/s = 173,289.2
2025-01-18 08:45:39.637 | DEBUG    | __main__:<module>:313 - Training step 9790: loss = 3.3548 | 3024.81ms | Tokens/s = 173,329.3
2025-01-18 08:46:09.891 | DEBUG    | __main__:<module>:313 - Training step 9800: loss = 3.2505 | 3025.80ms | Tokens/s = 173,272.5
2025-01-18 08:46:40.147 | DEBUG    | __main__:<module>:313 - Training step 9810: loss = 3.5055 | 3027.69ms | Tokens/s = 173,164.5
2025-01-18 08:47:10.432 | DEBUG    | __main__:<module>:313 - Training step 9820: loss = 3.3128 | 3028.68ms | Tokens/s = 173,107.9
2025-01-18 08:47:40.708 | DEBUG    | __main__:<module>:313 - Training step 9830: loss = 3.3311 | 3026.97ms | Tokens/s = 173,205.8
2025-01-18 08:48:10.959 | DEBUG    | __main__:<module>:313 - Training step 9840: loss = 3.3504 | 3025.46ms | Tokens/s = 173,291.8
2025-01-18 08:48:41.211 | DEBUG    | __main__:<module>:313 - Training step 9850: loss = 3.3425 | 3027.91ms | Tokens/s = 173,151.7
2025-01-18 08:49:11.495 | DEBUG    | __main__:<module>:313 - Training step 9860: loss = 3.3562 | 3028.37ms | Tokens/s = 173,125.4
2025-01-18 08:49:41.788 | DEBUG    | __main__:<module>:313 - Training step 9870: loss = 3.3263 | 3027.93ms | Tokens/s = 173,150.4
2025-01-18 08:50:12.058 | DEBUG    | __main__:<module>:313 - Training step 9880: loss = 3.3721 | 3025.32ms | Tokens/s = 173,299.8
2025-01-18 08:50:42.317 | DEBUG    | __main__:<module>:313 - Training step 9890: loss = 3.2802 | 3026.28ms | Tokens/s = 173,245.0
2025-01-18 08:51:12.566 | DEBUG    | __main__:<module>:313 - Training step 9900: loss = 3.1139 | 3023.30ms | Tokens/s = 173,416.0
2025-01-18 08:51:42.803 | DEBUG    | __main__:<module>:313 - Training step 9910: loss = 3.4529 | 3022.01ms | Tokens/s = 173,489.8
2025-01-18 08:52:13.042 | DEBUG    | __main__:<module>:313 - Training step 9920: loss = 3.3804 | 3024.01ms | Tokens/s = 173,375.1
2025-01-18 08:52:43.303 | DEBUG    | __main__:<module>:313 - Training step 9930: loss = 3.2006 | 3028.10ms | Tokens/s = 173,140.9
2025-01-18 08:53:13.589 | DEBUG    | __main__:<module>:313 - Training step 9940: loss = 3.2795 | 3027.47ms | Tokens/s = 173,177.2
2025-01-18 08:53:43.884 | DEBUG    | __main__:<module>:313 - Training step 9950: loss = 3.3600 | 3027.53ms | Tokens/s = 173,173.4
2025-01-18 08:54:14.145 | DEBUG    | __main__:<module>:313 - Training step 9960: loss = 3.2031 | 3025.64ms | Tokens/s = 173,281.8
2025-01-18 08:54:44.390 | DEBUG    | __main__:<module>:313 - Training step 9970: loss = 3.5226 | 3024.45ms | Tokens/s = 173,350.0
2025-01-18 08:55:14.634 | DEBUG    | __main__:<module>:313 - Training step 9980: loss = 3.2404 | 3024.23ms | Tokens/s = 173,362.5
2025-01-18 08:55:44.905 | DEBUG    | __main__:<module>:313 - Training step 9990: loss = 3.2998 | 3029.46ms | Tokens/s = 173,063.1
2025-01-18 08:56:18.658 | INFO     | __main__:<module>:265 - Step 10,000/20,000 loss: 3.3047 (T) 3.3172 (V) | lr=5.9e-03
2025-01-18 08:56:18.659 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 08:56:31.932 | DEBUG    | __main__:<module>:313 - Training step 10000: loss = 3.4170 | 19752.61ms | Tokens/s = 26,542.7
2025-01-18 08:57:02.075 | DEBUG    | __main__:<module>:313 - Training step 10010: loss = 3.2518 | 3021.87ms | Tokens/s = 173,498.0
2025-01-18 08:57:32.311 | DEBUG    | __main__:<module>:313 - Training step 10020: loss = 3.4181 | 3025.13ms | Tokens/s = 173,311.1
2025-01-18 08:58:02.581 | DEBUG    | __main__:<module>:313 - Training step 10030: loss = 3.3995 | 3027.68ms | Tokens/s = 173,165.0
2025-01-18 08:58:32.866 | DEBUG    | __main__:<module>:313 - Training step 10040: loss = 3.1578 | 3031.15ms | Tokens/s = 172,966.6
2025-01-18 08:59:03.130 | DEBUG    | __main__:<module>:313 - Training step 10050: loss = 3.5067 | 3025.92ms | Tokens/s = 173,265.5
2025-01-18 08:59:33.381 | DEBUG    | __main__:<module>:313 - Training step 10060: loss = 3.3238 | 3025.79ms | Tokens/s = 173,273.2
2025-01-18 09:00:03.620 | DEBUG    | __main__:<module>:313 - Training step 10070: loss = 3.3454 | 3023.35ms | Tokens/s = 173,413.1
2025-01-18 09:00:33.855 | DEBUG    | __main__:<module>:313 - Training step 10080: loss = 3.2524 | 3022.30ms | Tokens/s = 173,472.9
2025-01-18 09:01:04.087 | DEBUG    | __main__:<module>:313 - Training step 10090: loss = 3.3545 | 3023.61ms | Tokens/s = 173,398.0
2025-01-18 09:01:34.356 | DEBUG    | __main__:<module>:313 - Training step 10100: loss = 3.1755 | 3027.01ms | Tokens/s = 173,203.2
2025-01-18 09:02:04.648 | DEBUG    | __main__:<module>:313 - Training step 10110: loss = 3.1865 | 3029.49ms | Tokens/s = 173,061.5
2025-01-18 09:02:34.919 | DEBUG    | __main__:<module>:313 - Training step 10120: loss = 3.3186 | 3025.33ms | Tokens/s = 173,299.2
2025-01-18 09:03:05.174 | DEBUG    | __main__:<module>:313 - Training step 10130: loss = 3.2220 | 3026.94ms | Tokens/s = 173,207.2
2025-01-18 09:03:35.416 | DEBUG    | __main__:<module>:313 - Training step 10140: loss = 3.1787 | 3024.45ms | Tokens/s = 173,350.1
2025-01-18 09:04:05.645 | DEBUG    | __main__:<module>:313 - Training step 10150: loss = 3.3793 | 3023.06ms | Tokens/s = 173,429.7
2025-01-18 09:04:35.904 | DEBUG    | __main__:<module>:313 - Training step 10160: loss = 3.2878 | 3026.85ms | Tokens/s = 173,212.3
2025-01-18 09:05:06.185 | DEBUG    | __main__:<module>:313 - Training step 10170: loss = 3.3375 | 3028.43ms | Tokens/s = 173,122.1
2025-01-18 09:05:36.440 | DEBUG    | __main__:<module>:313 - Training step 10180: loss = 3.1095 | 3024.11ms | Tokens/s = 173,369.4
2025-01-18 09:06:06.678 | DEBUG    | __main__:<module>:313 - Training step 10190: loss = 3.3021 | 3023.64ms | Tokens/s = 173,396.1
2025-01-18 09:06:36.951 | DEBUG    | __main__:<module>:313 - Training step 10200: loss = 3.3964 | 3029.38ms | Tokens/s = 173,067.9
2025-01-18 09:07:07.236 | DEBUG    | __main__:<module>:313 - Training step 10210: loss = 3.1985 | 3027.34ms | Tokens/s = 173,184.2
2025-01-18 09:07:37.498 | DEBUG    | __main__:<module>:313 - Training step 10220: loss = 3.3493 | 3026.91ms | Tokens/s = 173,209.3
2025-01-18 09:08:07.757 | DEBUG    | __main__:<module>:313 - Training step 10230: loss = 3.5026 | 3024.49ms | Tokens/s = 173,347.7
2025-01-18 09:08:38.015 | DEBUG    | __main__:<module>:313 - Training step 10240: loss = 3.1813 | 3028.27ms | Tokens/s = 173,131.2
2025-01-18 09:09:08.304 | DEBUG    | __main__:<module>:313 - Training step 10250: loss = 3.3344 | 3028.88ms | Tokens/s = 173,096.1
2025-01-18 09:09:38.581 | DEBUG    | __main__:<module>:313 - Training step 10260: loss = 3.2130 | 3027.29ms | Tokens/s = 173,187.4
2025-01-18 09:10:08.836 | DEBUG    | __main__:<module>:313 - Training step 10270: loss = 3.4379 | 3025.48ms | Tokens/s = 173,291.0
2025-01-18 09:10:39.083 | DEBUG    | __main__:<module>:313 - Training step 10280: loss = 3.4778 | 3023.80ms | Tokens/s = 173,387.3
2025-01-18 09:11:09.346 | DEBUG    | __main__:<module>:313 - Training step 10290: loss = 3.4030 | 3028.40ms | Tokens/s = 173,123.8
2025-01-18 09:11:39.628 | DEBUG    | __main__:<module>:313 - Training step 10300: loss = 3.3965 | 3026.39ms | Tokens/s = 173,238.5
2025-01-18 09:12:09.917 | DEBUG    | __main__:<module>:313 - Training step 10310: loss = 3.3128 | 3030.31ms | Tokens/s = 173,014.4
2025-01-18 09:12:40.215 | DEBUG    | __main__:<module>:313 - Training step 10320: loss = 3.3316 | 3030.12ms | Tokens/s = 173,025.7
2025-01-18 09:13:10.483 | DEBUG    | __main__:<module>:313 - Training step 10330: loss = 3.2642 | 3025.98ms | Tokens/s = 173,262.0
2025-01-18 09:13:40.740 | DEBUG    | __main__:<module>:313 - Training step 10340: loss = 3.0785 | 3026.40ms | Tokens/s = 173,238.4
2025-01-18 09:14:10.971 | DEBUG    | __main__:<module>:313 - Training step 10350: loss = 3.2291 | 3021.98ms | Tokens/s = 173,491.3
2025-01-18 09:14:41.211 | DEBUG    | __main__:<module>:313 - Training step 10360: loss = 3.4760 | 3024.77ms | Tokens/s = 173,331.8
2025-01-18 09:15:11.453 | DEBUG    | __main__:<module>:313 - Training step 10370: loss = 3.4127 | 3021.63ms | Tokens/s = 173,511.6
2025-01-18 09:15:41.725 | DEBUG    | __main__:<module>:313 - Training step 10380: loss = 3.2174 | 3028.77ms | Tokens/s = 173,102.8
2025-01-18 09:16:12.009 | DEBUG    | __main__:<module>:313 - Training step 10390: loss = 3.2286 | 3028.26ms | Tokens/s = 173,131.6
2025-01-18 09:16:42.279 | DEBUG    | __main__:<module>:313 - Training step 10400: loss = 3.3043 | 3026.48ms | Tokens/s = 173,233.5
2025-01-18 09:17:12.538 | DEBUG    | __main__:<module>:313 - Training step 10410: loss = 3.2456 | 3026.00ms | Tokens/s = 173,260.9
2025-01-18 09:17:42.796 | DEBUG    | __main__:<module>:313 - Training step 10420: loss = 3.3822 | 3026.36ms | Tokens/s = 173,240.2
2025-01-18 09:18:13.074 | DEBUG    | __main__:<module>:313 - Training step 10430: loss = 3.4129 | 3028.31ms | Tokens/s = 173,128.7
2025-01-18 09:18:43.338 | DEBUG    | __main__:<module>:313 - Training step 10440: loss = 3.4140 | 3026.74ms | Tokens/s = 173,218.6
2025-01-18 09:19:13.622 | DEBUG    | __main__:<module>:313 - Training step 10450: loss = 3.2495 | 3029.39ms | Tokens/s = 173,067.0
2025-01-18 09:19:43.899 | DEBUG    | __main__:<module>:313 - Training step 10460: loss = 3.0165 | 3029.32ms | Tokens/s = 173,071.3
2025-01-18 09:20:14.180 | DEBUG    | __main__:<module>:313 - Training step 10470: loss = 3.4788 | 3027.90ms | Tokens/s = 173,152.3
2025-01-18 09:20:44.451 | DEBUG    | __main__:<module>:313 - Training step 10480: loss = 3.2551 | 3026.54ms | Tokens/s = 173,230.3
2025-01-18 09:21:14.720 | DEBUG    | __main__:<module>:313 - Training step 10490: loss = 3.3357 | 3028.12ms | Tokens/s = 173,140.0
2025-01-18 09:21:45.003 | DEBUG    | __main__:<module>:313 - Training step 10500: loss = 3.4701 | 3026.07ms | Tokens/s = 173,257.2
2025-01-18 09:22:15.268 | DEBUG    | __main__:<module>:313 - Training step 10510: loss = 3.2671 | 3025.39ms | Tokens/s = 173,296.1
2025-01-18 09:22:45.519 | DEBUG    | __main__:<module>:313 - Training step 10520: loss = 3.3119 | 3025.16ms | Tokens/s = 173,309.1
2025-01-18 09:23:15.792 | DEBUG    | __main__:<module>:313 - Training step 10530: loss = 3.2097 | 3028.84ms | Tokens/s = 173,098.4
2025-01-18 09:23:46.077 | DEBUG    | __main__:<module>:313 - Training step 10540: loss = 3.4353 | 3028.57ms | Tokens/s = 173,113.8
2025-01-18 09:24:16.350 | DEBUG    | __main__:<module>:313 - Training step 10550: loss = 3.2800 | 3027.45ms | Tokens/s = 173,178.2
2025-01-18 09:24:46.608 | DEBUG    | __main__:<module>:313 - Training step 10560: loss = 3.4216 | 3025.18ms | Tokens/s = 173,307.9
2025-01-18 09:25:16.862 | DEBUG    | __main__:<module>:313 - Training step 10570: loss = 3.1059 | 3025.78ms | Tokens/s = 173,273.5
2025-01-18 09:25:47.120 | DEBUG    | __main__:<module>:313 - Training step 10580: loss = 3.1984 | 3025.30ms | Tokens/s = 173,301.4
2025-01-18 09:26:17.359 | DEBUG    | __main__:<module>:313 - Training step 10590: loss = 3.2286 | 3024.37ms | Tokens/s = 173,354.7
2025-01-18 09:26:47.597 | DEBUG    | __main__:<module>:313 - Training step 10600: loss = 3.4377 | 3022.10ms | Tokens/s = 173,484.6
2025-01-18 09:27:17.822 | DEBUG    | __main__:<module>:313 - Training step 10610: loss = 3.3462 | 3022.19ms | Tokens/s = 173,479.4
2025-01-18 09:27:48.073 | DEBUG    | __main__:<module>:313 - Training step 10620: loss = 3.2002 | 3027.73ms | Tokens/s = 173,162.0
2025-01-18 09:28:18.356 | DEBUG    | __main__:<module>:313 - Training step 10630: loss = 3.4237 | 3029.57ms | Tokens/s = 173,056.7
2025-01-18 09:28:48.630 | DEBUG    | __main__:<module>:313 - Training step 10640: loss = 3.1673 | 3025.42ms | Tokens/s = 173,294.1
2025-01-18 09:29:18.882 | DEBUG    | __main__:<module>:313 - Training step 10650: loss = 3.2981 | 3024.98ms | Tokens/s = 173,319.5
2025-01-18 09:29:49.117 | DEBUG    | __main__:<module>:313 - Training step 10660: loss = 3.3972 | 3022.13ms | Tokens/s = 173,482.7
2025-01-18 09:30:19.352 | DEBUG    | __main__:<module>:313 - Training step 10670: loss = 3.4590 | 3022.68ms | Tokens/s = 173,451.5
2025-01-18 09:30:49.579 | DEBUG    | __main__:<module>:313 - Training step 10680: loss = 3.2887 | 3022.56ms | Tokens/s = 173,458.4
2025-01-18 09:31:19.804 | DEBUG    | __main__:<module>:313 - Training step 10690: loss = 3.3362 | 3023.64ms | Tokens/s = 173,396.3
2025-01-18 09:31:50.056 | DEBUG    | __main__:<module>:313 - Training step 10700: loss = 3.2651 | 3024.77ms | Tokens/s = 173,331.4
2025-01-18 09:32:20.331 | DEBUG    | __main__:<module>:313 - Training step 10710: loss = 3.2786 | 3026.33ms | Tokens/s = 173,242.2
2025-01-18 09:32:50.594 | DEBUG    | __main__:<module>:313 - Training step 10720: loss = 3.1813 | 3030.22ms | Tokens/s = 173,020.1
2025-01-18 09:33:20.869 | DEBUG    | __main__:<module>:313 - Training step 10730: loss = 3.2071 | 3026.83ms | Tokens/s = 173,213.3
2025-01-18 09:33:51.132 | DEBUG    | __main__:<module>:313 - Training step 10740: loss = 3.2465 | 3023.63ms | Tokens/s = 173,397.0
2025-01-18 09:34:21.377 | DEBUG    | __main__:<module>:313 - Training step 10750: loss = 3.1756 | 3023.11ms | Tokens/s = 173,426.5
2025-01-18 09:34:51.628 | DEBUG    | __main__:<module>:313 - Training step 10760: loss = 3.2500 | 3026.89ms | Tokens/s = 173,210.0
2025-01-18 09:35:21.897 | DEBUG    | __main__:<module>:313 - Training step 10770: loss = 3.1602 | 3024.57ms | Tokens/s = 173,343.1
2025-01-18 09:35:52.160 | DEBUG    | __main__:<module>:313 - Training step 10780: loss = 3.1108 | 3027.32ms | Tokens/s = 173,185.4
2025-01-18 09:36:22.401 | DEBUG    | __main__:<module>:313 - Training step 10790: loss = 3.1930 | 3025.34ms | Tokens/s = 173,299.0
2025-01-18 09:36:52.676 | DEBUG    | __main__:<module>:313 - Training step 10800: loss = 3.2729 | 3028.10ms | Tokens/s = 173,141.1
2025-01-18 09:37:22.974 | DEBUG    | __main__:<module>:313 - Training step 10810: loss = 3.2111 | 3031.88ms | Tokens/s = 172,925.1
2025-01-18 09:37:53.268 | DEBUG    | __main__:<module>:313 - Training step 10820: loss = 3.2625 | 3028.09ms | Tokens/s = 173,141.2
2025-01-18 09:38:23.537 | DEBUG    | __main__:<module>:313 - Training step 10830: loss = 3.3262 | 3025.82ms | Tokens/s = 173,271.1
2025-01-18 09:38:53.786 | DEBUG    | __main__:<module>:313 - Training step 10840: loss = 3.2002 | 3025.11ms | Tokens/s = 173,311.9
2025-01-18 09:39:24.031 | DEBUG    | __main__:<module>:313 - Training step 10850: loss = 3.2836 | 3023.37ms | Tokens/s = 173,411.8
2025-01-18 09:39:54.307 | DEBUG    | __main__:<module>:313 - Training step 10860: loss = 3.2028 | 3027.80ms | Tokens/s = 173,158.1
2025-01-18 09:40:24.592 | DEBUG    | __main__:<module>:313 - Training step 10870: loss = 3.1985 | 3027.63ms | Tokens/s = 173,167.7
2025-01-18 09:40:54.858 | DEBUG    | __main__:<module>:313 - Training step 10880: loss = 3.2800 | 3025.87ms | Tokens/s = 173,268.3
2025-01-18 09:41:25.145 | DEBUG    | __main__:<module>:313 - Training step 10890: loss = 3.2738 | 3030.56ms | Tokens/s = 173,000.2
2025-01-18 09:41:55.433 | DEBUG    | __main__:<module>:313 - Training step 10900: loss = 3.0904 | 3026.30ms | Tokens/s = 173,244.0
2025-01-18 09:42:25.703 | DEBUG    | __main__:<module>:313 - Training step 10910: loss = 3.2750 | 3026.51ms | Tokens/s = 173,231.6
2025-01-18 09:42:55.960 | DEBUG    | __main__:<module>:313 - Training step 10920: loss = 3.1878 | 3024.60ms | Tokens/s = 173,341.5
2025-01-18 09:43:26.212 | DEBUG    | __main__:<module>:313 - Training step 10930: loss = 3.1836 | 3023.76ms | Tokens/s = 173,389.7
2025-01-18 09:43:56.461 | DEBUG    | __main__:<module>:313 - Training step 10940: loss = 3.3129 | 3023.75ms | Tokens/s = 173,390.3
2025-01-18 09:44:26.690 | DEBUG    | __main__:<module>:313 - Training step 10950: loss = 3.2110 | 3023.49ms | Tokens/s = 173,404.8
2025-01-18 09:44:56.933 | DEBUG    | __main__:<module>:313 - Training step 10960: loss = 3.3071 | 3026.56ms | Tokens/s = 173,229.1
2025-01-18 09:45:27.205 | DEBUG    | __main__:<module>:313 - Training step 10970: loss = 3.2814 | 3028.23ms | Tokens/s = 173,133.3
2025-01-18 09:45:57.503 | DEBUG    | __main__:<module>:313 - Training step 10980: loss = 3.2238 | 3030.16ms | Tokens/s = 173,023.2
2025-01-18 09:46:27.782 | DEBUG    | __main__:<module>:313 - Training step 10990: loss = 3.2136 | 3027.34ms | Tokens/s = 173,184.7
2025-01-18 09:47:01.477 | INFO     | __main__:<module>:265 - Step 11,000/20,000 loss: 3.2709 (T) 3.2548 (V) | lr=5.0e-03
2025-01-18 09:47:01.478 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 09:47:14.602 | DEBUG    | __main__:<module>:313 - Training step 11000: loss = 3.1985 | 19587.87ms | Tokens/s = 26,766.0
2025-01-18 09:47:44.711 | DEBUG    | __main__:<module>:313 - Training step 11010: loss = 3.2499 | 3017.62ms | Tokens/s = 173,742.4
2025-01-18 09:48:14.933 | DEBUG    | __main__:<module>:313 - Training step 11020: loss = 3.3025 | 3026.10ms | Tokens/s = 173,255.3
2025-01-18 09:48:45.190 | DEBUG    | __main__:<module>:313 - Training step 11030: loss = 3.3399 | 3027.49ms | Tokens/s = 173,175.9
2025-01-18 09:49:15.480 | DEBUG    | __main__:<module>:313 - Training step 11040: loss = 3.3004 | 3028.92ms | Tokens/s = 173,094.0
2025-01-18 09:49:45.762 | DEBUG    | __main__:<module>:313 - Training step 11050: loss = 3.4092 | 3028.32ms | Tokens/s = 173,128.4
2025-01-18 09:50:16.021 | DEBUG    | __main__:<module>:313 - Training step 11060: loss = 3.2682 | 3025.87ms | Tokens/s = 173,268.2
2025-01-18 09:50:46.266 | DEBUG    | __main__:<module>:313 - Training step 11070: loss = 3.2374 | 3026.11ms | Tokens/s = 173,254.7
2025-01-18 09:51:16.546 | DEBUG    | __main__:<module>:313 - Training step 11080: loss = 3.1756 | 3028.50ms | Tokens/s = 173,118.1
2025-01-18 09:51:46.817 | DEBUG    | __main__:<module>:313 - Training step 11090: loss = 3.4218 | 3026.38ms | Tokens/s = 173,239.3
2025-01-18 09:52:17.077 | DEBUG    | __main__:<module>:313 - Training step 11100: loss = 3.1658 | 3025.08ms | Tokens/s = 173,313.5
2025-01-18 09:52:47.331 | DEBUG    | __main__:<module>:313 - Training step 11110: loss = 3.2235 | 3024.81ms | Tokens/s = 173,329.1
2025-01-18 09:53:17.578 | DEBUG    | __main__:<module>:313 - Training step 11120: loss = 3.3490 | 3024.71ms | Tokens/s = 173,334.8
2025-01-18 09:53:47.856 | DEBUG    | __main__:<module>:313 - Training step 11130: loss = 3.2911 | 3027.62ms | Tokens/s = 173,168.1
2025-01-18 09:54:18.149 | DEBUG    | __main__:<module>:313 - Training step 11140: loss = 3.1487 | 3032.81ms | Tokens/s = 172,872.2
2025-01-18 09:54:48.430 | DEBUG    | __main__:<module>:313 - Training step 11150: loss = 3.3537 | 3027.18ms | Tokens/s = 173,193.8
2025-01-18 09:55:18.687 | DEBUG    | __main__:<module>:313 - Training step 11160: loss = 3.3841 | 3024.63ms | Tokens/s = 173,339.3
2025-01-18 09:55:48.935 | DEBUG    | __main__:<module>:313 - Training step 11170: loss = 3.2188 | 3024.77ms | Tokens/s = 173,331.8
2025-01-18 09:56:19.171 | DEBUG    | __main__:<module>:313 - Training step 11180: loss = 3.1585 | 3023.30ms | Tokens/s = 173,415.6
2025-01-18 09:56:49.419 | DEBUG    | __main__:<module>:313 - Training step 11190: loss = 3.4178 | 3025.73ms | Tokens/s = 173,276.3
2025-01-18 09:57:19.701 | DEBUG    | __main__:<module>:313 - Training step 11200: loss = 3.2775 | 3027.53ms | Tokens/s = 173,173.7
2025-01-18 09:57:49.999 | DEBUG    | __main__:<module>:313 - Training step 11210: loss = 3.2732 | 3028.54ms | Tokens/s = 173,115.5
2025-01-18 09:58:20.270 | DEBUG    | __main__:<module>:313 - Training step 11220: loss = 3.1243 | 3026.94ms | Tokens/s = 173,207.5
2025-01-18 09:58:50.525 | DEBUG    | __main__:<module>:313 - Training step 11230: loss = 3.2731 | 3025.41ms | Tokens/s = 173,295.0
2025-01-18 09:59:20.761 | DEBUG    | __main__:<module>:313 - Training step 11240: loss = 3.1586 | 3023.50ms | Tokens/s = 173,404.2
2025-01-18 09:59:50.998 | DEBUG    | __main__:<module>:313 - Training step 11250: loss = 3.2351 | 3025.26ms | Tokens/s = 173,303.3
2025-01-18 10:00:21.274 | DEBUG    | __main__:<module>:313 - Training step 11260: loss = 3.1439 | 3028.84ms | Tokens/s = 173,098.5
2025-01-18 10:00:51.553 | DEBUG    | __main__:<module>:313 - Training step 11270: loss = 3.2995 | 3027.44ms | Tokens/s = 173,178.8
2025-01-18 10:01:21.815 | DEBUG    | __main__:<module>:313 - Training step 11280: loss = 3.3683 | 3022.97ms | Tokens/s = 173,434.5
2025-01-18 10:01:52.068 | DEBUG    | __main__:<module>:313 - Training step 11290: loss = 2.9930 | 3024.42ms | Tokens/s = 173,351.3
2025-01-18 10:02:22.323 | DEBUG    | __main__:<module>:313 - Training step 11300: loss = 3.2013 | 3028.75ms | Tokens/s = 173,103.9
2025-01-18 10:02:52.600 | DEBUG    | __main__:<module>:313 - Training step 11310: loss = 3.3382 | 3025.34ms | Tokens/s = 173,299.0
2025-01-18 10:03:22.853 | DEBUG    | __main__:<module>:313 - Training step 11320: loss = 2.9471 | 3024.29ms | Tokens/s = 173,359.2
2025-01-18 10:03:53.095 | DEBUG    | __main__:<module>:313 - Training step 11330: loss = 3.3310 | 3024.87ms | Tokens/s = 173,326.0
2025-01-18 10:04:23.332 | DEBUG    | __main__:<module>:313 - Training step 11340: loss = 3.1524 | 3024.96ms | Tokens/s = 173,320.7
2025-01-18 10:04:53.611 | DEBUG    | __main__:<module>:313 - Training step 11350: loss = 3.1448 | 3027.60ms | Tokens/s = 173,169.7
2025-01-18 10:05:23.890 | DEBUG    | __main__:<module>:313 - Training step 11360: loss = 3.2713 | 3026.74ms | Tokens/s = 173,218.8
2025-01-18 10:05:54.143 | DEBUG    | __main__:<module>:313 - Training step 11370: loss = 3.3045 | 3023.42ms | Tokens/s = 173,409.1
2025-01-18 10:06:24.393 | DEBUG    | __main__:<module>:313 - Training step 11380: loss = 3.2238 | 3023.30ms | Tokens/s = 173,415.9
2025-01-18 10:06:54.630 | DEBUG    | __main__:<module>:313 - Training step 11390: loss = 3.1277 | 3022.40ms | Tokens/s = 173,467.6
2025-01-18 10:07:24.898 | DEBUG    | __main__:<module>:313 - Training step 11400: loss = 3.2165 | 3028.35ms | Tokens/s = 173,126.7
2025-01-18 10:07:55.174 | DEBUG    | __main__:<module>:313 - Training step 11410: loss = 3.2802 | 3026.71ms | Tokens/s = 173,220.2
2025-01-18 10:08:25.433 | DEBUG    | __main__:<module>:313 - Training step 11420: loss = 3.1998 | 3025.34ms | Tokens/s = 173,298.6
2025-01-18 10:08:55.675 | DEBUG    | __main__:<module>:313 - Training step 11430: loss = 3.3769 | 3023.07ms | Tokens/s = 173,429.0
2025-01-18 10:09:25.934 | DEBUG    | __main__:<module>:313 - Training step 11440: loss = 3.1681 | 3027.29ms | Tokens/s = 173,187.4
2025-01-18 10:09:56.213 | DEBUG    | __main__:<module>:313 - Training step 11450: loss = 3.2065 | 3028.09ms | Tokens/s = 173,141.7
2025-01-18 10:10:26.516 | DEBUG    | __main__:<module>:313 - Training step 11460: loss = 3.1914 | 3028.49ms | Tokens/s = 173,118.5
2025-01-18 10:10:56.798 | DEBUG    | __main__:<module>:313 - Training step 11470: loss = 3.1174 | 3027.28ms | Tokens/s = 173,187.9
2025-01-18 10:11:27.054 | DEBUG    | __main__:<module>:313 - Training step 11480: loss = 3.4424 | 3023.62ms | Tokens/s = 173,397.5
2025-01-18 10:11:57.302 | DEBUG    | __main__:<module>:313 - Training step 11490: loss = 3.4263 | 3027.07ms | Tokens/s = 173,199.8
2025-01-18 10:12:27.577 | DEBUG    | __main__:<module>:313 - Training step 11500: loss = 3.2633 | 3029.71ms | Tokens/s = 173,048.6
2025-01-18 10:12:57.857 | DEBUG    | __main__:<module>:313 - Training step 11510: loss = 3.3021 | 3025.64ms | Tokens/s = 173,281.4
2025-01-18 10:13:28.116 | DEBUG    | __main__:<module>:313 - Training step 11520: loss = 3.1314 | 3024.09ms | Tokens/s = 173,370.3
2025-01-18 10:13:58.362 | DEBUG    | __main__:<module>:313 - Training step 11530: loss = 3.1449 | 3027.11ms | Tokens/s = 173,197.7
2025-01-18 10:14:28.625 | DEBUG    | __main__:<module>:313 - Training step 11540: loss = 3.1526 | 3026.80ms | Tokens/s = 173,215.4
2025-01-18 10:14:58.907 | DEBUG    | __main__:<module>:313 - Training step 11550: loss = 3.3158 | 3027.88ms | Tokens/s = 173,153.7
2025-01-18 10:15:29.201 | DEBUG    | __main__:<module>:313 - Training step 11560: loss = 3.3131 | 3030.46ms | Tokens/s = 173,005.8
2025-01-18 10:15:59.481 | DEBUG    | __main__:<module>:313 - Training step 11570: loss = 3.2389 | 3026.61ms | Tokens/s = 173,225.9
2025-01-18 10:16:29.741 | DEBUG    | __main__:<module>:313 - Training step 11580: loss = 3.2094 | 3025.46ms | Tokens/s = 173,292.1
2025-01-18 10:16:59.993 | DEBUG    | __main__:<module>:313 - Training step 11590: loss = 3.2507 | 3023.75ms | Tokens/s = 173,390.0
2025-01-18 10:17:30.254 | DEBUG    | __main__:<module>:313 - Training step 11600: loss = 3.1975 | 3026.92ms | Tokens/s = 173,208.2
2025-01-18 10:18:00.533 | DEBUG    | __main__:<module>:313 - Training step 11610: loss = 3.3774 | 3025.94ms | Tokens/s = 173,264.5
2025-01-18 10:18:30.798 | DEBUG    | __main__:<module>:313 - Training step 11620: loss = 3.2444 | 3026.48ms | Tokens/s = 173,233.8
2025-01-18 10:19:01.039 | DEBUG    | __main__:<module>:313 - Training step 11630: loss = 3.2484 | 3023.81ms | Tokens/s = 173,386.7
2025-01-18 10:19:31.294 | DEBUG    | __main__:<module>:313 - Training step 11640: loss = 3.2771 | 3026.52ms | Tokens/s = 173,231.1
2025-01-18 10:20:01.577 | DEBUG    | __main__:<module>:313 - Training step 11650: loss = 3.2800 | 3030.07ms | Tokens/s = 173,028.2
2025-01-18 10:20:31.871 | DEBUG    | __main__:<module>:313 - Training step 11660: loss = 3.2095 | 3028.09ms | Tokens/s = 173,141.5
2025-01-18 10:21:02.143 | DEBUG    | __main__:<module>:313 - Training step 11670: loss = 3.2565 | 3024.64ms | Tokens/s = 173,338.9
2025-01-18 10:21:32.392 | DEBUG    | __main__:<module>:313 - Training step 11680: loss = 3.2534 | 3023.78ms | Tokens/s = 173,388.6
2025-01-18 10:22:02.635 | DEBUG    | __main__:<module>:313 - Training step 11690: loss = 3.3221 | 3025.03ms | Tokens/s = 173,316.5
2025-01-18 10:22:32.864 | DEBUG    | __main__:<module>:313 - Training step 11700: loss = 3.1219 | 3020.99ms | Tokens/s = 173,548.5
2025-01-18 10:23:03.111 | DEBUG    | __main__:<module>:313 - Training step 11710: loss = 3.2607 | 3027.57ms | Tokens/s = 173,171.2
2025-01-18 10:23:33.383 | DEBUG    | __main__:<module>:313 - Training step 11720: loss = 3.3419 | 3027.44ms | Tokens/s = 173,178.9
2025-01-18 10:24:03.657 | DEBUG    | __main__:<module>:313 - Training step 11730: loss = 3.3444 | 3024.94ms | Tokens/s = 173,322.0
2025-01-18 10:24:33.911 | DEBUG    | __main__:<module>:313 - Training step 11740: loss = 3.2073 | 3025.23ms | Tokens/s = 173,305.3
2025-01-18 10:25:04.147 | DEBUG    | __main__:<module>:313 - Training step 11750: loss = 3.3386 | 3024.09ms | Tokens/s = 173,370.4
2025-01-18 10:25:34.379 | DEBUG    | __main__:<module>:313 - Training step 11760: loss = 3.1878 | 3021.43ms | Tokens/s = 173,523.3
2025-01-18 10:26:04.612 | DEBUG    | __main__:<module>:313 - Training step 11770: loss = 3.2833 | 3022.27ms | Tokens/s = 173,475.1
2025-01-18 10:26:34.869 | DEBUG    | __main__:<module>:313 - Training step 11780: loss = 3.5102 | 3028.14ms | Tokens/s = 173,138.5
2025-01-18 10:27:05.146 | DEBUG    | __main__:<module>:313 - Training step 11790: loss = 3.2964 | 3025.05ms | Tokens/s = 173,315.5
2025-01-18 10:27:35.401 | DEBUG    | __main__:<module>:313 - Training step 11800: loss = 3.2849 | 3024.85ms | Tokens/s = 173,326.7
2025-01-18 10:28:05.644 | DEBUG    | __main__:<module>:313 - Training step 11810: loss = 3.1896 | 3024.85ms | Tokens/s = 173,327.1
2025-01-18 10:28:35.880 | DEBUG    | __main__:<module>:313 - Training step 11820: loss = 3.5336 | 3026.34ms | Tokens/s = 173,241.9
2025-01-18 10:29:06.139 | DEBUG    | __main__:<module>:313 - Training step 11830: loss = 3.1318 | 3027.95ms | Tokens/s = 173,149.7
2025-01-18 10:29:36.415 | DEBUG    | __main__:<module>:313 - Training step 11840: loss = 3.1625 | 3027.08ms | Tokens/s = 173,199.1
2025-01-18 10:30:06.669 | DEBUG    | __main__:<module>:313 - Training step 11850: loss = 3.1973 | 3022.90ms | Tokens/s = 173,438.7
2025-01-18 10:30:36.914 | DEBUG    | __main__:<module>:313 - Training step 11860: loss = 3.1378 | 3023.55ms | Tokens/s = 173,401.5
2025-01-18 10:31:07.164 | DEBUG    | __main__:<module>:313 - Training step 11870: loss = 3.2603 | 3027.86ms | Tokens/s = 173,154.8
2025-01-18 10:31:37.437 | DEBUG    | __main__:<module>:313 - Training step 11880: loss = 3.2696 | 3026.56ms | Tokens/s = 173,228.9
2025-01-18 10:32:07.710 | DEBUG    | __main__:<module>:313 - Training step 11890: loss = 3.1247 | 3026.15ms | Tokens/s = 173,252.6
2025-01-18 10:32:37.963 | DEBUG    | __main__:<module>:313 - Training step 11900: loss = 3.3508 | 3023.30ms | Tokens/s = 173,416.0
2025-01-18 10:33:08.197 | DEBUG    | __main__:<module>:313 - Training step 11910: loss = 3.2780 | 3020.88ms | Tokens/s = 173,554.6
2025-01-18 10:33:38.432 | DEBUG    | __main__:<module>:313 - Training step 11920: loss = 2.8871 | 3024.50ms | Tokens/s = 173,346.7
2025-01-18 10:34:08.682 | DEBUG    | __main__:<module>:313 - Training step 11930: loss = 3.2319 | 3025.17ms | Tokens/s = 173,308.6
2025-01-18 10:34:38.939 | DEBUG    | __main__:<module>:313 - Training step 11940: loss = 3.1821 | 3027.09ms | Tokens/s = 173,198.4
2025-01-18 10:35:09.207 | DEBUG    | __main__:<module>:313 - Training step 11950: loss = 3.2411 | 3027.03ms | Tokens/s = 173,202.2
2025-01-18 10:35:39.461 | DEBUG    | __main__:<module>:313 - Training step 11960: loss = 3.1846 | 3024.74ms | Tokens/s = 173,333.2
2025-01-18 10:36:09.699 | DEBUG    | __main__:<module>:313 - Training step 11970: loss = 3.2135 | 3023.97ms | Tokens/s = 173,377.3
2025-01-18 10:36:39.927 | DEBUG    | __main__:<module>:313 - Training step 11980: loss = 3.4687 | 3020.81ms | Tokens/s = 173,558.8
2025-01-18 10:37:10.151 | DEBUG    | __main__:<module>:313 - Training step 11990: loss = 3.1377 | 3024.87ms | Tokens/s = 173,326.1
2025-01-18 10:37:43.852 | INFO     | __main__:<module>:265 - Step 12,000/20,000 loss: 3.2332 (T) 3.2407 (V) | lr=4.1e-03
2025-01-18 10:37:43.853 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 10:38:01.271 | DEBUG    | __main__:<module>:313 - Training step 12000: loss = 3.2729 | 23889.63ms | Tokens/s = 21,946.3
2025-01-18 10:38:31.346 | DEBUG    | __main__:<module>:313 - Training step 12010: loss = 3.0763 | 3015.77ms | Tokens/s = 173,848.9
2025-01-18 10:39:01.548 | DEBUG    | __main__:<module>:313 - Training step 12020: loss = 3.1737 | 3019.65ms | Tokens/s = 173,625.6
2025-01-18 10:39:31.802 | DEBUG    | __main__:<module>:313 - Training step 12030: loss = 3.2372 | 3026.24ms | Tokens/s = 173,247.6
2025-01-18 10:40:02.054 | DEBUG    | __main__:<module>:313 - Training step 12040: loss = 3.2872 | 3025.23ms | Tokens/s = 173,305.1
2025-01-18 10:40:32.328 | DEBUG    | __main__:<module>:313 - Training step 12050: loss = 3.3346 | 3029.95ms | Tokens/s = 173,035.3
2025-01-18 10:41:02.620 | DEBUG    | __main__:<module>:313 - Training step 12060: loss = 3.1211 | 3030.49ms | Tokens/s = 173,004.2
2025-01-18 10:41:32.893 | DEBUG    | __main__:<module>:313 - Training step 12070: loss = 3.1525 | 3025.65ms | Tokens/s = 173,280.9
2025-01-18 10:42:03.143 | DEBUG    | __main__:<module>:313 - Training step 12080: loss = 3.2712 | 3023.35ms | Tokens/s = 173,412.7
2025-01-18 10:42:33.381 | DEBUG    | __main__:<module>:313 - Training step 12090: loss = 3.1560 | 3022.90ms | Tokens/s = 173,438.5
2025-01-18 10:43:03.609 | DEBUG    | __main__:<module>:313 - Training step 12100: loss = 3.1077 | 3022.74ms | Tokens/s = 173,447.7
2025-01-18 10:43:33.867 | DEBUG    | __main__:<module>:313 - Training step 12110: loss = 3.2050 | 3026.66ms | Tokens/s = 173,223.2
2025-01-18 10:44:04.158 | DEBUG    | __main__:<module>:313 - Training step 12120: loss = 3.2466 | 3030.21ms | Tokens/s = 173,020.1
2025-01-18 10:44:34.451 | DEBUG    | __main__:<module>:313 - Training step 12130: loss = 3.2655 | 3027.75ms | Tokens/s = 173,161.0
2025-01-18 10:45:04.716 | DEBUG    | __main__:<module>:313 - Training step 12140: loss = 3.1969 | 3023.70ms | Tokens/s = 173,392.7
2025-01-18 10:45:34.970 | DEBUG    | __main__:<module>:313 - Training step 12150: loss = 3.3725 | 3022.89ms | Tokens/s = 173,439.4
2025-01-18 10:46:05.210 | DEBUG    | __main__:<module>:313 - Training step 12160: loss = 3.4232 | 3022.45ms | Tokens/s = 173,464.7
2025-01-18 10:46:35.466 | DEBUG    | __main__:<module>:313 - Training step 12170: loss = 3.1170 | 3026.66ms | Tokens/s = 173,223.4
2025-01-18 10:47:05.742 | DEBUG    | __main__:<module>:313 - Training step 12180: loss = 3.4755 | 3027.26ms | Tokens/s = 173,189.2
2025-01-18 10:47:36.007 | DEBUG    | __main__:<module>:313 - Training step 12190: loss = 3.2966 | 3027.47ms | Tokens/s = 173,177.0
2025-01-18 10:48:06.301 | DEBUG    | __main__:<module>:313 - Training step 12200: loss = 3.2308 | 3028.52ms | Tokens/s = 173,117.1
2025-01-18 10:48:36.576 | DEBUG    | __main__:<module>:313 - Training step 12210: loss = 3.2204 | 3026.35ms | Tokens/s = 173,240.8
2025-01-18 10:49:06.836 | DEBUG    | __main__:<module>:313 - Training step 12220: loss = 3.3010 | 3026.40ms | Tokens/s = 173,238.3
2025-01-18 10:49:37.112 | DEBUG    | __main__:<module>:313 - Training step 12230: loss = 3.1561 | 3029.76ms | Tokens/s = 173,046.1
2025-01-18 10:50:07.408 | DEBUG    | __main__:<module>:313 - Training step 12240: loss = 3.0314 | 3029.30ms | Tokens/s = 173,072.2
2025-01-18 10:50:37.679 | DEBUG    | __main__:<module>:313 - Training step 12250: loss = 3.1002 | 3024.23ms | Tokens/s = 173,362.7
2025-01-18 10:51:07.932 | DEBUG    | __main__:<module>:313 - Training step 12260: loss = 3.2439 | 3023.56ms | Tokens/s = 173,400.9
2025-01-18 10:51:38.182 | DEBUG    | __main__:<module>:313 - Training step 12270: loss = 3.0963 | 3028.96ms | Tokens/s = 173,092.0
2025-01-18 10:52:08.464 | DEBUG    | __main__:<module>:313 - Training step 12280: loss = 3.2767 | 3029.66ms | Tokens/s = 173,051.9
2025-01-18 10:52:38.748 | DEBUG    | __main__:<module>:313 - Training step 12290: loss = 3.2003 | 3027.33ms | Tokens/s = 173,184.8
2025-01-18 10:53:09.005 | DEBUG    | __main__:<module>:313 - Training step 12300: loss = 3.2441 | 3023.88ms | Tokens/s = 173,382.8
2025-01-18 10:53:39.257 | DEBUG    | __main__:<module>:313 - Training step 12310: loss = 3.2470 | 3026.31ms | Tokens/s = 173,243.2
2025-01-18 10:54:09.542 | DEBUG    | __main__:<module>:313 - Training step 12320: loss = 3.3601 | 3028.72ms | Tokens/s = 173,105.7
2025-01-18 10:54:39.827 | DEBUG    | __main__:<module>:313 - Training step 12330: loss = 3.2053 | 3030.34ms | Tokens/s = 173,013.2
2025-01-18 10:55:10.122 | DEBUG    | __main__:<module>:313 - Training step 12340: loss = 3.0581 | 3029.19ms | Tokens/s = 173,078.7
2025-01-18 10:55:40.392 | DEBUG    | __main__:<module>:313 - Training step 12350: loss = 3.3318 | 3024.62ms | Tokens/s = 173,340.4
2025-01-18 10:56:10.646 | DEBUG    | __main__:<module>:313 - Training step 12360: loss = 3.2211 | 3024.50ms | Tokens/s = 173,347.2
2025-01-18 10:56:40.891 | DEBUG    | __main__:<module>:313 - Training step 12370: loss = 3.1505 | 3026.63ms | Tokens/s = 173,225.2
2025-01-18 10:57:11.140 | DEBUG    | __main__:<module>:313 - Training step 12380: loss = 3.3606 | 3024.82ms | Tokens/s = 173,328.9
2025-01-18 10:57:41.384 | DEBUG    | __main__:<module>:313 - Training step 12390: loss = 3.3467 | 3024.50ms | Tokens/s = 173,346.8
2025-01-18 10:58:11.659 | DEBUG    | __main__:<module>:313 - Training step 12400: loss = 3.2697 | 3026.75ms | Tokens/s = 173,218.1
2025-01-18 10:58:41.938 | DEBUG    | __main__:<module>:313 - Training step 12410: loss = 3.1469 | 3027.07ms | Tokens/s = 173,199.9
2025-01-18 10:59:12.189 | DEBUG    | __main__:<module>:313 - Training step 12420: loss = 3.1332 | 3025.93ms | Tokens/s = 173,265.0
2025-01-18 10:59:42.440 | DEBUG    | __main__:<module>:313 - Training step 12430: loss = 3.2447 | 3025.00ms | Tokens/s = 173,318.6
2025-01-18 11:00:12.712 | DEBUG    | __main__:<module>:313 - Training step 12440: loss = 3.1965 | 3027.96ms | Tokens/s = 173,148.9
2025-01-18 11:00:43.006 | DEBUG    | __main__:<module>:313 - Training step 12450: loss = 3.1113 | 3030.52ms | Tokens/s = 173,002.5
2025-01-18 11:01:13.290 | DEBUG    | __main__:<module>:313 - Training step 12460: loss = 3.2145 | 3025.70ms | Tokens/s = 173,278.3
2025-01-18 11:01:43.549 | DEBUG    | __main__:<module>:313 - Training step 12470: loss = 2.9645 | 3025.10ms | Tokens/s = 173,312.6
2025-01-18 11:02:13.818 | DEBUG    | __main__:<module>:313 - Training step 12480: loss = 3.2545 | 3028.05ms | Tokens/s = 173,143.8
2025-01-18 11:02:44.119 | DEBUG    | __main__:<module>:313 - Training step 12490: loss = 3.3033 | 3030.78ms | Tokens/s = 172,987.6
2025-01-18 11:03:14.431 | DEBUG    | __main__:<module>:313 - Training step 12500: loss = 3.2056 | 3030.09ms | Tokens/s = 173,026.9
2025-01-18 11:03:44.719 | DEBUG    | __main__:<module>:313 - Training step 12510: loss = 3.1757 | 3029.78ms | Tokens/s = 173,045.0
2025-01-18 11:04:15.002 | DEBUG    | __main__:<module>:313 - Training step 12520: loss = 3.1616 | 3029.85ms | Tokens/s = 173,040.8
2025-01-18 11:04:45.295 | DEBUG    | __main__:<module>:313 - Training step 12530: loss = 3.2140 | 3028.75ms | Tokens/s = 173,103.7
2025-01-18 11:05:15.570 | DEBUG    | __main__:<module>:313 - Training step 12540: loss = 3.1980 | 3027.30ms | Tokens/s = 173,186.5
2025-01-18 11:05:45.845 | DEBUG    | __main__:<module>:313 - Training step 12550: loss = 3.3438 | 3026.28ms | Tokens/s = 173,245.1
2025-01-18 11:06:16.095 | DEBUG    | __main__:<module>:313 - Training step 12560: loss = 3.2238 | 3023.70ms | Tokens/s = 173,392.6
2025-01-18 11:06:46.342 | DEBUG    | __main__:<module>:313 - Training step 12570: loss = 3.2001 | 3024.22ms | Tokens/s = 173,363.2
2025-01-18 11:07:16.597 | DEBUG    | __main__:<module>:313 - Training step 12580: loss = 3.2456 | 3026.70ms | Tokens/s = 173,221.0
2025-01-18 11:07:46.872 | DEBUG    | __main__:<module>:313 - Training step 12590: loss = 3.2070 | 3027.16ms | Tokens/s = 173,194.7
2025-01-18 11:08:17.129 | DEBUG    | __main__:<module>:313 - Training step 12600: loss = 3.1603 | 3023.81ms | Tokens/s = 173,386.5
2025-01-18 11:08:47.375 | DEBUG    | __main__:<module>:313 - Training step 12610: loss = 3.1804 | 3024.14ms | Tokens/s = 173,367.4
2025-01-18 11:09:17.608 | DEBUG    | __main__:<module>:313 - Training step 12620: loss = 3.1744 | 3024.06ms | Tokens/s = 173,372.3
2025-01-18 11:09:47.870 | DEBUG    | __main__:<module>:313 - Training step 12630: loss = 3.3203 | 3026.83ms | Tokens/s = 173,213.7
2025-01-18 11:10:18.160 | DEBUG    | __main__:<module>:313 - Training step 12640: loss = 2.8910 | 3030.95ms | Tokens/s = 172,978.2
2025-01-18 11:10:48.461 | DEBUG    | __main__:<module>:313 - Training step 12650: loss = 3.2262 | 3029.78ms | Tokens/s = 173,044.7
2025-01-18 11:11:18.740 | DEBUG    | __main__:<module>:313 - Training step 12660: loss = 3.2354 | 3029.05ms | Tokens/s = 173,086.7
2025-01-18 11:11:48.993 | DEBUG    | __main__:<module>:313 - Training step 12670: loss = 3.2137 | 3024.53ms | Tokens/s = 173,345.5
2025-01-18 11:12:19.238 | DEBUG    | __main__:<module>:313 - Training step 12680: loss = 3.0875 | 3024.66ms | Tokens/s = 173,337.8
2025-01-18 11:12:49.480 | DEBUG    | __main__:<module>:313 - Training step 12690: loss = 3.1974 | 3024.98ms | Tokens/s = 173,319.4
2025-01-18 11:13:19.743 | DEBUG    | __main__:<module>:313 - Training step 12700: loss = 3.2919 | 3025.18ms | Tokens/s = 173,307.8
2025-01-18 11:13:49.993 | DEBUG    | __main__:<module>:313 - Training step 12710: loss = 3.3469 | 3028.15ms | Tokens/s = 173,138.0
2025-01-18 11:14:20.265 | DEBUG    | __main__:<module>:313 - Training step 12720: loss = 3.2195 | 3029.25ms | Tokens/s = 173,075.5
2025-01-18 11:14:50.556 | DEBUG    | __main__:<module>:313 - Training step 12730: loss = 3.1705 | 3028.87ms | Tokens/s = 173,096.9
2025-01-18 11:15:20.832 | DEBUG    | __main__:<module>:313 - Training step 12740: loss = 3.2234 | 3024.78ms | Tokens/s = 173,331.1
2025-01-18 11:15:51.083 | DEBUG    | __main__:<module>:313 - Training step 12750: loss = 3.3399 | 3026.05ms | Tokens/s = 173,257.9
2025-01-18 11:16:21.325 | DEBUG    | __main__:<module>:313 - Training step 12760: loss = 3.2306 | 3021.73ms | Tokens/s = 173,505.7
2025-01-18 11:16:51.573 | DEBUG    | __main__:<module>:313 - Training step 12770: loss = 3.1953 | 3026.59ms | Tokens/s = 173,227.5
2025-01-18 11:17:21.852 | DEBUG    | __main__:<module>:313 - Training step 12780: loss = 3.2321 | 3029.10ms | Tokens/s = 173,084.0
2025-01-18 11:17:52.139 | DEBUG    | __main__:<module>:313 - Training step 12790: loss = 3.2062 | 3027.05ms | Tokens/s = 173,201.1
2025-01-18 11:18:22.397 | DEBUG    | __main__:<module>:313 - Training step 12800: loss = 3.1253 | 3022.60ms | Tokens/s = 173,456.0
2025-01-18 11:18:52.625 | DEBUG    | __main__:<module>:313 - Training step 12810: loss = 3.1905 | 3019.98ms | Tokens/s = 173,606.3
2025-01-18 11:19:22.838 | DEBUG    | __main__:<module>:313 - Training step 12820: loss = 3.1923 | 3022.17ms | Tokens/s = 173,480.9
2025-01-18 11:19:53.090 | DEBUG    | __main__:<module>:313 - Training step 12830: loss = 3.0830 | 3025.11ms | Tokens/s = 173,312.3
2025-01-18 11:20:23.356 | DEBUG    | __main__:<module>:313 - Training step 12840: loss = 3.2495 | 3026.87ms | Tokens/s = 173,211.3
2025-01-18 11:20:53.632 | DEBUG    | __main__:<module>:313 - Training step 12850: loss = 3.1164 | 3028.17ms | Tokens/s = 173,136.7
2025-01-18 11:21:23.932 | DEBUG    | __main__:<module>:313 - Training step 12860: loss = 3.2357 | 3028.97ms | Tokens/s = 173,090.9
2025-01-18 11:21:54.205 | DEBUG    | __main__:<module>:313 - Training step 12870: loss = 3.2542 | 3026.09ms | Tokens/s = 173,256.0
2025-01-18 11:22:24.464 | DEBUG    | __main__:<module>:313 - Training step 12880: loss = 3.2423 | 3026.83ms | Tokens/s = 173,213.6
2025-01-18 11:22:54.745 | DEBUG    | __main__:<module>:313 - Training step 12890: loss = 3.2541 | 3029.34ms | Tokens/s = 173,069.8
2025-01-18 11:23:25.040 | DEBUG    | __main__:<module>:313 - Training step 12900: loss = 3.0988 | 3027.40ms | Tokens/s = 173,181.1
2025-01-18 11:23:55.307 | DEBUG    | __main__:<module>:313 - Training step 12910: loss = 3.1862 | 3025.60ms | Tokens/s = 173,284.0
2025-01-18 11:24:25.553 | DEBUG    | __main__:<module>:313 - Training step 12920: loss = 3.1427 | 3026.05ms | Tokens/s = 173,258.4
2025-01-18 11:24:55.787 | DEBUG    | __main__:<module>:313 - Training step 12930: loss = 3.3145 | 3022.91ms | Tokens/s = 173,438.2
2025-01-18 11:25:26.034 | DEBUG    | __main__:<module>:313 - Training step 12940: loss = 3.0669 | 3027.01ms | Tokens/s = 173,203.1
2025-01-18 11:25:56.312 | DEBUG    | __main__:<module>:313 - Training step 12950: loss = 3.2317 | 3024.23ms | Tokens/s = 173,362.2
2025-01-18 11:26:26.566 | DEBUG    | __main__:<module>:313 - Training step 12960: loss = 3.1176 | 3023.62ms | Tokens/s = 173,397.5
2025-01-18 11:26:56.818 | DEBUG    | __main__:<module>:313 - Training step 12970: loss = 3.2194 | 3026.75ms | Tokens/s = 173,218.4
2025-01-18 11:27:27.079 | DEBUG    | __main__:<module>:313 - Training step 12980: loss = 3.1829 | 3027.47ms | Tokens/s = 173,177.0
2025-01-18 11:27:57.365 | DEBUG    | __main__:<module>:313 - Training step 12990: loss = 3.3560 | 3027.93ms | Tokens/s = 173,150.5
2025-01-18 11:28:31.086 | INFO     | __main__:<module>:265 - Step 13,000/20,000 loss: 3.2048 (T) 3.2018 (V) | lr=3.3e-03
2025-01-18 11:28:31.087 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 11:28:44.340 | DEBUG    | __main__:<module>:313 - Training step 13000: loss = 3.2670 | 19716.95ms | Tokens/s = 26,590.7
2025-01-18 11:29:14.443 | DEBUG    | __main__:<module>:313 - Training step 13010: loss = 3.1028 | 3017.44ms | Tokens/s = 173,752.8
2025-01-18 11:29:44.661 | DEBUG    | __main__:<module>:313 - Training step 13020: loss = 3.2987 | 3023.15ms | Tokens/s = 173,424.2
2025-01-18 11:30:14.913 | DEBUG    | __main__:<module>:313 - Training step 13030: loss = 3.0832 | 3028.23ms | Tokens/s = 173,133.3
2025-01-18 11:30:45.190 | DEBUG    | __main__:<module>:313 - Training step 13040: loss = 3.2649 | 3027.68ms | Tokens/s = 173,165.0
2025-01-18 11:31:15.443 | DEBUG    | __main__:<module>:313 - Training step 13050: loss = 3.2030 | 3024.55ms | Tokens/s = 173,344.3
2025-01-18 11:31:45.710 | DEBUG    | __main__:<module>:313 - Training step 13060: loss = 3.3158 | 3026.82ms | Tokens/s = 173,214.3
2025-01-18 11:32:15.984 | DEBUG    | __main__:<module>:313 - Training step 13070: loss = 3.0128 | 3027.32ms | Tokens/s = 173,185.7
2025-01-18 11:32:46.234 | DEBUG    | __main__:<module>:313 - Training step 13080: loss = 3.2385 | 3023.44ms | Tokens/s = 173,407.8
2025-01-18 11:33:16.466 | DEBUG    | __main__:<module>:313 - Training step 13090: loss = 3.1445 | 3022.37ms | Tokens/s = 173,469.0
2025-01-18 11:33:46.715 | DEBUG    | __main__:<module>:313 - Training step 13100: loss = 3.1846 | 3025.82ms | Tokens/s = 173,271.6
2025-01-18 11:34:16.993 | DEBUG    | __main__:<module>:313 - Training step 13110: loss = 3.2748 | 3028.99ms | Tokens/s = 173,090.3
2025-01-18 11:34:47.274 | DEBUG    | __main__:<module>:313 - Training step 13120: loss = 3.3068 | 3026.87ms | Tokens/s = 173,211.2
2025-01-18 11:35:17.535 | DEBUG    | __main__:<module>:313 - Training step 13130: loss = 3.0295 | 3024.35ms | Tokens/s = 173,355.8
2025-01-18 11:35:47.781 | DEBUG    | __main__:<module>:313 - Training step 13140: loss = 3.3707 | 3025.03ms | Tokens/s = 173,316.9
2025-01-18 11:36:18.026 | DEBUG    | __main__:<module>:313 - Training step 13150: loss = 3.2819 | 3024.40ms | Tokens/s = 173,353.0
2025-01-18 11:36:48.316 | DEBUG    | __main__:<module>:313 - Training step 13160: loss = 3.1738 | 3030.82ms | Tokens/s = 172,985.5
2025-01-18 11:37:18.610 | DEBUG    | __main__:<module>:313 - Training step 13170: loss = 3.3340 | 3027.77ms | Tokens/s = 173,159.6
2025-01-18 11:37:48.870 | DEBUG    | __main__:<module>:313 - Training step 13180: loss = 3.2293 | 3025.09ms | Tokens/s = 173,313.1
2025-01-18 11:38:19.139 | DEBUG    | __main__:<module>:313 - Training step 13190: loss = 3.2375 | 3028.12ms | Tokens/s = 173,140.1
2025-01-18 11:38:49.426 | DEBUG    | __main__:<module>:313 - Training step 13200: loss = 3.2677 | 3027.33ms | Tokens/s = 173,184.9
2025-01-18 11:39:19.698 | DEBUG    | __main__:<module>:313 - Training step 13210: loss = 3.2261 | 3025.56ms | Tokens/s = 173,286.5
2025-01-18 11:39:49.950 | DEBUG    | __main__:<module>:313 - Training step 13220: loss = 3.1506 | 3024.43ms | Tokens/s = 173,351.2
2025-01-18 11:40:20.221 | DEBUG    | __main__:<module>:313 - Training step 13230: loss = 3.1903 | 3029.45ms | Tokens/s = 173,063.9
2025-01-18 11:40:50.518 | DEBUG    | __main__:<module>:313 - Training step 13240: loss = 3.2983 | 3028.80ms | Tokens/s = 173,101.0
2025-01-18 11:41:20.797 | DEBUG    | __main__:<module>:313 - Training step 13250: loss = 3.2714 | 3027.47ms | Tokens/s = 173,176.7
2025-01-18 11:41:51.059 | DEBUG    | __main__:<module>:313 - Training step 13260: loss = 3.1654 | 3024.09ms | Tokens/s = 173,370.2
2025-01-18 11:42:21.332 | DEBUG    | __main__:<module>:313 - Training step 13270: loss = 3.1732 | 3029.90ms | Tokens/s = 173,037.9
2025-01-18 11:42:51.627 | DEBUG    | __main__:<module>:313 - Training step 13280: loss = 3.2586 | 3030.24ms | Tokens/s = 173,018.9
2025-01-18 11:43:21.913 | DEBUG    | __main__:<module>:313 - Training step 13290: loss = 3.1996 | 3027.55ms | Tokens/s = 173,172.3
2025-01-18 11:43:52.183 | DEBUG    | __main__:<module>:313 - Training step 13300: loss = 3.1885 | 3024.68ms | Tokens/s = 173,336.4
2025-01-18 11:44:22.456 | DEBUG    | __main__:<module>:313 - Training step 13310: loss = 3.2287 | 3027.23ms | Tokens/s = 173,190.9
2025-01-18 11:44:52.752 | DEBUG    | __main__:<module>:313 - Training step 13320: loss = 3.2844 | 3031.04ms | Tokens/s = 172,972.7
2025-01-18 11:45:23.025 | DEBUG    | __main__:<module>:313 - Training step 13330: loss = 2.9528 | 3027.36ms | Tokens/s = 173,183.0
2025-01-18 11:45:53.284 | DEBUG    | __main__:<module>:313 - Training step 13340: loss = 3.3871 | 3025.89ms | Tokens/s = 173,267.1
2025-01-18 11:46:23.561 | DEBUG    | __main__:<module>:313 - Training step 13350: loss = 3.2574 | 3029.66ms | Tokens/s = 173,051.5
2025-01-18 11:46:53.866 | DEBUG    | __main__:<module>:313 - Training step 13360: loss = 3.2065 | 3033.75ms | Tokens/s = 172,818.6
2025-01-18 11:47:24.164 | DEBUG    | __main__:<module>:313 - Training step 13370: loss = 3.2788 | 3028.33ms | Tokens/s = 173,127.7
2025-01-18 11:47:54.438 | DEBUG    | __main__:<module>:313 - Training step 13380: loss = 3.1953 | 3026.01ms | Tokens/s = 173,260.2
2025-01-18 11:48:24.689 | DEBUG    | __main__:<module>:313 - Training step 13390: loss = 3.1009 | 3025.73ms | Tokens/s = 173,276.3
2025-01-18 11:48:54.923 | DEBUG    | __main__:<module>:313 - Training step 13400: loss = 3.2918 | 3021.57ms | Tokens/s = 173,515.2
2025-01-18 11:49:25.155 | DEBUG    | __main__:<module>:313 - Training step 13410: loss = 3.1771 | 3024.47ms | Tokens/s = 173,348.7
2025-01-18 11:49:55.413 | DEBUG    | __main__:<module>:313 - Training step 13420: loss = 3.1005 | 3027.30ms | Tokens/s = 173,186.7
2025-01-18 11:50:25.696 | DEBUG    | __main__:<module>:313 - Training step 13430: loss = 3.2355 | 3029.12ms | Tokens/s = 173,082.8
2025-01-18 11:50:55.978 | DEBUG    | __main__:<module>:313 - Training step 13440: loss = 3.0536 | 3026.50ms | Tokens/s = 173,232.2
2025-01-18 11:51:26.238 | DEBUG    | __main__:<module>:313 - Training step 13450: loss = 3.2865 | 3025.22ms | Tokens/s = 173,305.6
2025-01-18 11:51:56.500 | DEBUG    | __main__:<module>:313 - Training step 13460: loss = 3.3643 | 3027.14ms | Tokens/s = 173,195.9
2025-01-18 11:52:26.790 | DEBUG    | __main__:<module>:313 - Training step 13470: loss = 3.1404 | 3030.98ms | Tokens/s = 172,976.6
2025-01-18 11:52:57.084 | DEBUG    | __main__:<module>:313 - Training step 13480: loss = 2.9301 | 3026.25ms | Tokens/s = 173,246.8
2025-01-18 11:53:27.352 | DEBUG    | __main__:<module>:313 - Training step 13490: loss = 3.1837 | 3026.20ms | Tokens/s = 173,249.6
2025-01-18 11:53:57.604 | DEBUG    | __main__:<module>:313 - Training step 13500: loss = 3.0101 | 3024.01ms | Tokens/s = 173,375.0
2025-01-18 11:54:27.829 | DEBUG    | __main__:<module>:313 - Training step 13510: loss = 3.2535 | 3021.96ms | Tokens/s = 173,492.5
2025-01-18 11:54:58.080 | DEBUG    | __main__:<module>:313 - Training step 13520: loss = 3.2271 | 3026.59ms | Tokens/s = 173,227.2
2025-01-18 11:55:28.358 | DEBUG    | __main__:<module>:313 - Training step 13530: loss = 3.1739 | 3029.78ms | Tokens/s = 173,045.0
2025-01-18 11:55:58.644 | DEBUG    | __main__:<module>:313 - Training step 13540: loss = 3.1960 | 3027.67ms | Tokens/s = 173,165.3
2025-01-18 11:56:28.906 | DEBUG    | __main__:<module>:313 - Training step 13550: loss = 2.9944 | 3025.72ms | Tokens/s = 173,277.3
2025-01-18 11:56:59.155 | DEBUG    | __main__:<module>:313 - Training step 13560: loss = 3.1716 | 3025.14ms | Tokens/s = 173,310.6
2025-01-18 11:57:29.434 | DEBUG    | __main__:<module>:313 - Training step 13570: loss = 3.1357 | 3029.11ms | Tokens/s = 173,083.1
2025-01-18 11:57:59.730 | DEBUG    | __main__:<module>:313 - Training step 13580: loss = 3.3410 | 3030.24ms | Tokens/s = 173,018.8
2025-01-18 11:58:30.033 | DEBUG    | __main__:<module>:313 - Training step 13590: loss = 3.1809 | 3031.02ms | Tokens/s = 172,974.3
2025-01-18 11:59:00.343 | DEBUG    | __main__:<module>:313 - Training step 13600: loss = 3.2195 | 3030.53ms | Tokens/s = 173,002.1
2025-01-18 11:59:30.630 | DEBUG    | __main__:<module>:313 - Training step 13610: loss = 2.9663 | 3027.61ms | Tokens/s = 173,169.1
2025-01-18 12:00:00.893 | DEBUG    | __main__:<module>:313 - Training step 13620: loss = 3.1546 | 3025.56ms | Tokens/s = 173,286.3
2025-01-18 12:00:31.177 | DEBUG    | __main__:<module>:313 - Training step 13630: loss = 3.2342 | 3028.56ms | Tokens/s = 173,114.7
2025-01-18 12:01:01.455 | DEBUG    | __main__:<module>:313 - Training step 13640: loss = 3.0663 | 3028.81ms | Tokens/s = 173,100.3
2025-01-18 12:01:31.752 | DEBUG    | __main__:<module>:313 - Training step 13650: loss = 3.1866 | 3029.10ms | Tokens/s = 173,083.5
2025-01-18 12:02:02.030 | DEBUG    | __main__:<module>:313 - Training step 13660: loss = 3.2267 | 3028.22ms | Tokens/s = 173,134.3
2025-01-18 12:02:32.314 | DEBUG    | __main__:<module>:313 - Training step 13670: loss = 3.2520 | 3028.29ms | Tokens/s = 173,130.1
2025-01-18 12:03:02.612 | DEBUG    | __main__:<module>:313 - Training step 13680: loss = 3.2015 | 3027.83ms | Tokens/s = 173,156.1
2025-01-18 12:03:32.887 | DEBUG    | __main__:<module>:313 - Training step 13690: loss = 3.3022 | 3024.31ms | Tokens/s = 173,358.0
2025-01-18 12:04:03.136 | DEBUG    | __main__:<module>:313 - Training step 13700: loss = 3.1032 | 3022.30ms | Tokens/s = 173,473.2
2025-01-18 12:04:33.380 | DEBUG    | __main__:<module>:313 - Training step 13710: loss = 3.0809 | 3025.10ms | Tokens/s = 173,312.5
2025-01-18 12:05:03.647 | DEBUG    | __main__:<module>:313 - Training step 13720: loss = 3.0837 | 3027.09ms | Tokens/s = 173,198.4
2025-01-18 12:05:33.944 | DEBUG    | __main__:<module>:313 - Training step 13730: loss = 3.2755 | 3029.95ms | Tokens/s = 173,035.1
2025-01-18 12:06:04.233 | DEBUG    | __main__:<module>:313 - Training step 13740: loss = 3.2109 | 3025.71ms | Tokens/s = 173,277.8
2025-01-18 12:06:34.487 | DEBUG    | __main__:<module>:313 - Training step 13750: loss = 3.1129 | 3024.86ms | Tokens/s = 173,326.2
2025-01-18 12:07:04.734 | DEBUG    | __main__:<module>:313 - Training step 13760: loss = 3.2427 | 3026.85ms | Tokens/s = 173,212.6
2025-01-18 12:07:35.010 | DEBUG    | __main__:<module>:313 - Training step 13770: loss = 3.3010 | 3028.51ms | Tokens/s = 173,117.4
2025-01-18 12:08:05.299 | DEBUG    | __main__:<module>:313 - Training step 13780: loss = 3.2586 | 3024.63ms | Tokens/s = 173,339.5
2025-01-18 12:08:35.555 | DEBUG    | __main__:<module>:313 - Training step 13790: loss = 3.1574 | 3025.77ms | Tokens/s = 173,274.0
2025-01-18 12:09:05.828 | DEBUG    | __main__:<module>:313 - Training step 13800: loss = 3.2088 | 3026.25ms | Tokens/s = 173,246.9
2025-01-18 12:09:36.112 | DEBUG    | __main__:<module>:313 - Training step 13810: loss = 3.1980 | 3027.30ms | Tokens/s = 173,186.7
2025-01-18 12:10:06.373 | DEBUG    | __main__:<module>:313 - Training step 13820: loss = 3.2316 | 3026.09ms | Tokens/s = 173,256.2
2025-01-18 12:10:36.621 | DEBUG    | __main__:<module>:313 - Training step 13830: loss = 3.0051 | 3024.61ms | Tokens/s = 173,340.9
2025-01-18 12:11:06.880 | DEBUG    | __main__:<module>:313 - Training step 13840: loss = 3.0967 | 3026.23ms | Tokens/s = 173,247.7
2025-01-18 12:11:37.176 | DEBUG    | __main__:<module>:313 - Training step 13850: loss = 3.1605 | 3031.00ms | Tokens/s = 172,975.0
2025-01-18 12:12:07.460 | DEBUG    | __main__:<module>:313 - Training step 13860: loss = 3.0780 | 3024.61ms | Tokens/s = 173,340.8
2025-01-18 12:12:37.714 | DEBUG    | __main__:<module>:313 - Training step 13870: loss = 3.1208 | 3023.58ms | Tokens/s = 173,399.8
2025-01-18 12:13:07.964 | DEBUG    | __main__:<module>:313 - Training step 13880: loss = 3.1577 | 3023.98ms | Tokens/s = 173,376.8
2025-01-18 12:13:38.228 | DEBUG    | __main__:<module>:313 - Training step 13890: loss = 3.1135 | 3028.60ms | Tokens/s = 173,112.4
2025-01-18 12:14:08.524 | DEBUG    | __main__:<module>:313 - Training step 13900: loss = 3.1180 | 3030.57ms | Tokens/s = 173,000.0
2025-01-18 12:14:38.827 | DEBUG    | __main__:<module>:313 - Training step 13910: loss = 3.2680 | 3029.96ms | Tokens/s = 173,034.9
2025-01-18 12:15:09.108 | DEBUG    | __main__:<module>:313 - Training step 13920: loss = 2.9915 | 3026.89ms | Tokens/s = 173,209.9
2025-01-18 12:15:39.371 | DEBUG    | __main__:<module>:313 - Training step 13930: loss = 3.2351 | 3027.56ms | Tokens/s = 173,171.8
2025-01-18 12:16:09.660 | DEBUG    | __main__:<module>:313 - Training step 13940: loss = 3.2002 | 3028.78ms | Tokens/s = 173,102.1
2025-01-18 12:16:39.951 | DEBUG    | __main__:<module>:313 - Training step 13950: loss = 3.0592 | 3028.50ms | Tokens/s = 173,117.8
2025-01-18 12:17:10.220 | DEBUG    | __main__:<module>:313 - Training step 13960: loss = 3.2746 | 3027.50ms | Tokens/s = 173,175.2
2025-01-18 12:17:40.524 | DEBUG    | __main__:<module>:313 - Training step 13970: loss = 3.1972 | 3029.93ms | Tokens/s = 173,036.1
2025-01-18 12:18:10.827 | DEBUG    | __main__:<module>:313 - Training step 13980: loss = 3.0552 | 3028.71ms | Tokens/s = 173,106.1
2025-01-18 12:18:41.103 | DEBUG    | __main__:<module>:313 - Training step 13990: loss = 3.0395 | 3026.49ms | Tokens/s = 173,232.9
2025-01-18 12:19:14.792 | INFO     | __main__:<module>:265 - Step 14,000/20,000 loss: 3.1539 (T) 3.1850 (V) | lr=2.5e-03
2025-01-18 12:19:14.793 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 12:19:27.903 | DEBUG    | __main__:<module>:313 - Training step 14000: loss = 2.9082 | 19572.20ms | Tokens/s = 26,787.4
2025-01-18 12:19:58.028 | DEBUG    | __main__:<module>:313 - Training step 14010: loss = 3.2606 | 3018.95ms | Tokens/s = 173,665.7
2025-01-18 12:20:28.245 | DEBUG    | __main__:<module>:313 - Training step 14020: loss = 3.0716 | 3024.42ms | Tokens/s = 173,351.5
2025-01-18 12:20:58.511 | DEBUG    | __main__:<module>:313 - Training step 14030: loss = 3.0820 | 3026.59ms | Tokens/s = 173,227.3
2025-01-18 12:21:28.787 | DEBUG    | __main__:<module>:313 - Training step 14040: loss = 2.9965 | 3031.51ms | Tokens/s = 172,946.0
2025-01-18 12:21:59.071 | DEBUG    | __main__:<module>:313 - Training step 14050: loss = 3.2115 | 3026.50ms | Tokens/s = 173,232.3
2025-01-18 12:22:29.332 | DEBUG    | __main__:<module>:313 - Training step 14060: loss = 3.2801 | 3026.67ms | Tokens/s = 173,222.6
2025-01-18 12:22:59.611 | DEBUG    | __main__:<module>:313 - Training step 14070: loss = 3.1583 | 3029.36ms | Tokens/s = 173,069.1
2025-01-18 12:23:29.881 | DEBUG    | __main__:<module>:313 - Training step 14080: loss = 3.1551 | 3026.46ms | Tokens/s = 173,235.0
2025-01-18 12:24:00.128 | DEBUG    | __main__:<module>:313 - Training step 14090: loss = 3.0943 | 3024.11ms | Tokens/s = 173,369.5
2025-01-18 12:24:30.358 | DEBUG    | __main__:<module>:313 - Training step 14100: loss = 3.0105 | 3021.54ms | Tokens/s = 173,516.6
2025-01-18 12:25:00.587 | DEBUG    | __main__:<module>:313 - Training step 14110: loss = 3.1952 | 3022.03ms | Tokens/s = 173,488.8
2025-01-18 12:25:30.817 | DEBUG    | __main__:<module>:313 - Training step 14120: loss = 3.0043 | 3024.14ms | Tokens/s = 173,367.8
2025-01-18 12:26:01.066 | DEBUG    | __main__:<module>:313 - Training step 14130: loss = 3.2024 | 3027.55ms | Tokens/s = 173,172.1
2025-01-18 12:26:31.310 | DEBUG    | __main__:<module>:313 - Training step 14140: loss = 3.1205 | 3023.52ms | Tokens/s = 173,403.4
2025-01-18 12:27:01.552 | DEBUG    | __main__:<module>:313 - Training step 14150: loss = 3.4240 | 3024.29ms | Tokens/s = 173,359.3
2025-01-18 12:27:31.811 | DEBUG    | __main__:<module>:313 - Training step 14160: loss = 3.2930 | 3026.64ms | Tokens/s = 173,224.3
2025-01-18 12:28:02.093 | DEBUG    | __main__:<module>:313 - Training step 14170: loss = 3.2892 | 3028.78ms | Tokens/s = 173,102.1
2025-01-18 12:28:32.383 | DEBUG    | __main__:<module>:313 - Training step 14180: loss = 3.1464 | 3027.98ms | Tokens/s = 173,147.9
2025-01-18 12:29:02.649 | DEBUG    | __main__:<module>:313 - Training step 14190: loss = 3.2243 | 3025.78ms | Tokens/s = 173,273.8
2025-01-18 12:29:32.899 | DEBUG    | __main__:<module>:313 - Training step 14200: loss = 3.2278 | 3023.64ms | Tokens/s = 173,396.6
2025-01-18 12:30:03.135 | DEBUG    | __main__:<module>:313 - Training step 14210: loss = 3.0272 | 3021.01ms | Tokens/s = 173,547.1
2025-01-18 12:30:33.369 | DEBUG    | __main__:<module>:313 - Training step 14220: loss = 3.0899 | 3023.98ms | Tokens/s = 173,377.0
2025-01-18 12:31:03.624 | DEBUG    | __main__:<module>:313 - Training step 14230: loss = 3.0885 | 3026.54ms | Tokens/s = 173,230.2
2025-01-18 12:31:33.904 | DEBUG    | __main__:<module>:313 - Training step 14240: loss = 3.1074 | 3028.14ms | Tokens/s = 173,138.8
2025-01-18 12:32:04.163 | DEBUG    | __main__:<module>:313 - Training step 14250: loss = 3.0575 | 3025.63ms | Tokens/s = 173,282.4
2025-01-18 12:32:34.415 | DEBUG    | __main__:<module>:313 - Training step 14260: loss = 3.0571 | 3024.88ms | Tokens/s = 173,325.0
2025-01-18 12:33:04.690 | DEBUG    | __main__:<module>:313 - Training step 14270: loss = 3.1583 | 3027.23ms | Tokens/s = 173,190.9
2025-01-18 12:33:34.977 | DEBUG    | __main__:<module>:313 - Training step 14280: loss = 3.2672 | 3027.80ms | Tokens/s = 173,158.3
2025-01-18 12:34:05.242 | DEBUG    | __main__:<module>:313 - Training step 14290: loss = 3.1600 | 3023.20ms | Tokens/s = 173,421.8
2025-01-18 12:34:35.490 | DEBUG    | __main__:<module>:313 - Training step 14300: loss = 3.2495 | 3024.27ms | Tokens/s = 173,359.9
2025-01-18 12:35:05.762 | DEBUG    | __main__:<module>:313 - Training step 14310: loss = 3.0653 | 3027.82ms | Tokens/s = 173,156.7
2025-01-18 12:35:36.022 | DEBUG    | __main__:<module>:313 - Training step 14320: loss = 3.2170 | 3025.15ms | Tokens/s = 173,309.9
2025-01-18 12:36:06.277 | DEBUG    | __main__:<module>:313 - Training step 14330: loss = 3.0864 | 3026.76ms | Tokens/s = 173,217.5
2025-01-18 12:36:36.556 | DEBUG    | __main__:<module>:313 - Training step 14340: loss = 3.1811 | 3027.11ms | Tokens/s = 173,197.8
2025-01-18 12:37:06.822 | DEBUG    | __main__:<module>:313 - Training step 14350: loss = 3.2159 | 3025.97ms | Tokens/s = 173,262.6
2025-01-18 12:37:37.086 | DEBUG    | __main__:<module>:313 - Training step 14360: loss = 3.0832 | 3027.20ms | Tokens/s = 173,192.3
2025-01-18 12:38:07.372 | DEBUG    | __main__:<module>:313 - Training step 14370: loss = 2.9814 | 3026.31ms | Tokens/s = 173,243.1
2025-01-18 12:38:37.642 | DEBUG    | __main__:<module>:313 - Training step 14380: loss = 3.0812 | 3027.41ms | Tokens/s = 173,180.3
2025-01-18 12:39:07.940 | DEBUG    | __main__:<module>:313 - Training step 14390: loss = 3.5090 | 3029.97ms | Tokens/s = 173,033.8
2025-01-18 12:39:38.254 | DEBUG    | __main__:<module>:313 - Training step 14400: loss = 3.2015 | 3029.37ms | Tokens/s = 173,068.6
2025-01-18 12:40:08.547 | DEBUG    | __main__:<module>:313 - Training step 14410: loss = 3.3259 | 3030.07ms | Tokens/s = 173,028.2
2025-01-18 12:40:38.853 | DEBUG    | __main__:<module>:313 - Training step 14420: loss = 3.1817 | 3028.44ms | Tokens/s = 173,121.4
2025-01-18 12:41:09.159 | DEBUG    | __main__:<module>:313 - Training step 14430: loss = 3.1529 | 3028.58ms | Tokens/s = 173,113.5
2025-01-18 12:41:39.440 | DEBUG    | __main__:<module>:313 - Training step 14440: loss = 3.0766 | 3028.42ms | Tokens/s = 173,122.5
2025-01-18 12:42:09.703 | DEBUG    | __main__:<module>:313 - Training step 14450: loss = 3.1084 | 3026.00ms | Tokens/s = 173,261.1
2025-01-18 12:42:39.946 | DEBUG    | __main__:<module>:313 - Training step 14460: loss = 3.0202 | 3022.32ms | Tokens/s = 173,472.0
2025-01-18 12:43:10.200 | DEBUG    | __main__:<module>:313 - Training step 14470: loss = 3.0981 | 3028.01ms | Tokens/s = 173,146.2
2025-01-18 12:43:40.485 | DEBUG    | __main__:<module>:313 - Training step 14480: loss = 3.2853 | 3027.76ms | Tokens/s = 173,160.1
2025-01-18 12:44:10.759 | DEBUG    | __main__:<module>:313 - Training step 14490: loss = 3.0367 | 3027.05ms | Tokens/s = 173,201.1
2025-01-18 12:44:41.013 | DEBUG    | __main__:<module>:313 - Training step 14500: loss = 3.1657 | 3024.01ms | Tokens/s = 173,375.1
2025-01-18 12:45:11.256 | DEBUG    | __main__:<module>:313 - Training step 14510: loss = 3.2209 | 3025.39ms | Tokens/s = 173,296.2
2025-01-18 12:45:41.527 | DEBUG    | __main__:<module>:313 - Training step 14520: loss = 3.3200 | 3028.13ms | Tokens/s = 173,139.2
2025-01-18 12:46:11.819 | DEBUG    | __main__:<module>:313 - Training step 14530: loss = 3.1197 | 3029.53ms | Tokens/s = 173,059.5
2025-01-18 12:46:42.087 | DEBUG    | __main__:<module>:313 - Training step 14540: loss = 3.0366 | 3024.61ms | Tokens/s = 173,340.8
2025-01-18 12:47:12.339 | DEBUG    | __main__:<module>:313 - Training step 14550: loss = 3.1989 | 3027.81ms | Tokens/s = 173,157.7
2025-01-18 12:47:42.621 | DEBUG    | __main__:<module>:313 - Training step 14560: loss = 3.2472 | 3029.29ms | Tokens/s = 173,072.8
2025-01-18 12:48:12.887 | DEBUG    | __main__:<module>:313 - Training step 14570: loss = 3.2004 | 3024.16ms | Tokens/s = 173,366.3
2025-01-18 12:48:43.133 | DEBUG    | __main__:<module>:313 - Training step 14580: loss = 3.1301 | 3024.51ms | Tokens/s = 173,346.6
2025-01-18 12:49:13.367 | DEBUG    | __main__:<module>:313 - Training step 14590: loss = 3.1620 | 3026.48ms | Tokens/s = 173,233.5
2025-01-18 12:49:43.644 | DEBUG    | __main__:<module>:313 - Training step 14600: loss = 3.1992 | 3026.94ms | Tokens/s = 173,207.4
2025-01-18 12:50:13.931 | DEBUG    | __main__:<module>:313 - Training step 14610: loss = 3.1588 | 3028.09ms | Tokens/s = 173,141.6
2025-01-18 12:50:44.196 | DEBUG    | __main__:<module>:313 - Training step 14620: loss = 3.2665 | 3025.18ms | Tokens/s = 173,307.9
2025-01-18 12:51:14.437 | DEBUG    | __main__:<module>:313 - Training step 14630: loss = 3.0355 | 3022.89ms | Tokens/s = 173,439.4
2025-01-18 12:51:44.667 | DEBUG    | __main__:<module>:313 - Training step 14640: loss = 3.1276 | 3020.22ms | Tokens/s = 173,592.6
2025-01-18 12:52:14.917 | DEBUG    | __main__:<module>:313 - Training step 14650: loss = 3.2208 | 3026.39ms | Tokens/s = 173,239.0
2025-01-18 12:52:45.191 | DEBUG    | __main__:<module>:313 - Training step 14660: loss = 3.1530 | 3027.82ms | Tokens/s = 173,156.7
2025-01-18 12:53:15.479 | DEBUG    | __main__:<module>:313 - Training step 14670: loss = 3.0515 | 3029.27ms | Tokens/s = 173,074.1
2025-01-18 12:53:45.785 | DEBUG    | __main__:<module>:313 - Training step 14680: loss = 3.0198 | 3030.13ms | Tokens/s = 173,024.8
2025-01-18 12:54:16.059 | DEBUG    | __main__:<module>:313 - Training step 14690: loss = 3.2186 | 3027.08ms | Tokens/s = 173,199.4
2025-01-18 12:54:46.311 | DEBUG    | __main__:<module>:313 - Training step 14700: loss = 3.1625 | 3023.31ms | Tokens/s = 173,415.4
2025-01-18 12:55:16.552 | DEBUG    | __main__:<module>:313 - Training step 14710: loss = 3.0836 | 3024.39ms | Tokens/s = 173,353.5
2025-01-18 12:55:46.807 | DEBUG    | __main__:<module>:313 - Training step 14720: loss = 3.1054 | 3025.15ms | Tokens/s = 173,309.6
2025-01-18 12:56:17.077 | DEBUG    | __main__:<module>:313 - Training step 14730: loss = 2.9949 | 3028.26ms | Tokens/s = 173,131.7
2025-01-18 12:56:47.361 | DEBUG    | __main__:<module>:313 - Training step 14740: loss = 3.1018 | 3025.98ms | Tokens/s = 173,262.0
2025-01-18 12:57:17.626 | DEBUG    | __main__:<module>:313 - Training step 14750: loss = 3.0309 | 3028.70ms | Tokens/s = 173,106.8
2025-01-18 12:57:47.890 | DEBUG    | __main__:<module>:313 - Training step 14760: loss = 3.2252 | 3023.37ms | Tokens/s = 173,412.1
2025-01-18 12:58:18.135 | DEBUG    | __main__:<module>:313 - Training step 14770: loss = 3.1546 | 3025.30ms | Tokens/s = 173,301.0
2025-01-18 12:58:48.412 | DEBUG    | __main__:<module>:313 - Training step 14780: loss = 3.3013 | 3029.17ms | Tokens/s = 173,079.6
2025-01-18 12:59:18.698 | DEBUG    | __main__:<module>:313 - Training step 14790: loss = 3.1321 | 3028.65ms | Tokens/s = 173,109.4
2025-01-18 12:59:48.988 | DEBUG    | __main__:<module>:313 - Training step 14800: loss = 3.0339 | 3027.95ms | Tokens/s = 173,149.6
2025-01-18 13:00:19.257 | DEBUG    | __main__:<module>:313 - Training step 14810: loss = 3.2340 | 3026.90ms | Tokens/s = 173,209.4
2025-01-18 13:00:49.515 | DEBUG    | __main__:<module>:313 - Training step 14820: loss = 3.0938 | 3025.21ms | Tokens/s = 173,306.3
2025-01-18 13:01:19.756 | DEBUG    | __main__:<module>:313 - Training step 14830: loss = 3.1045 | 3021.56ms | Tokens/s = 173,515.6
2025-01-18 13:01:49.989 | DEBUG    | __main__:<module>:313 - Training step 14840: loss = 3.1677 | 3026.68ms | Tokens/s = 173,222.0
2025-01-18 13:02:20.263 | DEBUG    | __main__:<module>:313 - Training step 14850: loss = 3.2192 | 3028.54ms | Tokens/s = 173,115.9
2025-01-18 13:02:50.547 | DEBUG    | __main__:<module>:313 - Training step 14860: loss = 3.2376 | 3027.83ms | Tokens/s = 173,156.2
2025-01-18 13:03:20.802 | DEBUG    | __main__:<module>:313 - Training step 14870: loss = 3.1336 | 3025.28ms | Tokens/s = 173,302.0
2025-01-18 13:03:51.071 | DEBUG    | __main__:<module>:313 - Training step 14880: loss = 3.1214 | 3027.47ms | Tokens/s = 173,176.9
2025-01-18 13:04:21.341 | DEBUG    | __main__:<module>:313 - Training step 14890: loss = 3.0090 | 3026.88ms | Tokens/s = 173,210.9
2025-01-18 13:04:51.589 | DEBUG    | __main__:<module>:313 - Training step 14900: loss = 3.3830 | 3024.65ms | Tokens/s = 173,338.5
2025-01-18 13:05:21.833 | DEBUG    | __main__:<module>:313 - Training step 14910: loss = 3.0633 | 3025.68ms | Tokens/s = 173,279.7
2025-01-18 13:05:52.105 | DEBUG    | __main__:<module>:313 - Training step 14920: loss = 3.0931 | 3027.51ms | Tokens/s = 173,174.4
2025-01-18 13:06:22.383 | DEBUG    | __main__:<module>:313 - Training step 14930: loss = 3.0827 | 3027.04ms | Tokens/s = 173,201.4
2025-01-18 13:06:52.662 | DEBUG    | __main__:<module>:313 - Training step 14940: loss = 3.1077 | 3029.31ms | Tokens/s = 173,071.9
2025-01-18 13:07:22.950 | DEBUG    | __main__:<module>:313 - Training step 14950: loss = 2.9420 | 3029.49ms | Tokens/s = 173,061.2
2025-01-18 13:07:53.235 | DEBUG    | __main__:<module>:313 - Training step 14960: loss = 3.1624 | 3026.86ms | Tokens/s = 173,211.8
2025-01-18 13:08:23.513 | DEBUG    | __main__:<module>:313 - Training step 14970: loss = 3.1235 | 3027.83ms | Tokens/s = 173,156.3
2025-01-18 13:08:53.791 | DEBUG    | __main__:<module>:313 - Training step 14980: loss = 3.1919 | 3028.56ms | Tokens/s = 173,114.8
2025-01-18 13:09:24.080 | DEBUG    | __main__:<module>:313 - Training step 14990: loss = 3.1441 | 3028.22ms | Tokens/s = 173,134.0
2025-01-18 13:09:57.792 | INFO     | __main__:<module>:265 - Step 15,000/20,000 loss: 3.1279 (T) 3.1272 (V) | lr=1.8e-03
2025-01-18 13:09:57.794 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 13:10:11.067 | DEBUG    | __main__:<module>:313 - Training step 15000: loss = 3.0629 | 19737.53ms | Tokens/s = 26,563.0
2025-01-18 13:10:41.175 | DEBUG    | __main__:<module>:313 - Training step 15010: loss = 3.1469 | 3016.31ms | Tokens/s = 173,817.4
2025-01-18 13:11:11.373 | DEBUG    | __main__:<module>:313 - Training step 15020: loss = 3.1807 | 3021.37ms | Tokens/s = 173,526.7
2025-01-18 13:11:41.603 | DEBUG    | __main__:<module>:313 - Training step 15030: loss = 3.1173 | 3025.46ms | Tokens/s = 173,292.0
2025-01-18 13:12:11.843 | DEBUG    | __main__:<module>:313 - Training step 15040: loss = 3.1215 | 3024.99ms | Tokens/s = 173,318.7
2025-01-18 13:12:42.088 | DEBUG    | __main__:<module>:313 - Training step 15050: loss = 3.0772 | 3025.32ms | Tokens/s = 173,300.1
2025-01-18 13:13:12.343 | DEBUG    | __main__:<module>:313 - Training step 15060: loss = 3.0459 | 3025.30ms | Tokens/s = 173,301.0
2025-01-18 13:13:42.603 | DEBUG    | __main__:<module>:313 - Training step 15070: loss = 3.0862 | 3025.53ms | Tokens/s = 173,287.8
2025-01-18 13:14:12.870 | DEBUG    | __main__:<module>:313 - Training step 15080: loss = 3.1157 | 3029.53ms | Tokens/s = 173,059.0
2025-01-18 13:14:43.141 | DEBUG    | __main__:<module>:313 - Training step 15090: loss = 3.0487 | 3027.99ms | Tokens/s = 173,147.0
2025-01-18 13:15:13.421 | DEBUG    | __main__:<module>:313 - Training step 15100: loss = 3.1047 | 3026.28ms | Tokens/s = 173,244.9
2025-01-18 13:15:43.670 | DEBUG    | __main__:<module>:313 - Training step 15110: loss = 2.9768 | 3025.93ms | Tokens/s = 173,264.9
2025-01-18 13:16:13.912 | DEBUG    | __main__:<module>:313 - Training step 15120: loss = 3.2472 | 3025.24ms | Tokens/s = 173,304.7
2025-01-18 13:16:44.174 | DEBUG    | __main__:<module>:313 - Training step 15130: loss = 3.1082 | 3027.67ms | Tokens/s = 173,165.7
2025-01-18 13:17:14.464 | DEBUG    | __main__:<module>:313 - Training step 15140: loss = 3.0209 | 3030.17ms | Tokens/s = 173,022.4
2025-01-18 13:17:44.761 | DEBUG    | __main__:<module>:313 - Training step 15150: loss = 2.8724 | 3029.11ms | Tokens/s = 173,083.2
2025-01-18 13:18:15.032 | DEBUG    | __main__:<module>:313 - Training step 15160: loss = 3.0825 | 3025.79ms | Tokens/s = 173,272.8
2025-01-18 13:18:45.278 | DEBUG    | __main__:<module>:313 - Training step 15170: loss = 2.9837 | 3025.51ms | Tokens/s = 173,289.0
2025-01-18 13:19:15.511 | DEBUG    | __main__:<module>:313 - Training step 15180: loss = 3.1681 | 3021.13ms | Tokens/s = 173,540.4
2025-01-18 13:19:45.762 | DEBUG    | __main__:<module>:313 - Training step 15190: loss = 3.1400 | 3028.69ms | Tokens/s = 173,107.2
2025-01-18 13:20:16.032 | DEBUG    | __main__:<module>:313 - Training step 15200: loss = 3.2446 | 3025.63ms | Tokens/s = 173,282.5
2025-01-18 13:20:46.301 | DEBUG    | __main__:<module>:313 - Training step 15210: loss = 2.9281 | 3025.03ms | Tokens/s = 173,316.4
2025-01-18 13:21:16.543 | DEBUG    | __main__:<module>:313 - Training step 15220: loss = 3.0472 | 3023.40ms | Tokens/s = 173,410.3
2025-01-18 13:21:46.807 | DEBUG    | __main__:<module>:313 - Training step 15230: loss = 3.0929 | 3027.24ms | Tokens/s = 173,190.1
2025-01-18 13:22:17.092 | DEBUG    | __main__:<module>:313 - Training step 15240: loss = 3.1712 | 3029.60ms | Tokens/s = 173,055.1
2025-01-18 13:22:47.370 | DEBUG    | __main__:<module>:313 - Training step 15250: loss = 3.2327 | 3025.87ms | Tokens/s = 173,268.6
2025-01-18 13:23:17.618 | DEBUG    | __main__:<module>:313 - Training step 15260: loss = 3.0677 | 3023.61ms | Tokens/s = 173,398.2
2025-01-18 13:23:47.882 | DEBUG    | __main__:<module>:313 - Training step 15270: loss = 3.0857 | 3026.86ms | Tokens/s = 173,212.0
2025-01-18 13:24:18.153 | DEBUG    | __main__:<module>:313 - Training step 15280: loss = 3.0238 | 3028.21ms | Tokens/s = 173,134.8
2025-01-18 13:24:48.432 | DEBUG    | __main__:<module>:313 - Training step 15290: loss = 2.9811 | 3025.53ms | Tokens/s = 173,288.2
2025-01-18 13:25:18.696 | DEBUG    | __main__:<module>:313 - Training step 15300: loss = 3.0841 | 3025.99ms | Tokens/s = 173,261.8
2025-01-18 13:25:48.950 | DEBUG    | __main__:<module>:313 - Training step 15310: loss = 3.1009 | 3026.43ms | Tokens/s = 173,236.7
2025-01-18 13:26:19.229 | DEBUG    | __main__:<module>:313 - Training step 15320: loss = 2.9813 | 3029.58ms | Tokens/s = 173,056.3
2025-01-18 13:26:49.523 | DEBUG    | __main__:<module>:313 - Training step 15330: loss = 3.1875 | 3029.85ms | Tokens/s = 173,040.9
2025-01-18 13:27:19.824 | DEBUG    | __main__:<module>:313 - Training step 15340: loss = 3.1585 | 3028.69ms | Tokens/s = 173,107.4
2025-01-18 13:27:50.101 | DEBUG    | __main__:<module>:313 - Training step 15350: loss = 3.1566 | 3026.11ms | Tokens/s = 173,254.6
2025-01-18 13:28:20.356 | DEBUG    | __main__:<module>:313 - Training step 15360: loss = 3.0256 | 3024.57ms | Tokens/s = 173,343.1
2025-01-18 13:28:50.596 | DEBUG    | __main__:<module>:313 - Training step 15370: loss = 3.1152 | 3024.18ms | Tokens/s = 173,365.1
2025-01-18 13:29:20.855 | DEBUG    | __main__:<module>:313 - Training step 15380: loss = 3.0342 | 3026.13ms | Tokens/s = 173,253.8
2025-01-18 13:29:51.137 | DEBUG    | __main__:<module>:313 - Training step 15390: loss = 3.1293 | 3027.98ms | Tokens/s = 173,147.8
2025-01-18 13:30:21.432 | DEBUG    | __main__:<module>:313 - Training step 15400: loss = 3.0405 | 3028.02ms | Tokens/s = 173,145.3
2025-01-18 13:30:51.700 | DEBUG    | __main__:<module>:313 - Training step 15410: loss = 3.0187 | 3025.78ms | Tokens/s = 173,273.8
2025-01-18 13:31:21.943 | DEBUG    | __main__:<module>:313 - Training step 15420: loss = 2.9082 | 3024.10ms | Tokens/s = 173,370.1
2025-01-18 13:31:52.203 | DEBUG    | __main__:<module>:313 - Training step 15430: loss = 3.0968 | 3026.18ms | Tokens/s = 173,250.9
2025-01-18 13:32:22.480 | DEBUG    | __main__:<module>:313 - Training step 15440: loss = 3.1961 | 3027.91ms | Tokens/s = 173,152.0
2025-01-18 13:32:52.769 | DEBUG    | __main__:<module>:313 - Training step 15450: loss = 3.1905 | 3028.30ms | Tokens/s = 173,129.5
2025-01-18 13:33:23.062 | DEBUG    | __main__:<module>:313 - Training step 15460: loss = 3.1028 | 3030.78ms | Tokens/s = 172,988.0
2025-01-18 13:33:53.328 | DEBUG    | __main__:<module>:313 - Training step 15470: loss = 3.1300 | 3026.26ms | Tokens/s = 173,246.2
2025-01-18 13:34:23.585 | DEBUG    | __main__:<module>:313 - Training step 15480: loss = 3.0839 | 3025.86ms | Tokens/s = 173,269.1
2025-01-18 13:34:53.881 | DEBUG    | __main__:<module>:313 - Training step 15490: loss = 3.1462 | 3031.45ms | Tokens/s = 172,949.6
2025-01-18 13:35:24.190 | DEBUG    | __main__:<module>:313 - Training step 15500: loss = 3.1774 | 3029.01ms | Tokens/s = 173,089.1
2025-01-18 13:35:54.492 | DEBUG    | __main__:<module>:313 - Training step 15510: loss = 3.0994 | 3030.70ms | Tokens/s = 172,992.3
2025-01-18 13:36:24.791 | DEBUG    | __main__:<module>:313 - Training step 15520: loss = 3.1718 | 3029.93ms | Tokens/s = 173,036.4
2025-01-18 13:36:55.098 | DEBUG    | __main__:<module>:313 - Training step 15530: loss = 2.9827 | 3031.82ms | Tokens/s = 172,928.6
2025-01-18 13:37:25.405 | DEBUG    | __main__:<module>:313 - Training step 15540: loss = 3.2885 | 3031.20ms | Tokens/s = 172,963.6
2025-01-18 13:37:55.713 | DEBUG    | __main__:<module>:313 - Training step 15550: loss = 3.1092 | 3029.79ms | Tokens/s = 173,044.1
2025-01-18 13:38:26.015 | DEBUG    | __main__:<module>:313 - Training step 15560: loss = 3.2541 | 3029.30ms | Tokens/s = 173,072.4
2025-01-18 13:38:56.315 | DEBUG    | __main__:<module>:313 - Training step 15570: loss = 3.1617 | 3030.16ms | Tokens/s = 173,023.0
2025-01-18 13:39:26.617 | DEBUG    | __main__:<module>:313 - Training step 15580: loss = 2.9432 | 3030.82ms | Tokens/s = 172,985.3
2025-01-18 13:39:56.916 | DEBUG    | __main__:<module>:313 - Training step 15590: loss = 3.0016 | 3030.10ms | Tokens/s = 173,026.5
2025-01-18 13:40:27.225 | DEBUG    | __main__:<module>:313 - Training step 15600: loss = 2.9268 | 3031.29ms | Tokens/s = 172,959.0
2025-01-18 13:40:57.532 | DEBUG    | __main__:<module>:313 - Training step 15610: loss = 2.9037 | 3031.56ms | Tokens/s = 172,943.3
2025-01-18 13:41:27.847 | DEBUG    | __main__:<module>:313 - Training step 15620: loss = 3.1225 | 3031.91ms | Tokens/s = 172,923.4
2025-01-18 13:41:58.167 | DEBUG    | __main__:<module>:313 - Training step 15630: loss = 2.9100 | 3030.61ms | Tokens/s = 172,997.4
2025-01-18 13:42:28.488 | DEBUG    | __main__:<module>:313 - Training step 15640: loss = 3.0955 | 3031.22ms | Tokens/s = 172,962.8
2025-01-18 13:42:58.808 | DEBUG    | __main__:<module>:313 - Training step 15650: loss = 3.1079 | 3032.48ms | Tokens/s = 172,891.0
2025-01-18 13:43:29.109 | DEBUG    | __main__:<module>:313 - Training step 15660: loss = 3.0930 | 3028.77ms | Tokens/s = 173,102.7
2025-01-18 13:43:59.390 | DEBUG    | __main__:<module>:313 - Training step 15670: loss = 3.0632 | 3026.00ms | Tokens/s = 173,261.2
2025-01-18 13:44:29.662 | DEBUG    | __main__:<module>:313 - Training step 15680: loss = 2.9256 | 3028.19ms | Tokens/s = 173,136.0
2025-01-18 13:44:59.908 | DEBUG    | __main__:<module>:313 - Training step 15690: loss = 3.0621 | 3023.88ms | Tokens/s = 173,382.7
2025-01-18 13:45:30.149 | DEBUG    | __main__:<module>:313 - Training step 15700: loss = 2.9544 | 3023.33ms | Tokens/s = 173,414.1
2025-01-18 13:46:00.416 | DEBUG    | __main__:<module>:313 - Training step 15710: loss = 2.9670 | 3029.49ms | Tokens/s = 173,061.6
2025-01-18 13:46:30.719 | DEBUG    | __main__:<module>:313 - Training step 15720: loss = 3.0509 | 3030.39ms | Tokens/s = 173,010.1
2025-01-18 13:47:01.021 | DEBUG    | __main__:<module>:313 - Training step 15730: loss = 3.0318 | 3031.79ms | Tokens/s = 172,930.1
2025-01-18 13:47:31.332 | DEBUG    | __main__:<module>:313 - Training step 15740: loss = 3.1317 | 3030.23ms | Tokens/s = 173,019.1
2025-01-18 13:48:01.647 | DEBUG    | __main__:<module>:313 - Training step 15750: loss = 3.0429 | 3031.24ms | Tokens/s = 172,961.3
2025-01-18 13:48:31.950 | DEBUG    | __main__:<module>:313 - Training step 15760: loss = 3.0019 | 3029.09ms | Tokens/s = 173,084.5
2025-01-18 13:49:02.259 | DEBUG    | __main__:<module>:313 - Training step 15770: loss = 3.0926 | 3031.83ms | Tokens/s = 172,927.8
2025-01-18 13:49:32.574 | DEBUG    | __main__:<module>:313 - Training step 15780: loss = 3.1447 | 3032.59ms | Tokens/s = 172,884.7
2025-01-18 13:50:02.887 | DEBUG    | __main__:<module>:313 - Training step 15790: loss = 3.0381 | 3030.53ms | Tokens/s = 173,001.9
2025-01-18 13:50:33.189 | DEBUG    | __main__:<module>:313 - Training step 15800: loss = 3.0124 | 3029.02ms | Tokens/s = 173,088.5
2025-01-18 13:51:03.490 | DEBUG    | __main__:<module>:313 - Training step 15810: loss = 3.0370 | 3030.86ms | Tokens/s = 172,983.1
2025-01-18 13:51:33.797 | DEBUG    | __main__:<module>:313 - Training step 15820: loss = 2.9954 | 3029.99ms | Tokens/s = 173,032.7
2025-01-18 13:52:04.099 | DEBUG    | __main__:<module>:313 - Training step 15830: loss = 3.1170 | 3029.11ms | Tokens/s = 173,083.4
2025-01-18 13:52:34.401 | DEBUG    | __main__:<module>:313 - Training step 15840: loss = 3.1991 | 3031.55ms | Tokens/s = 172,943.7
2025-01-18 13:53:04.710 | DEBUG    | __main__:<module>:313 - Training step 15850: loss = 3.0257 | 3031.83ms | Tokens/s = 172,928.1
2025-01-18 13:53:35.023 | DEBUG    | __main__:<module>:313 - Training step 15860: loss = 3.2349 | 3032.22ms | Tokens/s = 172,905.5
2025-01-18 13:54:05.344 | DEBUG    | __main__:<module>:313 - Training step 15870: loss = 3.1251 | 3032.16ms | Tokens/s = 172,908.9
2025-01-18 13:54:35.662 | DEBUG    | __main__:<module>:313 - Training step 15880: loss = 2.9154 | 3031.55ms | Tokens/s = 172,944.1
2025-01-18 13:55:05.981 | DEBUG    | __main__:<module>:313 - Training step 15890: loss = 2.9487 | 3031.82ms | Tokens/s = 172,928.3
2025-01-18 13:55:36.300 | DEBUG    | __main__:<module>:313 - Training step 15900: loss = 3.2327 | 3033.29ms | Tokens/s = 172,844.5
2025-01-18 13:56:06.622 | DEBUG    | __main__:<module>:313 - Training step 15910: loss = 3.0202 | 3033.38ms | Tokens/s = 172,839.8
2025-01-18 13:56:36.943 | DEBUG    | __main__:<module>:313 - Training step 15920: loss = 3.0493 | 3031.94ms | Tokens/s = 172,921.7
2025-01-18 13:57:07.255 | DEBUG    | __main__:<module>:313 - Training step 15930: loss = 3.2223 | 3032.53ms | Tokens/s = 172,888.0
2025-01-18 13:57:37.577 | DEBUG    | __main__:<module>:313 - Training step 15940: loss = 3.1263 | 3033.18ms | Tokens/s = 172,851.1
2025-01-18 13:58:07.902 | DEBUG    | __main__:<module>:313 - Training step 15950: loss = 2.8958 | 3031.91ms | Tokens/s = 172,923.3
2025-01-18 13:58:38.220 | DEBUG    | __main__:<module>:313 - Training step 15960: loss = 3.0667 | 3031.63ms | Tokens/s = 172,939.3
2025-01-18 13:59:08.535 | DEBUG    | __main__:<module>:313 - Training step 15970: loss = 2.9971 | 3030.44ms | Tokens/s = 173,007.2
2025-01-18 13:59:38.853 | DEBUG    | __main__:<module>:313 - Training step 15980: loss = 3.0698 | 3030.89ms | Tokens/s = 172,981.2
2025-01-18 14:00:09.176 | DEBUG    | __main__:<module>:313 - Training step 15990: loss = 3.0493 | 3032.12ms | Tokens/s = 172,911.1
2025-01-18 14:00:42.942 | INFO     | __main__:<module>:265 - Step 16,000/20,000 loss: 3.0682 (T) 3.0851 (V) | lr=1.2e-03
2025-01-18 14:00:42.943 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 14:00:59.285 | DEBUG    | __main__:<module>:313 - Training step 16000: loss = 3.1754 | 22818.50ms | Tokens/s = 22,976.4
2025-01-18 14:01:29.431 | DEBUG    | __main__:<module>:313 - Training step 16010: loss = 3.1532 | 3020.82ms | Tokens/s = 173,558.0
2025-01-18 14:01:59.705 | DEBUG    | __main__:<module>:313 - Training step 16020: loss = 3.0774 | 3029.57ms | Tokens/s = 173,056.9
2025-01-18 14:02:29.991 | DEBUG    | __main__:<module>:313 - Training step 16030: loss = 3.0332 | 3029.46ms | Tokens/s = 173,063.4
2025-01-18 14:03:00.299 | DEBUG    | __main__:<module>:313 - Training step 16040: loss = 3.0638 | 3031.61ms | Tokens/s = 172,940.3
2025-01-18 14:03:30.607 | DEBUG    | __main__:<module>:313 - Training step 16050: loss = 3.1755 | 3029.99ms | Tokens/s = 173,032.7
2025-01-18 14:04:00.910 | DEBUG    | __main__:<module>:313 - Training step 16060: loss = 3.1358 | 3029.35ms | Tokens/s = 173,069.4
2025-01-18 14:04:31.218 | DEBUG    | __main__:<module>:313 - Training step 16070: loss = 3.0611 | 3029.00ms | Tokens/s = 173,089.2
2025-01-18 14:05:01.536 | DEBUG    | __main__:<module>:313 - Training step 16080: loss = 3.1521 | 3032.23ms | Tokens/s = 172,905.0
2025-01-18 14:05:31.843 | DEBUG    | __main__:<module>:313 - Training step 16090: loss = 3.2526 | 3031.38ms | Tokens/s = 172,953.5
2025-01-18 14:06:02.156 | DEBUG    | __main__:<module>:313 - Training step 16100: loss = 3.0590 | 3030.33ms | Tokens/s = 173,013.6
2025-01-18 14:06:32.480 | DEBUG    | __main__:<module>:313 - Training step 16110: loss = 3.0960 | 3032.59ms | Tokens/s = 172,884.6
2025-01-18 14:07:02.798 | DEBUG    | __main__:<module>:313 - Training step 16120: loss = 3.1107 | 3031.72ms | Tokens/s = 172,934.3
2025-01-18 14:07:33.116 | DEBUG    | __main__:<module>:313 - Training step 16130: loss = 2.9419 | 3031.26ms | Tokens/s = 172,960.2
2025-01-18 14:08:03.433 | DEBUG    | __main__:<module>:313 - Training step 16140: loss = 3.1478 | 3030.90ms | Tokens/s = 172,981.2
2025-01-18 14:08:33.751 | DEBUG    | __main__:<module>:313 - Training step 16150: loss = 2.8740 | 3032.45ms | Tokens/s = 172,892.6
2025-01-18 14:09:04.067 | DEBUG    | __main__:<module>:313 - Training step 16160: loss = 2.8528 | 3031.35ms | Tokens/s = 172,955.5
2025-01-18 14:09:34.384 | DEBUG    | __main__:<module>:313 - Training step 16170: loss = 3.0133 | 3031.55ms | Tokens/s = 172,943.9
2025-01-18 14:10:04.699 | DEBUG    | __main__:<module>:313 - Training step 16180: loss = 2.8643 | 3033.00ms | Tokens/s = 172,861.2
2025-01-18 14:10:35.013 | DEBUG    | __main__:<module>:313 - Training step 16190: loss = 3.1324 | 3032.11ms | Tokens/s = 172,912.1
2025-01-18 14:11:05.325 | DEBUG    | __main__:<module>:313 - Training step 16200: loss = 3.1182 | 3031.73ms | Tokens/s = 172,933.7
2025-01-18 14:11:35.633 | DEBUG    | __main__:<module>:313 - Training step 16210: loss = 3.0494 | 3030.04ms | Tokens/s = 173,030.0
2025-01-18 14:12:05.939 | DEBUG    | __main__:<module>:313 - Training step 16220: loss = 3.0741 | 3029.08ms | Tokens/s = 173,085.1
2025-01-18 14:12:36.237 | DEBUG    | __main__:<module>:313 - Training step 16230: loss = 3.0462 | 3030.59ms | Tokens/s = 172,998.5
2025-01-18 14:13:06.532 | DEBUG    | __main__:<module>:313 - Training step 16240: loss = 3.0442 | 3029.74ms | Tokens/s = 173,047.3
2025-01-18 14:13:36.824 | DEBUG    | __main__:<module>:313 - Training step 16250: loss = 3.1143 | 3027.74ms | Tokens/s = 173,161.3
2025-01-18 14:14:07.117 | DEBUG    | __main__:<module>:313 - Training step 16260: loss = 3.1199 | 3029.11ms | Tokens/s = 173,083.1
2025-01-18 14:14:37.411 | DEBUG    | __main__:<module>:313 - Training step 16270: loss = 2.9819 | 3029.71ms | Tokens/s = 173,048.7
2025-01-18 14:15:07.711 | DEBUG    | __main__:<module>:313 - Training step 16280: loss = 2.9962 | 3030.92ms | Tokens/s = 172,980.0
2025-01-18 14:15:38.004 | DEBUG    | __main__:<module>:313 - Training step 16290: loss = 3.1228 | 3029.14ms | Tokens/s = 173,081.2
2025-01-18 14:16:08.312 | DEBUG    | __main__:<module>:313 - Training step 16300: loss = 3.0033 | 3030.89ms | Tokens/s = 172,981.8
2025-01-18 14:16:38.626 | DEBUG    | __main__:<module>:313 - Training step 16310: loss = 3.1245 | 3030.04ms | Tokens/s = 173,029.9
2025-01-18 14:17:08.939 | DEBUG    | __main__:<module>:313 - Training step 16320: loss = 3.0708 | 3029.29ms | Tokens/s = 173,073.1
2025-01-18 14:17:39.255 | DEBUG    | __main__:<module>:313 - Training step 16330: loss = 3.1107 | 3031.38ms | Tokens/s = 172,953.8
2025-01-18 14:18:09.578 | DEBUG    | __main__:<module>:313 - Training step 16340: loss = 2.9426 | 3030.02ms | Tokens/s = 173,031.3
2025-01-18 14:18:39.892 | DEBUG    | __main__:<module>:313 - Training step 16350: loss = 3.0454 | 3031.37ms | Tokens/s = 172,954.1
2025-01-18 14:19:10.199 | DEBUG    | __main__:<module>:313 - Training step 16360: loss = 3.1892 | 3030.70ms | Tokens/s = 172,992.5
2025-01-18 14:19:40.506 | DEBUG    | __main__:<module>:313 - Training step 16370: loss = 2.8112 | 3030.51ms | Tokens/s = 173,003.2
2025-01-18 14:20:10.818 | DEBUG    | __main__:<module>:313 - Training step 16380: loss = 2.8212 | 3030.88ms | Tokens/s = 172,982.0
2025-01-18 14:20:41.125 | DEBUG    | __main__:<module>:313 - Training step 16390: loss = 3.0463 | 3030.54ms | Tokens/s = 173,001.3
2025-01-18 14:21:11.428 | DEBUG    | __main__:<module>:313 - Training step 16400: loss = 2.9314 | 3028.45ms | Tokens/s = 173,120.9
2025-01-18 14:21:41.724 | DEBUG    | __main__:<module>:313 - Training step 16410: loss = 3.0729 | 3029.09ms | Tokens/s = 173,084.5
2025-01-18 14:22:12.021 | DEBUG    | __main__:<module>:313 - Training step 16420: loss = 2.9562 | 3029.73ms | Tokens/s = 173,047.6
2025-01-18 14:22:42.318 | DEBUG    | __main__:<module>:313 - Training step 16430: loss = 3.2112 | 3026.89ms | Tokens/s = 173,210.2
2025-01-18 14:23:12.622 | DEBUG    | __main__:<module>:313 - Training step 16440: loss = 2.9638 | 3029.39ms | Tokens/s = 173,067.3
2025-01-18 14:23:42.920 | DEBUG    | __main__:<module>:313 - Training step 16450: loss = 3.0935 | 3029.41ms | Tokens/s = 173,066.0
2025-01-18 14:24:13.219 | DEBUG    | __main__:<module>:313 - Training step 16460: loss = 2.9113 | 3028.11ms | Tokens/s = 173,140.1
2025-01-18 14:24:43.518 | DEBUG    | __main__:<module>:313 - Training step 16470: loss = 3.2272 | 3031.16ms | Tokens/s = 172,966.4
2025-01-18 14:25:13.824 | DEBUG    | __main__:<module>:313 - Training step 16480: loss = 2.9907 | 3030.36ms | Tokens/s = 173,011.9
2025-01-18 14:25:44.120 | DEBUG    | __main__:<module>:313 - Training step 16490: loss = 2.9646 | 3027.66ms | Tokens/s = 173,166.3
2025-01-18 14:26:14.420 | DEBUG    | __main__:<module>:313 - Training step 16500: loss = 3.1325 | 3029.34ms | Tokens/s = 173,070.2
2025-01-18 14:26:44.722 | DEBUG    | __main__:<module>:313 - Training step 16510: loss = 2.9579 | 3030.89ms | Tokens/s = 172,981.7
2025-01-18 14:27:15.028 | DEBUG    | __main__:<module>:313 - Training step 16520: loss = 3.0346 | 3030.35ms | Tokens/s = 173,012.4
2025-01-18 14:27:45.329 | DEBUG    | __main__:<module>:313 - Training step 16530: loss = 3.0789 | 3029.18ms | Tokens/s = 173,079.3
2025-01-18 14:28:15.631 | DEBUG    | __main__:<module>:313 - Training step 16540: loss = 2.9606 | 3028.76ms | Tokens/s = 173,103.1
2025-01-18 14:28:45.938 | DEBUG    | __main__:<module>:313 - Training step 16550: loss = 2.8972 | 3030.99ms | Tokens/s = 172,975.9
2025-01-18 14:29:16.242 | DEBUG    | __main__:<module>:313 - Training step 16560: loss = 3.0556 | 3030.26ms | Tokens/s = 173,017.2
2025-01-18 14:29:46.541 | DEBUG    | __main__:<module>:313 - Training step 16570: loss = 3.0188 | 3030.35ms | Tokens/s = 173,012.1
2025-01-18 14:30:16.842 | DEBUG    | __main__:<module>:313 - Training step 16580: loss = 3.0765 | 3031.58ms | Tokens/s = 172,942.3
2025-01-18 14:30:47.149 | DEBUG    | __main__:<module>:313 - Training step 16590: loss = 3.0613 | 3032.12ms | Tokens/s = 172,911.1
2025-01-18 14:31:17.459 | DEBUG    | __main__:<module>:313 - Training step 16600: loss = 3.0377 | 3030.51ms | Tokens/s = 173,003.0
2025-01-18 14:31:47.768 | DEBUG    | __main__:<module>:313 - Training step 16610: loss = 3.0068 | 3031.44ms | Tokens/s = 172,950.1
2025-01-18 14:32:18.066 | DEBUG    | __main__:<module>:313 - Training step 16620: loss = 3.2716 | 3030.19ms | Tokens/s = 173,021.6
2025-01-18 14:32:48.370 | DEBUG    | __main__:<module>:313 - Training step 16630: loss = 3.1428 | 3029.51ms | Tokens/s = 173,060.2
2025-01-18 14:33:18.676 | DEBUG    | __main__:<module>:313 - Training step 16640: loss = 3.0813 | 3030.39ms | Tokens/s = 173,010.1
2025-01-18 14:33:48.986 | DEBUG    | __main__:<module>:313 - Training step 16650: loss = 3.0098 | 3031.89ms | Tokens/s = 172,924.4
2025-01-18 14:34:19.276 | DEBUG    | __main__:<module>:313 - Training step 16660: loss = 3.0877 | 3028.82ms | Tokens/s = 173,099.7
2025-01-18 14:34:49.571 | DEBUG    | __main__:<module>:313 - Training step 16670: loss = 2.8449 | 3029.04ms | Tokens/s = 173,087.2
2025-01-18 14:35:19.868 | DEBUG    | __main__:<module>:313 - Training step 16680: loss = 3.0379 | 3029.28ms | Tokens/s = 173,073.3
2025-01-18 14:35:50.157 | DEBUG    | __main__:<module>:313 - Training step 16690: loss = 3.0974 | 3028.74ms | Tokens/s = 173,104.5
2025-01-18 14:36:20.453 | DEBUG    | __main__:<module>:313 - Training step 16700: loss = 3.0560 | 3028.89ms | Tokens/s = 173,095.8
2025-01-18 14:36:50.752 | DEBUG    | __main__:<module>:313 - Training step 16710: loss = 3.2322 | 3030.09ms | Tokens/s = 173,027.1
2025-01-18 14:37:21.044 | DEBUG    | __main__:<module>:313 - Training step 16720: loss = 2.9731 | 3030.60ms | Tokens/s = 172,997.9
2025-01-18 14:37:51.334 | DEBUG    | __main__:<module>:313 - Training step 16730: loss = 3.0443 | 3027.91ms | Tokens/s = 173,151.8
2025-01-18 14:38:21.636 | DEBUG    | __main__:<module>:313 - Training step 16740: loss = 2.9527 | 3029.61ms | Tokens/s = 173,054.4
2025-01-18 14:38:51.929 | DEBUG    | __main__:<module>:313 - Training step 16750: loss = 3.0307 | 3027.94ms | Tokens/s = 173,150.0
2025-01-18 14:39:22.214 | DEBUG    | __main__:<module>:313 - Training step 16760: loss = 3.0747 | 3028.31ms | Tokens/s = 173,129.1
2025-01-18 14:39:52.510 | DEBUG    | __main__:<module>:313 - Training step 16770: loss = 2.9809 | 3030.88ms | Tokens/s = 172,982.3
2025-01-18 14:40:22.799 | DEBUG    | __main__:<module>:313 - Training step 16780: loss = 2.8711 | 3027.69ms | Tokens/s = 173,164.4
2025-01-18 14:40:53.091 | DEBUG    | __main__:<module>:313 - Training step 16790: loss = 2.9927 | 3029.53ms | Tokens/s = 173,059.3
2025-01-18 14:41:23.383 | DEBUG    | __main__:<module>:313 - Training step 16800: loss = 3.0441 | 3026.52ms | Tokens/s = 173,231.2
2025-01-18 14:41:53.697 | DEBUG    | __main__:<module>:313 - Training step 16810: loss = 2.9147 | 3031.98ms | Tokens/s = 172,919.2
2025-01-18 14:42:24.005 | DEBUG    | __main__:<module>:313 - Training step 16820: loss = 2.9061 | 3030.88ms | Tokens/s = 172,982.2
2025-01-18 14:42:54.314 | DEBUG    | __main__:<module>:313 - Training step 16830: loss = 2.9451 | 3032.28ms | Tokens/s = 172,902.1
2025-01-18 14:43:24.620 | DEBUG    | __main__:<module>:313 - Training step 16840: loss = 3.1649 | 3030.63ms | Tokens/s = 172,996.2
2025-01-18 14:43:54.919 | DEBUG    | __main__:<module>:313 - Training step 16850: loss = 3.0848 | 3029.57ms | Tokens/s = 173,056.7
2025-01-18 14:44:25.218 | DEBUG    | __main__:<module>:313 - Training step 16860: loss = 3.1587 | 3029.47ms | Tokens/s = 173,062.9
2025-01-18 14:44:55.510 | DEBUG    | __main__:<module>:313 - Training step 16870: loss = 3.0061 | 3029.38ms | Tokens/s = 173,067.6
2025-01-18 14:45:25.808 | DEBUG    | __main__:<module>:313 - Training step 16880: loss = 2.9226 | 3028.72ms | Tokens/s = 173,105.7
2025-01-18 14:45:56.104 | DEBUG    | __main__:<module>:313 - Training step 16890: loss = 3.0044 | 3028.02ms | Tokens/s = 173,145.3
2025-01-18 14:46:26.406 | DEBUG    | __main__:<module>:313 - Training step 16900: loss = 2.8882 | 3030.56ms | Tokens/s = 173,000.5
2025-01-18 14:46:56.703 | DEBUG    | __main__:<module>:313 - Training step 16910: loss = 3.2017 | 3029.62ms | Tokens/s = 173,054.2
2025-01-18 14:47:27.003 | DEBUG    | __main__:<module>:313 - Training step 16920: loss = 2.8831 | 3030.27ms | Tokens/s = 173,017.2
2025-01-18 14:47:57.298 | DEBUG    | __main__:<module>:313 - Training step 16930: loss = 3.1198 | 3028.53ms | Tokens/s = 173,116.4
2025-01-18 14:48:27.593 | DEBUG    | __main__:<module>:313 - Training step 16940: loss = 2.9542 | 3029.99ms | Tokens/s = 173,033.2
2025-01-18 14:48:57.896 | DEBUG    | __main__:<module>:313 - Training step 16950: loss = 2.8711 | 3028.62ms | Tokens/s = 173,111.1
2025-01-18 14:49:28.196 | DEBUG    | __main__:<module>:313 - Training step 16960: loss = 2.9955 | 3030.37ms | Tokens/s = 173,011.0
2025-01-18 14:49:58.495 | DEBUG    | __main__:<module>:313 - Training step 16970: loss = 2.9635 | 3030.92ms | Tokens/s = 172,980.0
2025-01-18 14:50:28.796 | DEBUG    | __main__:<module>:313 - Training step 16980: loss = 2.9977 | 3029.63ms | Tokens/s = 173,053.4
2025-01-18 14:50:59.098 | DEBUG    | __main__:<module>:313 - Training step 16990: loss = 3.0430 | 3029.55ms | Tokens/s = 173,058.3
2025-01-18 14:51:32.839 | INFO     | __main__:<module>:265 - Step 17,000/20,000 loss: 3.0379 (T) 3.0330 (V) | lr=6.7e-04
2025-01-18 14:51:32.841 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 14:51:45.959 | DEBUG    | __main__:<module>:313 - Training step 17000: loss = 2.9915 | 19589.37ms | Tokens/s = 26,763.9
2025-01-18 14:52:16.124 | DEBUG    | __main__:<module>:313 - Training step 17010: loss = 3.1484 | 3025.62ms | Tokens/s = 173,282.7
2025-01-18 14:52:46.394 | DEBUG    | __main__:<module>:313 - Training step 17020: loss = 3.0618 | 3029.89ms | Tokens/s = 173,038.7
2025-01-18 14:53:16.708 | DEBUG    | __main__:<module>:313 - Training step 17030: loss = 3.2369 | 3029.90ms | Tokens/s = 173,038.0
2025-01-18 14:53:47.019 | DEBUG    | __main__:<module>:313 - Training step 17040: loss = 3.0168 | 3030.79ms | Tokens/s = 172,987.2
2025-01-18 14:54:17.322 | DEBUG    | __main__:<module>:313 - Training step 17050: loss = 3.2534 | 3030.53ms | Tokens/s = 173,001.9
2025-01-18 14:54:47.624 | DEBUG    | __main__:<module>:313 - Training step 17060: loss = 3.1053 | 3028.06ms | Tokens/s = 173,143.4
2025-01-18 14:55:17.927 | DEBUG    | __main__:<module>:313 - Training step 17070: loss = 2.8802 | 3030.19ms | Tokens/s = 173,021.5
2025-01-18 14:55:48.213 | DEBUG    | __main__:<module>:313 - Training step 17080: loss = 2.9380 | 3028.99ms | Tokens/s = 173,089.9
2025-01-18 14:56:18.506 | DEBUG    | __main__:<module>:313 - Training step 17090: loss = 3.0048 | 3029.09ms | Tokens/s = 173,084.5
2025-01-18 14:56:48.793 | DEBUG    | __main__:<module>:313 - Training step 17100: loss = 3.0041 | 3026.41ms | Tokens/s = 173,237.7
2025-01-18 14:57:19.079 | DEBUG    | __main__:<module>:313 - Training step 17110: loss = 2.9478 | 3028.79ms | Tokens/s = 173,101.5
2025-01-18 14:57:49.376 | DEBUG    | __main__:<module>:313 - Training step 17120: loss = 2.9490 | 3030.98ms | Tokens/s = 172,976.3
2025-01-18 14:58:19.664 | DEBUG    | __main__:<module>:313 - Training step 17130: loss = 2.9085 | 3028.05ms | Tokens/s = 173,143.6
2025-01-18 14:58:49.967 | DEBUG    | __main__:<module>:313 - Training step 17140: loss = 3.1521 | 3029.46ms | Tokens/s = 173,063.0
2025-01-18 14:59:20.264 | DEBUG    | __main__:<module>:313 - Training step 17150: loss = 2.9633 | 3030.56ms | Tokens/s = 173,000.5
2025-01-18 14:59:50.559 | DEBUG    | __main__:<module>:313 - Training step 17160: loss = 2.9572 | 3029.02ms | Tokens/s = 173,088.4
2025-01-18 15:00:20.871 | DEBUG    | __main__:<module>:313 - Training step 17170: loss = 2.9798 | 3031.54ms | Tokens/s = 172,944.4
2025-01-18 15:00:51.184 | DEBUG    | __main__:<module>:313 - Training step 17180: loss = 2.9873 | 3031.54ms | Tokens/s = 172,944.7
2025-01-18 15:01:21.505 | DEBUG    | __main__:<module>:313 - Training step 17190: loss = 2.9781 | 3031.29ms | Tokens/s = 172,958.8
2025-01-18 15:01:51.824 | DEBUG    | __main__:<module>:313 - Training step 17200: loss = 3.1950 | 3029.58ms | Tokens/s = 173,056.5
2025-01-18 15:02:22.135 | DEBUG    | __main__:<module>:313 - Training step 17210: loss = 3.0927 | 3029.60ms | Tokens/s = 173,055.5
2025-01-18 15:02:52.440 | DEBUG    | __main__:<module>:313 - Training step 17220: loss = 3.0433 | 3028.69ms | Tokens/s = 173,107.0
2025-01-18 15:03:22.739 | DEBUG    | __main__:<module>:313 - Training step 17230: loss = 3.1853 | 3029.04ms | Tokens/s = 173,087.4
2025-01-18 15:03:53.038 | DEBUG    | __main__:<module>:313 - Training step 17240: loss = 3.0354 | 3029.34ms | Tokens/s = 173,069.8
2025-01-18 15:04:23.327 | DEBUG    | __main__:<module>:313 - Training step 17250: loss = 2.9154 | 3029.36ms | Tokens/s = 173,068.9
2025-01-18 15:04:53.624 | DEBUG    | __main__:<module>:313 - Training step 17260: loss = 2.9012 | 3030.10ms | Tokens/s = 173,026.4
2025-01-18 15:05:23.926 | DEBUG    | __main__:<module>:313 - Training step 17270: loss = 3.0571 | 3030.57ms | Tokens/s = 172,999.7
2025-01-18 15:05:54.221 | DEBUG    | __main__:<module>:313 - Training step 17280: loss = 2.9681 | 3029.63ms | Tokens/s = 173,053.7
2025-01-18 15:06:24.513 | DEBUG    | __main__:<module>:313 - Training step 17290: loss = 3.0777 | 3027.67ms | Tokens/s = 173,165.4
2025-01-18 15:06:54.810 | DEBUG    | __main__:<module>:313 - Training step 17300: loss = 2.9411 | 3028.68ms | Tokens/s = 173,107.7
2025-01-18 15:07:25.109 | DEBUG    | __main__:<module>:313 - Training step 17310: loss = 2.9508 | 3029.85ms | Tokens/s = 173,041.0
2025-01-18 15:07:55.409 | DEBUG    | __main__:<module>:313 - Training step 17320: loss = 2.9885 | 3029.63ms | Tokens/s = 173,053.7
2025-01-18 15:08:25.699 | DEBUG    | __main__:<module>:313 - Training step 17330: loss = 2.8379 | 3030.32ms | Tokens/s = 173,014.3
2025-01-18 15:08:55.996 | DEBUG    | __main__:<module>:313 - Training step 17340: loss = 2.9436 | 3029.10ms | Tokens/s = 173,084.0
2025-01-18 15:09:26.288 | DEBUG    | __main__:<module>:313 - Training step 17350: loss = 3.0506 | 3030.01ms | Tokens/s = 173,032.0
2025-01-18 15:09:56.575 | DEBUG    | __main__:<module>:313 - Training step 17360: loss = 3.0440 | 3029.22ms | Tokens/s = 173,077.1
2025-01-18 15:10:26.880 | DEBUG    | __main__:<module>:313 - Training step 17370: loss = 3.0435 | 3028.79ms | Tokens/s = 173,101.4
2025-01-18 15:10:57.186 | DEBUG    | __main__:<module>:313 - Training step 17380: loss = 3.0146 | 3030.45ms | Tokens/s = 173,006.7
2025-01-18 15:11:27.473 | DEBUG    | __main__:<module>:313 - Training step 17390: loss = 2.9001 | 3029.51ms | Tokens/s = 173,060.1
2025-01-18 15:11:57.758 | DEBUG    | __main__:<module>:313 - Training step 17400: loss = 2.9113 | 3026.52ms | Tokens/s = 173,231.3
2025-01-18 15:12:28.039 | DEBUG    | __main__:<module>:313 - Training step 17410: loss = 2.8580 | 3028.49ms | Tokens/s = 173,118.7
2025-01-18 15:12:58.320 | DEBUG    | __main__:<module>:313 - Training step 17420: loss = 3.1243 | 3027.26ms | Tokens/s = 173,188.9
2025-01-18 15:13:28.603 | DEBUG    | __main__:<module>:313 - Training step 17430: loss = 2.9717 | 3029.41ms | Tokens/s = 173,065.8
2025-01-18 15:13:58.916 | DEBUG    | __main__:<module>:313 - Training step 17440: loss = 3.0985 | 3029.95ms | Tokens/s = 173,035.2
2025-01-18 15:14:29.213 | DEBUG    | __main__:<module>:313 - Training step 17450: loss = 3.0276 | 3028.26ms | Tokens/s = 173,131.6
2025-01-18 15:14:59.507 | DEBUG    | __main__:<module>:313 - Training step 17460: loss = 2.9372 | 3030.27ms | Tokens/s = 173,017.1
2025-01-18 15:15:29.791 | DEBUG    | __main__:<module>:313 - Training step 17470: loss = 3.1914 | 3028.42ms | Tokens/s = 173,122.8
2025-01-18 15:16:00.076 | DEBUG    | __main__:<module>:313 - Training step 17480: loss = 3.0338 | 3029.46ms | Tokens/s = 173,063.0
2025-01-18 15:16:30.366 | DEBUG    | __main__:<module>:313 - Training step 17490: loss = 2.8649 | 3030.68ms | Tokens/s = 172,993.6
2025-01-18 15:17:00.675 | DEBUG    | __main__:<module>:313 - Training step 17500: loss = 2.8407 | 3028.08ms | Tokens/s = 173,142.1
2025-01-18 15:17:30.968 | DEBUG    | __main__:<module>:313 - Training step 17510: loss = 2.6951 | 3029.05ms | Tokens/s = 173,086.6
2025-01-18 15:18:01.259 | DEBUG    | __main__:<module>:313 - Training step 17520: loss = 2.9756 | 3029.91ms | Tokens/s = 173,037.2
2025-01-18 15:18:31.550 | DEBUG    | __main__:<module>:313 - Training step 17530: loss = 3.0094 | 3028.45ms | Tokens/s = 173,120.9
2025-01-18 15:19:01.833 | DEBUG    | __main__:<module>:313 - Training step 17540: loss = 3.0857 | 3027.42ms | Tokens/s = 173,179.7
2025-01-18 15:19:32.125 | DEBUG    | __main__:<module>:313 - Training step 17550: loss = 3.0844 | 3029.28ms | Tokens/s = 173,073.4
2025-01-18 15:20:02.423 | DEBUG    | __main__:<module>:313 - Training step 17560: loss = 2.8226 | 3027.82ms | Tokens/s = 173,156.9
2025-01-18 15:20:32.702 | DEBUG    | __main__:<module>:313 - Training step 17570: loss = 3.0477 | 3026.51ms | Tokens/s = 173,231.9
2025-01-18 15:21:02.983 | DEBUG    | __main__:<module>:313 - Training step 17580: loss = 3.1996 | 3030.10ms | Tokens/s = 173,026.4
2025-01-18 15:21:33.273 | DEBUG    | __main__:<module>:313 - Training step 17590: loss = 3.1006 | 3027.43ms | Tokens/s = 173,179.2
2025-01-18 15:22:03.555 | DEBUG    | __main__:<module>:313 - Training step 17600: loss = 3.1256 | 3027.11ms | Tokens/s = 173,197.6
2025-01-18 15:22:33.842 | DEBUG    | __main__:<module>:313 - Training step 17610: loss = 2.9544 | 3029.97ms | Tokens/s = 173,033.9
2025-01-18 15:23:04.146 | DEBUG    | __main__:<module>:313 - Training step 17620: loss = 3.1316 | 3028.93ms | Tokens/s = 173,093.3
2025-01-18 15:23:34.452 | DEBUG    | __main__:<module>:313 - Training step 17630: loss = 2.9487 | 3031.53ms | Tokens/s = 172,944.9
2025-01-18 15:24:04.738 | DEBUG    | __main__:<module>:313 - Training step 17640: loss = 3.1847 | 3029.23ms | Tokens/s = 173,076.3
2025-01-18 15:24:35.022 | DEBUG    | __main__:<module>:313 - Training step 17650: loss = 2.9627 | 3028.33ms | Tokens/s = 173,127.8
2025-01-18 15:25:05.298 | DEBUG    | __main__:<module>:313 - Training step 17660: loss = 3.1052 | 3028.60ms | Tokens/s = 173,112.0
2025-01-18 15:25:35.579 | DEBUG    | __main__:<module>:313 - Training step 17670: loss = 3.0382 | 3026.49ms | Tokens/s = 173,232.8
2025-01-18 15:26:05.856 | DEBUG    | __main__:<module>:313 - Training step 17680: loss = 2.9816 | 3027.05ms | Tokens/s = 173,201.0
2025-01-18 15:26:36.134 | DEBUG    | __main__:<module>:313 - Training step 17690: loss = 3.0798 | 3027.36ms | Tokens/s = 173,183.3
2025-01-18 15:27:06.423 | DEBUG    | __main__:<module>:313 - Training step 17700: loss = 2.9288 | 3027.58ms | Tokens/s = 173,170.8
2025-01-18 15:27:36.707 | DEBUG    | __main__:<module>:313 - Training step 17710: loss = 2.8801 | 3028.04ms | Tokens/s = 173,144.2
2025-01-18 15:28:06.991 | DEBUG    | __main__:<module>:313 - Training step 17720: loss = 3.0433 | 3028.49ms | Tokens/s = 173,118.9
2025-01-18 15:28:37.277 | DEBUG    | __main__:<module>:313 - Training step 17730: loss = 3.0220 | 3027.58ms | Tokens/s = 173,170.4
2025-01-18 15:29:07.569 | DEBUG    | __main__:<module>:313 - Training step 17740: loss = 2.9415 | 3030.61ms | Tokens/s = 172,997.8
2025-01-18 15:29:37.858 | DEBUG    | __main__:<module>:313 - Training step 17750: loss = 3.1240 | 3029.70ms | Tokens/s = 173,049.3
2025-01-18 15:30:08.140 | DEBUG    | __main__:<module>:313 - Training step 17760: loss = 3.1726 | 3028.56ms | Tokens/s = 173,114.8
2025-01-18 15:30:38.419 | DEBUG    | __main__:<module>:313 - Training step 17770: loss = 3.0915 | 3028.22ms | Tokens/s = 173,134.3
2025-01-18 15:31:08.707 | DEBUG    | __main__:<module>:313 - Training step 17780: loss = 3.1289 | 3029.48ms | Tokens/s = 173,061.8
2025-01-18 15:31:38.999 | DEBUG    | __main__:<module>:313 - Training step 17790: loss = 3.1090 | 3030.20ms | Tokens/s = 173,021.2
2025-01-18 15:32:09.290 | DEBUG    | __main__:<module>:313 - Training step 17800: loss = 3.0956 | 3027.90ms | Tokens/s = 173,152.1
2025-01-18 15:32:39.577 | DEBUG    | __main__:<module>:313 - Training step 17810: loss = 3.2421 | 3028.21ms | Tokens/s = 173,134.7
2025-01-18 15:33:09.870 | DEBUG    | __main__:<module>:313 - Training step 17820: loss = 3.0605 | 3030.80ms | Tokens/s = 172,987.0
2025-01-18 15:33:40.167 | DEBUG    | __main__:<module>:313 - Training step 17830: loss = 3.0262 | 3031.13ms | Tokens/s = 172,968.0
2025-01-18 15:34:10.468 | DEBUG    | __main__:<module>:313 - Training step 17840: loss = 2.8653 | 3029.44ms | Tokens/s = 173,064.1
2025-01-18 15:34:40.770 | DEBUG    | __main__:<module>:313 - Training step 17850: loss = 3.1596 | 3029.06ms | Tokens/s = 173,086.0
2025-01-18 15:35:11.065 | DEBUG    | __main__:<module>:313 - Training step 17860: loss = 3.0724 | 3029.97ms | Tokens/s = 173,033.8
2025-01-18 15:35:41.362 | DEBUG    | __main__:<module>:313 - Training step 17870: loss = 2.9824 | 3031.13ms | Tokens/s = 172,967.7
2025-01-18 15:36:11.663 | DEBUG    | __main__:<module>:313 - Training step 17880: loss = 2.9739 | 3029.81ms | Tokens/s = 173,043.4
2025-01-18 15:36:41.952 | DEBUG    | __main__:<module>:313 - Training step 17890: loss = 2.8155 | 3026.75ms | Tokens/s = 173,218.4
2025-01-18 15:37:12.226 | DEBUG    | __main__:<module>:313 - Training step 17900: loss = 3.0722 | 3024.09ms | Tokens/s = 173,370.3
2025-01-18 15:37:42.488 | DEBUG    | __main__:<module>:313 - Training step 17910: loss = 3.0371 | 3028.31ms | Tokens/s = 173,129.1
2025-01-18 15:38:12.780 | DEBUG    | __main__:<module>:313 - Training step 17920: loss = 3.0086 | 3031.54ms | Tokens/s = 172,944.3
2025-01-18 15:38:43.069 | DEBUG    | __main__:<module>:313 - Training step 17930: loss = 3.0641 | 3026.57ms | Tokens/s = 173,228.5
2025-01-18 15:39:13.343 | DEBUG    | __main__:<module>:313 - Training step 17940: loss = 2.9247 | 3027.40ms | Tokens/s = 173,180.7
2025-01-18 15:39:43.598 | DEBUG    | __main__:<module>:313 - Training step 17950: loss = 2.9216 | 3024.32ms | Tokens/s = 173,357.4
2025-01-18 15:40:13.854 | DEBUG    | __main__:<module>:313 - Training step 17960: loss = 3.0493 | 3025.13ms | Tokens/s = 173,311.1
2025-01-18 15:40:44.106 | DEBUG    | __main__:<module>:313 - Training step 17970: loss = 3.1172 | 3025.27ms | Tokens/s = 173,302.9
2025-01-18 15:41:14.380 | DEBUG    | __main__:<module>:313 - Training step 17980: loss = 2.8923 | 3028.68ms | Tokens/s = 173,107.9
2025-01-18 15:41:44.679 | DEBUG    | __main__:<module>:313 - Training step 17990: loss = 2.9214 | 3030.33ms | Tokens/s = 173,013.6
2025-01-18 15:42:18.426 | INFO     | __main__:<module>:265 - Step 18,000/20,000 loss: 2.9833 (T) 2.9901 (V) | lr=3.0e-04
2025-01-18 15:42:18.427 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 15:42:32.015 | DEBUG    | __main__:<module>:313 - Training step 18000: loss = 3.0762 | 20058.23ms | Tokens/s = 26,138.3
2025-01-18 15:43:02.150 | DEBUG    | __main__:<module>:313 - Training step 18010: loss = 3.0377 | 3019.69ms | Tokens/s = 173,623.0
2025-01-18 15:43:32.393 | DEBUG    | __main__:<module>:313 - Training step 18020: loss = 2.8582 | 3027.11ms | Tokens/s = 173,197.5
2025-01-18 15:44:02.674 | DEBUG    | __main__:<module>:313 - Training step 18030: loss = 3.0685 | 3027.62ms | Tokens/s = 173,168.2
2025-01-18 15:44:32.950 | DEBUG    | __main__:<module>:313 - Training step 18040: loss = 2.8521 | 3024.71ms | Tokens/s = 173,334.7
2025-01-18 15:45:03.215 | DEBUG    | __main__:<module>:313 - Training step 18050: loss = 3.0105 | 3027.00ms | Tokens/s = 173,204.1
2025-01-18 15:45:33.473 | DEBUG    | __main__:<module>:313 - Training step 18060: loss = 3.0234 | 3026.66ms | Tokens/s = 173,223.3
2025-01-18 15:46:03.766 | DEBUG    | __main__:<module>:313 - Training step 18070: loss = 2.9760 | 3032.07ms | Tokens/s = 172,914.4
2025-01-18 15:46:34.081 | DEBUG    | __main__:<module>:313 - Training step 18080: loss = 2.9783 | 3031.72ms | Tokens/s = 172,934.4
2025-01-18 15:47:04.396 | DEBUG    | __main__:<module>:313 - Training step 18090: loss = 3.1401 | 3031.71ms | Tokens/s = 172,934.8
2025-01-18 15:47:34.703 | DEBUG    | __main__:<module>:313 - Training step 18100: loss = 2.8710 | 3028.24ms | Tokens/s = 173,132.7
2025-01-18 15:48:05.001 | DEBUG    | __main__:<module>:313 - Training step 18110: loss = 2.9208 | 3029.68ms | Tokens/s = 173,050.3
2025-01-18 15:48:35.301 | DEBUG    | __main__:<module>:313 - Training step 18120: loss = 2.8610 | 3028.99ms | Tokens/s = 173,089.9
2025-01-18 15:49:05.607 | DEBUG    | __main__:<module>:313 - Training step 18130: loss = 2.9319 | 3031.01ms | Tokens/s = 172,974.8
2025-01-18 15:49:35.901 | DEBUG    | __main__:<module>:313 - Training step 18140: loss = 3.0778 | 3028.71ms | Tokens/s = 173,105.8
2025-01-18 15:50:06.200 | DEBUG    | __main__:<module>:313 - Training step 18150: loss = 3.0360 | 3028.31ms | Tokens/s = 173,129.0
2025-01-18 15:50:36.511 | DEBUG    | __main__:<module>:313 - Training step 18160: loss = 3.0939 | 3030.44ms | Tokens/s = 173,007.4
2025-01-18 15:51:06.820 | DEBUG    | __main__:<module>:313 - Training step 18170: loss = 2.8829 | 3030.43ms | Tokens/s = 173,008.0
2025-01-18 15:51:37.124 | DEBUG    | __main__:<module>:313 - Training step 18180: loss = 3.0180 | 3030.57ms | Tokens/s = 173,000.1
2025-01-18 15:52:07.429 | DEBUG    | __main__:<module>:313 - Training step 18190: loss = 3.1490 | 3029.94ms | Tokens/s = 173,036.0
2025-01-18 15:52:37.743 | DEBUG    | __main__:<module>:313 - Training step 18200: loss = 2.8512 | 3029.44ms | Tokens/s = 173,064.4
2025-01-18 15:53:08.046 | DEBUG    | __main__:<module>:313 - Training step 18210: loss = 3.0396 | 3030.29ms | Tokens/s = 173,016.0
2025-01-18 15:53:38.354 | DEBUG    | __main__:<module>:313 - Training step 18220: loss = 2.9200 | 3029.80ms | Tokens/s = 173,044.0
2025-01-18 15:54:08.659 | DEBUG    | __main__:<module>:313 - Training step 18230: loss = 3.0394 | 3029.94ms | Tokens/s = 173,036.0
2025-01-18 15:54:38.976 | DEBUG    | __main__:<module>:313 - Training step 18240: loss = 2.7880 | 3030.18ms | Tokens/s = 173,022.2
2025-01-18 15:55:09.290 | DEBUG    | __main__:<module>:313 - Training step 18250: loss = 3.1712 | 3031.28ms | Tokens/s = 172,959.2
2025-01-18 15:55:39.610 | DEBUG    | __main__:<module>:313 - Training step 18260: loss = 2.9550 | 3034.14ms | Tokens/s = 172,796.0
2025-01-18 15:56:09.920 | DEBUG    | __main__:<module>:313 - Training step 18270: loss = 3.1034 | 3030.11ms | Tokens/s = 173,026.0
2025-01-18 15:56:40.233 | DEBUG    | __main__:<module>:313 - Training step 18280: loss = 2.9134 | 3031.25ms | Tokens/s = 172,961.1
2025-01-18 15:57:10.550 | DEBUG    | __main__:<module>:313 - Training step 18290: loss = 3.0226 | 3032.29ms | Tokens/s = 172,901.4
2025-01-18 15:57:40.872 | DEBUG    | __main__:<module>:313 - Training step 18300: loss = 2.7090 | 3029.30ms | Tokens/s = 173,072.4
2025-01-18 15:58:11.186 | DEBUG    | __main__:<module>:313 - Training step 18310: loss = 2.9350 | 3030.72ms | Tokens/s = 172,991.4
2025-01-18 15:58:41.502 | DEBUG    | __main__:<module>:313 - Training step 18320: loss = 3.1449 | 3030.88ms | Tokens/s = 172,982.3
2025-01-18 15:59:11.821 | DEBUG    | __main__:<module>:313 - Training step 18330: loss = 2.9874 | 3032.71ms | Tokens/s = 172,877.9
2025-01-18 15:59:42.140 | DEBUG    | __main__:<module>:313 - Training step 18340: loss = 3.1561 | 3033.52ms | Tokens/s = 172,831.6
2025-01-18 16:00:12.455 | DEBUG    | __main__:<module>:313 - Training step 18350: loss = 3.0000 | 3029.96ms | Tokens/s = 173,034.6
2025-01-18 16:00:42.767 | DEBUG    | __main__:<module>:313 - Training step 18360: loss = 2.9987 | 3031.66ms | Tokens/s = 172,937.3
2025-01-18 16:01:13.068 | DEBUG    | __main__:<module>:313 - Training step 18370: loss = 2.7732 | 3030.56ms | Tokens/s = 173,000.6
2025-01-18 16:01:43.377 | DEBUG    | __main__:<module>:313 - Training step 18380: loss = 2.9412 | 3032.69ms | Tokens/s = 172,878.7
2025-01-18 16:02:13.674 | DEBUG    | __main__:<module>:313 - Training step 18390: loss = 2.8529 | 3029.80ms | Tokens/s = 173,043.6
2025-01-18 16:02:43.978 | DEBUG    | __main__:<module>:313 - Training step 18400: loss = 3.0549 | 3030.17ms | Tokens/s = 173,022.5
2025-01-18 16:03:14.301 | DEBUG    | __main__:<module>:313 - Training step 18410: loss = 3.0530 | 3033.19ms | Tokens/s = 172,850.2
2025-01-18 16:03:44.626 | DEBUG    | __main__:<module>:313 - Training step 18420: loss = 3.0398 | 3032.51ms | Tokens/s = 172,889.1
2025-01-18 16:04:14.946 | DEBUG    | __main__:<module>:313 - Training step 18430: loss = 2.9122 | 3031.94ms | Tokens/s = 172,921.6
2025-01-18 16:04:45.261 | DEBUG    | __main__:<module>:313 - Training step 18440: loss = 2.9978 | 3032.24ms | Tokens/s = 172,904.3
2025-01-18 16:05:15.575 | DEBUG    | __main__:<module>:313 - Training step 18450: loss = 2.9035 | 3030.91ms | Tokens/s = 172,980.1
2025-01-18 16:05:45.882 | DEBUG    | __main__:<module>:313 - Training step 18460: loss = 2.9407 | 3031.26ms | Tokens/s = 172,960.5
2025-01-18 16:06:16.198 | DEBUG    | __main__:<module>:313 - Training step 18470: loss = 3.0836 | 3031.38ms | Tokens/s = 172,953.8
2025-01-18 16:06:46.509 | DEBUG    | __main__:<module>:313 - Training step 18480: loss = 2.9797 | 3033.27ms | Tokens/s = 172,846.0
2025-01-18 16:07:16.817 | DEBUG    | __main__:<module>:313 - Training step 18490: loss = 2.8299 | 3031.51ms | Tokens/s = 172,946.2
2025-01-18 16:07:47.133 | DEBUG    | __main__:<module>:313 - Training step 18500: loss = 2.9235 | 3031.21ms | Tokens/s = 172,963.2
2025-01-18 16:08:17.448 | DEBUG    | __main__:<module>:313 - Training step 18510: loss = 2.7697 | 3031.50ms | Tokens/s = 172,946.9
2025-01-18 16:08:47.768 | DEBUG    | __main__:<module>:313 - Training step 18520: loss = 3.0592 | 3031.79ms | Tokens/s = 172,930.1
2025-01-18 16:09:18.098 | DEBUG    | __main__:<module>:313 - Training step 18530: loss = 2.9287 | 3034.07ms | Tokens/s = 172,800.0
2025-01-18 16:09:48.430 | DEBUG    | __main__:<module>:313 - Training step 18540: loss = 3.0478 | 3032.51ms | Tokens/s = 172,889.1
2025-01-18 16:10:18.727 | DEBUG    | __main__:<module>:313 - Training step 18550: loss = 2.9609 | 3027.76ms | Tokens/s = 173,160.3
2025-01-18 16:10:48.978 | DEBUG    | __main__:<module>:313 - Training step 18560: loss = 2.9253 | 3023.67ms | Tokens/s = 173,394.4
2025-01-18 16:11:19.210 | DEBUG    | __main__:<module>:313 - Training step 18570: loss = 2.9412 | 3023.27ms | Tokens/s = 173,417.8
2025-01-18 16:11:49.445 | DEBUG    | __main__:<module>:313 - Training step 18580: loss = 3.0716 | 3024.63ms | Tokens/s = 173,339.4
2025-01-18 16:12:19.719 | DEBUG    | __main__:<module>:313 - Training step 18590: loss = 2.9058 | 3031.06ms | Tokens/s = 172,971.8
2025-01-18 16:12:50.027 | DEBUG    | __main__:<module>:313 - Training step 18600: loss = 2.9677 | 3029.06ms | Tokens/s = 173,086.2
2025-01-18 16:13:20.326 | DEBUG    | __main__:<module>:313 - Training step 18610: loss = 2.9813 | 3029.13ms | Tokens/s = 173,082.0
2025-01-18 16:13:50.620 | DEBUG    | __main__:<module>:313 - Training step 18620: loss = 2.9598 | 3028.47ms | Tokens/s = 173,119.9
2025-01-18 16:14:20.909 | DEBUG    | __main__:<module>:313 - Training step 18630: loss = 2.8392 | 3028.20ms | Tokens/s = 173,135.1
2025-01-18 16:14:51.207 | DEBUG    | __main__:<module>:313 - Training step 18640: loss = 2.9051 | 3030.26ms | Tokens/s = 173,017.7
2025-01-18 16:15:21.512 | DEBUG    | __main__:<module>:313 - Training step 18650: loss = 2.9863 | 3029.48ms | Tokens/s = 173,062.0
2025-01-18 16:15:51.826 | DEBUG    | __main__:<module>:313 - Training step 18660: loss = 2.9359 | 3030.98ms | Tokens/s = 172,976.7
2025-01-18 16:16:22.142 | DEBUG    | __main__:<module>:313 - Training step 18670: loss = 2.9173 | 3032.17ms | Tokens/s = 172,908.7
2025-01-18 16:16:52.456 | DEBUG    | __main__:<module>:313 - Training step 18680: loss = 2.7405 | 3030.82ms | Tokens/s = 172,985.3
2025-01-18 16:17:22.772 | DEBUG    | __main__:<module>:313 - Training step 18690: loss = 3.0327 | 3034.25ms | Tokens/s = 172,790.3
2025-01-18 16:17:53.076 | DEBUG    | __main__:<module>:313 - Training step 18700: loss = 3.0091 | 3028.73ms | Tokens/s = 173,104.8
2025-01-18 16:18:23.378 | DEBUG    | __main__:<module>:313 - Training step 18710: loss = 2.9938 | 3031.36ms | Tokens/s = 172,954.8
2025-01-18 16:18:53.677 | DEBUG    | __main__:<module>:313 - Training step 18720: loss = 3.0613 | 3029.97ms | Tokens/s = 173,034.1
2025-01-18 16:19:23.966 | DEBUG    | __main__:<module>:313 - Training step 18730: loss = 2.8047 | 3028.01ms | Tokens/s = 173,145.9
2025-01-18 16:19:54.266 | DEBUG    | __main__:<module>:313 - Training step 18740: loss = 2.7383 | 3028.42ms | Tokens/s = 173,122.8
2025-01-18 16:20:24.578 | DEBUG    | __main__:<module>:313 - Training step 18750: loss = 3.0532 | 3033.76ms | Tokens/s = 172,817.7
2025-01-18 16:20:54.885 | DEBUG    | __main__:<module>:313 - Training step 18760: loss = 2.9817 | 3031.71ms | Tokens/s = 172,934.7
2025-01-18 16:21:25.196 | DEBUG    | __main__:<module>:313 - Training step 18770: loss = 3.0492 | 3032.26ms | Tokens/s = 172,903.5
2025-01-18 16:21:55.506 | DEBUG    | __main__:<module>:313 - Training step 18780: loss = 2.9643 | 3031.74ms | Tokens/s = 172,932.8
2025-01-18 16:22:25.822 | DEBUG    | __main__:<module>:313 - Training step 18790: loss = 2.9445 | 3032.13ms | Tokens/s = 172,910.6
2025-01-18 16:22:56.135 | DEBUG    | __main__:<module>:313 - Training step 18800: loss = 2.9868 | 3030.88ms | Tokens/s = 172,981.9
2025-01-18 16:23:26.438 | DEBUG    | __main__:<module>:313 - Training step 18810: loss = 3.0101 | 3030.05ms | Tokens/s = 173,029.4
2025-01-18 16:23:56.741 | DEBUG    | __main__:<module>:313 - Training step 18820: loss = 2.8923 | 3030.32ms | Tokens/s = 173,014.2
2025-01-18 16:24:27.041 | DEBUG    | __main__:<module>:313 - Training step 18830: loss = 3.0282 | 3029.11ms | Tokens/s = 173,083.2
2025-01-18 16:24:57.346 | DEBUG    | __main__:<module>:313 - Training step 18840: loss = 3.0324 | 3029.56ms | Tokens/s = 173,057.6
2025-01-18 16:25:27.647 | DEBUG    | __main__:<module>:313 - Training step 18850: loss = 2.8994 | 3029.05ms | Tokens/s = 173,086.9
2025-01-18 16:25:57.955 | DEBUG    | __main__:<module>:313 - Training step 18860: loss = 2.9731 | 3033.76ms | Tokens/s = 172,817.9
2025-01-18 16:26:28.266 | DEBUG    | __main__:<module>:313 - Training step 18870: loss = 2.9827 | 3030.85ms | Tokens/s = 172,983.8
2025-01-18 16:26:58.576 | DEBUG    | __main__:<module>:313 - Training step 18880: loss = 3.0016 | 3031.07ms | Tokens/s = 172,971.5
2025-01-18 16:27:28.881 | DEBUG    | __main__:<module>:313 - Training step 18890: loss = 2.8368 | 3030.07ms | Tokens/s = 173,028.1
2025-01-18 16:27:59.180 | DEBUG    | __main__:<module>:313 - Training step 18900: loss = 2.9834 | 3028.28ms | Tokens/s = 173,130.6
2025-01-18 16:28:29.486 | DEBUG    | __main__:<module>:313 - Training step 18910: loss = 3.0292 | 3030.73ms | Tokens/s = 172,990.8
2025-01-18 16:28:59.792 | DEBUG    | __main__:<module>:313 - Training step 18920: loss = 3.0635 | 3029.84ms | Tokens/s = 173,041.7
2025-01-18 16:29:30.100 | DEBUG    | __main__:<module>:313 - Training step 18930: loss = 2.9016 | 3031.63ms | Tokens/s = 172,939.1
2025-01-18 16:30:00.410 | DEBUG    | __main__:<module>:313 - Training step 18940: loss = 3.0585 | 3030.42ms | Tokens/s = 173,008.1
2025-01-18 16:30:30.725 | DEBUG    | __main__:<module>:313 - Training step 18950: loss = 2.9263 | 3032.35ms | Tokens/s = 172,898.2
2025-01-18 16:31:01.032 | DEBUG    | __main__:<module>:313 - Training step 18960: loss = 2.8069 | 3029.39ms | Tokens/s = 173,067.5
2025-01-18 16:31:31.337 | DEBUG    | __main__:<module>:313 - Training step 18970: loss = 3.0159 | 3030.35ms | Tokens/s = 173,012.5
2025-01-18 16:32:01.647 | DEBUG    | __main__:<module>:313 - Training step 18980: loss = 2.9169 | 3029.55ms | Tokens/s = 173,058.0
2025-01-18 16:32:31.957 | DEBUG    | __main__:<module>:313 - Training step 18990: loss = 3.1152 | 3030.01ms | Tokens/s = 173,031.9
2025-01-18 16:33:05.705 | INFO     | __main__:<module>:265 - Step 19,000/20,000 loss: 2.9752 (T) 2.9814 (V) | lr=7.6e-05
2025-01-18 16:33:05.706 | INFO     | __main__:<module>:287 - Saving model to /home/v-youransun/repgpt/model... 
2025-01-18 16:33:19.584 | DEBUG    | __main__:<module>:313 - Training step 19000: loss = 2.9249 | 20349.84ms | Tokens/s = 25,763.7
2025-01-18 16:33:49.740 | DEBUG    | __main__:<module>:313 - Training step 19010: loss = 3.1043 | 3020.96ms | Tokens/s = 173,550.0
2025-01-18 16:34:20.008 | DEBUG    | __main__:<module>:313 - Training step 19020: loss = 2.8377 | 3028.21ms | Tokens/s = 173,134.6
2025-01-18 16:34:50.289 | DEBUG    | __main__:<module>:313 - Training step 19030: loss = 2.9957 | 3029.98ms | Tokens/s = 173,033.5
2025-01-18 16:35:20.572 | DEBUG    | __main__:<module>:313 - Training step 19040: loss = 3.1616 | 3029.01ms | Tokens/s = 173,089.1
2025-01-18 16:35:50.847 | DEBUG    | __main__:<module>:313 - Training step 19050: loss = 2.8302 | 3026.97ms | Tokens/s = 173,205.4
2025-01-18 16:36:21.120 | DEBUG    | __main__:<module>:313 - Training step 19060: loss = 3.0563 | 3028.81ms | Tokens/s = 173,100.4
2025-01-18 16:36:51.409 | DEBUG    | __main__:<module>:313 - Training step 19070: loss = 2.8312 | 3030.99ms | Tokens/s = 172,975.6
2025-01-18 16:37:21.707 | DEBUG    | __main__:<module>:313 - Training step 19080: loss = 3.0317 | 3029.86ms | Tokens/s = 173,040.3
2025-01-18 16:37:52.009 | DEBUG    | __main__:<module>:313 - Training step 19090: loss = 3.0207 | 3027.41ms | Tokens/s = 173,180.6
2025-01-18 16:38:22.312 | DEBUG    | __main__:<module>:313 - Training step 19100: loss = 3.1852 | 3029.19ms | Tokens/s = 173,078.5
2025-01-18 16:38:52.623 | DEBUG    | __main__:<module>:313 - Training step 19110: loss = 3.0731 | 3031.04ms | Tokens/s = 172,972.7
2025-01-18 16:39:22.925 | DEBUG    | __main__:<module>:313 - Training step 19120: loss = 3.0402 | 3031.24ms | Tokens/s = 172,961.6
2025-01-18 16:39:53.227 | DEBUG    | __main__:<module>:313 - Training step 19130: loss = 2.9478 | 3031.79ms | Tokens/s = 172,930.0
2025-01-18 16:40:23.532 | DEBUG    | __main__:<module>:313 - Training step 19140: loss = 2.9413 | 3031.80ms | Tokens/s = 172,929.4
2025-01-18 16:40:53.847 | DEBUG    | __main__:<module>:313 - Training step 19150: loss = 2.9532 | 3030.43ms | Tokens/s = 173,008.0
2025-01-18 16:41:24.160 | DEBUG    | __main__:<module>:313 - Training step 19160: loss = 2.8789 | 3030.30ms | Tokens/s = 173,015.2
2025-01-18 16:41:54.474 | DEBUG    | __main__:<module>:313 - Training step 19170: loss = 2.8631 | 3030.77ms | Tokens/s = 172,988.4
2025-01-18 16:42:24.787 | DEBUG    | __main__:<module>:313 - Training step 19180: loss = 2.9882 | 3030.64ms | Tokens/s = 172,995.7
2025-01-18 16:42:55.098 | DEBUG    | __main__:<module>:313 - Training step 19190: loss = 2.8341 | 3031.77ms | Tokens/s = 172,931.3
2025-01-18 16:43:25.407 | DEBUG    | __main__:<module>:313 - Training step 19200: loss = 2.9649 | 3029.10ms | Tokens/s = 173,083.8
2025-01-18 16:43:55.715 | DEBUG    | __main__:<module>:313 - Training step 19210: loss = 2.9486 | 3030.14ms | Tokens/s = 173,024.4
2025-01-18 16:44:26.020 | DEBUG    | __main__:<module>:313 - Training step 19220: loss = 2.9884 | 3030.62ms | Tokens/s = 172,997.0
2025-01-18 16:44:56.320 | DEBUG    | __main__:<module>:313 - Training step 19230: loss = 2.8481 | 3030.05ms | Tokens/s = 173,029.6
2025-01-18 16:45:26.622 | DEBUG    | __main__:<module>:313 - Training step 19240: loss = 2.8873 | 3030.01ms | Tokens/s = 173,031.9
2025-01-18 16:45:56.935 | DEBUG    | __main__:<module>:313 - Training step 19250: loss = 3.0061 | 3031.57ms | Tokens/s = 172,942.9
2025-01-18 16:46:27.250 | DEBUG    | __main__:<module>:313 - Training step 19260: loss = 2.8008 | 3031.16ms | Tokens/s = 172,966.1
2025-01-18 16:46:57.563 | DEBUG    | __main__:<module>:313 - Training step 19270: loss = 3.0588 | 3031.72ms | Tokens/s = 172,934.3
2025-01-18 16:47:27.875 | DEBUG    | __main__:<module>:313 - Training step 19280: loss = 2.8964 | 3031.68ms | Tokens/s = 172,936.4
2025-01-18 16:47:58.187 | DEBUG    | __main__:<module>:313 - Training step 19290: loss = 3.0134 | 3031.25ms | Tokens/s = 172,960.8
2025-01-18 16:48:28.497 | DEBUG    | __main__:<module>:313 - Training step 19300: loss = 2.9266 | 3029.71ms | Tokens/s = 173,048.7
2025-01-18 16:48:58.812 | DEBUG    | __main__:<module>:313 - Training step 19310: loss = 2.8516 | 3033.09ms | Tokens/s = 172,855.8
2025-01-18 16:49:29.128 | DEBUG    | __main__:<module>:313 - Training step 19320: loss = 2.9190 | 3031.69ms | Tokens/s = 172,935.6
2025-01-18 16:49:59.446 | DEBUG    | __main__:<module>:313 - Training step 19330: loss = 3.0108 | 3030.32ms | Tokens/s = 173,014.3
2025-01-18 16:50:29.762 | DEBUG    | __main__:<module>:313 - Training step 19340: loss = 3.0508 | 3031.17ms | Tokens/s = 172,965.4
2025-01-18 16:51:00.079 | DEBUG    | __main__:<module>:313 - Training step 19350: loss = 2.8726 | 3031.36ms | Tokens/s = 172,954.7
2025-01-18 16:51:30.393 | DEBUG    | __main__:<module>:313 - Training step 19360: loss = 2.9975 | 3031.44ms | Tokens/s = 172,950.4
2025-01-18 16:52:00.715 | DEBUG    | __main__:<module>:313 - Training step 19370: loss = 2.9842 | 3029.70ms | Tokens/s = 173,049.6
2025-01-18 16:52:31.029 | DEBUG    | __main__:<module>:313 - Training step 19380: loss = 2.8406 | 3030.72ms | Tokens/s = 172,991.0
2025-01-18 16:53:01.352 | DEBUG    | __main__:<module>:313 - Training step 19390: loss = 2.8554 | 3032.07ms | Tokens/s = 172,914.4
2025-01-18 16:53:31.672 | DEBUG    | __main__:<module>:313 - Training step 19400: loss = 3.0426 | 3030.72ms | Tokens/s = 172,991.3
2025-01-18 16:54:01.988 | DEBUG    | __main__:<module>:313 - Training step 19410: loss = 2.9825 | 3030.86ms | Tokens/s = 172,983.5
2025-01-18 16:54:32.312 | DEBUG    | __main__:<module>:313 - Training step 19420: loss = 2.9412 | 3033.30ms | Tokens/s = 172,844.1
2025-01-18 16:55:02.633 | DEBUG    | __main__:<module>:313 - Training step 19430: loss = 3.1291 | 3030.82ms | Tokens/s = 172,985.4
2025-01-18 16:55:32.952 | DEBUG    | __main__:<module>:313 - Training step 19440: loss = 2.9328 | 3031.45ms | Tokens/s = 172,949.5
2025-01-18 16:56:03.265 | DEBUG    | __main__:<module>:313 - Training step 19450: loss = 3.0524 | 3030.89ms | Tokens/s = 172,981.5
2025-01-18 16:56:33.580 | DEBUG    | __main__:<module>:313 - Training step 19460: loss = 2.9604 | 3030.97ms | Tokens/s = 172,976.9
2025-01-18 16:57:03.898 | DEBUG    | __main__:<module>:313 - Training step 19470: loss = 2.9822 | 3031.60ms | Tokens/s = 172,941.0
2025-01-18 16:57:34.210 | DEBUG    | __main__:<module>:313 - Training step 19480: loss = 3.0386 | 3029.93ms | Tokens/s = 173,036.1
2025-01-18 16:58:04.527 | DEBUG    | __main__:<module>:313 - Training step 19490: loss = 2.7906 | 3031.12ms | Tokens/s = 172,968.3
2025-01-18 16:58:34.841 | DEBUG    | __main__:<module>:313 - Training step 19500: loss = 3.0192 | 3032.09ms | Tokens/s = 172,913.3
2025-01-18 16:59:05.150 | DEBUG    | __main__:<module>:313 - Training step 19510: loss = 2.9601 | 3030.32ms | Tokens/s = 173,014.1
2025-01-18 16:59:35.477 | DEBUG    | __main__:<module>:313 - Training step 19520: loss = 2.8998 | 3032.33ms | Tokens/s = 172,899.4
2025-01-18 17:00:05.801 | DEBUG    | __main__:<module>:313 - Training step 19530: loss = 3.0498 | 3032.94ms | Tokens/s = 172,864.5
2025-01-18 17:00:36.127 | DEBUG    | __main__:<module>:313 - Training step 19540: loss = 3.0613 | 3032.87ms | Tokens/s = 172,868.7
2025-01-18 17:01:06.453 | DEBUG    | __main__:<module>:313 - Training step 19550: loss = 2.9129 | 3032.45ms | Tokens/s = 172,892.8
2025-01-18 17:01:36.774 | DEBUG    | __main__:<module>:313 - Training step 19560: loss = 2.7988 | 3031.26ms | Tokens/s = 172,960.2
2025-01-18 17:02:07.095 | DEBUG    | __main__:<module>:313 - Training step 19570: loss = 2.8983 | 3032.55ms | Tokens/s = 172,886.6
2025-01-18 17:02:37.409 | DEBUG    | __main__:<module>:313 - Training step 19580: loss = 2.9255 | 3029.75ms | Tokens/s = 173,046.5
2025-01-18 17:03:07.720 | DEBUG    | __main__:<module>:313 - Training step 19590: loss = 2.8749 | 3029.60ms | Tokens/s = 173,055.0
2025-01-18 17:03:38.025 | DEBUG    | __main__:<module>:313 - Training step 19600: loss = 3.0021 | 3028.26ms | Tokens/s = 173,131.8
2025-01-18 17:04:08.335 | DEBUG    | __main__:<module>:313 - Training step 19610: loss = 3.0068 | 3032.41ms | Tokens/s = 172,894.9
2025-01-18 17:04:38.643 | DEBUG    | __main__:<module>:313 - Training step 19620: loss = 3.0035 | 3031.10ms | Tokens/s = 172,969.3
2025-01-18 17:05:08.947 | DEBUG    | __main__:<module>:313 - Training step 19630: loss = 3.0155 | 3031.67ms | Tokens/s = 172,936.8
2025-01-18 17:05:39.262 | DEBUG    | __main__:<module>:313 - Training step 19640: loss = 2.9541 | 3031.14ms | Tokens/s = 172,967.3
2025-01-18 17:06:09.573 | DEBUG    | __main__:<module>:313 - Training step 19650: loss = 2.9693 | 3029.93ms | Tokens/s = 173,036.2
2025-01-18 17:06:39.879 | DEBUG    | __main__:<module>:313 - Training step 19660: loss = 2.9522 | 3029.15ms | Tokens/s = 173,080.9
2025-01-18 17:07:10.195 | DEBUG    | __main__:<module>:313 - Training step 19670: loss = 2.9661 | 3031.36ms | Tokens/s = 172,954.6
2025-01-18 17:07:40.514 | DEBUG    | __main__:<module>:313 - Training step 19680: loss = 2.7870 | 3030.88ms | Tokens/s = 172,982.1
2025-01-18 17:08:10.842 | DEBUG    | __main__:<module>:313 - Training step 19690: loss = 2.9078 | 3032.89ms | Tokens/s = 172,867.7
2025-01-18 17:08:41.169 | DEBUG    | __main__:<module>:313 - Training step 19700: loss = 3.0984 | 3029.56ms | Tokens/s = 173,057.5
2025-01-18 17:09:11.490 | DEBUG    | __main__:<module>:313 - Training step 19710: loss = 2.8698 | 3032.36ms | Tokens/s = 172,897.9
2025-01-18 17:09:41.805 | DEBUG    | __main__:<module>:313 - Training step 19720: loss = 3.0406 | 3031.25ms | Tokens/s = 172,961.2
2025-01-18 17:10:12.114 | DEBUG    | __main__:<module>:313 - Training step 19730: loss = 2.7942 | 3030.81ms | Tokens/s = 172,986.1
2025-01-18 17:10:42.422 | DEBUG    | __main__:<module>:313 - Training step 19740: loss = 2.9565 | 3029.85ms | Tokens/s = 173,040.8
2025-01-18 17:11:12.734 | DEBUG    | __main__:<module>:313 - Training step 19750: loss = 3.0681 | 3031.48ms | Tokens/s = 172,947.8
2025-01-18 17:11:43.037 | DEBUG    | __main__:<module>:313 - Training step 19760: loss = 2.9658 | 3031.27ms | Tokens/s = 172,960.1
2025-01-18 17:12:13.353 | DEBUG    | __main__:<module>:313 - Training step 19770: loss = 3.0203 | 3031.04ms | Tokens/s = 172,972.7
2025-01-18 17:12:43.670 | DEBUG    | __main__:<module>:313 - Training step 19780: loss = 2.8262 | 3029.16ms | Tokens/s = 173,080.3
2025-01-18 17:13:13.980 | DEBUG    | __main__:<module>:313 - Training step 19790: loss = 2.9264 | 3032.22ms | Tokens/s = 172,905.8
2025-01-18 17:13:44.291 | DEBUG    | __main__:<module>:313 - Training step 19800: loss = 3.0456 | 3030.23ms | Tokens/s = 173,018.9
2025-01-18 17:14:14.602 | DEBUG    | __main__:<module>:313 - Training step 19810: loss = 3.1051 | 3032.35ms | Tokens/s = 172,898.3
2025-01-18 17:14:44.910 | DEBUG    | __main__:<module>:313 - Training step 19820: loss = 2.9197 | 3031.18ms | Tokens/s = 172,964.9
2025-01-18 17:15:15.230 | DEBUG    | __main__:<module>:313 - Training step 19830: loss = 2.9888 | 3033.32ms | Tokens/s = 172,843.0
2025-01-18 17:15:45.551 | DEBUG    | __main__:<module>:313 - Training step 19840: loss = 2.9191 | 3031.83ms | Tokens/s = 172,928.0
2025-01-18 17:16:15.870 | DEBUG    | __main__:<module>:313 - Training step 19850: loss = 2.9813 | 3032.50ms | Tokens/s = 172,889.6
2025-01-18 17:16:46.194 | DEBUG    | __main__:<module>:313 - Training step 19860: loss = 3.0052 | 3033.05ms | Tokens/s = 172,858.5
2025-01-18 17:17:16.519 | DEBUG    | __main__:<module>:313 - Training step 19870: loss = 2.8742 | 3032.27ms | Tokens/s = 172,902.5
2025-01-18 17:17:46.837 | DEBUG    | __main__:<module>:313 - Training step 19880: loss = 2.9784 | 3031.61ms | Tokens/s = 172,940.5
2025-01-18 17:18:17.153 | DEBUG    | __main__:<module>:313 - Training step 19890: loss = 2.8854 | 3029.38ms | Tokens/s = 173,067.7
2025-01-18 17:18:47.460 | DEBUG    | __main__:<module>:313 - Training step 19900: loss = 2.9022 | 3031.21ms | Tokens/s = 172,963.3
2025-01-18 17:19:17.796 | DEBUG    | __main__:<module>:313 - Training step 19910: loss = 3.0152 | 3034.25ms | Tokens/s = 172,789.9
2025-01-18 17:19:48.127 | DEBUG    | __main__:<module>:313 - Training step 19920: loss = 3.0023 | 3033.13ms | Tokens/s = 172,853.7
2025-01-18 17:20:18.451 | DEBUG    | __main__:<module>:313 - Training step 19930: loss = 3.0294 | 3035.43ms | Tokens/s = 172,723.0
2025-01-18 17:20:48.773 | DEBUG    | __main__:<module>:313 - Training step 19940: loss = 2.7651 | 3033.90ms | Tokens/s = 172,810.2
2025-01-18 17:21:19.104 | DEBUG    | __main__:<module>:313 - Training step 19950: loss = 3.0592 | 3033.45ms | Tokens/s = 172,835.6
2025-01-18 17:21:49.434 | DEBUG    | __main__:<module>:313 - Training step 19960: loss = 3.0792 | 3032.35ms | Tokens/s = 172,898.1
2025-01-18 17:22:19.752 | DEBUG    | __main__:<module>:313 - Training step 19970: loss = 2.9100 | 3030.81ms | Tokens/s = 172,985.8
2025-01-18 17:22:50.080 | DEBUG    | __main__:<module>:313 - Training step 19980: loss = 2.8989 | 3032.17ms | Tokens/s = 172,908.6
2025-01-18 17:23:20.409 | DEBUG    | __main__:<module>:313 - Training step 19990: loss = 2.9066 | 3033.47ms | Tokens/s = 172,834.4
2025-01-18 17:23:47.597 | INFO     | __main__:<module>:319 - job_name='gpt2-training-124M-2025-01-18-00-28-44' finished in 60897.46s
2025-01-18 17:23:47.597 | INFO     | __main__:<module>:320 - Trained for 20,000 steps total_training_tokens=10,485,760,000 and achieved  best eval loss=2.9814019203186035
2025-01-18 17:23:54.177 | INFO     | __main__:<module>:345 - Loss: 2.9604 (T) 2.9706 (V) | 60907.334010601044s
