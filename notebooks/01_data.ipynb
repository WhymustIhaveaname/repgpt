{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "The purpose of this notebook is to provide an understanding of the data we are feeding into the model. The complete preprocessing code is contained in src/repgpt/process.py.\n",
    "\n",
    "### 1.1 Accessing the Data\n",
    "\n",
    "The original dataset (Web Text) from GPT2 is not public. However, there is an approximately similar data source (Open Web Text) created by (created by [Aaron Gokaslan](https://twitter.com/SkyLi0n)).\n",
    "\n",
    "We can download the [openwebtext dataset](https://huggingface.co/datasets/Skylion007/openwebtext)  from HuggingFace's datasets library. Depending on your internet connection, it may take several minutes to download (on my machine, it takes ~8 minutes).\n",
    "\n",
    "After the initial download, the data will be stored in local cache:\n",
    "\n",
    "`~/.cache/huggingface/datasets/downloads/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davjosia/opt/miniconda3/envs/how-to-reproduce-gpt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"openwebtext\", num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the data contains 8M \"rows\" which in this case refers to documents extracted from reddit URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a hold-out set for evaluation using the built-in huggingface functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "split_dataset[\"val\"] = split_dataset.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8009762\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example row from the dataset (first 1k characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is NOT an April Fools joke!\n",
      "\n",
      "Quite frankly I’m shocked that I’ve made it to day 60 of this project and only now selected a Dark Star. That seems almost criminal.\n",
      "\n",
      "For many Dark Star is the quintessential Grateful Dead song. Well, song may be a somewhat liberal term here. To say it’s an anything-goes jam held together by a few key riffs might be more accurate, but regardless of the label you use to describe it one this is clear: Dark Star is awesome.\n",
      "\n",
      "The only real issue I have with this recording is that Keith is virtually absent. I assume he was there that night, but I listened to this on good headphones and could detect nary a hint of piano, keys, anything. This is especially disappointing because we’ve already seen how well Keith integrated with the band (see yesterday’s Bertha post).\n",
      "\n",
      "This one starts off in a very laid back manner. No noodling here, but the band is certainly not in a rush. Right around the 2 minute mark Jerry seems to be running through scales up and down the \n"
     ]
    }
   ],
   "source": [
    "print(split_dataset['train'][11]['text'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use openai's GPT2 tokenizer (n_vocab=50257) to produce numeric encodings of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50,257 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11929, 281, 3035, 376, 10141, 9707, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"There are {enc.n_vocab:,} tokens in the vocabulary\")\n",
    "\n",
    "enc.encode_ordinary(\"NOT an April Fools joke!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT an April Fools joke!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(enc.encode_ordinary(\"NOT an April Fools joke!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually inspect what some of the tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Index | Token           |\n",
      "---------------------------\n",
      "|     0 | !               |\n",
      "|     1 | \"               |\n",
      "|     2 | #               |\n",
      "|     3 | $               |\n",
      "|   100 | �               |\n",
      "|  1000 | ale             |\n",
      "|  1001 |  Se             |\n",
      "|  1003 | //              |\n",
      "|  1004 |  Le             |\n",
      "|  2000 |  mind           |\n",
      "|  2001 | aff             |\n",
      "| 10000 |  pocket         |\n",
      "| 20000 |  Junior         |\n",
      "| 50254 |  informants     |\n",
      "| 50255 |  gazed          |\n",
      "| 50256 | <|endoftext|>   |\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'| Index':<5} | {'Token': <15} |\")\n",
    "print(\"-\"*27)\n",
    "for i in [0, 1, 2, 3, 100, 1_000, 1_001, 1_003, 1_004, 2_000, 2_001, 10_000, 20_000, enc.n_vocab - 3, enc.n_vocab - 2, enc.n_vocab - 1]:\n",
    "    print(f\"| {i:>5} | {enc.decode([i]):<15} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Structuring data for training\n",
    "\n",
    "The first step is to create a giant array, with each document separated by an `<|endoftext|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8585,   262,  1772,    25, 25334,  8120,    17, 43959, 44947,   318])\n",
      "5627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def process_document(example, enc) -> dict:\n",
    "    ids = enc.encode_ordinary(example[\"text\"])\n",
    "    ids.append(enc.eot_token)\n",
    "    return ids\n",
    "\n",
    "train_tokens = []\n",
    "for i in range(3):\n",
    "    train_tokens += process_document(split_dataset['train'][i], enc)\n",
    "\n",
    "train_tokens = torch.tensor(train_tokens)\n",
    "\n",
    "print(train_tokens[:10])\n",
    "print(len(train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sample positions from this data, and create our X and y variables, each with `(batch_size, context_size)` dimension, with the elements from y being shifted by one token to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[ 257, 4048, 8066, 1560],\n",
      "        [ 284, 2652, 1200, 1203]])\n",
      "y=tensor([[4048, 8066, 1560,  345],\n",
      "        [2652, 1200, 1203,  290]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, context_size):\n",
    "    indices = torch.randint(low=0, high=data.shape[0] - context_size, size=(batch_size,))\n",
    "    X = torch.stack([data[idx:idx+context_size]for idx in indices])\n",
    "    y = torch.stack([data[idx+1:idx+context_size+1]for idx in indices])\n",
    "    return X, y\n",
    "\n",
    "X, y = get_batch(train_tokens, batch_size=2, context_size=4)\n",
    "print(f\"{X=}\")\n",
    "print(f\"{y=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You will observe the shift in this data. e.g., `X[0,2] == y[0, 1]`\n",
    "* To simplify training, we preprocess all of our data into tokens and save to disk before even starting the training process. \n",
    "* The script preprocessing is here: `src/repgpt/preprocess.py`.\n",
    "* Preview of what's to come: later on, we will perform an embedding lookup, which will be the third dimension in the data: `(batch_size, context_size, n_embd)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
