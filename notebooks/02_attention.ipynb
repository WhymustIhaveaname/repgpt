{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention\n",
    "\n",
    "The purpose is towfold: (1) gain understanding of the attention mechanism, and (2) ensure that our implementation is correct by cross-checking with a built-in attention function.\n",
    "\n",
    "* Taking an average\n",
    "* Vectorizing our weighted average\n",
    "* Learning weights for the wectorized weighted average\n",
    "* Cross checking with a built-in attention function\n",
    "* Expanding to mult-headed attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Taking an Average\n",
    "\n",
    "A naive method for predicting the next word would be to take an average of the word features that come before it. Let's look at the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (batch_size=2, context_size=3, n_embd=4):\n",
      " tensor([[[5., 9., 1., 3.],\n",
      "         [1., 8., 5., 2.],\n",
      "         [5., 8., 5., 9.]],\n",
      "\n",
      "        [[5., 9., 2., 5.],\n",
      "         [4., 5., 8., 6.],\n",
      "         [1., 7., 4., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(103)\n",
    "\n",
    "batch_size = 2\n",
    "context_size = 3\n",
    "n_embd = 4\n",
    "\n",
    "x = torch.randint(high=10, size=(batch_size, context_size, n_embd), dtype=torch.float32)\n",
    "print(f\"Input ({batch_size=}, {context_size=}, {n_embd=}):\\n {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to therefore generate predictions for each position, so we are effectively getting multiple batches of examples from each sequence (as well as multiple independent sequences).\n",
    "\n",
    "As a reminder: when you slice you don't get the value correspnding to the right integer e.g. `[:4]` it takes all elements up to but not including `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (batch_size=2, context_size=3):\n",
      " tensor([[[5.0000, 9.0000, 1.0000, 3.0000],\n",
      "         [3.0000, 8.5000, 3.0000, 2.5000],\n",
      "         [3.6667, 8.3333, 3.6667, 4.6667]],\n",
      "\n",
      "        [[5.0000, 9.0000, 2.0000, 5.0000],\n",
      "         [4.5000, 7.0000, 5.0000, 5.5000],\n",
      "         [3.3333, 7.0000, 4.6667, 5.0000]]])\n"
     ]
    }
   ],
   "source": [
    "y_output = torch.zeros((batch_size, context_size, n_embd))\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_size):\n",
    "        x_prev = x[b, : t + 1, :]\n",
    "        y_output[b, t, :] = x_prev.mean(dim=0)\n",
    "print(f\"Output ({batch_size=}, {context_size=}):\\n {y_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vectorizing our moving average\n",
    "\n",
    "We can remove the for loop and vectorize the previous operation by the following technique: creating a square matrix of dimension `context_size`, then multiplying that matrix by our input matrix. \n",
    "\n",
    "And this is actually numerically equivalent to what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(context_size, context_size))\n",
    "triangle_masked_weights = mask / mask.sum(dim=1, keepdim=True)\n",
    "triangle_masked_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have is a square matrix: `(context_size, context_size)` multiplied by a `(context_size, n_embd)` matrix. \n",
    "\n",
    "And torch is going to broadcast this matrix operation across the batch dimension to give us ultimately what we want.\n",
    "\n",
    "Which is a `(batch_size, context_size, n_embd)` matrix that is equivalent to what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My outputs are equivalent: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[5.0000, 9.0000, 1.0000, 3.0000],\n",
       "         [3.0000, 8.5000, 3.0000, 2.5000],\n",
       "         [3.6667, 8.3333, 3.6667, 4.6667]],\n",
       "\n",
       "        [[5.0000, 9.0000, 2.0000, 5.0000],\n",
       "         [4.5000, 7.0000, 5.0000, 5.5000],\n",
       "         [3.3333, 7.0000, 4.6667, 5.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output_2 = triangle_masked_weights @ x\n",
    "print(f\"My outputs are equivalent: {torch.allclose(y_output, y_output_2)}\")\n",
    "y_output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work? \n",
    "* The rows in the triangle matrix are being multiplied by the columns (which corresponds to the context/sequence length) in the input matrix. \n",
    "* The first row in the triangle matix zero's out the all but the very first element of the input matrix columns. Likewise, the second row in the triangle matrix zeros out all by the very first two elements of the input matrix columns. And so on and so forth.\n",
    "\n",
    "What is happening here? \n",
    "* Well, this is a moving average with uniformly weighted across all prior positions of the input. \n",
    "* The first row has a 1, the second row has a 1/2, the third row has a 1/3, etc...\n",
    "* But what if we wanted to take a weighted moving average, and ulimately learn the weights to use in the average? \n",
    "* This is what the attention mechanism does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Self-Attention Mechanism\n",
    "\n",
    "We are going to learn the weights to use by comparing the query and the key. Here we are using the terminology from the world of databases and hash tables. Your query is like your request. The key is the index of the data to be returned. And the values are what you ultimately are caring about. It doesn't map 1:1 in the world of LLMs, but this is the terminology used.\n",
    "* Change 1: we are learning weights associated with projections of the input sequence.\n",
    "* Change 2: we are creating a weight matrix that corresponds to the similarity of the query and the key. (Previously this was a uniform)\n",
    "* Change 3: we are replacing the zeros with -infinity, because since e^-inf = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (batch_size=2, context_size=3, n_embd=4):\n",
      "tensor([[[ 3.5171,  1.2840, -3.4459, -4.4998],\n",
      "         [ 3.1850,  1.2937,  0.2993, -3.4091],\n",
      "         [ 0.3842,  4.1011,  0.9978, -1.5259]],\n",
      "\n",
      "        [[ 2.6459,  2.1178, -2.2431, -3.6423],\n",
      "         [ 0.0575,  3.9665,  2.8881, -1.3670],\n",
      "         [ 1.8560,  1.7649,  0.8492, -1.9648]]], grad_fn=<ViewBackward0>)\n",
      "Weights (context_size=3,context_size=3) :\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.6018, 0.3982, 0.0000],\n",
      "         [0.3446, 0.2355, 0.4198]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.0133, 0.9867, 0.0000],\n",
      "         [0.6703, 0.0380, 0.2918]]], grad_fn=<SoftmaxBackward0>)\n",
      "Output (batch_size=2, context_size=3, n_embd=4):\n",
      "tensor([[[ 3.5171,  1.2840, -3.4459, -4.4998],\n",
      "         [ 3.3848,  1.2878, -1.9546, -4.0655],\n",
      "         [ 2.1236,  2.4689, -0.6982, -2.9944]],\n",
      "\n",
      "        [[ 2.6459,  2.1178, -2.2431, -3.6423],\n",
      "         [ 0.0920,  3.9419,  2.8196, -1.3973],\n",
      "         [ 2.3172,  2.0850, -1.1461, -3.0664]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(243)\n",
    "\n",
    "key_layer = nn.Linear(n_embd, n_embd)\n",
    "query_layer = nn.Linear(n_embd, n_embd)\n",
    "value_layer = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "q = key_layer(x)\n",
    "k = query_layer(x)\n",
    "v = value_layer(x)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) * 1 / math.sqrt(k.shape[-1])\n",
    "weights = weights.masked_fill(torch.tril(torch.ones(context_size, context_size)) == 0, float(\"-inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "output = weights @ v\n",
    "print(f\"Input ({batch_size=}, {context_size=}, {n_embd=}):\\n{v}\")\n",
    "print(f\"Weights ({context_size=},{context_size=}) :\\n{weights}\")\n",
    "print(f\"Output ({batch_size=}, {context_size=}, {n_embd=}):\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Change 4: We are applying a scaling factor of 1 / sqrt(d_k). This allows the softmax output to be more spread out when the values are large (the attention logit can become very large). \n",
    "* Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no scaling: tensor([4.5336e-05, 1.2324e-04, 3.3499e-04, 9.1059e-04, 9.9859e-01, 0.0000e+00])\n",
      "with scaling: tensor([0.0548, 0.0704, 0.0904, 0.1161, 0.6682, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"no scaling:\", F.softmax(torch.tensor([0, 1, 2, 3, 10, -1e4]).float(), dim=0))\n",
    "print(\"with scaling:\", F.softmax(torch.tensor([0, 1, 2, 3, 10, -1e4]).float() / n_embd, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Checking our work\n",
    "\n",
    "Let's to a cross-check using the pytorch attention function to make sure we're doing this correctly. This is a function, so it expects you to pass in the key, query, value data to it.\n",
    "\n",
    "And we can see that we're getting the exact same results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (batch_size=2, context_size=3, n_embd=4):\n",
      "tensor([[[ 3.5171,  1.2840, -3.4459, -4.4998],\n",
      "         [ 3.3848,  1.2878, -1.9546, -4.0655],\n",
      "         [ 2.1236,  2.4689, -0.6982, -2.9944]],\n",
      "\n",
      "        [[ 2.6459,  2.1178, -2.2431, -3.6423],\n",
      "         [ 0.0920,  3.9419,  2.8196, -1.3973],\n",
      "         [ 2.3172,  2.0850, -1.1461, -3.0664]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Your implementation is correct: True\n"
     ]
    }
   ],
   "source": [
    "expected_output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "print(f\"Output ({batch_size=}, {context_size=}, {n_embd=}):\\n{expected_output}\")\n",
    "print(f\"Your implementation is correct: {torch.allclose(output, expected_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multi-headed Attention\n",
    "\n",
    "Now we can expand this to the setting of a multi-headed attention. The idea here is to have a bunch of these attention heads processing completely independently. We could create these independently, and then loop through them, and then concatenate them together. However, there's a slightly more efficient way of doing this. Let's make our input data slightly bigger to illustrate and use an example of 4 heads.\n",
    "\n",
    "* It's not obvious until you draw it out, but we create our keys/queries/values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (batch_size=2, context_size=3, n_embd=12):\n",
      " tensor([[[5., 9., 1., 3., 1., 8., 5., 2., 5., 8., 5., 9.],\n",
      "         [5., 9., 2., 5., 4., 5., 8., 6., 1., 7., 4., 4.],\n",
      "         [6., 9., 5., 3., 6., 8., 9., 0., 5., 2., 0., 6.]],\n",
      "\n",
      "        [[7., 5., 5., 3., 6., 5., 3., 7., 4., 9., 0., 5.],\n",
      "         [3., 9., 4., 7., 0., 3., 7., 1., 5., 5., 2., 6.],\n",
      "         [2., 7., 5., 1., 0., 6., 2., 7., 4., 7., 3., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(103)\n",
    "\n",
    "batch_size = 2\n",
    "context_size = 3\n",
    "n_embd = 12\n",
    "n_heads = 4\n",
    "\n",
    "x = torch.randint(high=10, size=(batch_size, context_size, n_embd), dtype=torch.float32)\n",
    "print(f\"Input ({batch_size=}, {context_size=}, {n_embd=}):\\n {x}\")\n",
    "\n",
    "key_layer = nn.Linear(n_embd, n_embd)\n",
    "query_layer = nn.Linear(n_embd, n_embd)\n",
    "value_layer = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "q = key_layer(x)\n",
    "k = query_layer(x)\n",
    "v = value_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can convert the head into a batch dimension, so now we have two independent batch dimensions.\n",
    "\n",
    "The first corresponding to the batch of sequences, the second corresponding to the attention head.\n",
    "\n",
    "So our final shape is going to be `(batch_size, n_heads, context_size, n_embd // n_heads)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.view(batch_size, context_size, n_heads, n_embd // n_heads).transpose(1, 2)\n",
    "k = k.view(batch_size, context_size, n_heads, n_embd // n_heads).transpose(1, 2)\n",
    "v = v.view(batch_size, context_size, n_heads, n_embd // n_heads).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do the exact same thing as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (torch.Size([2, 4, 3, 3])):\n",
      "tensor([[[[ 8.7784e+00, -4.8753e+00,  1.1932e+00],\n",
      "          [ 5.0240e+00, -2.5953e-01, -1.5118e+00],\n",
      "          [ 5.3240e+00, -1.2399e+00, -1.7522e+00]],\n",
      "\n",
      "         [[-3.5147e-01, -1.9413e+00, -2.5365e+00],\n",
      "          [-2.6749e-01, -1.2959e+00,  3.5526e-01],\n",
      "          [-2.1949e-01, -2.6145e+00, -2.5183e+00]],\n",
      "\n",
      "         [[ 5.9869e-01, -3.0716e+00,  4.2714e-01],\n",
      "          [-2.2677e-01, -4.4387e+00,  7.7444e-01],\n",
      "          [-1.5823e+00, -2.8861e+00,  7.8539e-01]],\n",
      "\n",
      "         [[-5.0772e-01,  5.2677e+00, -2.1102e+00],\n",
      "          [ 2.1462e-03,  3.1952e+00, -1.2774e+00],\n",
      "          [-1.9456e-02,  3.5555e+00, -1.9040e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 4.9868e+00,  9.7068e-01, -2.1595e+00],\n",
      "          [ 8.2696e+00, -3.3180e+00,  4.1589e-01],\n",
      "          [ 6.1282e+00, -2.8202e+00, -6.9385e-01]],\n",
      "\n",
      "         [[ 2.5676e+00,  1.4350e+00,  6.6007e-03],\n",
      "          [-6.6369e-01, -4.2443e-01, -1.8966e+00],\n",
      "          [ 1.8987e+00,  2.0759e+00,  5.6721e-01]],\n",
      "\n",
      "         [[ 9.5145e-01, -3.8799e+00, -4.8346e-02],\n",
      "          [-6.1339e-01, -4.3079e+00,  1.9957e+00],\n",
      "          [ 6.5115e-01, -2.7964e+00, -7.8118e-01]],\n",
      "\n",
      "         [[ 2.0551e-02,  2.5232e+00, -3.6808e+00],\n",
      "          [-2.6770e+00,  5.0224e+00, -1.9393e+00],\n",
      "          [-8.5821e-01,  1.9502e+00, -3.3283e+00]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Weights (torch.Size([2, 4, 3, 3])) :\n",
      "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3922e-02, 9.6608e-01, 0.0000e+00],\n",
      "          [9.1938e-04, 9.9905e-01, 3.1413e-05]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.0127e-01, 4.9873e-01, 0.0000e+00],\n",
      "          [6.0892e-01, 3.0459e-01, 8.6483e-02]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.6776e-02, 9.7322e-01, 0.0000e+00],\n",
      "          [1.6803e-07, 2.0890e-01, 7.9110e-01]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [8.6664e-04, 9.9913e-01, 0.0000e+00],\n",
      "          [2.4198e-03, 9.8725e-01, 1.0333e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.3332e-01, 7.6668e-01, 0.0000e+00],\n",
      "          [8.0253e-02, 2.6809e-01, 6.5166e-01]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9524e-01, 4.7646e-03, 0.0000e+00],\n",
      "          [9.7849e-01, 1.9539e-02, 1.9691e-03]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.2130e-01, 2.7870e-01, 0.0000e+00],\n",
      "          [7.3458e-01, 2.0714e-01, 5.8280e-02]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [8.8353e-01, 1.1647e-01, 0.0000e+00],\n",
      "          [7.8683e-03, 1.2009e-01, 8.7205e-01]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Output (output.shape=torch.Size([2, 4, 3, 3])):\n",
      "tensor([[[[ 8.7784e+00, -4.8753e+00,  1.1932e+00],\n",
      "          [ 5.1513e+00, -4.1610e-01, -1.4201e+00],\n",
      "          [ 5.0275e+00, -2.6380e-01, -1.5094e+00]],\n",
      "\n",
      "         [[-3.5147e-01, -1.9413e+00, -2.5365e+00],\n",
      "          [-3.0959e-01, -1.6194e+00, -1.0943e+00],\n",
      "          [-3.1448e-01, -1.8029e+00, -1.6541e+00]],\n",
      "\n",
      "         [[ 5.9869e-01, -3.0716e+00,  4.2714e-01],\n",
      "          [-2.0467e-01, -4.4021e+00,  7.6514e-01],\n",
      "          [-1.2991e+00, -3.2104e+00,  7.8310e-01]],\n",
      "\n",
      "         [[-5.0772e-01,  5.2677e+00, -2.1102e+00],\n",
      "          [ 1.7044e-03,  3.1970e+00, -1.2781e+00],\n",
      "          [ 6.8927e-04,  3.2039e+00, -1.2858e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 4.9868e+00,  9.7068e-01, -2.1595e+00],\n",
      "          [ 7.5037e+00, -2.3174e+00, -1.8502e-01],\n",
      "          [ 6.6107e+00, -2.6495e+00, -5.1397e-01]],\n",
      "\n",
      "         [[ 2.5676e+00,  1.4350e+00,  6.6007e-03],\n",
      "          [ 2.5522e+00,  1.4261e+00, -2.4673e-03],\n",
      "          [ 2.5032e+00,  1.3999e+00, -2.9482e-02]],\n",
      "\n",
      "         [[ 9.5145e-01, -3.8799e+00, -4.8346e-02],\n",
      "          [ 5.1534e-01, -3.9992e+00,  5.2132e-01],\n",
      "          [ 6.0982e-01, -3.9054e+00,  3.3234e-01]],\n",
      "\n",
      "         [[ 2.0551e-02,  2.5232e+00, -3.6808e+00],\n",
      "          [-2.9361e-01,  2.8142e+00, -3.4780e+00],\n",
      "          [-1.0697e+00,  2.3236e+00, -3.1643e+00]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(243)\n",
    "\n",
    "key_layer = nn.Linear(n_embd, n_embd)\n",
    "query_layer = nn.Linear(n_embd, n_embd)\n",
    "value_layer = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) * 1 / math.sqrt(k.shape[-1])\n",
    "weights = weights.masked_fill(torch.tril(torch.ones(context_size, context_size)) == 0, float(\"-inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "output = weights @ v\n",
    "print(f\"Input ({v.shape}):\\n{v}\")\n",
    "print(f\"Weights ({weights.shape}) :\\n{weights}\")\n",
    "print(f\"Output ({output.shape=}):\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we can perform our sanity check to ensure we did it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (batch_size=2, n_heads=4, context_size=3, n_embd=12): torch.Size([2, 4, 3, 3])\n",
      "Your implementation is correct: True\n"
     ]
    }
   ],
   "source": [
    "expected_output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "print(f\"Output ({batch_size=}, {n_heads=}, {context_size=}, {n_embd=}): {expected_output.shape}\")\n",
    "print(f\"Your implementation is correct: {torch.allclose(output, expected_output)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
