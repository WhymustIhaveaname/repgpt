{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer in a deep learning context refers to the combination of mult-headed attention, combined with a couple of additional elements: layer normalization, feedforward network, and residual skip connections. Let's take each of these one-by-one:\n",
    "\n",
    "### 1. Layer Normalization\n",
    "\n",
    "> Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine ([torch docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)).\n",
    "\n",
    "When you think of a single token's representation, it can get spikey. In an effort to smooth them out, we can use layer normalization. What does this mean? \n",
    "\n",
    "You subtract the mean and divide by the standard deviation. And you learn parameters, one or two for each embedding dimension. One if you disable the bias, two if you enable it.\n",
    "\n",
    "$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "Here's what it looks like in PyTorch and vanilla matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 0., 1., 3.],\n",
      "         [2., 2., 0., 2.],\n",
      "         [1., 0., 3., 3.]],\n",
      "\n",
      "        [[2., 2., 1., 1.],\n",
      "         [1., 2., 3., 2.],\n",
      "         [0., 0., 1., 0.]]])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9622, -1.3471, -0.5773,  0.9622],\n",
       "         [ 0.5773,  0.5773, -1.7320,  0.5773],\n",
       "         [-0.5773, -1.3471,  0.9622,  0.9622]],\n",
       "\n",
       "        [[ 1.0000,  1.0000, -1.0000, -1.0000],\n",
       "         [-1.4142,  0.0000,  1.4142,  0.0000],\n",
       "         [-0.5773, -0.5773,  1.7320, -0.5773]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 2\n",
    "context_size = 3\n",
    "n_embd = 4\n",
    "bias=False\n",
    "x = torch.randint(high=4, size=(batch_size, context_size, n_embd), dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "layer_norm = nn.LayerNorm(n_embd, bias=bias)\n",
    "print(layer_norm.weight)\n",
    "\n",
    "output = layer_norm(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9622, -1.3471, -0.5773,  0.9622],\n",
       "         [ 0.5773,  0.5773, -1.7320,  0.5773],\n",
       "         [-0.5773, -1.3471,  0.9622,  0.9622]],\n",
       "\n",
       "        [[ 1.0000,  1.0000, -1.0000, -1.0000],\n",
       "         [-1.4142,  0.0000,  1.4142,  0.0000],\n",
       "         [-0.5773, -0.5773,  1.7320, -0.5773]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "output2 = (x - torch.mean(x, dim=-1, keepdim=True)) / (torch.std(x, dim=-1, keepdim=True, unbiased=False) + eps)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = torch.ones(n_embd) * 2\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9245, -2.6943, -1.1547,  1.9245],\n",
       "         [ 1.1547,  1.1547, -3.4641,  1.1547],\n",
       "         [-1.1547, -2.6943,  1.9245,  1.9245]],\n",
       "\n",
       "        [[ 2.0000,  2.0000, -2.0000, -2.0000],\n",
       "         [-2.8284,  0.0000,  2.8284,  0.0000],\n",
       "         [-1.1547, -1.1547,  3.4640, -1.1547]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equivalent: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Outputs are equivalent: {torch.allclose(output, output2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feed Forward\n",
    "\n",
    "* This operates on a per-token level, across the entire embedding space.\n",
    "* Information from other tokens is gathered by the dot-product from the Attention.\n",
    "* Then the model needs to \"think\" on that information it has gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0568,  0.0302, -0.0024,  0.1023],\n",
       "         [ 0.0030,  0.0199, -0.0254,  0.0903],\n",
       "         [-0.0646, -0.0231, -0.0872,  0.0039]],\n",
       "\n",
       "        [[-0.0100, -0.0171, -0.1008, -0.0094],\n",
       "         [-0.0560,  0.0301,  0.0239,  0.0919],\n",
       "         [ 0.0095, -0.0186, -0.0808, -0.0112]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, bias=False):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "ffwd = FeedForward(n_embd)\n",
    "ffwd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number params = 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number params = {sum(p.numel() for p in ffwd.parameters() if p.requires_grad)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
