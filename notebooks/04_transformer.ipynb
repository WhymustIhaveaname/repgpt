{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer in a deep learning context refers to the combination of mult-headed attention, combined with a couple of additional elements: layer normalization, feedforward network, and residual skip connections. Let's take each of these one-by-one:\n",
    "\n",
    "### 1. Layer Normalization\n",
    "\n",
    "> Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine ([torch docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)).\n",
    "\n",
    "When you think of a single token's representation, it can get spikey. In an effort to smooth them out, we can use layer normalization. What does this mean? \n",
    "\n",
    "You subtract the mean and divide by the standard deviation. And you learn parameters, one or two for each embedding dimension. One if you disable the bias, two if you enable it.\n",
    "\n",
    "$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "Here's what it looks like in PyTorch and vanilla matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False, linewidth=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 0., 1., 3.],\n",
      "         [2., 2., 0., 2.],\n",
      "         [1., 0., 3., 3.]],\n",
      "\n",
      "        [[2., 2., 1., 1.],\n",
      "         [1., 2., 3., 2.],\n",
      "         [0., 0., 1., 0.]]])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9622, -1.3471, -0.5773,  0.9622],\n",
       "         [ 0.5773,  0.5773, -1.7320,  0.5773],\n",
       "         [-0.5773, -1.3471,  0.9622,  0.9622]],\n",
       "\n",
       "        [[ 1.0000,  1.0000, -1.0000, -1.0000],\n",
       "         [-1.4142,  0.0000,  1.4142,  0.0000],\n",
       "         [-0.5773, -0.5773,  1.7320, -0.5773]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 2\n",
    "context_size = 3\n",
    "n_embd = 4\n",
    "bias=False\n",
    "x = torch.randint(high=4, size=(batch_size, context_size, n_embd), dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "layer_norm = nn.LayerNorm(n_embd, bias=bias)\n",
    "print(layer_norm.weight)\n",
    "\n",
    "output = layer_norm(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9622, -1.3471, -0.5773,  0.9622],\n",
       "         [ 0.5773,  0.5773, -1.7320,  0.5773],\n",
       "         [-0.5773, -1.3471,  0.9622,  0.9622]],\n",
       "\n",
       "        [[ 1.0000,  1.0000, -1.0000, -1.0000],\n",
       "         [-1.4142,  0.0000,  1.4142,  0.0000],\n",
       "         [-0.5773, -0.5773,  1.7320, -0.5773]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "output2 = (x - torch.mean(x, dim=-1, keepdim=True)) / (torch.std(x, dim=-1, keepdim=True, unbiased=False) + eps)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = torch.ones(n_embd) * 2\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9245, -2.6943, -1.1547,  1.9245],\n",
       "         [ 1.1547,  1.1547, -3.4641,  1.1547],\n",
       "         [-1.1547, -2.6943,  1.9245,  1.9245]],\n",
       "\n",
       "        [[ 2.0000,  2.0000, -2.0000, -2.0000],\n",
       "         [-2.8284,  0.0000,  2.8284,  0.0000],\n",
       "         [-1.1547, -1.1547,  3.4640, -1.1547]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equivalent: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Outputs are equivalent: {torch.allclose(output, output2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feed Forward\n",
    "\n",
    "* This operates on a per-token level, across the entire embedding space.\n",
    "* Information from other tokens is gathered by the dot-product from the Attention.\n",
    "* Then the model needs to \"think\" on that information it has gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0568,  0.0302, -0.0024,  0.1023],\n",
       "         [ 0.0030,  0.0199, -0.0254,  0.0903],\n",
       "         [-0.0646, -0.0231, -0.0872,  0.0039]],\n",
       "\n",
       "        [[-0.0100, -0.0171, -0.1008, -0.0094],\n",
       "         [-0.0560,  0.0301,  0.0239,  0.0919],\n",
       "         [ 0.0095, -0.0186, -0.0808, -0.0112]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, bias=False):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "ffwd = FeedForward(n_embd)\n",
    "ffwd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number params = 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number params = {sum(p.numel() for p in ffwd.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention\n",
    "\n",
    "* Information is gathered from other tokens in the context sequence.\n",
    "* The mechanism is the humble pairwise dot product between all tokens combination.\n",
    "* A single sequence provides multiple training examples through triangular masking.\n",
    "* Input and output dimension is the same, `(batch_size, context_size, n_embd)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1911,  0.0591, -0.1139,  0.1038],\n",
       "         [-0.0683, -0.0674, -0.1091, -0.0690],\n",
       "         [-0.1098, -0.0891, -0.1046, -0.0989]],\n",
       "\n",
       "        [[ 0.1770,  0.0769, -0.0124,  0.1006],\n",
       "         [ 0.1136,  0.0387, -0.0391,  0.0522],\n",
       "         [ 0.1072,  0.0291, -0.0672,  0.0477]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(538)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_embd, context_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, n_embd = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attn_logits = q @ k.transpose(-1, -2) * 1 / math.sqrt(k.shape[-1])\n",
    "        mask = torch.tril(torch.ones(context_size, context_size))\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -1e9)\n",
    "        attn_prob = F.softmax(attn_logits, dim=-1)\n",
    "        attn_out = attn_prob @ v\n",
    "        return self.proj(attn_out)\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "attn = Attention(n_embd, context_size)\n",
    "attn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_dot_product_attention: \n",
    "* Compute the pairwise similarity of all of the tokens in the sequence.\n",
    "* Much faster, requires pytorch 2.x\n",
    "* The mask is applied within the function due to `is_causal=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1911,  0.0591, -0.1139,  0.1038],\n",
       "         [-0.0683, -0.0674, -0.1091, -0.0690],\n",
       "         [-0.1098, -0.0891, -0.1046, -0.0989]],\n",
       "\n",
       "        [[ 0.1770,  0.0769, -0.0124,  0.1006],\n",
       "         [ 0.1136,  0.0387, -0.0391,  0.0522],\n",
       "         [ 0.1072,  0.0291, -0.0672,  0.0477]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(538)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_embd, context_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, n_embd = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True)\n",
    "        return self.proj(out)\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "attn = Attention(n_embd, context_size)\n",
    "attn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Multi-Head Attention\n",
    "\n",
    "* Information is gathered from other tokens in the context sequence.\n",
    "* The mechanism is the humble pairwise dot product between all tokens combination.\n",
    "* A single sequence provides multiple training examples through triangular masking.\n",
    "* Input and output dimension is the same, `(batch_size, context_size, n_embd)`.\n",
    "\n",
    "Mult-head logic:\n",
    "\n",
    "* Compute self-attention for each head independently.\n",
    "* Convert the head to a batch dimension by:\n",
    "    (1) breaking up the embedding dimension into their individual heads, then\n",
    "    (2) swapping the context_size and n_head\n",
    "* Intermediate dimensionality is  `(batch_size, n_head, context_size,  n_embd // n_head)`\n",
    "\n",
    "scaled_dot_product_attention: \n",
    "* Compute the pairwise similarity of all of the tokens in the sequence.\n",
    "* Batch dimensions are the training example and the attention head.\n",
    "* The mask is applied within the function due to `is_causal=True` argument.\n",
    "\n",
    "\n",
    "Output transform:\n",
    "\n",
    "* Convert back to 3D shape\n",
    "* Contiguous is required here, which refers to creating a new tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0695, -0.1569, -0.0522,  0.0398,  0.1019,  0.1882,  0.2080,  0.0817, -0.0100,  0.0229,  0.0451, -0.2700],\n",
       "         [-0.1275, -0.1344, -0.1049, -0.0612,  0.1591,  0.2479,  0.1442,  0.1388, -0.0788, -0.0085, -0.0199, -0.2941],\n",
       "         [-0.0926, -0.0455, -0.0927, -0.1061,  0.1230,  0.2879,  0.1255,  0.1476, -0.1061,  0.0566, -0.0785, -0.2065]],\n",
       "\n",
       "        [[-0.0419, -0.0335, -0.0098, -0.0156,  0.0757,  0.3924,  0.3331,  0.0867,  0.0588,  0.1902, -0.0457, -0.2093],\n",
       "         [-0.0660, -0.0605, -0.0492, -0.0656,  0.0940,  0.2376,  0.2284,  0.1606,  0.0476,  0.0316, -0.0006, -0.1983],\n",
       "         [-0.1252, -0.0439, -0.0597, -0.0248,  0.0417,  0.3308,  0.1892,  0.1849, -0.0597,  0.0695, -0.0121, -0.1741]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiheadedAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_head, n_embd, context_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, n_embd = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        k = k.view(batch_size, context_size, self.n_head, n_embd // self.n_head).transpose(1, 2)\n",
    "        q = q.view(batch_size, context_size, self.n_head, n_embd // self.n_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, context_size, self.n_head, n_embd // self.n_head).transpose(1, 2)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, context_size, n_embd)\n",
    "        return self.proj(out)\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 12; n_head = 4\n",
    "\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "mha = MultiheadedAttention(n_embd // n_head, n_head, n_embd, context_size)\n",
    "mha(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
