{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer in a deep learning context refers to the combination of two sub-layers: multi-headed attention and feedforward, interspersed with layer normalization, and residual skip connections that encompase the sub-layers. \n",
    "\n",
    "Let's take each of these one-by-one:\n",
    "\n",
    "### 1. Layer Normalization\n",
    "\n",
    "$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "* When you think of a single token's representation, it can get spikey. In an effort to smooth them out, we can use layer normalization.\n",
    "* You subtract the mean and divide by the standard deviation where these values are calculated across the token's embedding. \n",
    "* Then you multiply each token embedding by a learned vector $\\gamma$. Optionally you also add a learned bias vector $\\beta$.\n",
    "\n",
    "References: \n",
    "* torch docs: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False, linewidth=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Creating data input\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "x = torch.randint(high=4, size=(batch_size, context_size, n_embd), dtype=torch.float)\n",
    "\n",
    "# 2. Calculating the normalization manually\n",
    "eps = 1e-5\n",
    "gamma = torch.ones(n_embd)\n",
    "beta = torch.zeros(n_embd)\n",
    "numerator = (x - torch.mean(x, dim=-1, keepdim=True))\n",
    "denomenator = torch.sqrt(torch.var(x, dim=-1, keepdim=True, correction=0) + eps)\n",
    "output1 =  numerator / denomenator * gamma + beta\n",
    "\n",
    "# 3. Using the LayerNorm module\n",
    "layer_norm = nn.LayerNorm(n_embd)\n",
    "output2 = layer_norm(x)\n",
    "torch.allclose(output1, output2, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feed Forward\n",
    "\n",
    "* This operates on a per-token level, across the entire embedding space.\n",
    "* Information from other tokens is gathered by the dot-product from the Attention.\n",
    "* Then the model needs to \"think\" on that information it has gathered.\n",
    "* In dense transformers, the feedforward network takes up the majority of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0505,  0.0126, -0.0237, -0.0140],\n",
       "         [-0.0584,  0.0065, -0.0475,  0.0038],\n",
       "         [-0.0210,  0.0022, -0.0162, -0.0439]],\n",
       "\n",
       "        [[ 0.0992, -0.0262, -0.0854, -0.1161],\n",
       "         [ 0.0382, -0.0444, -0.1140, -0.0680],\n",
       "         [ 0.0538,  0.0129, -0.1184, -0.0631]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, bias=False):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "ffwd = FeedForward(n_embd)\n",
    "ffwd(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention\n",
    "\n",
    "* Information is gathered from other tokens in the context sequence.\n",
    "* The mechanism is the humble pairwise dot product between all tokens combination.\n",
    "* A single sequence provides multiple training examples through triangular masking.\n",
    "* Input and output dimension is the same, `(batch_size, context_size, n_embd)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1911,  0.0591, -0.1139,  0.1038],\n",
       "         [-0.0683, -0.0674, -0.1091, -0.0690],\n",
       "         [-0.1098, -0.0891, -0.1046, -0.0989]],\n",
       "\n",
       "        [[ 0.1770,  0.0769, -0.0124,  0.1006],\n",
       "         [ 0.1136,  0.0387, -0.0391,  0.0522],\n",
       "         [ 0.1072,  0.0291, -0.0672,  0.0477]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(538)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_embd, context_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, n_embd = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        attn_logits = q @ k.transpose(-1, -2) * 1 / math.sqrt(k.shape[-1])\n",
    "        mask = torch.tril(torch.ones(context_size, context_size))\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -1e9)\n",
    "        attn_prob = F.softmax(attn_logits, dim=-1)\n",
    "        attn_out = attn_prob @ v\n",
    "        return self.proj(attn_out)\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "attn = Attention(n_embd, context_size)\n",
    "output1 = attn(x)\n",
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_dot_product_attention: \n",
    "* Much faster attention calculation, requires pytorch 2.x\n",
    "* The mask is applied within the function due to `is_causal=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1911,  0.0591, -0.1139,  0.1038],\n",
       "         [-0.0683, -0.0674, -0.1091, -0.0690],\n",
       "         [-0.1098, -0.0891, -0.1046, -0.0989]],\n",
       "\n",
       "        [[ 0.1770,  0.0769, -0.0124,  0.1006],\n",
       "         [ 0.1136,  0.0387, -0.0391,  0.0522],\n",
       "         [ 0.1072,  0.0291, -0.0672,  0.0477]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(538)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_embd, context_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, n_embd = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True)\n",
    "        return self.proj(out)\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 4\n",
    "\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "attn = Attention(n_embd, context_size)\n",
    "output2 = attn(x)\n",
    "print(torch.allclose(output1, output2))\n",
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Multi-Head Attention\n",
    "\n",
    "* Information is gathered from other tokens in the context sequence.\n",
    "* The mechanism is the humble pairwise dot product between all tokens combination.\n",
    "* A single sequence provides multiple training examples through triangular masking.\n",
    "* Input and output dimension is the same, `(batch_size, context_size, n_embd)`.\n",
    "\n",
    "Mult-head logic:\n",
    "\n",
    "* Compute self-attention for each head independently.\n",
    "* Convert the head to a batch dimension by:\n",
    "    (1) breaking up the embedding dimension into their individual using `view` and\n",
    "    (2) swapping the context_size and n_head using `transpose`\n",
    "* Intermediate dimensionality is  `(batch_size, n_head, context_size,  n_embd // n_head)`\n",
    "\n",
    "scaled_dot_product_attention: \n",
    "* Compute the pairwise similarity of all of the tokens in the sequence.\n",
    "* Batch dimensions are the training example and the attention head.\n",
    "* The mask is applied within the function due to `is_causal=True` argument.\n",
    "\n",
    "Output transform:\n",
    "\n",
    "* Convert back to 3D shape\n",
    "* Contiguous is required here, which refers to creating a new tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0281, -0.2927, -0.2525, -0.0889,  0.1167,  0.1468, -0.0820, -0.2655,  0.1458,  0.0630,  0.0043,  0.1676],\n",
       "         [-0.0009, -0.1137, -0.3365, -0.0507,  0.2371,  0.0397, -0.1004, -0.1876,  0.0511,  0.1017,  0.0875, -0.0008],\n",
       "         [-0.0237, -0.0629, -0.3037, -0.0972,  0.2089,  0.0214, -0.0721, -0.1648,  0.0580,  0.0751,  0.0676,  0.0171]],\n",
       "\n",
       "        [[-0.0217, -0.2904, -0.3444, -0.0450,  0.1474,  0.2053, -0.2469, -0.3188,  0.0560,  0.1473, -0.0262,  0.0488],\n",
       "         [ 0.0701, -0.1827, -0.3194, -0.0396,  0.2232,  0.1187, -0.0864, -0.2168,  0.0954,  0.0748,  0.0209,  0.0241],\n",
       "         [ 0.0656, -0.1826, -0.3278, -0.1010,  0.2486,  0.0924, -0.0423, -0.2427,  0.0861,  0.0443, -0.0071,  0.0052]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiheadedAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_head, n_embd, context_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, n_embd = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        k = k.view(batch_size, context_size, self.n_head, n_embd // self.n_head).transpose(1, 2)\n",
    "        q = q.view(batch_size, context_size, self.n_head, n_embd // self.n_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, context_size, self.n_head, n_embd // self.n_head).transpose(1, 2)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, context_size, n_embd)\n",
    "        return self.proj(out)\n",
    "\n",
    "batch_size = 2; context_size = 3; n_embd = 12; n_head = 4\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "mha = MultiheadedAttention(n_embd // n_head, n_head, n_embd, context_size)\n",
    "mha(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Transformer\n",
    "\n",
    "* We need to assemble the multi-headed attention and the feedforward sub-layers.\n",
    "* But there's more to the Transformer block than this.\n",
    "* Also need layer normalization and residual skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1371, -0.0388,  0.6705,  ...,  0.9291,  1.2065,  0.0958],\n",
       "         [-0.0254,  1.2155,  0.4800,  ...,  0.9933,  1.2587,  0.2396],\n",
       "         [ 0.1490,  0.4489,  0.0122,  ...,  1.1132,  0.5381,  1.2893],\n",
       "         ...,\n",
       "         [ 0.1049,  0.1704,  0.5994,  ...,  0.7017,  0.9758,  0.0885],\n",
       "         [ 0.2474,  0.5638,  0.4740,  ...,  0.5415,  1.2365,  0.7050],\n",
       "         [ 0.5544,  1.2055,  0.7379,  ...,  0.1003,  0.9279,  0.3755]],\n",
       "\n",
       "        [[ 0.3582,  0.8588, -0.1149,  ...,  0.8905,  0.6593, -0.1255],\n",
       "         [ 0.8621,  0.4575,  0.5921,  ...,  0.7179,  0.1728,  0.2621],\n",
       "         [ 1.5342,  0.8590,  0.3469,  ...,  0.1434,  0.4204,  0.4043],\n",
       "         ...,\n",
       "         [ 0.6552,  0.6969,  0.8733,  ...,  0.0807,  0.6024,  1.0817],\n",
       "         [ 0.4097,  0.4047,  0.4242,  ...,  0.8062,  0.8283,  0.6442],\n",
       "         [ 1.0054,  0.7435,  0.6510,  ..., -0.0248,  0.4735,  0.1544]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        head_size = config.n_embd // config.n_head\n",
    "        self.layer_norm1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.multi_headed_attention = MultiheadedAttention(head_size, config.n_head, config.n_embd, config.context_size, config.bias)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.feed_forward = FeedForward(config.n_embd, config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_headed_attention(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "class Config:\n",
    "    n_layer = 12\n",
    "    n_embd = 768\n",
    "    n_head = 12\n",
    "    context_size = 1024\n",
    "    vocab_size = 50_304\n",
    "    bias = False\n",
    "\n",
    "batch_size = 2; context_size = 1024; n_embd = 768\n",
    "x = torch.rand(size=(batch_size, context_size, n_embd))\n",
    "\n",
    "model = Transformer(Config())\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting the model all together\n",
    "\n",
    "Finally, we need to add three things:\n",
    "* The input: position embedding and token embeddding\n",
    "* The output: final layer normalization and linear projection back into the vocabulary space\n",
    "* The initialization: scaling all weights by $Normal(0, 0.02)$ and additional scaling of $1\\sqrt{N}$ for the weights of residual layers, where $N$ is the number of residual layers (i.e., final linear layer before the residual skip connection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[     0.6351,     -0.0949,     -0.5541,  ...,     -0.1742,      0.1028,      0.2494],\n",
       "         [     0.7023,     -0.6657,     -0.4743,  ...,      0.3498,      0.3731,     -0.0730],\n",
       "         [     0.1394,     -0.9582,     -0.8774,  ...,      0.6104,      0.0692,     -0.5602],\n",
       "         ...,\n",
       "         [     0.9415,      0.8835,     -0.5578,  ...,      0.5964,      0.0697,      0.0509],\n",
       "         [     0.8785,      0.8142,     -0.0001,  ...,      0.1103,      1.2653,     -0.5562],\n",
       "         [     0.8092,      0.9330,     -0.5170,  ...,      0.2967,      0.3159,     -0.9122]],\n",
       "\n",
       "        [[     0.7075,     -0.1322,      0.7380,  ...,      0.3899,     -0.1218,     -0.5173],\n",
       "         [     0.4188,     -0.0548,      0.3190,  ...,      0.6858,      0.2561,     -0.8678],\n",
       "         [     0.0219,     -0.3555,     -0.1466,  ...,      0.5057,      0.0858,      0.0092],\n",
       "         ...,\n",
       "         [     0.7707,      0.1874,     -0.8775,  ...,      0.4231,      0.2930,     -0.3817],\n",
       "         [     1.3118,      0.1258,     -0.5397,  ...,      0.3277,      0.2669,     -0.5991],\n",
       "         [     0.6873,      0.1844,     -0.5190,  ...,      0.5468,      0.2728,     -0.5752]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.context_size = config.context_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.context_size, config.n_embd)\n",
    "        self.transformers = nn.Sequential(*[Transformer(config) for _ in range(config.n_layer)])\n",
    "        self.layer_norm_final = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=config.bias)\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        n_residual_layers = 2 * config.n_layer\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"proj.weight\") or pn.endswith(\"linear2.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(n_residual_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        pos_idx = torch.arange(context_size, dtype=torch.long, device=device)\n",
    "        x = self.token_embedding_table(idx) + self.position_embedding_table(pos_idx)\n",
    "        x = self.transformers(x)\n",
    "        x = self.layer_norm_final(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    n_layer = 12\n",
    "    n_embd = 768\n",
    "    n_head = 12\n",
    "    context_size = 1024\n",
    "    vocab_size = 50_304\n",
    "    bias = False\n",
    "\n",
    "batch_size = 2; context_size = 1024; n_embd = 768\n",
    "idx = torch.randint(high=50_254, size=(batch_size, context_size))\n",
    "\n",
    "model = GPT2(Config())\n",
    "model(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
