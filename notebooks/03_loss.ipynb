{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "The goal for this notebook is to demonstrate the loss function - cross entropy loss - commonly used in language modeling.\n",
    "\n",
    "* Data: For next-word prediction, we calculate the loss `context_size` * `batch_size` times.\n",
    "* Logsoftmax: It's just log(softmax(x))\n",
    "* Negative log liklihood loss is the negative log liklihood of the correct class\n",
    "* Cross Entropy Loss is a convenient combination of the previous two steps: Negative Log Liklhoood Loss ( LogSoftmax )\n",
    "\n",
    "### For next-word prediction, we calculate the loss `context_size` * `batch_size` times.\n",
    "\n",
    "The training goal is to predict the next token. In data, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens:\n",
      "tensor([[7., 9., 4., 7., 4.],\n",
      "        [9., 3., 0., 4., 9.],\n",
      "        [0., 0., 7., 0., 5.]])\n",
      "--------------------------------------------------\n",
      "| Seq    | Context                   | Target |\n",
      "--------------------------------------------------\n",
      "| 0      | [7.0]                     | 9.0    |\n",
      "| 0      | [7.0, 9.0]                | 4.0    |\n",
      "| 0      | [7.0, 9.0, 4.0]           | 7.0    |\n",
      "| 0      | [7.0, 9.0, 4.0, 7.0]      | 4.0    |\n",
      "| 1      | [9.0]                     | 3.0    |\n",
      "| 1      | [9.0, 3.0]                | 0.0    |\n",
      "| 1      | [9.0, 3.0, 0.0]           | 4.0    |\n",
      "| 1      | [9.0, 3.0, 0.0, 4.0]      | 9.0    |\n",
      "| 2      | [0.0]                     | 0.0    |\n",
      "| 2      | [0.0, 0.0]                | 7.0    |\n",
      "| 2      | [0.0, 0.0, 7.0]           | 0.0    |\n",
      "| 2      | [0.0, 0.0, 7.0, 0.0]      | 5.0    |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "torch.manual_seed(538)\n",
    "\n",
    "batch_size = 3\n",
    "context_size = 4\n",
    "\n",
    "data_batch = torch.randint(high=10, size=(batch_size, context_size + 1), dtype=torch.float32)\n",
    "x_batch = data_batch[:, :context_size]\n",
    "y_batch = data_batch[:, 1:context_size+1]\n",
    "\n",
    "print(f\"Input tokens:\\n{data_batch}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"| {'Seq': <6} | {'Context':<25} | {'Target'} |\")\n",
    "print(\"-\" * 50)\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_size):\n",
    "        context = x_batch[b, : t + 1]\n",
    "        target = y_batch[b, t]\n",
    "        print(f\"| {b: <6} | {str(context.tolist()):<25} | {target:<6} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means: for any given sequence, we will get context_size number of training batches from it to calculate the loss on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `F.log_softmax` is equivalent to `torch.log(F.softmax)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are the same: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "context_size = 4\n",
    "n_vocab = 8\n",
    "\n",
    "logits = torch.randn(size=(batch_size * context_size, n_vocab), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "output = torch.log(F.softmax(logits, dim=-1))\n",
    "output2 = F.log_softmax(logits, dim=-1)\n",
    "print(f\"Outputs are the same: {torch.allclose(output, output2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But IRL you should use `F.log_softmax` for better numerical properties.\n",
    "\n",
    "### Negative log liklihood loss is the negative log probability of the correct class\n",
    "\n",
    "* First, you take the neural network outputs (i.e., logits) and compute their probabilities (i.e., Softmax).\n",
    "* Second, you take the log of these probabilities, these are the log likelihood values.\n",
    "* Third, you retrieve the log liklihood value for the correct class and multiply by negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3182, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3182, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 3\n",
    "context_size = 4\n",
    "n_vocab = 8\n",
    "\n",
    "# This is the form of the data during training (i.e., from nn output and the dataloader)\n",
    "logits = torch.randn(size=(batch_size, context_size, n_vocab), dtype=torch.float32, requires_grad=True)\n",
    "targets = torch.randint(high=n_vocab, size=(batch_size, context_size), dtype=torch.long)\n",
    "\n",
    "# This is the format we format it to for computing loss\n",
    "# logits are floats 0...1 with dimension (N = Number of observations, C = Number of classes)\n",
    "# target are integers 0...C-1 with dimension (N = Number of observations,) \n",
    "logits = logits.view(batch_size * context_size, n_vocab)\n",
    "targets = targets.view(batch_size * context_size)\n",
    "\n",
    "output = F.nll_loss(F.log_softmax(logits, dim= -1), targets)\n",
    "print(output)\n",
    "\n",
    "n_examples = logits.shape[0]\n",
    "log_probs = F.log_softmax(logits, dim= -1)\n",
    "nll_losses = torch.zeros(n_examples)\n",
    "for i in range(n_examples):\n",
    "    nll_losses[i] = -log_probs[i, targets[i]]\n",
    "\n",
    "output2 = nll_losses.mean()\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss combines three functions (Negative Log Liklihood Loss, Log, and Softmax) into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8195, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8195, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "context_size = 4\n",
    "n_vocab = 8\n",
    "\n",
    "# This is the form of the data during training (i.e., from nn output and the dataloader)\n",
    "logits = torch.randn(size=(batch_size, context_size, n_vocab), dtype=torch.float32, requires_grad=True)\n",
    "targets = torch.randint(high=n_vocab, size=(batch_size, context_size), dtype=torch.long)\n",
    "\n",
    "# This is the format we format it to for computing loss\n",
    "# logits are floats 0...1 with dimension (N = Number of observations, C = Number of classes)\n",
    "# target are integers 0...C-1 with dimension (N = Number of observations,) \n",
    "logits = logits.view(batch_size * context_size, n_vocab)\n",
    "targets = targets.view(batch_size * context_size)\n",
    "\n",
    "output = F.cross_entropy(logits, targets)\n",
    "print(output)\n",
    "output2 = F.nll_loss(F.log_softmax(logits, dim= -1), targets)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "* https://cs231n.github.io/neural-networks-case-study/#grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
