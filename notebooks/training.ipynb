{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* Training Loop\n",
    "* Automatic Mixed Precision (AMP)\n",
    "* Distributed Data Parallelism (DDP)\n",
    "* DDP with Gradient Accumulation\n",
    "* Logging\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[5, 1, 2, 8, 6, 6, 0, 7],\n",
      "        [5, 7, 4, 7, 1, 6, 6, 2]]), torch.Size([2, 8])\n",
      "y=tensor([[1, 2, 8, 6, 6, 0, 7, 5],\n",
      "        [7, 4, 7, 1, 6, 6, 2, 2]]), torch.Size([2, 8])\n",
      "logits = tensor([[[ 0.0162, -0.0736, -0.0593, -0.0633,  0.0910,  0.0398,  0.0282,  0.0741, -0.0131,  0.0068],\n",
      "         [ 0.0436,  0.0058, -0.0128, -0.0425, -0.0049,  0.0162,  0.0598,  0.0395, -0.0528, -0.0426],\n",
      "         [-0.0181,  0.0252,  0.0098, -0.0738, -0.0507, -0.0679, -0.0526, -0.1016,  0.1065, -0.0413],\n",
      "         [ 0.0678, -0.0511, -0.0521,  0.0580,  0.0096,  0.0632, -0.0470, -0.0654, -0.0079,  0.0072],\n",
      "         [ 0.0352, -0.1104,  0.0142, -0.0234,  0.0122,  0.0239, -0.0714, -0.0795,  0.0549,  0.0719],\n",
      "         [ 0.0352, -0.1104,  0.0142, -0.0234,  0.0122,  0.0239, -0.0714, -0.0795,  0.0549,  0.0719],\n",
      "         [-0.0184,  0.0244, -0.0355,  0.0152, -0.0456, -0.0781, -0.0111, -0.0562, -0.0044,  0.0702],\n",
      "         [ 0.0811, -0.0203, -0.1606,  0.1077, -0.0264,  0.0409,  0.0093, -0.1120, -0.0560, -0.0291]],\n",
      "\n",
      "        [[ 0.0162, -0.0736, -0.0593, -0.0633,  0.0910,  0.0398,  0.0282,  0.0741, -0.0131,  0.0068],\n",
      "         [ 0.0811, -0.0203, -0.1606,  0.1077, -0.0264,  0.0409,  0.0093, -0.1120, -0.0560, -0.0291],\n",
      "         [ 0.0480, -0.0645,  0.0989, -0.0989, -0.0209,  0.0126, -0.0566, -0.0562, -0.0336, -0.0424],\n",
      "         [ 0.0811, -0.0203, -0.1606,  0.1077, -0.0264,  0.0409,  0.0093, -0.1120, -0.0560, -0.0291],\n",
      "         [ 0.0436,  0.0058, -0.0128, -0.0425, -0.0049,  0.0162,  0.0598,  0.0395, -0.0528, -0.0426],\n",
      "         [ 0.0352, -0.1104,  0.0142, -0.0234,  0.0122,  0.0239, -0.0714, -0.0795,  0.0549,  0.0719],\n",
      "         [ 0.0352, -0.1104,  0.0142, -0.0234,  0.0122,  0.0239, -0.0714, -0.0795,  0.0549,  0.0719],\n",
      "         [-0.0181,  0.0252,  0.0098, -0.0738, -0.0507, -0.0679, -0.0526, -0.1016,  0.1065, -0.0413]]], grad_fn=<ViewBackward0>), torch.Size([2, 8, 10])\n",
      "loss at init = 2.3004, expected loss at init = 2.3026\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(740)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=160)\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_size = 100; n_embd = 1000; vocab_size = 10\n",
    "batch_size = 2; context_size = 8\n",
    "train_tokens = torch.randint(low=0, high=vocab_size, size=(data_size,)).type(torch.int64)\n",
    "\n",
    "def get_batch(data, batch_size = batch_size, context_size = context_size):\n",
    "    indices = torch.randint(low=0, high=data.shape[0] - context_size, size=(batch_size,))\n",
    "    X = torch.stack([data[idx:idx+context_size] for idx in indices])\n",
    "    y = torch.stack([data[idx+1:idx+context_size+1] for idx in indices])\n",
    "    return X, y\n",
    "\n",
    "X, y = get_batch(train_tokens)\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.linear_out = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        x = self.linear1(tok_emb)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        logits = self.linear_out(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                    logits.view(batch_size * context_size, vocab_size), \n",
    "                    targets.view(batch_size * context_size)\n",
    "                )\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "model = SimpleModel(n_embd=n_embd, vocab_size=vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0001)\n",
    "\n",
    "max_steps = 100\n",
    "print(f\"{X=}, {X.shape}\")\n",
    "print(f\"{y=}, {y.shape}\")\n",
    "print(f\"logits = {model(X, y)[0]}, {model(X)[0].shape}\")\n",
    "print(f\"loss at init = {model(X, y)[1]:.4f}, expected loss at init = {-math.log(1/vocab_size):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    X, y = get_batch(train_tokens)\n",
    "    logits, loss = model(X, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Automatic Mixed Precision  (AMP)\n",
    "\n",
    "There are two precision formats we use here: one is brain float 16 (BF16), and one is floating point 32 called (full precision).\n",
    "\n",
    "1. Starting out with the weights in FP32 you then copy the weights to BF16 format.\n",
    "2. Forward pass: compute the outputs of the NN with the BF16 weights (`model(x)`)\n",
    "3. Compute the gradients in BF16 (`loss.backward()`)\n",
    "4. Copy the BF16 gradients back to FP32\n",
    "5. Update the FP32 weights using the optimizer (`optimizer.step()`).\n",
    "\n",
    "To summarize, you are doing the forward pass and the calculation of the gradients in BF16, and you are doing the weight update (and the loss calculation) in FP32.\n",
    "\n",
    "This is all handled behind the scenes within a context manager provided by torch called `autocast`. They explain it [in the following way](https://pytorch.org/docs/stable/amp.html#autocasting):\n",
    "\n",
    "> When entering an autocast-enabled region, Tensors may be any type. You should not call half() or bfloat16() on your model(s) or inputs when using autocasting. `autocast` should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    X, y = get_batch(train_tokens)\n",
    "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "        logits, loss = model(X, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Distributed Data Parallelism (DDP)\n",
    "\n",
    "DDP refers to the process of creating multiple processes running on separate machines, each machine has a copy of the weights, and these weights are trained using different batches and then the gradients are syncronized before being used to update the model weights.\n",
    "\n",
    "\n",
    "This works when the model will git on a single GPU. (Larger models require different parallel techniques, such as [FDSP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html).)\n",
    "\n",
    "When you run the training, instead of running with python you use `torchrun` command e.g.,: \n",
    "\n",
    "```\n",
    "# Original\n",
    "python train.py  --max_steps 600000\n",
    "\n",
    "# DDP\n",
    "torchrun  --standalone --nproc_per_node=8 train.py --max_steps 600000\n",
    "```\n",
    "\n",
    "To implement DDP, you need to modify your training code in 4 places.\n",
    "\n",
    "```python\n",
    "# 0. Initialize the DDP process\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "init_process_group(backend=\"nccl\") # https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "# 1. Specify which cuda device you are using (set from torchrun)\n",
    "ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# 2. Wrap your model in a DDP container (access model using model.module)\n",
    "model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "# Same as before\n",
    "for step in range(max_steps):\n",
    "    X, y = get_batch(train_tokens)\n",
    "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "        logits, loss = model(X, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# 3. Cleanup at training end\n",
    "destroy_process_group()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with DDP and Gradient Accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "backend = \"nccl\"  # use nccl for distributed GPU per https://pytorch.org/docs/stable/distributed.html\n",
    "init_process_group(backend=backend)\n",
    "ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    ...\n",
    "    # Iterate through each substep to simulate the larger batch size\n",
    "    # Save gradient sync for the last gradient calculation (if using gradient accumulation)\n",
    "    # Use it so that gradients don't sync during each sub_step.\n",
    "    for sub_step in range(gradient_accumulation_steps_per_gpu):\n",
    "        model.require_backward_grad_sync = (sub_step + 1) == gradient_accumulation_steps_per_gpu\n",
    "        \n",
    "        with ctx:\n",
    "            logits, loss = model(xb, yb)\n",
    "            loss = loss / gradient_accumulation_steps_per_gpu\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)    \n",
    "\n",
    "destroy_process_group()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
