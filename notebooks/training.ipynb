{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* Training Loop\n",
    "* Automatic Mixed Precision (AMP)\n",
    "* Distributed Data Parallelism (DDP)\n",
    "* DDP with Gradient Accumulation\n",
    "* Logging\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(740)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=160)\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_size = 100; n_embd = 1000; vocab_size = 10\n",
    "batch_size = 2; context_size = 8\n",
    "train_tokens = torch.randint(low=0, high=vocab_size, size=(data_size,)).type(torch.int64)\n",
    "valid_tokens = torch.randint(low=0, high=vocab_size, size=(data_size,)).type(torch.int64)\n",
    "\n",
    "def get_batch(data, device = device, batch_size = batch_size, context_size = context_size):\n",
    "    indices = torch.randint(low=0, high=data.shape[0] - context_size, size=(batch_size,))\n",
    "    X = torch.stack([data[idx:idx+context_size] for idx in indices]).to(device)\n",
    "    y = torch.stack([data[idx+1:idx+context_size+1] for idx in indices]).to(device)\n",
    "    return X, y\n",
    "\n",
    "X, y = get_batch(train_tokens)\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.linear_out = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        x = self.linear1(tok_emb)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        logits = self.linear_out(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                    logits.view(batch_size * context_size, vocab_size), \n",
    "                    targets.view(batch_size * context_size)\n",
    "                )\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "model = SimpleModel(n_embd=n_embd, vocab_size=vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0001)\n",
    "\n",
    "max_steps = 100\n",
    "print(f\"{X=}, {X.shape}\")\n",
    "print(f\"{y=}, {y.shape}\")\n",
    "print(f\"logits = {model(X, y)[0]}, {model(X)[0].shape}\")\n",
    "print(f\"loss at init = {model(X, y)[1]:.4f}, expected loss at init = {-math.log(1/vocab_size):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    X, y = get_batch(train_tokens)\n",
    "    logits, loss = model(X, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Automatic Mixed Precision  (AMP)\n",
    "\n",
    "There are two precision formats we use here: one is brain float 16 (BF16), and one is floating point 32 called (full precision).\n",
    "\n",
    "1. Starting out with the weights in FP32 you then copy the weights to BF16 format.\n",
    "2. Forward pass: compute the outputs of the NN with the BF16 weights (`model(x)`)\n",
    "3. Compute the gradients in BF16 (`loss.backward()`)\n",
    "4. Copy the BF16 gradients back to FP32\n",
    "5. Update the FP32 weights using the optimizer (`optimizer.step()`).\n",
    "\n",
    "To summarize, you are doing the forward pass and the calculation of the gradients in BF16, and you are doing the weight update (and the loss calculation) in FP32.\n",
    "\n",
    "This is all handled behind the scenes within a context manager provided by torch called `autocast`. They explain it [in the following way](https://pytorch.org/docs/stable/amp.html#autocasting):\n",
    "\n",
    "> When entering an autocast-enabled region, Tensors may be any type. You should not call half() or bfloat16() on your model(s) or inputs when using autocasting. `autocast` should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    X, y = get_batch(train_tokens)\n",
    "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "        logits, loss = model(X, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Distributed Data Parallelism (DDP)\n",
    "\n",
    "DDP refers to the process of creating multiple processes running on separate machines, each machine has a copy of the weights, and these weights are trained using different batches and then the gradients are syncronized before being used to update the model weights.\n",
    "\n",
    "\n",
    "This works when the model will git on a single GPU. (Larger models require different parallel techniques, such as [FDSP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html).)\n",
    "\n",
    "When you run the training, instead of running with python you use `torchrun` command e.g.,: \n",
    "\n",
    "```\n",
    "# Original\n",
    "python train.py  --max_steps 600000\n",
    "\n",
    "# DDP\n",
    "torchrun  --standalone --nproc_per_node=8 train.py --max_steps 600000\n",
    "```\n",
    "\n",
    "To implement DDP, you need to modify your training code in 4 places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Initialize the DDP process\n",
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "init_process_group(backend=\"nccl\") # https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "# 1. Incorporate environment variables (set from torchrun)\n",
    "ddp_rank = int(os.environ[\"RANK\"])\n",
    "ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "main_process = ddp_rank == 0\n",
    "\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# 2. Wrap your model in a DDP container (access model using model.module)\n",
    "model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "# Same as before\n",
    "for step in range(max_steps):\n",
    "    X, y = get_batch(train_tokens, device)\n",
    "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "        logits, loss = model(X, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# 3. Cleanup at training end\n",
    "destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with DDP and Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Initialize the DDP process\n",
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "init_process_group(backend=\"nccl\") # https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "# 1. Incorporate environment variables (set from torchrun)\n",
    "ddp_rank = int(os.environ[\"RANK\"])\n",
    "ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "main_process = ddp_rank == 0\n",
    "\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# 2. Wrap your model in a DDP container (access model using model.module)\n",
    "model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Iterate through each substep to simulate the larger batch size\n",
    "    # Example: batch_size_per_gpu = 16, gradient_accumulation_steps_per_gpu = 4, n_gpus = 8\n",
    "    # So effective batch_size is 512, meaning parameter update is based off of 4 * 16 * 8 = 512 training examples.\n",
    "    for sub_step in range(gradient_accumulation_steps_per_gpu):\n",
    "        X, y = get_batch(train_tokens, device)\n",
    "\n",
    "        # Sync gradients only on the last gradient accumulation step\n",
    "        model.require_backward_grad_sync = (sub_step + 1) == gradient_accumulation_steps_per_gpu\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(X, y)\n",
    "            loss = loss / gradient_accumulation_steps_per_gpu    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# 3. Cleanup at training end\n",
    "destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "# 0. Before Training\n",
    "writer = SummaryWriter(log_dir=f\"/tmp/data/output/tensorboard/training-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "\n",
    "init_process_group(backend=\"nccl\") # https://pytorch.org/docs/stable/distributed.html\n",
    "ddp_rank = int(os.environ[\"RANK\"])\n",
    "ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "main_process = ddp_rank == 0\n",
    "\n",
    "device = f\"cuda:{ddp_local_rank}\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    for sub_step in range(gradient_accumulation_steps_per_gpu):\n",
    "        X, y = get_batch(train_tokens, device)\n",
    "        \n",
    "        # 1. Within Training\n",
    "        losses = estimate_loss(model)\n",
    "        writer.add_scalar(\"Loss/train\", losses[\"train\"], step)\n",
    "        writer.add_scalar(\"Loss/eval\", losses[\"eval\"], step)\n",
    "        writer.add_scalar(\"learning_rate\", lr, step)\n",
    "\n",
    "        model.require_backward_grad_sync = (sub_step + 1) == gradient_accumulation_steps_per_gpu\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(X, y)\n",
    "            loss = loss / gradient_accumulation_steps_per_gpu    \n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "# After training\n",
    "hparam_dict = {\n",
    "    \"max_learning_rate\": max_learning_rate,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_embd\": n_embd,\n",
    "    \"context_size\": context_size,\n",
    "    \"total_training_tokens\": total_training_tokens,\n",
    "    \"n_times_through_data\": total_training_tokens / total_training_tokens_unique,\n",
    "}\n",
    "metric_dict = {\"hparam/loss\": best_eval_loss.item()}\n",
    "writer.add_hparams(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "writer.flush()\n",
    "destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "how-to-reproduce-gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
